{"paper": "ABSTRACT\nThis paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modiﬁed WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a signiﬁcant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech\n\n1. INTRODUCTION\nGenerating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the ﬁeld. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds mufﬂed and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require signiﬁcant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simpliﬁes the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Grifﬁn-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Grifﬁn-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a uniﬁed, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modiﬁed WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difﬁcult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs signiﬁcantly.\n\n2. MODEL ARCHITECTURE\nOur proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modiﬁed version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high ﬁdelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Grifﬁn-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modiﬁed WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in signiﬁcantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel ﬁlterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the ﬁlterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 ﬁlters with shape 5 × 1, i.e., where each ﬁlter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the ﬁnal convolutional layer is passed into a single bi-directional [19] LSTM [20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a ﬁxed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages the model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution ﬁlters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is ﬁrst passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 ﬁlters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the ﬁnal layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difﬁcult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a ﬁxed duration. Speciﬁcally, generation completes at the ﬁrst frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modiﬁed version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic distributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.\n\n3. EXPERIMENTS & RESULTS\n3.1. Training Setup Our training process involves ﬁrst training the feature prediction network on its own, followed by training a modiﬁed WaveNet independently on the outputs generated by the ﬁrst network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with β1 = 0.9, β2 = 0.999, ϵ = 10−6 and a learning rate of 10−3 exponentially decaying to 10−5 starting after 50,000 iterations. We also apply L2 regularization with weight 10−6. We then train our modiﬁed WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with β1 = 0.9, β2 = 0.999, ϵ = 10−8 and a ﬁxed learning rate of 10−4. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing conﬁguration used for training. We randomly selected 100 ﬁxed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inﬂated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modiﬁcations to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Grifﬁn-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We ﬁnd that the proposed system signiﬁcantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio. † System MOS Parametric 3.492 ± 0.096 Tacotron (Grifﬁn-Lim) 4.001 ± 0.087 Concatenative 4.166 ± 0.091 WaveNet (Linguistic) 4.341 ± 0.051 Ground truth 4.582 ± 0.053 Tacotron 2 (this paper) 4.526 ± 0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% conﬁdence intervals computed from the t-distribution for various systems. We also conduct a side-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of −0.270 ± 0.155 shows that raters have a small but statistically signiﬁcant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.148±0.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 ± 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insigniﬁcant preference towards our results by 0.142 ± 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difﬁculties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 ± 0.066 4.449 ± 0.060 Ground truth 4.362 ± 0.066 4.522 ± 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Grifﬁn-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 ± 0.091 Tacotron 2 (Linear + WaveNet) 4.510 ± 0.054 Tacotron 2 (Mel + WaveNet) 4.526 ± 0.066 Table 3. Comparison of evaluated MOS for Grifﬁn-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel spectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Grifﬁn-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better choice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429 ± 0.071, compared to 4.526 ± 0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A deﬁning feature of WaveNet is its use of dilated convolution to increase the receptive ﬁeld exponentially with the number of layers. We evaluate models with varying receptive ﬁeld sizes and number of layers to test our hypothesis that a shallow network with a small receptive ﬁeld may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we ﬁnd that our model can generate highquality audio using as few as 12 layers with a receptive ﬁeld of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results conﬁrm the observations in [9] that a large receptive ﬁeld size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive ﬁeld becomes two orders of magnitude smaller than the baseline and the quality degrades signiﬁcantly even though the stack is as deep as the baseline model. This indicates that the model requires sufﬁcient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive ﬁeld (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526 ± 0.066 24 4 6 505 / 21.0 4.547 ± 0.056 12 2 6 253 / 10.5 4.481 ± 0.059 30 30 1 61 / 2.5 3.930 ± 0.076 Table 4. WaveNet with various layer and receptive ﬁeld sizes.\n\n4. CONCLUSION\nThis paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modiﬁed WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.\n\n5. ACKNOWLEDGMENTS\nThe authors thank Jan Chorowski, Samy Bengio, A¨aron van den Oord, and the WaveNet and Machine Hearing teams for their helpful discussions and advice, as well as Heiga Zen and the Google TTS team for their feedback and assistance with running evaluations. The authors are also grateful to the very thorough reviewers.\n\n6. REFERENCES\n[1] P. Taylor, Text-to-Speech Synthesis, Cambridge University Press, New York, NY, USA, 1st edition, 2009. [2] A. J. Hunt and A. W. Black, “Unit selection in a concatenative speech synthesis system using a large speech database,” in Proc. ICASSP, 1996, pp. 373–376. [3] A. W. Black and P. Taylor, “Automatically clustering similar units for unit selection in speech synthesis,” in Proc. Eurospeech, September 1997, pp. 601–604. [4] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and T. Kitamura, “Speech parameter generation algorithms for HMMbased speech synthesis,” in Proc. ICASSP, 2000, pp. 1315– 1318. [5] H. Zen, K. Tokuda, and A. W. Black, “Statistical parametric speech synthesis,” Speech Communication, vol. 51, no. 11, pp. 1039–1064, 2009. [6] H. Zen, A. Senior, and M. Schuster, “Statistical parametric speech synthesis using deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–7966. [7] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and\nK. Oura, “Speech synthesis based on hidden Markov models,”\nProc. IEEE, vol. 101, no. 5, pp. 1234–1252, 2013. [8] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu,\n“WaveNet: A generative model for raw audio,” CoRR, vol. abs/1609.03499, 2016. [9] S. ¨O. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky, Y. Kang, X. Li, J. Miller, J. Raiman, S. Sengupta, and\nM. Shoeybi, “Deep voice: Real-time neural text-to-speech,”\nCoRR, vol. abs/1702.07825, 2017. [10] S. ¨O. Arik, G. F. Diamos, A. Gibiansky, J. Miller, K. Peng,\nW. Ping, J. Raiman, and Y. Zhou, “Deep voice 2: Multi-speaker\nneural text-to-speech,” CoRR, vol. abs/1705.08947, 2017. [11] W. Ping, K. Peng, A. Gibiansky, S. ¨O. Arik, A. Kannan,\nS. Narang, J. Raiman, and J. Miller, “Deep voice 3: 2000speaker neural text-to-speech,” CoRR, vol. abs/1710.07654,\n2017. [12] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le,\nY. Agiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron:\nTowards end-to-end speech synthesis,” in Proc. Interspeech, Aug. 2017, pp. 4006–4010. [13] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks.,” in Proc. NIPS, Z. Ghahramani,\nM. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,\nEds., 2014, pp. 3104–3112. [14] D. W. Grifﬁn and J. S. Lim, “Signal estimation from modiﬁed short-time Fourier transform,” IEEE Transactions on Acoustics, Speech and Signal Processing, pp. 236–243, 1984. [15] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and\nT. Toda,\n“Speaker-dependent WaveNet vocoder,” in Proc. Interspeech, 2017, pp. 1118–1122. [17] S. Davis and P. Mermelstein, “Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,” IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 28, no. 4, pp. 357 – 366, 1980. [18] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in Proc. ICML, 2015, pp. 448–456. [19] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673–2681, Nov. 1997. [20] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, Nov. 1997. [21] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Proc. NIPS, 2015, pp. 577–585. [22] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in Proc. ICLR, 2015. [23] C. M. Bishop, “Mixture density networks,” Tech. Rep., 1994. [24] M. Schuster, On supervised learning from sequential data with of Science and Technology, 1999. [25] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural networks from overﬁtting.,” Journal of Machine Learning\nResearch, vol. 15, no. 1, pp. 1929–1958, 2014. [26] D. Krueger, T. Maharaj, J. Kram´ar, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio, H. Larochelle, A. Courville, et al., “Zoneout: Regularizing RNNs by randomly preserving hidden activations,” in Proc. ICLR, 2017. [27] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modiﬁcations,” in Proc. ICLR, 2017. [28] A. van den Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals,\nK. Kavukcuoglu, G. van den Driessche, E. Lockhart, L. C. Cobo,\nF. Stimberg, N. Casagrande, D. Grewe, S. Noury, S. Dieleman,\nE. Elsen, N. Kalchbrenner, H. Zen, A. Graves, H. King, T. Walters, D. Belov, and D. Hassabis, “Parallel WaveNet: Fast HighFidelity Speech Synthesis,” CoRR, vol. abs/1711.10433, Nov.\n2017. [29] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. ICLR, 2015. [30] X. Gonzalvo, S. Tazari, C.-a. Chan, M. Becker, A. Gutkin, and\nH. Silen, “Recent advances in Google real-time HMM-driven\nunit selection synthesizer,” in Proc. Interspeech, 2016. [31] H. Zen, Y. Agiomyrgiannakis, N. Egberts, F. Henderson, and\nP. Szczepaniak, “Fast, compact, and high quality LSTM-RNN\nbased statistical parametric speech synthesizers for mobile devices,” in Proc. Interspeech, 2016.\n--- FOOTNOTES 1Google, Inc., 2University of California, Berkeley, 5 Conv Layer 2 Layer 2 LSTM 3 Conv applications for speech recognition, Ph.D. thesis, Nara Institute", "persona": "Student", "response": "### 1. Overview\nConventional Text-to-Speech Approaches The task of computer speech synthesis has long been the focus of scientists and engineers. However, classic approaches do not synthesize speech indistinguishable from that of a human. That is why, here, as in many other areas, deep learning has come to the rescue. This method is based on the pre-recording of short audio sequences, which are then combined to create coherent speech. It turns out very clean and clear but is absolutely devoid of emotion and tone components, so it sounds unnatural. This happens because it is impossible to get an audio recording of all possible words uttered in all possible combinations of emotions and prosody. The use of the concatenated TTS method is limited due to a large amount of data and enormous development time required.\n\n### 2. Background\nTherefore, a statistical parametric speech synthesis method, which explores the very nature of the data, was developed. It generates speech by combining certain parameters such as frequency, amplitude spectrum, etc. Parametric synthesis consists of two stages: These manually-configured parameters, along with linguistic features, are transmitted to the vocoder model, which performs many complex transformations to generate a sound wave. In this case, the vocoder evaluates speech parameters such as phase, prosody, tone, and others. If we can approximate the parameters that define speech on each of its samples, then we can create a parametric model.\n\n### 3. Methods\nThe input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results. Another crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data: Attention defines the part of the encoder data that should be used at the current decoder step. Then the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet. At each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step.\n\n### 4. Results\nApplying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, αi-1 — previous attention weights, hj — j-th hidden encoder state, αi-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5.\n\n### 5. Takeaways\nNow we have data representing text and audio that are suitable for processing by a neural network. Let’s consider the architecture of the feature prediction net, which we will call Tacotron 2, named after the central element of the entire synthesis system. Tacotron 2 Architecture Explained Tacotron 2 is not one network, but two: Feature prediction net and NN-vocoder WaveNet. Feature prediction net is considered as the main network, while the WaveNet vocoder plays the role of a supplemental system. Tacotron2 has sequence to sequence architecture. It consists of an encoder, which creates internal representation of the input signal (symbolic tokens), and a decoder, which turns this representation into a Mel-spectrogram. A very important element of the network is the PostNet, designed to improve the spectrogram generated by the decoder. Some of the modules use information from the previous step of the decoder. But on the first step, the information will be zero-value tensors, which is a common approach in creating recurrent structures. The Workflow with Tacontron 2 The output of PreNet in concatenation with the context vector, obtained as a result of the attention mechanism, is directed to the entrance of the unidirectional two-layer LSTM network with 1024 neurons in each layer. Visualization of Tacotron 2 Processing However, after all the modules started working properly, something like this is displayed: We have accumulated some thoughts on the development process with text to speech synthesis so far. Some of them are common, others are more specific and unique.", "paper_id": "23", "story_id": "23-05"}
{"paper": "ABSTRACT\nThis paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modiﬁed WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a signiﬁcant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech\n\n1. INTRODUCTION\nGenerating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the ﬁeld. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds mufﬂed and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require signiﬁcant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simpliﬁes the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Grifﬁn-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Grifﬁn-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a uniﬁed, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modiﬁed WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difﬁcult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs signiﬁcantly.\n\n2. MODEL ARCHITECTURE\nOur proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modiﬁed version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high ﬁdelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Grifﬁn-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modiﬁed WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in signiﬁcantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel ﬁlterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the ﬁlterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 ﬁlters with shape 5 × 1, i.e., where each ﬁlter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the ﬁnal convolutional layer is passed into a single bi-directional [19] LSTM [20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a ﬁxed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages the model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution ﬁlters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is ﬁrst passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 ﬁlters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the ﬁnal layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difﬁcult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a ﬁxed duration. Speciﬁcally, generation completes at the ﬁrst frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modiﬁed version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic distributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.\n\n3. EXPERIMENTS & RESULTS\n3.1. Training Setup Our training process involves ﬁrst training the feature prediction network on its own, followed by training a modiﬁed WaveNet independently on the outputs generated by the ﬁrst network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with β1 = 0.9, β2 = 0.999, ϵ = 10−6 and a learning rate of 10−3 exponentially decaying to 10−5 starting after 50,000 iterations. We also apply L2 regularization with weight 10−6. We then train our modiﬁed WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with β1 = 0.9, β2 = 0.999, ϵ = 10−8 and a ﬁxed learning rate of 10−4. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing conﬁguration used for training. We randomly selected 100 ﬁxed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inﬂated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modiﬁcations to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Grifﬁn-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We ﬁnd that the proposed system signiﬁcantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio. † System MOS Parametric 3.492 ± 0.096 Tacotron (Grifﬁn-Lim) 4.001 ± 0.087 Concatenative 4.166 ± 0.091 WaveNet (Linguistic) 4.341 ± 0.051 Ground truth 4.582 ± 0.053 Tacotron 2 (this paper) 4.526 ± 0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% conﬁdence intervals computed from the t-distribution for various systems. We also conduct a side-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of −0.270 ± 0.155 shows that raters have a small but statistically signiﬁcant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.148±0.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 ± 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insigniﬁcant preference towards our results by 0.142 ± 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difﬁculties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 ± 0.066 4.449 ± 0.060 Ground truth 4.362 ± 0.066 4.522 ± 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Grifﬁn-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 ± 0.091 Tacotron 2 (Linear + WaveNet) 4.510 ± 0.054 Tacotron 2 (Mel + WaveNet) 4.526 ± 0.066 Table 3. Comparison of evaluated MOS for Grifﬁn-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel spectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Grifﬁn-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better choice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429 ± 0.071, compared to 4.526 ± 0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A deﬁning feature of WaveNet is its use of dilated convolution to increase the receptive ﬁeld exponentially with the number of layers. We evaluate models with varying receptive ﬁeld sizes and number of layers to test our hypothesis that a shallow network with a small receptive ﬁeld may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we ﬁnd that our model can generate highquality audio using as few as 12 layers with a receptive ﬁeld of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results conﬁrm the observations in [9] that a large receptive ﬁeld size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive ﬁeld becomes two orders of magnitude smaller than the baseline and the quality degrades signiﬁcantly even though the stack is as deep as the baseline model. This indicates that the model requires sufﬁcient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive ﬁeld (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526 ± 0.066 24 4 6 505 / 21.0 4.547 ± 0.056 12 2 6 253 / 10.5 4.481 ± 0.059 30 30 1 61 / 2.5 3.930 ± 0.076 Table 4. WaveNet with various layer and receptive ﬁeld sizes.\n\n4. CONCLUSION\nThis paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modiﬁed WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.\n\n5. ACKNOWLEDGMENTS\nThe authors thank Jan Chorowski, Samy Bengio, A¨aron van den Oord, and the WaveNet and Machine Hearing teams for their helpful discussions and advice, as well as Heiga Zen and the Google TTS team for their feedback and assistance with running evaluations. The authors are also grateful to the very thorough reviewers.\n\n6. REFERENCES\n[1] P. Taylor, Text-to-Speech Synthesis, Cambridge University Press, New York, NY, USA, 1st edition, 2009. [2] A. J. Hunt and A. W. Black, “Unit selection in a concatenative speech synthesis system using a large speech database,” in Proc. ICASSP, 1996, pp. 373–376. [3] A. W. Black and P. Taylor, “Automatically clustering similar units for unit selection in speech synthesis,” in Proc. Eurospeech, September 1997, pp. 601–604. [4] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi, and T. Kitamura, “Speech parameter generation algorithms for HMMbased speech synthesis,” in Proc. ICASSP, 2000, pp. 1315– 1318. [5] H. Zen, K. Tokuda, and A. W. Black, “Statistical parametric speech synthesis,” Speech Communication, vol. 51, no. 11, pp. 1039–1064, 2009. [6] H. Zen, A. Senior, and M. Schuster, “Statistical parametric speech synthesis using deep neural networks,” in Proc. ICASSP, 2013, pp. 7962–7966. [7] K. Tokuda, Y. Nankaku, T. Toda, H. Zen, J. Yamagishi, and\nK. Oura, “Speech synthesis based on hidden Markov models,”\nProc. IEEE, vol. 101, no. 5, pp. 1234–1252, 2013. [8] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals,\nA. Graves, N. Kalchbrenner, A. W. Senior, and K. Kavukcuoglu,\n“WaveNet: A generative model for raw audio,” CoRR, vol. abs/1609.03499, 2016. [9] S. ¨O. Arik, M. Chrzanowski, A. Coates, G. Diamos, A. Gibiansky, Y. Kang, X. Li, J. Miller, J. Raiman, S. Sengupta, and\nM. Shoeybi, “Deep voice: Real-time neural text-to-speech,”\nCoRR, vol. abs/1702.07825, 2017. [10] S. ¨O. Arik, G. F. Diamos, A. Gibiansky, J. Miller, K. Peng,\nW. Ping, J. Raiman, and Y. Zhou, “Deep voice 2: Multi-speaker\nneural text-to-speech,” CoRR, vol. abs/1705.08947, 2017. [11] W. Ping, K. Peng, A. Gibiansky, S. ¨O. Arik, A. Kannan,\nS. Narang, J. Raiman, and J. Miller, “Deep voice 3: 2000speaker neural text-to-speech,” CoRR, vol. abs/1710.07654,\n2017. [12] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss,\nN. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le,\nY. Agiomyrgiannakis, R. Clark, and R. A. Saurous, “Tacotron:\nTowards end-to-end speech synthesis,” in Proc. Interspeech, Aug. 2017, pp. 4006–4010. [13] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks.,” in Proc. NIPS, Z. Ghahramani,\nM. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger,\nEds., 2014, pp. 3104–3112. [14] D. W. Grifﬁn and J. S. Lim, “Signal estimation from modiﬁed short-time Fourier transform,” IEEE Transactions on Acoustics, Speech and Signal Processing, pp. 236–243, 1984. [15] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and\nT. Toda,\n“Speaker-dependent WaveNet vocoder,” in Proc. Interspeech, 2017, pp. 1118–1122. [17] S. Davis and P. Mermelstein, “Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,” IEEE Transactions on Acoustics, Speech and Signal Processing, vol. 28, no. 4, pp. 357 – 366, 1980. [18] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in Proc. ICML, 2015, pp. 448–456. [19] M. Schuster and K. K. Paliwal, “Bidirectional recurrent neural networks,” IEEE Transactions on Signal Processing, vol. 45, no. 11, pp. 2673–2681, Nov. 1997. [20] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural Computation, vol. 9, no. 8, pp. 1735–1780, Nov. 1997. [21] J. K. Chorowski, D. Bahdanau, D. Serdyuk, K. Cho, and Y. Bengio, “Attention-based models for speech recognition,” in Proc. NIPS, 2015, pp. 577–585. [22] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in Proc. ICLR, 2015. [23] C. M. Bishop, “Mixture density networks,” Tech. Rep., 1994. [24] M. Schuster, On supervised learning from sequential data with of Science and Technology, 1999. [25] N. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov, “Dropout: a simple way to prevent neural networks from overﬁtting.,” Journal of Machine Learning\nResearch, vol. 15, no. 1, pp. 1929–1958, 2014. [26] D. Krueger, T. Maharaj, J. Kram´ar, M. Pezeshki, N. Ballas, N. R. Ke, A. Goyal, Y. Bengio, H. Larochelle, A. Courville, et al., “Zoneout: Regularizing RNNs by randomly preserving hidden activations,” in Proc. ICLR, 2017. [27] T. Salimans, A. Karpathy, X. Chen, and D. P. Kingma, “PixelCNN++: Improving the PixelCNN with discretized logistic mixture likelihood and other modiﬁcations,” in Proc. ICLR, 2017. [28] A. van den Oord, Y. Li, I. Babuschkin, K. Simonyan, O. Vinyals,\nK. Kavukcuoglu, G. van den Driessche, E. Lockhart, L. C. Cobo,\nF. Stimberg, N. Casagrande, D. Grewe, S. Noury, S. Dieleman,\nE. Elsen, N. Kalchbrenner, H. Zen, A. Graves, H. King, T. Walters, D. Belov, and D. Hassabis, “Parallel WaveNet: Fast HighFidelity Speech Synthesis,” CoRR, vol. abs/1711.10433, Nov.\n2017. [29] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. ICLR, 2015. [30] X. Gonzalvo, S. Tazari, C.-a. Chan, M. Becker, A. Gutkin, and\nH. Silen, “Recent advances in Google real-time HMM-driven\nunit selection synthesizer,” in Proc. Interspeech, 2016. [31] H. Zen, Y. Agiomyrgiannakis, N. Egberts, F. Henderson, and\nP. Szczepaniak, “Fast, compact, and high quality LSTM-RNN\nbased statistical parametric speech synthesizers for mobile devices,” in Proc. Interspeech, 2016.\n--- FOOTNOTES 1Google, Inc., 2University of California, Berkeley, 5 Conv Layer 2 Layer 2 LSTM 3 Conv applications for speech recognition, Ph.D. thesis, Nara Institute", "persona": "Developer", "response": "### 1. Overview\nConverting text into high quality, natural-sounding speech in real time has been a challenging conversational AI task for decades. State-of-the-art speech synthesis models are based on parametric neural networks1. Text-to-speech (TTS) synthesis is typically done in two steps. Input: Output: WaveGlow model2 instead of WaveNet to synthesize waveforms. After reading this blog, try the Jupyter notebook to get hands-on experience generating audio from text in real-time. First step transforms the text into time-aligned features, such as mel spectrogram, or F0 frequencies and other linguistic features; “William Shakespeare was an English poet, playwright and actor, widely regarded as the greatest writer in the English language and the world’s greatest dramatist. He is often called England’s national poet and the ‘Bard of Avon’.” After following the steps in the Jupyter notebook, you will be able to provide English text to the model and it will generate an audio output file.\n\n### 2. Background\nAll of the scripts to reproduce the results have been published on GitHub in our NVIDIA Deep Learning Examples repository, which contains several high-performance training recipes that use Tensor Cores. Additionally, we developed a Jupyter notebook for users to create their own container image, then download the dataset and reproduce the training and inference results step-by-step. Tacotron 22 is a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms, as shown in Figure 1. The network is composed of an encoder (blue) and a decoder (orange) with attention.\n\n### 3. Methods\nHere is an example of what you can achieve using this model: The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer. WaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram. The Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow.\n\n### 4. Results\nDropout instead of Zoneout to regularize the LSTM layers; Mixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures. Using mixed precision training previously required two steps: Porting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values. Mixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed. Mixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook. Training Performance Table 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch. Number of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86 Number of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22 Table 2: Training performance results for WaveGlow model As shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size.\n\n### 5. Takeaways\nThe optimized Tacotron2 model2 and the new WaveGlow model1 take advantage of Tensor Cores on NVIDIA Volta and Turing GPUs to convert text into high quality natural sounding speech in real-time. The generated audio has a clear human-like voice without background noise. Our TTS system is a combination of two neural network models: The Tacotron 2 and WaveGlow model form a TTS system that enables users to synthesize natural sounding speech from raw transcripts without any additional prosody information. Tacotron 2 Model Our implementation of the Tacotron 2 model differs from the model described in1, we use: Table 1: Training performance results for modified Tacotron 2 model Table 3 and Table 4 show inference statistics for the Tacotron2 and WaveGlow text-to-speech system, gathered from 1,000 inference runs, on 1-V100 and 1-T4 GPU, respectively. Latency is measured from the start of Tacotron2 inference to the end of WaveGlow inference. The tables include average latency, standard deviation, and latency confidence intervals (percent values). Throughput is measured as the number of generated audio samples per second. RTF is the real-time factor which tells how many seconds of speech are generated in 1 second of wall time. Table 3: Inference statistics for Tacotron2 and WaveGlow system on 1-V100 GPU Table 4: Inference statistics for Tacotron2 and WaveGlow system on 1-T4 GPU Follow the scripts on GitHub or run the Jupyter notebook step-by-step, to train Tacotron 2 and WaveGlow v1.5 models. In the Jupyter notebook, we provided scripts that are fully automated to download and pre-process the LJ Speech dataset; After the data preparation step, use the provided Dockerfile to build the modified Tacotron 2 and WaveGlow container, and start a detached session in the container; To train our model using AMP with Tensor Cores or using FP32, perform the training step using the default parameters of the Tacrotron 2 and WaveGlow models using a single GPU or multiple GPUs. After training Tacotron 2 and WaveGlow models, or downloaded the pre-trained checkpoints for the respective models, you can perform inference which takes text as input, and produces an audio file.", "paper_id": "23", "story_id": "23-04"}
{"paper": "Abstract\nWe propose a cross-lingual neural codec language model, VALL-E X, for crosslingual speech synthesis. Speciﬁcally, we extend VALL-E [Wang et al., 2023] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-tospeech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at https://aka.ms/vallex. VALL-E X (Cross-Lingual VALL-E) Cross-Lingual Neural Codec Language Modeling Multilingual G2P Multilingual G2P Audio Codec Encoder Source Text Target Text Source Speech Figure 1: The overall framework of VALL-E X, which can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training, and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation. Personalized Target Speech Audio Codec Decoder Language ID Source phoneme sequences Target phoneme sequences Source acoustic tokens Target acoustic tokens 1\n\nIntroduction\nRecent years have witnessed signiﬁcant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity [Li et al., 2019, Ren et al., 2019, Tan et al., 2022]. However, these models can only generate high-quality speech for a speciﬁc speaker in a speciﬁc language. Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difﬁcult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. Previous methods to tackle these challenges typically augment end-to-end TTS models with speciﬁc subnets for speaker and language control [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2020, Ellinas et al., 2022, Cai et al., 2023]. For example, based on the multi-speaker TTS model, Nachmani and Wolf [2019] introduce multiple encoders for each language and additional loss to keep the speaker’s identity. Zhang et al. [2020] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities. Yang and He [2020] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively. Yang and He [2022] further propose a multi-task learning method with additional tasks of speaker similarity and language identiﬁcation. Moreover, Cai et al. [2023] investigates cross-lingual multi-speaker text-to-speech synthesis with sufﬁcient or limited bilingual speech training data. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [Zhang et al., 2019, Lee et al., 2022]. Table 1: A comparison between VALL-E X and previous cross-lingual TTS systems. Previous Systems VALL-E X Intermediate representation Mel spectrogram Audio codec codes Training data < 13K hours Speech accent Foreign Native Speaker similarity Relative low High In-context learning \u0017 \u0013 Zero-shot cross-lingual TTS \u0017 \u0013 In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, VALL-E X, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis. Based on the knowledge learned from large-scale multi-lingual speech data, VALL-E X is able to transfer the speech characteristics, including the speaker’s voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems. More speciﬁcally, we ﬁrst obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data. Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an ofﬂine neural codec encoder. Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model. As illustrated in Figure 1, after training, VALL-E X can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language. The generated acoustic token sequence is decompressed to the target speech waveform by an ofﬂine audio codec decoder. VALL-E X is trained on two large-scale multi-speaker datasets1, LibriLight [Kahn et al., 2020] and WenetSpeech [Zhang et al., 2022a], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively. The combination of LibriLight and WenetSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which signiﬁcantly improves the coverage of different speakers and enhances VALL-E X’s generalization capacity. The comparison between VALL-E X and the previous cross-lingual TTS systems are listed in Table 1. We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot crosslingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST). For cross-lingual text-to-speech synthesis, the proposed VALL-E X is evaluated with LibriSpeech [Panayotov et al., 2015] and EMIME [Wester, 2010] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers. For zero-shot speech-to-speech translation, EMIME [Wester, 2010] dataset is used for the evaluation of VALL-E X on bidirectional Chinese↔English translation tasks, and it contains bilingual audio recordings by the same speakers. We evaluate the proposed VALL-E X framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS). Speciﬁcally, due to the strong in-context learning capability, VALL-E X achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker. By training on large-scale speech-transcription data, the proposed VALL-E X signiﬁcantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness. Furthermore, the human evaluation shows that our VALL-E X outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST). Our contributions can be summarized as follows: • We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data. VALL-E X is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts. • The multi-lingual in-context learning framework enables VALL-E X to generate crosslingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language. • Based on the learned cross-lingual speech modeling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can signiﬁcantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks. • We apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation. We encourage readers to listen to the audio samples on our demo page: https://aka.ms/vallex. 2\n\nBackground\nOur VALL-E X is the cross-lingual version of text-to-speech synthesizer VALL-E [Wang et al., 2023], which was recently proposed to leverage a neural codec language model to achieve textto-speech synthesis. Unlike conventional TTS methods that adopt the continuous regression task, e.g., mel-spectrogram generation, VALL-E regards TTS as a conditional language modeling task with neural codec codes, i.e. acoustic tokens, as an intermediate representation of speech. VALL-E 𝒜:,𝑙∣𝑙= 2, … , 8 VALL-E X Training: Multi-lingual Non-Autoregressive Codec LM ሚ 𝒜:,1:8 𝒜:,1:𝑙−1 𝒮 𝒜:,1 <eos> E Multi-lingual Autoregressive Codec LM B 𝑎1,1 𝑎2,1 <bos> … 𝑎𝑁,1 𝒮 Figure 2: Training illustration of the cross-lingual neural codec language model VALL-E X, consisting of a multi-lingual autoregressive codec LM (φMAR) and a multi-lingual non-autoregressive codec LM (φMNAR). Multi-lingual acoustic tokens (A) and phoneme sequences (S) are converted from speech and transcription using an audio codec encoder and G2P tool, respectively. During training, we use paired S and A from different languages to optimize these two models. employs two-stage modeling, which ﬁrst generates the codec codes of the ﬁrst quantizer of EnCodec [Défossez et al., 2022] from the paired phoneme sequences using an autoregressive language model, and then generates the codes of the rest quantizers in parallel using a non-autoregressive model. After training on the large-scale English speech-transcription dataset LibriLight, VALL-E shows strong in-context learning capabilities. It can generate personalized speech by taking only a 3-second speech fragment as a prompt. Based on VALL-E, our VALL-E X extend to train a cross-lingual neural codec language model, enabling zero-shot cross-lingual capability and supporting cross-lingual TTS or speech-to-speech translation tasks.\n\n3.2 Model Framework Inspired by VALL-E, the cross-lingual codec language model VALL-E X (denoted as φ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM The multi-lingual autoregressive codec LM φMAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efﬁcient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM φMAR is only used to predict the acoustic tokens from the ﬁrst quantizer of EnCodec model. Formally, based on paired speech-transcription data in any language, let S denote the transcribed phoneme sequence, and A:,1 ≜{ai,1|i = 1, . . . , N} denotes the ﬁrst-layer acoustic tokens extracted from the speech X. The decoder φMAR, modeling the concatenated sequence ⟨S, A:,1⟩, is trained to predict A:,1 autoregressively. It is optimized by maximizing the log-likelihood, Quantization & Phonemization: Quantized tokens Layer 𝐿 12 504 31 8 743 … 𝒜:,2:𝐿 …… Layer 2 325 71 435 90 7 … 𝒜:,1 731 284 78 32 669 … Layer 1 audio codec encoder (Quantization) Multi-lingual speech-transcription pairs (phonemization) G2P tool Phoneme tokens HH AH L OW … D 𝒮 N Y Multi-lingual Non-Autoregressive Codec LM Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM φMNAR is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the ﬁrst layer. It is prompted by the phoneme sequence of the current sentence (S) and the acoustic token sequence of another sentence with the same speaker ( ˜ A). Here ˜ A is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer l ∈[2, 8], the embeddings of l −1 layers’ acoustic tokens (A:,1:l−1) are summed up layerwise as input. The learning objective for the l-layer acoustic tokens A:,l can be calculated as 8 X l=2 log pNAR \u0010 A:,l | D S, ˜ A:,1:8, A:,1:l−1 E ; φMNAR \u0011 (2) LMNAR = where ⟨⟩means the sequence concatenation. pNAR(.) computes the pointwise probabilities of A:,l. 3.3 Multi-lingual Training In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-tospeech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (Ss, As) and (St, At) to train our multi-lingual codec LMs φMAR and φMNAR, where s and t represent two different (source and target) languages. Language ID Module Following multi-lingual TTS, we leverage a language ID to guide the speech generation for speciﬁc languages in VALL-E X. On the one hand, without language ID, VALL-E X may be confused to select suitable acoustic tokens for the speciﬁc language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difﬁculty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM φMAR is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens. 3.4 Cross-Lingual Inference After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we ﬁrst concatenate source phonemes Ss and target phonemes St as prompts, and take the ﬁrst-layer source acoustic tokens As :,1 as the decoding preﬁx, condition on which the multi-lingual autoregressive codec LM φMAR generates the ﬁrst-layer target acoustic tokens At :,1, ˆat i,1 ∼pAR \u0000 at i,1 | Ss, St, As :,1, At <i,1 ; φMAR \u0001 , i = 1, . . . , (3) where ∼means probability-based sampling. The sampling is stopped until the <end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the ﬁnal generated speech. After obtaining the ﬁrst-layer target acoustic tokens At :,1 from φMAR, multi-lingual non-autoregressive codec LM φMNAR is used to predict the rest layers of acoustic tokens n At :,l | l = 2, . . . , 8 o by greedy search, i.e., choosing the tokens with maximum probabilities, At :,l = argmax At :,l pNAR \u0000 At :,l | St, As :,1:8, At :,1:l−1 ; φMNAR \u0001 , l = 2, . . . , 8. (4) Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens At :,1:8. 𝑡∣𝑙= 2, … , 8 VALL-E X Inference: 𝒜:,𝑙 Multi-lingual Non-Autoregressive Codec LM 𝑡 𝑠 𝒜:,1:𝑙−1 𝒮𝑡 𝒜:,1:8 𝑡 𝒜:,1 Multi-lingual Autoregressive Codec LM B 𝑡 𝑎𝑁,1 𝑡 𝑎2,1 𝑡 … 𝑠 𝑎1,1 <bos> 𝒮𝑠 𝒮𝑡 𝒜:,1 Figure 3: Inference illustration of the cross-lingual neural codec language model VALL-E X, with two-stage decoding strategies. VALL-E X can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks. 4 VALL-E X Application VALL-E X can be applied to various cross-lingual speech generation tasks. In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in Figure 3. 4.1 Zero-Shot Cross-Lingual TTS The proposed VALL-E X is naturally suitable for zero-shot cross-lingual TTS tasks. Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker’s voice. Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability. Thanks to the in-context learning capability of large language models, VALL-E X surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis. More speciﬁcally, given the source speech, source transcript, and target text, we ﬁrst convert source speech into source acoustic token As using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes Ss and target phonemes St using G2P tool. More speciﬁcally, as introduced in Section 3.4, we let St be the phonemes extracted from the target text, Ss and As be the phonemes and acoustic tokens extracted from the source speech. Then VALL-E X generates the full-layer target acoustic tokens, which are ﬁnally decompressed into the target speech by EnCodec decoder. 4.2 Zero-Shot Speech-to-Speech Translation We can also apply our VALL-E X to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences. Speech Recognition & Translation Model We leverage the improved SpeechUT [Zhang et al., 2022c] as our speech recognition & translation model, which is a uniﬁed-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Speciﬁcally, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss) and MT corpus (source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more 𝒜𝑠 Application: Zero-shot cross-lingual TTS 𝒮𝑠 𝒮𝑡 Codec Encoder G2P Tool G2P Tool Source text Target text Source speech Application: Zero-shot S2ST 𝒮𝑠 𝒮𝑡 𝒜𝑠 E Codec Encoder Semantic Semantic Encoder Decoder Speech Encoder Source Speech pre-training details about this model. After pre-training, the model is ﬁne-tuned with (X s, Ss, St) triplet data derived from the ST corpus. Speciﬁcally, we perform multi-task learning with the CTC [Graves et al., 2006] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes. Inference Figure 3 shows the inference process of speech-to-speech translation. Given a source speech X s, the speech recognition & translation model ﬁrst generates the source phonemes Ss from the semantic encoder and the target phonemes St from the semantic decoder. Besides, we use the EnCodec encoder to compress X s into source acoustic tokens As. Then, we concatenate Ss, St, and As, as the input of VALL-E X, to produce the acoustic token sequence for the target speech, as introduced in Section 3.4. The generated acoustic tokens are converted to the ﬁnal target speech with the decoder of EnCodec. 4.3 Evaluation The proposed model is veriﬁed using various evaluation criteria, including speaker similarity (ASVScore), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation. Speciﬁcally, we measure speaker similarity between synthesized target speech and groudtruth target speech or source speech as an automatic speaker veriﬁcation (ASV) task, where a WavLM [Chen et al., 2022] based ASV model is used to calculate the score. To verify the quality of generated speech, we ﬁrst utilize the ASR system from the released HuBERT-Large model [Hsu et al., 2021] to recognize it into text. For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text. For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text. Finally, to better verify our proposed VALL-E X systems, we adopt the open-source NISQA3 [Mittag and Möller, 2021] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS). 5\n\nExperiments\nWe evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including Chinese→English and English→Chinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5, OpenSubtitles20186 and WMT20207, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama8, and news domains, respectively. The English→Chinese ST data is from GigaST [Ye et al., 2022], which is created by translating the transcripts in GigaSpeech [Chen et al., 2021] using a strong machine translation system. Similarly, we create the Chinese→English ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above. 3https://github.com/gabrielmittag/NISQA 4https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech 5https://challenger.ai/competition/translation 6https://opus.nlpl.eu/OpenSubtitles2018.php 7https://www.statmt.org/wmt20/translation-task.html 8http://www.opensubtitles.org/ We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [Wester, 2010], which contains bilingual Chinese/English speech recorded by the same speakers. There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350. Zero-shot cross-lingual TTS is evaluated using Librispeech [Panayotov et al., 2015] dev-clean set and EMIME dataset providing English and Chinese data, respectively. We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts. 5.2 Experimental Setup Phonemization & Quantization The right picture of Figure 2 illustrates the phonemization & quantization processes for different languages. All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets. We use a uniﬁed phoneme set called BigCiDian9 for two languages which are based on International Phonetic Alphabet (IPA). The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools10 for additional alignment information used for the pre-training of speech recognition & translation model. The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model EnCodec11, which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks. Model Architecture For the cross-lingual codec language models, φMAR and φMNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096. The autoregression is implemented by attention masking in the φMAR model. Sinuous position embedding is separately computed for each prompt sequence in φMAR and φMNAR models. Besides, the φMNAR model uses individual layer normalization for generating each layer of acoustic tokens. We also introduce the model architecture of speech recognition & translation for S2ST in Appendix A.1.2. We call our cross-lingual TTS model and S2ST model as VALL-E X and VALL-E X Trans in the subsequent experiments, respectively. Training Details We optimize each module of VALL-E X individually, including φMAR and φMNAR. For both modules, The maximum sentence length is set to 20 seconds, so we re-segment the LibriLight data to an average utterance duration of 12 seconds by detecting the consecutive silence phonemes. Fortunately, the WenetSpeech data has already been segmented into short utterances. The maximum learning rate is 5e-4 with warm-up steps of 8,000. The models are trained on 32 V100 GPUs for 800k steps. φMAR is trained with the batch size of 120 seconds per GPU, which is 66 seconds for φMNAR due to the memory constraint. When optimizing φMNAR, instead of accumulating all layer’s loss in Eqn. (2), we randomly select one layer at each optimization step for efﬁciency. For speech recognition & translation model, the training details can be found in Appendix A.1.3. Baselines We adopt YourTTS12 [Casanova et al., 2022] as our baseline for zero-shot cross-lingual TTS. YourTTS is a zero-shot multi-speaker TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech. Since previous work shows that current end-to-end S2ST systems underperform cascaded S2ST systems [Jia et al., 2022a, Lee et al., 2021b], we also build an S2ST baseline which is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS. The source speech serves as the reference speech when synthesizing the target speech using YourTTS. The ASR model is the released HuBERT model introduced in Section 4.3, and the MT model is a vanilla Transformer trained by ourselves on the MT data introduced in Section 5.1. Since YourTTS is built only for English, we don’t get its performance for English→Chinese translation direction. Table 2: Zero-shot cross-lingual TTS evaluation for English TTS with Chinese speech as prompts and Chinese TTS with English speech as prompts, using automatic evaluation matrices, including ASV-Score (hypothesis vs. prompt), ASR-WER, and Naturalness. ASV-Score ASR-WER Naturalness English TTS with Chinese as prompts Baseline (YourTTS) 0.30±0.10 8.53 3.36 VALL-E X 0.36±0.11 4.07 3.54 Chinese TTS with English as prompts VALL-E X 0.29±0.10 8.52 3.36 5.3 Zero-Shot Cross-Lingual TTS Evaluation We ﬁrst select samples with a length between 4 and 10 seconds from LibriSpeech dev-clean set, resulting in 40 speakers and 1373 samples. For English TTS, we randomly select one audio from EMIME set as the Chinese prompt for each target sentence in LibriSpeech dev-clean set. For Chinese TTS, we use extra 149 Chinese text sentences provided by the EMIME set and repeat them to the total number of 1373 so that they can be prompted by the LibriSpeech audios one-by-one. When synthesizing the target language speech, the whole sequence of the source language speech is used as the prompt. Automatic Evaluation Table 2 summarizes the results of cross-lingual zero-shot TTS tasks, including English TTS prompted by Chinese speech and Chinese TTS prompted by English speech. We measure the speaker similarity using the automatic speaker veriﬁcation (ASV) model, ranging from -1 to +1 given two speech utterances. The larger the value, the more similar the speakers of the two utterances are. The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of VALL-E X is superior to that of the baseline (0.36 vs 0.30). (2) VALL-E X reduces the WER signiﬁcantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method. (3) VALL-E X has better speech naturalness than the baseline thanks to the large-scale training data and the large language model capacity. The results of Chinese TTS with English prompts are also listed. Table 3: Human evaluation for zero-shot cross-lingual TTS. SMOS means similarity MOS between generated speech and prompt, and CMOS means comparative MOS based on Baseline. SMOS CMOS (v.s. Baseline) Baseline (YourTTS) 3.42±0.19 0.00 VALL-E X 4.00±0.20 +0.24 Human Evaluation We further conduct the human evaluation on 50 randomly selected speech records for zero-shot cross-lingual English TTS with Chinese speech as prompts, including SMOS and CMOS. Note that SMOS ranges from 1 to 5 where the larger the value, the higher the voice similarity, and CMOS ranges from -3 to 3 where the positive number means the new system is better than the baseline. The results are listed in Table 3. Baseline gets 3.42 SMOS scores between generated speech and prompts, while our VALL-E X achieves 4.00, which further demonstrates the model’s superiority in keeping the speech characteristic in the cross-lingual setting. Moreover, to directly compare the speech synthesis quality between the proposed VALL-E X and baseline, we calculate the CMOS score between them evaluated by native speakers on the 50 sentences. The last column of Table 3 shows that VALL-E X obtains the gain of +0.24 CMOS scores than the baseline. Table 4: S2ST performance on EMIME dataset for Chinese↔English directions. Baseline is a cascaded S2ST system based on speaker embedding. Automatic evaluation matrices include ASVScore, ASR-BLEU, and Naturalness. ASV-Score ASR-BLEU Naturalness tgt vs. src hyp vs. src hyp vs. tgt Chinese→English S2ST 0.28±0.10 0.27±0.12 27.49 3.44 - w/ oracle target text 0.28±0.10 0.29±0.11 80.30 3.43 VALL-E X Trans 0.37±0.10 0.37±0.11 30.66 3.54 - w/ oracle target text 0.39±0.10 0.38±0.10 86.78 3.54 Baseline (S2ST) 0.58±0.09 English→Chinese S2ST VALL-E X Trans 0.58±0.09 0.48±0.11 0.53±0.11 34.45 3.41 - w/ oracle target text 0.47±0.12 0.55±0.11 84.00 3.42 Speaker Similarity We ﬁrst evaluate whether the speaker’s voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in Table 4. Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs. src, hyp vs. src, and hyp vs. tgt). From Table 4 we can ﬁnd that: (1) For Chinese→English, the ASV score of VALL-E X Trans signiﬁcantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker’s voice. (2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs. src) for the English→Chinese direction, which suggests that the cross-lingual voice transferability still has the improvement space. (3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not increase notably, indicating that voice transferability is less affected by the quality of translation. Translation Quality Table 4 also shows the translation performance of VALL-E X Trans. Note that ASR-BLEU with oracle target text as the input of VALL-E X can be seen as the upper bound when translations are exactly correct. With oracle target text as input, VALL-E X Trans can achieve the performance of about 84∼87 BLEU scores, which also reﬂects the high performance of our neural codec language model. For Chinese→English, VALL-E X Trans achieves higher BLEU over the baseline (30.66 vs. 27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task. Speech Naturalness We also evaluate the Naturalness with the open-source NISQA [Mittag and Möller, 2021] for S2ST outputs. As shown in the last column of Table 4, compared to the baseline, VALL-E X Trans achieves a better naturalness score (3.54 vs. 3.44), which shows that VALL-E X can generate more natural target language speech than the baseline. Human Evaluation We randomly sample 56 translation pairs13 to perform a human evaluation using SMOS and MOS matrics for both Chinese→English and English→Chinese directions. Table 5 lists the results of VALL-E X Trans as well as the Chinese→English baseline. We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation. For speaker similarity evaluation, VALL-E X Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs. 3.06), demonstrating its superior ability to model speaker property of the proposed VALL-E X. Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91). For speech quality, our VALL-E X slightly outperforms the baseline in Chinese→English S2ST in terms of MOS score (3.87 vs. 3.81). Table 5: Subjection evaluation with SMOS and MOS scores on bidirectional Chinese↔English S2ST tasks. SMOS is measured by comparing with the ground-truth target speech. English→Chinese S2ST baseline is not reported since it is not supported by the released YourTTS. Chinese→English English→Chinese SMOS MOS SMOS MOS Baseline (S2ST) 3.06±0.14 3.81±0.19 - - VALL-E X Trans 4.12±0.13 3.87±0.21 3.94±0.15 3.48±0.13 Source speech prompt 4.91±0.05 - 4.64±0.06 - Oracle target speech - 3.92±0.17 - 3.88±0.13 5.5 Analysis In this section, we ﬁrst analyze the effect of language ID, then explore the foreign accent problems, and qualitatively investigate the ability to maintain voice emotion and synthesize code-switch speech of our proposed model. Effect of Language ID Our VALL-E X is trained with multi-lingual ASR data, which might increase the modeling difﬁculty for each speciﬁc language. We address it by adding language IDs to guide speech synthesis in the autoregressive language codec model. Here, we verify the effectiveness by removing the language ID (LID) or adding the wrong LID (i.e. the source LID). The ASV-Score and ASR-BLEU are reported in Table 6. Without LID or with the wrong language ID, the translation quality decreases, while the speaker similarity between the hypothesis and source speech increases. These results demonstrate the importance of language ID for the accuracy of the content. It also indicates that target LID reduces the transfer of information, which means the model without LID or with source LID will better maintain the sound of the original speaker. Table 6: Evaluation for the effect of language ID on Chinese↔English EMIME dataset. ASV-Score is computed between synthesized speech and source prompt speech. The last column lists the subjection evaluation score of the foreign accent (from 1 to 5 scores). ASV-Score (vs. src) ASR-BLEU Accent Score Chinese→English S2ST VALL-E X Trans 0.37±0.10 30.66 4.10 w/o Language ID 0.41±0.10 29.04 2.98 w/ wrong Language ID 0.41±0.10 29.07 2.55 English→Chinese S2ST VALL-E X Trans 0.48±0.11 34.45 4.03 w/o Language ID 0.49±0.11 30.86 2.35 w/ wrong Language ID 0.50±0.11 29.70 2.25 Foreign Accent Control L2 (second-language, or foreign) accent problem, the synthesized speech sounds like the accents of a foreigner, has arisen in cross-lingual TTS systems [Zhang et al., 2019, Lee et al., 2022]. Automatic Evaluation has shown that adding LID can boost speech quality. Besides, we conduct a subjection evaluation to label foreign accents from 1 to 5 on randomly selected 20 synthesized speech for both English and Chinese, where each sample is measured with a score from 1 to 5 denoting high-status foreign speakers, low-status foreign speakers, middle-status speakers, low-status native speakers, and high-status native speakers, respectively. As summarized in the last column of Table 6, we observed that our VALL-E X can control the accent for the target speech by LID modules. For example, in English→Chinese, VALL-E X Trans with right LID and without LID get the score of 4.03 and 2.35, respectively. This indicates that by using correct LID embedding, VALL-E X Trans is able to alleviate the foreign accent problem. Please also see the demo for audio examples of VALL-E X Trans with or without language ID. [Um et al., 2020]. Moreover, it is more tempting to reserve the source speaker’s emotion in generated target speech for the S2ST task, which is not explored in previous S2ST work. In these experiments, we adopt the source prompts from the emotional voices dataset EmoV-DB [Um et al., 2020] as inputs of VALL-E X Trans to generate the translated target speech, whose samples are listed on our demo page. We found that the proposed VALL-E X can maintain emotional consistency to a certain extent between the source prompt and the synthesized speech. The underlying reasons are (1) our VALL-E X is trained with large-scale multi-lingual multi-speaker speech-transcription data, which contains various emotional speech records, and (2) the strong in-context learning ability of VALL-E X, like GPT-3 [Brown et al., 2020], promotes the generated speech to reserve the characteristic of the source prompt. Code-Switch Speech Synthesis It is a common phenomenon to use code-switch utterances in bilingual or multi-lingual communities [Cao et al., 2020, Zhao et al., 2020, Manghat et al., 2022]. Code-switch speech synthesis aims to produce a ﬂuent and consistent voice for code-switch text. Although our proposed VALL-E X is trained on multiple monolingual speech data, without special optimization for code-switch setting, VALL-E X provides a promising solution to code-switch speech synthesis. We put the code-switch samples on our demo page, demonstrating that due to its strong in-context learning ability, VALL-E X can synthesize ﬂuent code-switch speech with a consistent voice. 6\n\nRelated Work\nSpeech/Audio Synthesis With the rapid development and application of neural networks, speech and audio synthesis have made tremendous progress with different network frameworks, such as WaveNet [Oord et al., 2016], HiFi-GAN [Kong et al., 2020a], and Diffwave Kong et al. [2020b]. Academic and industrial communities also pay increasing attention to synthesizing speech or sound from text, namely text-to-speech (TTS) [Li et al., 2019, Ren et al., 2019] or text-to-sound [Yang et al., 2022, Kreuk et al., 2022]. Recently, it is emerging to apply discrete audio representation learning to audio synthesis, e.g., AudioGen [Kreuk et al., 2022] and AudioLM [Borsos et al., 2022]. AudioGen, consisting of an audio encoder, a text encoder, a Transformer encoder, and an audio decoder, is an autoregressive audio generation model with textual descriptions as inputs. AudioLM reviews high-quality audio generation as unidirectional language modeling. In AudioLM, the input audio is mapped to semantic tokens using w2v-BERT [Chung et al., 2021] and acoustic tokens using SoundStream [Zeghidour et al., 2021]. Through three subsequent stages, AudioLM can accomplish speech continuation, acoustic generation, unconditional generation tasks, and so on. The most related work to ours is VALL-E [Wang et al., 2023], which was recently proposed to utilize a neural codec language model to achieve monolingual text-to-speech synthesis. Trained on large-scale speech data, VALL-E shows a strong in-content learning capability and can synthesize high-quality personalized speech prompted by a short recording of an unseen speaker. Different from the above work, this paper focuses on cross-lingual speech synthesis, and the goal is to retain the source language speaker’s voice in the synthesized speech of the target language. Cross-Lingual TTS In cross-lingual speech synthesis, the goal is to synthesize the speech of another language for a monolingual speaker, which is more challenging than conventional monolingual TTS [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2022, Cai et al., 2023]. By using shared phonemic input representation across languages and incorporating an adversarial objective to disentangle the speaker’s identity and speech content, Zhang et al. [2019] is able to achieve cross-lingual voice cloning within limited speakers. Liu and Mak [2020] also investigate the cross-lingual speech synthesis with speakers’ voices enrolled in their native language. In this system, they achieve it using a Tacotron-based synthesizer with a speaker encoder module and introduce a shared phoneme set with IPA to enhance the cross-lingual capability. Aiming at improving the speaker similarity between the synthesized speech and the recordings of the native speaker, authors in Yang and He [2022] propose multi-task learning by jointly training speaker classiﬁcation and cross-lingual TTS models. Cai et al. [2023] explores cross-lingual multi-speaker speech synthesis under the scenarios of sufﬁcient and limited bilingual training data. In the data-limited scenario, they employ a series of modules including a linguistic feature classiﬁer, a speaker representation extractor, a non-autoregressive multi-speaker voice conversion module, and a neural vocoder, to achieve crosslingual synthesis. Although previous work has made considerable achievements in cross-lingual TTS, they still suffer from the issue of low speaker similarity and the lack of zero-shot ability. In contrast, leveraging large-scale multi-lingual multi-speaker ASR data, our proposed framework with a neural codec language model demonstrates a strong in-context learning ability to alleviate the above issues. Speech to Speech Translation (S2ST) S2ST aims to translate the speech of one language to the speech of another language. The initial research and application mainly focus on cascaded S2ST systems [Lavie et al., 1997, Nakamura et al., 2006, Wahlster, 2013], consisting of speech recognition (ASR), machine translation (MT), and speech synthesis (TTS) models. Recently, end-to-end S2ST models have been explored [Jia et al., 2019, Lee et al., 2021a, Jia et al., 2021, Lee et al., 2021b, Wei et al., 2022, Huang et al., 2022, Li et al., 2022], achieving the direct conversion from source speech to target speech. However, there is still an unsolved problem to reserve the source sound characteristics (e.g. speaker, emotion, and the speech background) in generated speech. This challenge is largely due to the zero-shot nature as the bilingual speech data from the same speakers are hard to collect. Though researchers have put much effort into constructing speech-to-speech translation corpora, such as Voxvopule [Wang et al., 2021], CVSS [Jia et al., 2022b], and SpeechMatrix [Duquenne et al., 2022], they are either synthesized from text or mined from multilingual speech corpora thus can not meet the requirement that bilingual data come from the same speakers. At the same time, Translatotron [Jia et al., 2019] tries to synthesize target speech conditioned by the speaker embedding extracted from the source speech, but it misses richer voice information due to the limitation of the speaker embedding. Translatotron 2 [Jia et al., 2021] retrains the speaker voices relying on the pseudo bilingual speech data of the same speakers generated by multi-speaker TTS systems, while the synthetic speech does not completely simulate the speech of the real world. To address these challenges, we propose to equip the cross-lingual neural codec language model with translation modules and show its zero-shot capability to reserve the sound characteristics in the S2ST task.\n\nConclusion\nIn this work, we propose VALL-E X, a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. VALL-E X is free of the requirement for cross-lingual paired data from the same speakers. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.\n\nReferences\nZalán Borsos, Raphaël Marinier, Damien Vincent, Eugene Kharitonov, Olivier Pietquin, Matt Shariﬁ, Olivier Teboul, David Grangier, Marco Tagliasacchi, and Neil Zeghidour. Audiolm: a language modeling approach to audio generation. arXiv preprint arXiv:2209.03143, 2022. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020. Zexin Cai, Yaogen Yang, and Ming Li. Cross-lingual multi-speaker speech synthesis with limited bilingual training data. Computer Speech & Language, 77:101427, 2023. Yuewen Cao, Songxiang Liu, Xixin Wu, Shiyin Kang, Peng Liu, Zhiyong Wu, Xunying Liu, Dan Su, Dong Yu, and Helen Meng. Code-switched speech synthesis using bilingual phonetic posteriorgram with only monolingual corpora. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7619–7623. IEEE, 2020. Edresson Casanova, Julian Weber, Christopher D Shulby, Arnaldo Candido Junior, Eren Gölge, and Moacir A Ponti. Yourtts: Towards zero-shot multi-speaker tts and zero-shot voice conversion for everyone. In International Conference on Machine Learning, pages 2709–2720. PMLR, 2022. Guoguo Chen, Shuzhou Chai, Guanbo Wang, Jiayu Du, Wei-Qiang Zhang, Chao Weng, Dan Su, Daniel Povey, Jan Trmal, Junbo Zhang, et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. Yu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 244–250. IEEE, 2021. Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High ﬁdelity neural audio compression. arXiv preprint arXiv:2210.13438, 2022. Paul-Ambroise Duquenne, Hongyu Gong, Ning Dong, Jingfei Du, Ann Lee, Vedanuj Goswani, Changhan Wang, Juan Pino, Benoît Sagot, and Holger Schwenk. Speechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations. arXiv preprint arXiv:2211.04508, 2022. Nikolaos Ellinas, Georgios Vamvoukakis, Konstantinos Markopoulos, Georgia Maniati, Panos Kakoulidis, June Sig Sung, Inchul Hwang, Spyros Raptis, Aimilios Chalamandaris, and Pirros Tsiakoulis. Cross-lingual text-to-speech with ﬂow-based voice conversion for improved pronunciation. arXiv preprint arXiv:2210.17264, 2022. Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, ICML ’06, page 369–376, New York, NY, USA, 2006. Association for Computing Machinery. ISBN 1595933832. doi: Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. Hubert: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451–3460, 2021. Rongjie Huang, Zhou Zhao, Jinglin Liu, Huadai Liu, Yi Ren, Lichao Zhang, and Jinzheng He. Transpeech: Speech-to-speech translation with bilateral perturbation. arXiv preprint arXiv:2205.12523, 2022. Ye Jia, Ron J Weiss, Fadi Biadsy, Wolfgang Macherey, Melvin Johnson, Zhifeng Chen, and Yonghui Wu. Direct speech-to-speech translation with a sequence-to-sequence model. arXiv preprint arXiv:1904.06037, 2019. Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, and Roi Pomerantz. Translatotron 2: Robust direct speech-to-speech translation. arXiv preprint arXiv:2107.08661, 2021. Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, and Roi Pomerantz. Translatotron 2: Highquality direct speech-to-speech translation with voice preservation. In International Conference on Machine Learning, pages 10120–10134. PMLR, 2022a. Ye Jia, Michelle Tadmor Ramanovich, Quan Wang, and Heiga Zen. Cvss corpus and massively multilingual speech-to-speech translation. arXiv preprint arXiv:2201.03713, 2022b.\nJ. Kahn, M. Rivière, W. Zheng, E. Kharitonov, Q. Xu, P.E. Mazaré, J. Karadayi, V. Liptchinsky,\nR. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux.\nLibri-light: A benchmark for asr with limited or no supervision. In ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7669– 7673, 2020. doi: 10.1109/ICASSP40776.2020.9052942. Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae. Hiﬁ-gan: Generative adversarial networks for efﬁcient and high ﬁdelity speech synthesis. Advances in Neural Information Processing Systems, 33:17022–17033, 2020a. Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020b. Alon Lavie, Alex Waibel, Lori Levin, Michael Finke, Donna Gates, Marsal Gavalda, Torsten Zeppenfeld, and Puming Zhan. Janus-iii: Speech-to-speech translation in multiple languages. In ICASSP, volume 1, pages 99–102. IEEE, 1997. Ann Lee, Peng-Jen Chen, Changhan Wang, Jiatao Gu, Xutai Ma, Adam Polyak, Yossi Adi, Qing He, Yun Tang, Juan Pino, et al. Direct speech-to-speech translation with discrete units. arXiv preprint arXiv:2107.05604, 2021a. Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, Holger Schwenk, Peng-Jen Chen, Changhan Wang, Sravya Popuri, Juan Pino, Jiatao Gu, and Wei-Ning Hsu. Textless speech-to-speech translation on real data. arXiv preprint arXiv:2112.08352, 2021b. Jihwan Lee, Jae-Sung Bae, Seongkyu Mun, Heejin Choi, Joun Yeop Lee, Hoon-Young Cho, and Chanwoo Kim. An empirical study on l2 accents of cross-lingual text-to-speech systems via vowel space. arXiv preprint arXiv:2211.03078, 2022. Naihan Li, Shujie Liu, Yanqing Liu, Sheng Zhao, and Ming Liu. Neural speech synthesis with transformer network. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 6706–6713, 2019. Xinjian Li, Ye Jia, and Chung-Cheng Chiu. Textless direct speech-to-speech translation with discrete speech representation. arXiv preprint arXiv:2211.00115, 2022. Zhaoyu Liu and Brian Mak. Multi-lingual multi-speaker text-to-speech synthesis for voice cloning with online speaker enrollment. In INTERSPEECH, pages 2932–2936, 2020. Sreeram Manghat, Sreeja Manghat, and Tanja Schultz. Normalization of code-switched text for speech synthesis. Proc. Interspeech 2022, pages 4297–4301, 2022. Gabriel Mittag and Sebastian Möller. Deep learning based assessment of synthetic speech naturalness. arXiv preprint arXiv:2104.11673, 2021. Eliya Nachmani and Lior Wolf. Unsupervised polyglot text-to-speech. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7055–7059. IEEE, 2019. Satoshi Nakamura, Konstantin Markov, Hiromi Nakaiwa, Gen-ichiro Kikui, Hisashi Kawai, Takatoshi Jitsuhiro, J-S Zhang, Hirofumi Yamamoto, Eiichiro Sumita, and Seiichi Yamamoto. The atr multilingual speech-to-speech translation system. IEEE Transactions on Audio, Speech, and Language Processing, 14(2):365–376, 2006. Aaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech: An asr corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5206–5210, 2015. doi: 10.1109/ICASSP.2015.7178964. Yi Ren, Yangjun Ruan, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu. Fastspeech: Fast, robust and controllable text to speech. Advances in Neural Information Processing Systems, 32, 2019. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 464– 468, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10. 18653/v1/N18-2074. URL https://aclanthology.org/N18-2074. Se-Yun Um, Sangshin Oh, Kyungguen Byun, Inseon Jang, ChungHyun Ahn, and Hong-Goo Kang. Emotional speech synthesis with rich and granularized control. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7254–\n7258. IEEE, 2020.\nWolfgang Wahlster. Verbmobil: foundations of speech-to-speech translation. Springer Science & Business Media, 2013. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390, 2021. Chengyi Wang, Sanyuan Chen, Yu Wu, Ziqiang Zhang, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang, Jinyu Li, et al. Neural codec language models are zero-shot text to speech synthesizers. arXiv preprint arXiv:2301.02111, 2023. Kun Wei, Long Zhou, Ziqiang Zhang, Liping Chen, Shujie Liu, Lei He, Jinyu Li, and Furu Wei. Joint pre-training with speech and bilingual text for direct speech to speech translation. arXiv preprint arXiv:2210.17027, 2022. Mirjam Wester. The emime bilingual database. Technical report, The University of Edinburgh, 2010. Dongchao Yang, Jianwei Yu, Helin Wang, Wen Wang, Chao Weng, Yuexian Zou, and Dong Yu. Diffsound: Discrete diffusion model for text-to-sound generation. arXiv preprint arXiv:2207.09983, 2022. Jingzhou Yang and Lei He. Towards universal text-to-speech. In Interspeech, pages 3171–3175, 2020. Jingzhou Yang and Lei He. Cross-lingual text-to-speech using multi-task learning and speaker classiﬁer joint training. arXiv preprint arXiv:2201.08124, 2022. Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, and Jun Cao. Gigast: A 10,000-hour pseudo speech translation corpus. arXiv preprint arXiv:2204.03939, 2022. Neil Zeghidour, Alejandro Luebs, Ahmed Omran, Jan Skoglund, and Marco Tagliasacchi. Soundstream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 30:495–507, 2021. Binbin Zhang, Hang Lv, Pengcheng Guo, Qijie Shao, Chao Yang, Lei Xie, Xin Xu, Hui Bu, Xiaoyu Chen, Chenchen Zeng, Di Wu, and Zhendong Peng. Wenetspeech: A 10000+ hours multidomain mandarin corpus for speech recognition. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6182–6186, 2022a. doi: 10.1109/ICASSP43922.2022.9746682. Yu Zhang, Ron J Weiss, Heiga Zen, Yonghui Wu, Zhifeng Chen, RJ Skerry-Ryan, Ye Jia, Andrew Rosenberg, and Bhuvana Ramabhadran. Learning to speak ﬂuently in a foreign language: Multilingual speech synthesis and cross-language voice cloning. arXiv preprint arXiv:1907.04448, 2019. Yu Zhang, Ron J Weiss, Byungha Chun, Yonghui Wu, Zhifeng Chen, Russell John Wyatt SkerryRyan, Ye Jia, Andrew M Rosenberg, and Bhuvana Ramabhadran. Multilingual speech synthesis and cross-language voice cloning, December 3 2020. US Patent App. 16/855,042. Ziqiang Zhang, Sanyuan Chen, Long Zhou, Yu Wu, Shuo Ren, Shujie Liu, Zhuoyuan Yao, Xun Gong, Lirong Dai, Jinyu Li, et al. Speechlm: Enhanced speech pre-training with unpaired textual data. arXiv preprint arXiv:2209.15329, 2022b. Ziqiang Zhang, Long Zhou, Junyi Ao, Shujie Liu, Lirong Dai, Jinyu Li, and Furu Wei. Speechut: Bridging speech and text with hidden-unit for encoder-decoder based speech-text pre-training. arXiv preprint arXiv:2210.03730, 2022c. A\n\nAppendix\nA.1 Speech Recognition & Translation Model A.1.1 Model Pre-training Speciﬁcally, speech recognition & translation model consists of a speech encoder (θenc1), a semantic encoder (θenc2), and a semantic decoder (θdec). Given a speech waveform X s and the corresponding phonemes Ss ≜{ss i|i = 1, . . . , N} where N is the sequence length, the speech-side pre-training objective is to predict the phonemes from the top of the speech encoder and semantic encoder, formalized as Lspeech = − X i∈M (log p (ss i|X s; θenc1) + log p (ss i|X s; θenc1, θenc2)) (5) where M is a set of masked positions, and the p(.) is parameterized as the same way with original SpeechUT. Then, given bilingual phoneme sequences, Ss and St, the text-side pre-training objective is to perform sequence-to-sequence translation autoregressively, formalized as |St| X i=1 log p \u0000 st i|St <i, Ss; θenc2, θdec \u0001 (6) Ltext = − In this way, each of the three components can be pre-trained with one or two learning objectives. The ﬁnal pre-training objective is Lpt = Lspeech + Ltext. A.1.2 Model Architecture For the speech recognition & translation model, we leverage the Base architecture of the SpeechUT model, where all encoder/decoders consist of 6 Transformer layers with relative position bias [Shaw et al., 2018]. The FFN dimension is 3072 and the attention dimension is 768. Besides, a speech pre-net is equipped before the speech encoder, which contains several 1-D convolutional layers with 512 channels and kernel sizes of [10,3,3,3,3,2,2]. It can downsample the speech waveform by 320 and convert it to ﬁx-dimensional embeddings. A.1.3 Training Details The speech recognition & translation model is pre-trained following the hyper-parameter setting of Zhang et al. [2022c]. The speech mask probability is 8% and the mask length is 10. The embedding mixing mechanism of the original SpeechUT is also performed. The batch sizes of speech and phonemes on each GPU are 1,400,000 (87.5 seconds) and 3,000, respectively. The maximum learning rate is 5e-4 with warm-up steps of 32,000. The model is pre-trained on 32 V100 GPUs for 400K steps. After pre-training, we perform ASR/ST joint ﬁne-tuning, where the transcription phonemes are predicted on the top of the semantic encoder through a nonlinear CTC layer, and the translation phonemes are predicted through the semantic decoder. the transcription phonemes are reduced by removing the repetitive phonemes. During ﬁne-tuning, we empirically set the weight of the CTC loss to 0.2. The models are tuned on 32 GPUs with a batch size of 2,000,000 (125 seconds) per GPU for\n--- FOOTNOTES 70K hours 200K steps.", "persona": "Student", "response": "### 1. Overview\nThe page linked below contains actual examples of voices synthesized with each system. The results of English voice synthesis with Japanese vocal qualities, found at the end of the page in section `Japanese zero-shot cross-lingual text-to-speech` , are particularly illustrative. You can notice that it’s not just the vocal qualities of the original voice that are captured, but also the emotions. The AR Decoder then generates an intermediate representation A^t_1. Subsequently, S^t, A^s, and A^t_1 are used as inputs to the NAR Decoder to calculate the final intermediate representations A^t_2 to A^t_8. This process enables VALL-E-X to create a nuanced synthesis that mirrors the characteristics of the source voice while delivering the desired speech content. Model Size vocos: 40.4MB whisper medium: 1.42GB First, clone the repository. The following commands can be used for speech synthesis. Currently, BLAS runs faster than GPU on macOS, so if necessary, give -e 1 to run using BLAS. VALL-E-X, unlike RVC (Retrieval-based Voice Conversion), directly reflects the characteristics of short audio samples. Therefore, it might be beneficial to prepare several audio prompts with different emotions or intonations and switch between them as needed. This approach allows for greater flexibility and variety in the synthesized speech, adapting to different emotional tones or speaking styles.\n\n### 2. Background\nOverview Architecture AudioGen consists of four models: an audio encoder, text encoder, transformer encoder, and an audio decoder. The audio decoder uses an Auto Regressive Audio Generation Model to synthesize speech from text. AudioLM transforms the input voice into semantic tokens using w2v-BERT, and then converts these into acoustic tokens using SoundStream (a model equivalent to EnCodec), subsequently generating the continuation of the input voice. Model structure Source Text: This is the original text that corresponds to the source speech. It’s what the speaker in the source audio is saying Source Speech: This is an audio file containing the speech that you want the synthesized voice to emulate in terms of vocal qualities and style. Target Text: This is the text you want to be synthesized. It’s the content that you want the output speech to contain. The texts are converted into phonemes by G2P, then tokenized. These tokens are processed by two Transformers (AR and NAR) and finally, the audio waveform is output from the tokens by a neural vocoder. In the decoder, an Auto Regressive (AR) Audio Generation Model is used to generate tokens step-by-step, and then a Non Auto Regressive (NAR) Audio Generation Model processes these tokens in batches. The generation of intermediate representations 2 to 8 from intermediate representation 1 in VALL-E-X is because EnCodec’s intermediate representations are expressed in multiple layers due to a Quantizer. The Quantizer in this context serves as a mechanism that allows the encoding of the audio data into a more complex, layered representation.\n\n### 3. Methods\nWhen these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024). Positional Embeddings For positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha. In the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution. For example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2. Optimisation using kv_cache VALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer. A Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time. To optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step. This kv_cache mechanism significantly speeds up the operation of the Transformer.\n\n### 4. Results\nMain Features Traditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio. The weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts: vallex: 1.48GB Below is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt. VALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later. ax Inc.\n\n### 5. Takeaways\nRVC is a voice-to-voice model suitable for applications like live chat in games. If this kind of voice-to-voice process is needed, VALL-E-X, being a text-to-speech model, requires converting the original voice to text using tools like Whisper before synthesizing speech. Context To use a reference voice, enter the audio file and the transcription of the audio file in the transcript option. The transcription of the voice file can be generated by Whisper or other software if necessary. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference.", "paper_id": "09", "story_id": "09-05"}
{"paper": "BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)\n\nWearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1  Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4  with a lifetime risk as high as 1 in 3.5  Atrial fibrillation is associated with a quintupling of the risk of stroke.6  The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7  Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8  Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.\n\nStudy Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with a health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (<65 and ≥65 years), yielding 97.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.\n\nBaseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (±SD) of 13±16 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse notifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-Day Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.\n\nThe Apple Heart Study was a prospective, singlegroup study that was based on a siteless, pragmatic design. Of the 419,297 participants enrolled, only 0.52% received an irregular pulse notification, and among those with an initial notification who returned an ECG patch, 84% (95% CI, 76 to 92) of their subsequent notifications were confirmed to be atrial fibrillation. Of participants 65 years of age and older, 3.2% received notifications. These estimates may help providers better understand the implications of irregular pulse notifications when patients present for clinical care. The overall yield of atrial fibrillation on an ECG patch was 34% among those who received notifications. This finding is clinically relevant because these participants had a relatively high burden of atrial fibrillation, with a majority of episodes lasting more than 1 hour. The absence of atrial fibrillation on a subsequent ECG patch does not imply that the initial notification was a false positive. Rather, atrial fibrillation may have been paroxysmal and infrequent, which is the most common pattern in early-stage atrial fibrillation. The index atrial fibrillation episode may have ended by the time the ECG patch was worn, which was, on average, 13 days after the initial notification. Although the percentage of participants younger than 40 years of age who received notifications (0.16%) was low, the atrial fibrillation yield on ECG patch monitoring in this group was also lower (18%) than in other age groups. This may be a reflection of the paroxysmal nature of atrial fibrillation at the earlier stages of disease, but further studies are needed to better understand the public health implications of identifying irregular pulse in persons younger than 40 years of age. The positive predictive value of an individual tachogram was 0.71 (97.5% CI, 0.69 to 0.74) and the positive predictive value of an irregular pulse notification was 0.84 (95% CI, 0.76 to 0.92), which suggests that algorithms that rely on confirmation of multiple irregular tachograms before triggering a notification improve accuracy. Many of the irregular tachograms not adjudicated as atrial fibrillation were instead concordant with rhythms that may warrant further clinical attention and require additional study. The positive predictive values were measured for participants who had already received an irregular pulse notification and are therefore only an estimate of the positive predictive value of an initial notification in the overall cohort. This study also provides insight into the way digital alerts result in engagement with the health care system. That 76% of notified participants who returned a survey contacted either the telemedicine provider or a nonstudy provider suggests that many actively sought medical attention. The remaining may have ignored the notification because they knew they had atrial fibrillation, were asymptomatic, did not trust the notification, or did not feel that the notification, even if true, required follow-up. There are several limitations to the study. Participants did not initiate contact with the study provider after notification and fewer returned ECG patches (450 of 2161 notified) than anticipated. As a result, the targeted statistical precision for estimating the yield of atrial fibrillation on patch monitoring, which was one of our primary end points, was not met. The reported confidence intervals appropriately reflect the uncertainty of our key quantities of interest among participants who returned their ECG patches; however, the generalizability of these estimates to participants who did not return ECG patches remains uncertain. Nevertheless, no qualitative differences were observed between those notified and excluded from the analysis and those notified who provided ECG patches with data that could be analyzed. The study was not designed to assess the algorithm as a screening tool or to measure sensitivity, specificity, or false positive results. The algorithm was designed to minimize false positive findings,3 and the low incidence of notifications reflects this intent. Furthermore, the algorithm was not designed to detect short episodes of atrial fibrillation, and participants with a low burden of atrial fibrillation could have been missed. The study objective was not to address the use of the Apple Watch as a population screening tool. Patients using this technology should be aware that the absence of an irregular pulse notification does not exclude possible arrhythmias. Conversely, notification based on an irregular pulse from a photoplethysmography signal should not be used for a definitive diagnosis of atrial fibrillation. Since rhythm-detection technologies are rapidly evolving, additional studies using features such as wearable ECG monitoring devices will need to be performed as the technology becomes available. Nevertheless, uncertainty remains about the benefits of diagnosing and treating asymptomatic atrial fibrillation, particularly in persons whose episodes of atrial fibrillation are of 6 hours’ duration or less. There was no direct physical contact with participants from the time of enrollment and consent to interaction with the telemedicine provider and ECG patch monitoring. Although our siteless, pragmatic study design allowed us to enroll more than 400,000 participants in 8 months, we relied on the participants’ assessments regarding their eligibility for inclusion and regarding outcomes. Substantial loss to follow-up results in uncertain validity and generalizability inherent to this design. At enrollment, persons with previous atrial fibrillation were asked not to participate, but several participants who received notifications later reported a history of atrial fibrillation. Although we mitigated this misclassification by verifying enrollment criteria at the study visit, this kind of misclassification illustrates the challenges of relying on the participants themselves to assess enrollment eligibility and outcomes. In the future, studies may be able to leverage health record data directly from smartphones. As the number of app-based studies grows, development of methods to maximize engagement and the accuracy of data reported by participants is an important area of investigation. Although the participants we enrolled were geographically, racially, and ethnically diverse, the cohort was skewed toward a younger demographic, reflective of smartwatch owners. Studies using similar designs will need to consider these factors to ensure that all affected age and socioeconomic groups are represented. We found that the probability that a participant was notified of an irregular pulse was low, but among participants who were notified of an irregular pulse, more than one third had atrial fibrillation identified on a subsequently worn ECG patch monitor, and among those notified who returned an ECG patch, positive notifications were concordant with atrial fibrillation 84% (95% CI, 76 to 92) of the time. We believe that these data support the ability of the algorithm to correctly identify atrial fibrillation in users whom it notifies of irregular pulses. Rigorous investigation of this technology and of its use in a clinical setting is needed, including the ways this technology can guide further evaluation and treatment to improve clinical outcomes. Finally, this study provides a foundation on which further research in digital health can be conducted.\n\nReferences 1. Strain T, Wijndaele K, Brage S. Physical activity surveillance through smartphone apps and wearable trackers: examining the UK potential for nationally representative sampling. JMIR Mhealth Uhealth 2019;7(1):e11898. 2. Tison GH, Sanchez JM, Ballinger B, et al. Passive detection of atrial fibrillation using a commercially available smartwatch. JAMA Cardiol 2018;3:409-16. 3. Apple. Using Apple Watch for arrhythmia detection. December 2018 (https://www.apple.com/healthcare/site/ docs/Apple_Watch_Arrhythmia _Detection.pdf). 4. Colilla S, Crow A, Petkun W, Singer DE, Simon T, Liu X. Estimates of current and future incidence and prevalence of atrial fibrillation in the U.S. adult population. Am J Cardiol 2013;112:1142-7. 5. Weng LC, Preis SR, Hulme OL, et al. Genetic predisposition, clinical risk factor burden, and lifetime risk of atrial fibrillation. Circulation 2018;137:1027-38. 6. Wolf PA, Abbott RD, Kannel WB. Atrial fibrillation as an independent risk factor for stroke: the Framingham Study. Stroke 1991;22:983-8. 7. Sanna T, Diener H-C, Passman RS, et al. Cryptogenic stroke and underlying atrial fibrillation. N Engl J Med 2014;370: 2478-86. 8. Turakhia MP, Shafrin J, Bognar K, et al. Estimated prevalence of undiagnosed atrial fibrillation in the United States. PLoS One 2018;13(4):e0195088. 9. Healey JS, Connolly SJ, Gold MR, et al. Subclinical atrial fibrillation and the risk of stroke. N Engl J Med 2012;366:120-9. 10. Belkin MN, Soria CE, Waldo AL, et al. Incidence and clinical significance of new-onset device-detected atrial tachyarrhythmia: a meta-analysis. Circ Arrhythm Electrophysiol 2018;11(3):e005393. 11. Steinhubl SR, Waalen J, Edwards AM, et al. Effect of a home-based wearable continuous ECG monitoring patch on detection of undiagnosed atrial fibrillation: the mSToPS randomized clinical trial. JAMA 2018;320:146-55. 12. Halcox JPJ, Wareham K. Response by Halcox and Wareham to letter regarding article, “Assessment of remote heart rhythm sampling using the AliveCor heart monitor to screen for atrial fibrillation: the REHEARSE-AF study.” Circulation 2018;137:2193-4. 13. Turakhia MP, Desai M, Hedlin H, et al. Rationale and design of a large-scale, app-based study to identify cardiac arrhythmias using a smartwatch: the Apple Heart Study. Am Heart J 2019;207:66-75. Copyright © 2019 Massachusetts Medical Society.", "persona": "Student", "response": "### 1. Overview\nWearable technology can safely identify heart rate irregularities that subsequent clinical evaluations confirmed to be atrial fibrillation, reports a study from the Stanford University School of Medicine and Apple now published in the New England Journal of Medicine (NEJM). \"The study's findings will help patients and clinicians understand how devices like Apple Watch can play a role in identifying atrial fibrillation, a deadly and often undiagnosed disease,\" said Mintu Turakhia, MD, associate professor of cardiovascular medicine. \"Additionally, these important findings lay the foundation for further research into the use of emerging wearable technologies in clinical practice and demonstrate the unique potential of large-scale app-based studies.\" The study was launched through a research sponsorship by Apple, Inc., in November 2017 to determine whether software on the Apple Watch could use data from the Watch's heart-rate pulse sensor to identify atrial fibrillation, which is one of the most commonly diagnosed significant cardiac arrhythmias in the United States, affecting up to 6 million people. \"The performance and accuracy we observed in this study provides important information as we seek to understand the potential impact of wearable technology on the health system,\" said Perez. \"What the Apple Heart Study shows us is that atrial fibrillation is just the beginning. We can look ahead to other areas of preventive medicine. Further research will help people make more informed health decisions.\"\n\n### 2. Background\nDuring ECG patch monitoring, participants' Apple Watches continued to monitor pulse irregularities. If a participant had an irregular pulse detected, 84% of the time this was confirmed to be atrial fibrillation on the simultaneous ECG patch. This, said Perez, demonstrates that the algorithm in the Apple Watch can successfully identify atrial fibrillation. Information from this study could be used to inform further clinical evaluation. The study also showed how digital health alerts can enhance engagement with the health care system overall.\n\n### 3. Methods\nCourtesy of Apple With more than 400,000 participants enrolled in eight months, the Apple Heart Study is the largest virtual study to date. Launching app-based research \"As the number of app-based health studies grows, developing additional methods to maximize self-report data accuracy and engagement will be an important area of investigation,\" said Turakhia. The design of the study provided several valuable insights regarding patient engagement and interventions that will prove helpful in designing further studies. For one, the study's completely virtual nature eliminated the need for participants to be physically present and allowed for the implementation of a massive recruitment strategy in a relatively short period of time. Also, because screening was done via interacting with the app, the study was administered at scale with little incremental cost. To participate in the study, each individual needed an Apple Watch (series 1, 2 or 3) and an iPhone. (The Series 4 and 5 Apple Watch, which feature a built-in ECG, weren't part of the study, as they was released after the study's launch.) The Apple Heart Study app intermittently checked the heart-rate pulse sensor for measurements of an irregular pulse. If it detected an irregular pulse, the participant received a notification and was asked to schedule a telehealth consultation with a doctor involved in the study through American Well.\n\n### 4. Results\nTurakhia and Manisha Desai, PhD, professor of medicine and of biomedical data science, are the senior authors of the study. Marco Perez, MD, associate professor of cardiovascular medicine, is the lead author. The study chair is Kenneth Mahaffey, MD, professor of cardiovascular medicine. Participants were then sent ECG patches from BioTelemetry, which recorded the electrical rhythm of their hearts for up to a week. Researchers from the Lankenau Heart Institute, Jefferson Medical College, the University of Colorado School of Medicine, Cooper Medical School of Rowan University, StopAfib.org, the American Foundation for Women's Health and Duke University also contributed to the study.\n\n### 5. Takeaways\nThe study found that only 0.52% of participants received an irregular pulse notification, assuaging concerns about potential over-notification in healthy participants. Atrial fibrillation, a type of irregular heart rhythm, is a leading cause of stroke and hospitalization in the United States, but due to its elusive and often sporadic symptoms, the condition often goes undetected. During the study, the researchers found that only 0.52% of participants received an irregular pulse notification, assuaging concerns about potential over-notification in healthy participants. Those who were flagged for an irregular pulse received follow-up care through a heart-monitoring technique called an electrocardiography (ECG) patch, which continuously monitors electrical impulses generated by the heart, for one week. Of those who received a notification and were monitored by the ECG patch about two weeks later, 34% were found to have atrial fibrillation. Because atrial fibrillation is an intermittent condition, it's not surprising for it to go undetected in subsequent ECG patch monitoring. Comparison between irregular pulse-detection on Apple Watch and simultaneous ECG patch recordings showed the pulse detection algorithm has an 84% positive predictive value.", "paper_id": "62", "story_id": "62-01"}
{"paper": "BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)\n\nWearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1  Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4  with a lifetime risk as high as 1 in 3.5  Atrial fibrillation is associated with a quintupling of the risk of stroke.6  The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7  Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8  Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.\n\nStudy Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with a health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (<65 and ≥65 years), yielding 97.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.\n\nBaseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (±SD) of 13±16 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse notifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-Day Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.\n\nThe Apple Heart Study was a prospective, singlegroup study that was based on a siteless, pragmatic design. Of the 419,297 participants enrolled, only 0.52% received an irregular pulse notification, and among those with an initial notification who returned an ECG patch, 84% (95% CI, 76 to 92) of their subsequent notifications were confirmed to be atrial fibrillation. Of participants 65 years of age and older, 3.2% received notifications. These estimates may help providers better understand the implications of irregular pulse notifications when patients present for clinical care. The overall yield of atrial fibrillation on an ECG patch was 34% among those who received notifications. This finding is clinically relevant because these participants had a relatively high burden of atrial fibrillation, with a majority of episodes lasting more than 1 hour. The absence of atrial fibrillation on a subsequent ECG patch does not imply that the initial notification was a false positive. Rather, atrial fibrillation may have been paroxysmal and infrequent, which is the most common pattern in early-stage atrial fibrillation. The index atrial fibrillation episode may have ended by the time the ECG patch was worn, which was, on average, 13 days after the initial notification. Although the percentage of participants younger than 40 years of age who received notifications (0.16%) was low, the atrial fibrillation yield on ECG patch monitoring in this group was also lower (18%) than in other age groups. This may be a reflection of the paroxysmal nature of atrial fibrillation at the earlier stages of disease, but further studies are needed to better understand the public health implications of identifying irregular pulse in persons younger than 40 years of age. The positive predictive value of an individual tachogram was 0.71 (97.5% CI, 0.69 to 0.74) and the positive predictive value of an irregular pulse notification was 0.84 (95% CI, 0.76 to 0.92), which suggests that algorithms that rely on confirmation of multiple irregular tachograms before triggering a notification improve accuracy. Many of the irregular tachograms not adjudicated as atrial fibrillation were instead concordant with rhythms that may warrant further clinical attention and require additional study. The positive predictive values were measured for participants who had already received an irregular pulse notification and are therefore only an estimate of the positive predictive value of an initial notification in the overall cohort. This study also provides insight into the way digital alerts result in engagement with the health care system. That 76% of notified participants who returned a survey contacted either the telemedicine provider or a nonstudy provider suggests that many actively sought medical attention. The remaining may have ignored the notification because they knew they had atrial fibrillation, were asymptomatic, did not trust the notification, or did not feel that the notification, even if true, required follow-up. There are several limitations to the study. Participants did not initiate contact with the study provider after notification and fewer returned ECG patches (450 of 2161 notified) than anticipated. As a result, the targeted statistical precision for estimating the yield of atrial fibrillation on patch monitoring, which was one of our primary end points, was not met. The reported confidence intervals appropriately reflect the uncertainty of our key quantities of interest among participants who returned their ECG patches; however, the generalizability of these estimates to participants who did not return ECG patches remains uncertain. Nevertheless, no qualitative differences were observed between those notified and excluded from the analysis and those notified who provided ECG patches with data that could be analyzed. The study was not designed to assess the algorithm as a screening tool or to measure sensitivity, specificity, or false positive results. The algorithm was designed to minimize false positive findings,3 and the low incidence of notifications reflects this intent. Furthermore, the algorithm was not designed to detect short episodes of atrial fibrillation, and participants with a low burden of atrial fibrillation could have been missed. The study objective was not to address the use of the Apple Watch as a population screening tool. Patients using this technology should be aware that the absence of an irregular pulse notification does not exclude possible arrhythmias. Conversely, notification based on an irregular pulse from a photoplethysmography signal should not be used for a definitive diagnosis of atrial fibrillation. Since rhythm-detection technologies are rapidly evolving, additional studies using features such as wearable ECG monitoring devices will need to be performed as the technology becomes available. Nevertheless, uncertainty remains about the benefits of diagnosing and treating asymptomatic atrial fibrillation, particularly in persons whose episodes of atrial fibrillation are of 6 hours’ duration or less. There was no direct physical contact with participants from the time of enrollment and consent to interaction with the telemedicine provider and ECG patch monitoring. Although our siteless, pragmatic study design allowed us to enroll more than 400,000 participants in 8 months, we relied on the participants’ assessments regarding their eligibility for inclusion and regarding outcomes. Substantial loss to follow-up results in uncertain validity and generalizability inherent to this design. At enrollment, persons with previous atrial fibrillation were asked not to participate, but several participants who received notifications later reported a history of atrial fibrillation. Although we mitigated this misclassification by verifying enrollment criteria at the study visit, this kind of misclassification illustrates the challenges of relying on the participants themselves to assess enrollment eligibility and outcomes. In the future, studies may be able to leverage health record data directly from smartphones. As the number of app-based studies grows, development of methods to maximize engagement and the accuracy of data reported by participants is an important area of investigation. Although the participants we enrolled were geographically, racially, and ethnically diverse, the cohort was skewed toward a younger demographic, reflective of smartwatch owners. Studies using similar designs will need to consider these factors to ensure that all affected age and socioeconomic groups are represented. We found that the probability that a participant was notified of an irregular pulse was low, but among participants who were notified of an irregular pulse, more than one third had atrial fibrillation identified on a subsequently worn ECG patch monitor, and among those notified who returned an ECG patch, positive notifications were concordant with atrial fibrillation 84% (95% CI, 76 to 92) of the time. We believe that these data support the ability of the algorithm to correctly identify atrial fibrillation in users whom it notifies of irregular pulses. Rigorous investigation of this technology and of its use in a clinical setting is needed, including the ways this technology can guide further evaluation and treatment to improve clinical outcomes. Finally, this study provides a foundation on which further research in digital health can be conducted.\n\nReferences 1. Strain T, Wijndaele K, Brage S. Physical activity surveillance through smartphone apps and wearable trackers: examining the UK potential for nationally representative sampling. JMIR Mhealth Uhealth 2019;7(1):e11898. 2. Tison GH, Sanchez JM, Ballinger B, et al. Passive detection of atrial fibrillation using a commercially available smartwatch. JAMA Cardiol 2018;3:409-16. 3. Apple. Using Apple Watch for arrhythmia detection. December 2018 (https://www.apple.com/healthcare/site/ docs/Apple_Watch_Arrhythmia _Detection.pdf). 4. Colilla S, Crow A, Petkun W, Singer DE, Simon T, Liu X. Estimates of current and future incidence and prevalence of atrial fibrillation in the U.S. adult population. Am J Cardiol 2013;112:1142-7. 5. Weng LC, Preis SR, Hulme OL, et al. Genetic predisposition, clinical risk factor burden, and lifetime risk of atrial fibrillation. Circulation 2018;137:1027-38. 6. Wolf PA, Abbott RD, Kannel WB. Atrial fibrillation as an independent risk factor for stroke: the Framingham Study. Stroke 1991;22:983-8. 7. Sanna T, Diener H-C, Passman RS, et al. Cryptogenic stroke and underlying atrial fibrillation. N Engl J Med 2014;370: 2478-86. 8. Turakhia MP, Shafrin J, Bognar K, et al. Estimated prevalence of undiagnosed atrial fibrillation in the United States. PLoS One 2018;13(4):e0195088. 9. Healey JS, Connolly SJ, Gold MR, et al. Subclinical atrial fibrillation and the risk of stroke. N Engl J Med 2012;366:120-9. 10. Belkin MN, Soria CE, Waldo AL, et al. Incidence and clinical significance of new-onset device-detected atrial tachyarrhythmia: a meta-analysis. Circ Arrhythm Electrophysiol 2018;11(3):e005393. 11. Steinhubl SR, Waalen J, Edwards AM, et al. Effect of a home-based wearable continuous ECG monitoring patch on detection of undiagnosed atrial fibrillation: the mSToPS randomized clinical trial. JAMA 2018;320:146-55. 12. Halcox JPJ, Wareham K. Response by Halcox and Wareham to letter regarding article, “Assessment of remote heart rhythm sampling using the AliveCor heart monitor to screen for atrial fibrillation: the REHEARSE-AF study.” Circulation 2018;137:2193-4. 13. Turakhia MP, Desai M, Hedlin H, et al. Rationale and design of a large-scale, app-based study to identify cardiac arrhythmias using a smartwatch: the Apple Heart Study. Am Heart J 2019;207:66-75. Copyright © 2019 Massachusetts Medical Society.", "persona": "General Public", "response": "### 1. Overview\nThe study recruited 419,297 people who had an Apple Watch. It tested technology in the watch that uses an optical sensor to detect the wearer’s pulse. Of the cohort, 0.5 percent — or 2,161 people — were notified that they might have atrial fibrillation, a relatively common arrhythmia that can lead to feelings of weakness and increases the risk of stroke and other conditions. And the unknowns could be expensive.\n\n### 2. Background\nNEW ORLEANS — New results of a gigantic study testing how well the Apple Watch detects important heart-rhythm changes offer a tantalizing glimpse of how consumer devices might change medicine and lead to new ways to conduct science — but little more. Unfortunately, the data don’t tell us much about whether anyone should buy an Apple Watch, or whether the device is good for the population at large. advertisement The Apple Heart Study is being released at the annual meeting of the American College of Cardiology here. Full results will be presented before thousands of doctors at the opening of the convention Saturday morning. STAT was given a press release in advance containing some data from the study, which was shared with the researchers who commented for this story.\n\n### 3. Methods\nadvertisement Newsletters Sign up for ACC in 30 Seconds Your guide to what's happening at the 2019 ACC meeting in New Orleans. Please enter a valid email address. Privacy Policy “The technology has arrived. I am not one of those cardiologists that are blaming Apple for giving us too much information,” said Dr. Rajat Deo, an associate professor of medicine at the University of Pennsylvania. It’s doctors’ job, he said, to figure out how to use this information.\n\n### 4. Results\n“If we had everybody go to CVS or Walgreens and get a blood pressure check, we’d probably save a lot more lives and it would cost a lot less money,” said Dr. Wilber Su, chief of cardiac electrophysiology at Banner Health in Phoenix. “Most of us have mixed feelings about it,” Su said. “It’s not uncommon for patients to go into complete panic thinking they’re going to have a stroke the next moment because this is positive.” He said that 84 percent rate was low enough to be “concerning,” but he expects it to improve over time.\n\n### 5. Takeaways\n“At least it is not an epidemic of false alarms,” said Dr. Eric Topol, director and founder of the Scripps Research Translational Institute. “The important thing here is that only 0.5 percent of people were getting notified,” said Dr. Marco Perez, an associate professor at Stanford University and one of the lead investigators of the study. “If you look at the young people, it was 0.16 percent. The notification rates are really low. These are needles in a haystack.” What happened next: 945 people reported the notification to the researchers and had a visit with a telemedicine company, American Well. Some were told to go to see a doctor immediately, or excluded because they already had been diagnosed with atrial fibrillation. Then 658 had a patch shipped to them that could perform an electrocardiogram, or EKG, and 450 patients actually used the EKG patch. When the watch again signaled an atrial fibrillation rhythm, it agreed with the EKG 71 percent of the time. But the watch would not have notified the patient until it received five such rhythms; such notifications matched up with the EKG patch 84 percent of the time. (Apple announced EKG capabilities for the watch last year, but these were not part of the study.) If the 84 percent concurrence rate holds and the population in the study is similar to that of Apple Watch buyers (that is a very big “if”), one would expect that for every 400,000 watch buyers, there would be roughly 2,000 people notified they have atrial fibrillation.", "paper_id": "62", "story_id": "62-05"}
{"paper": "BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)\n\nWearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1  Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4  with a lifetime risk as high as 1 in 3.5  Atrial fibrillation is associated with a quintupling of the risk of stroke.6  The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7  Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8  Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.\n\nStudy Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with a health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (<65 and ≥65 years), yielding 97.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.\n\nBaseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (±SD) of 13±16 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse notifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-Day Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.\n\nThe Apple Heart Study was a prospective, singlegroup study that was based on a siteless, pragmatic design. Of the 419,297 participants enrolled, only 0.52% received an irregular pulse notification, and among those with an initial notification who returned an ECG patch, 84% (95% CI, 76 to 92) of their subsequent notifications were confirmed to be atrial fibrillation. Of participants 65 years of age and older, 3.2% received notifications. These estimates may help providers better understand the implications of irregular pulse notifications when patients present for clinical care. The overall yield of atrial fibrillation on an ECG patch was 34% among those who received notifications. This finding is clinically relevant because these participants had a relatively high burden of atrial fibrillation, with a majority of episodes lasting more than 1 hour. The absence of atrial fibrillation on a subsequent ECG patch does not imply that the initial notification was a false positive. Rather, atrial fibrillation may have been paroxysmal and infrequent, which is the most common pattern in early-stage atrial fibrillation. The index atrial fibrillation episode may have ended by the time the ECG patch was worn, which was, on average, 13 days after the initial notification. Although the percentage of participants younger than 40 years of age who received notifications (0.16%) was low, the atrial fibrillation yield on ECG patch monitoring in this group was also lower (18%) than in other age groups. This may be a reflection of the paroxysmal nature of atrial fibrillation at the earlier stages of disease, but further studies are needed to better understand the public health implications of identifying irregular pulse in persons younger than 40 years of age. The positive predictive value of an individual tachogram was 0.71 (97.5% CI, 0.69 to 0.74) and the positive predictive value of an irregular pulse notification was 0.84 (95% CI, 0.76 to 0.92), which suggests that algorithms that rely on confirmation of multiple irregular tachograms before triggering a notification improve accuracy. Many of the irregular tachograms not adjudicated as atrial fibrillation were instead concordant with rhythms that may warrant further clinical attention and require additional study. The positive predictive values were measured for participants who had already received an irregular pulse notification and are therefore only an estimate of the positive predictive value of an initial notification in the overall cohort. This study also provides insight into the way digital alerts result in engagement with the health care system. That 76% of notified participants who returned a survey contacted either the telemedicine provider or a nonstudy provider suggests that many actively sought medical attention. The remaining may have ignored the notification because they knew they had atrial fibrillation, were asymptomatic, did not trust the notification, or did not feel that the notification, even if true, required follow-up. There are several limitations to the study. Participants did not initiate contact with the study provider after notification and fewer returned ECG patches (450 of 2161 notified) than anticipated. As a result, the targeted statistical precision for estimating the yield of atrial fibrillation on patch monitoring, which was one of our primary end points, was not met. The reported confidence intervals appropriately reflect the uncertainty of our key quantities of interest among participants who returned their ECG patches; however, the generalizability of these estimates to participants who did not return ECG patches remains uncertain. Nevertheless, no qualitative differences were observed between those notified and excluded from the analysis and those notified who provided ECG patches with data that could be analyzed. The study was not designed to assess the algorithm as a screening tool or to measure sensitivity, specificity, or false positive results. The algorithm was designed to minimize false positive findings,3 and the low incidence of notifications reflects this intent. Furthermore, the algorithm was not designed to detect short episodes of atrial fibrillation, and participants with a low burden of atrial fibrillation could have been missed. The study objective was not to address the use of the Apple Watch as a population screening tool. Patients using this technology should be aware that the absence of an irregular pulse notification does not exclude possible arrhythmias. Conversely, notification based on an irregular pulse from a photoplethysmography signal should not be used for a definitive diagnosis of atrial fibrillation. Since rhythm-detection technologies are rapidly evolving, additional studies using features such as wearable ECG monitoring devices will need to be performed as the technology becomes available. Nevertheless, uncertainty remains about the benefits of diagnosing and treating asymptomatic atrial fibrillation, particularly in persons whose episodes of atrial fibrillation are of 6 hours’ duration or less. There was no direct physical contact with participants from the time of enrollment and consent to interaction with the telemedicine provider and ECG patch monitoring. Although our siteless, pragmatic study design allowed us to enroll more than 400,000 participants in 8 months, we relied on the participants’ assessments regarding their eligibility for inclusion and regarding outcomes. Substantial loss to follow-up results in uncertain validity and generalizability inherent to this design. At enrollment, persons with previous atrial fibrillation were asked not to participate, but several participants who received notifications later reported a history of atrial fibrillation. Although we mitigated this misclassification by verifying enrollment criteria at the study visit, this kind of misclassification illustrates the challenges of relying on the participants themselves to assess enrollment eligibility and outcomes. In the future, studies may be able to leverage health record data directly from smartphones. As the number of app-based studies grows, development of methods to maximize engagement and the accuracy of data reported by participants is an important area of investigation. Although the participants we enrolled were geographically, racially, and ethnically diverse, the cohort was skewed toward a younger demographic, reflective of smartwatch owners. Studies using similar designs will need to consider these factors to ensure that all affected age and socioeconomic groups are represented. We found that the probability that a participant was notified of an irregular pulse was low, but among participants who were notified of an irregular pulse, more than one third had atrial fibrillation identified on a subsequently worn ECG patch monitor, and among those notified who returned an ECG patch, positive notifications were concordant with atrial fibrillation 84% (95% CI, 76 to 92) of the time. We believe that these data support the ability of the algorithm to correctly identify atrial fibrillation in users whom it notifies of irregular pulses. Rigorous investigation of this technology and of its use in a clinical setting is needed, including the ways this technology can guide further evaluation and treatment to improve clinical outcomes. Finally, this study provides a foundation on which further research in digital health can be conducted.\n\nReferences 1. Strain T, Wijndaele K, Brage S. Physical activity surveillance through smartphone apps and wearable trackers: examining the UK potential for nationally representative sampling. JMIR Mhealth Uhealth 2019;7(1):e11898. 2. Tison GH, Sanchez JM, Ballinger B, et al. Passive detection of atrial fibrillation using a commercially available smartwatch. JAMA Cardiol 2018;3:409-16. 3. Apple. Using Apple Watch for arrhythmia detection. December 2018 (https://www.apple.com/healthcare/site/ docs/Apple_Watch_Arrhythmia _Detection.pdf). 4. Colilla S, Crow A, Petkun W, Singer DE, Simon T, Liu X. Estimates of current and future incidence and prevalence of atrial fibrillation in the U.S. adult population. Am J Cardiol 2013;112:1142-7. 5. Weng LC, Preis SR, Hulme OL, et al. Genetic predisposition, clinical risk factor burden, and lifetime risk of atrial fibrillation. Circulation 2018;137:1027-38. 6. Wolf PA, Abbott RD, Kannel WB. Atrial fibrillation as an independent risk factor for stroke: the Framingham Study. Stroke 1991;22:983-8. 7. Sanna T, Diener H-C, Passman RS, et al. Cryptogenic stroke and underlying atrial fibrillation. N Engl J Med 2014;370: 2478-86. 8. Turakhia MP, Shafrin J, Bognar K, et al. Estimated prevalence of undiagnosed atrial fibrillation in the United States. PLoS One 2018;13(4):e0195088. 9. Healey JS, Connolly SJ, Gold MR, et al. Subclinical atrial fibrillation and the risk of stroke. N Engl J Med 2012;366:120-9. 10. Belkin MN, Soria CE, Waldo AL, et al. Incidence and clinical significance of new-onset device-detected atrial tachyarrhythmia: a meta-analysis. Circ Arrhythm Electrophysiol 2018;11(3):e005393. 11. Steinhubl SR, Waalen J, Edwards AM, et al. Effect of a home-based wearable continuous ECG monitoring patch on detection of undiagnosed atrial fibrillation: the mSToPS randomized clinical trial. JAMA 2018;320:146-55. 12. Halcox JPJ, Wareham K. Response by Halcox and Wareham to letter regarding article, “Assessment of remote heart rhythm sampling using the AliveCor heart monitor to screen for atrial fibrillation: the REHEARSE-AF study.” Circulation 2018;137:2193-4. 13. Turakhia MP, Desai M, Hedlin H, et al. Rationale and design of a large-scale, app-based study to identify cardiac arrhythmias using a smartwatch: the Apple Heart Study. Am Heart J 2019;207:66-75. Copyright © 2019 Massachusetts Medical Society.", "persona": "Student", "response": "### 1. Overview\nOffering a glimpse at how wearable technology may help flag potential health problems, the Apple Watch was able to detect atrial fibrillation (AFib) in a small group of people who had received an alert of an irregular heartbeat, said researchers presenting results from the Apple Heart Study at ACC.19 in New Orleans. Clinical Topics: Arrhythmias and Clinical EP, Sports and Exercise Cardiology, Implantable Devices, SCD/Ventricular Arrhythmias, Atrial Fibrillation/Supraventricular Arrhythmias The Apple Watch and corresponding Heart Study app uses photoplethysmography to intermittently measure blood flow activity and detect subtle changes that might indicate an irregular heartbeat. A tachogram is then created and is analyzed by an algorithm. The aim of the study, presented by Mintu Turakhia, MD, MS, was to identify patients with an irregular pulse watch notification who have AFib on a subsequent electrocardiogram (ECG) patch. A total of 419,297 people self-enrolled in the study.\n\n### 2. Background\nParticipants could not have AFib or be taking anticoagulants and were required to have an Apple Watch and compatible iPhone. They were given information about the study when they downloaded the Heart Study app. Detection of five of six repeat tachograms of an irregular pulse within a 48-hour period triggered a notification to be sent via the app. Participants receiving a notification were prompted to contact the study doctor through the app for a video consultation to determine if the participant should wear an ECG patch. The patch was worn for up to seven days.\n\n### 3. Methods\nThe study had several limitations, including reliance on self-reported data from participants and the potential for a high number of false positive heart rhythms that could then lead to further unnecessary tests and undue anxiety for patients. Additionally, the target enrollment of 500,000 participants, with 75,000 aged 65 or older, was not reached. ACC.org Editor-in-Chief Kim A. Eagle, MD, MACC, also noted that while the watch and corresponding app \"offers promise,\" its accuracy is still far short of more traditional and currently used monitoring techniques. \"This is just a glimpse of the future, but we have a ways to go,\" he said. Turakhia said the trial does represent \"a paradigm shift\" for how clinical studies can be conducted.\n\n### 4. Results\nThe positive predictive value (PPV) for the tachogram was 71 percent and the PPV for notification was 84 percent. About half of participants receiving an irregular pulse notification contacted a study doctor. Subsequent surveys showed that 57 percent of participants who received an alert sought medical care outside of the study regardless of whether they had been seen virtually by a study doctor. \"This is encouraging because it tracks with our understanding of AFib as being more common as you get older,\" Turakhia said, adding that the overall study population represented a striking cross section of cardiovascular risk. \"We don't have to bring people into a brick and mortar clinic and give the study intervention,\" he said.\n\n### 5. Takeaways\nThe primary endpoints were AFib >30 seconds on ECG patch and simultaneous AFib on ECG patch and tachogram. A pulse notification was received by 2,161 participants (0.52 percent). Notification rates were most frequent in participants over age 65 (slightly >3.0 percent) and lowest among those under 40 (0.16 percent). Patches were sent to 658 participants and 450 were returned and included in the analysis. AFib was identified in 34 percent of those who received a notification and wore the ECG patch. \"AFib can come and go, particularly early on in the course of the disease. It's not surprising for it to go undetected in subsequent ECG patch monitoring. So while only 34 percent of people who were still having AFib on the ambulatory ECG, that doesn't mean that 66 percent didn't have AFib. It just means that AFib may not have been there at the time,\" Turakhia said. \"These parameters help us understand how we, as clinicians, should think about these notifications.\"", "paper_id": "62", "story_id": "62-04"}
{"paper": "Abstract: Communication barriers pose significant challenges for the Deaf and Hard-ofHearing (DHH) community, limiting their access to essential services, social interactions, and professional opportunities. To bridge this gap, assistive technologies leveraging artificial intelligence (AI) and deep learning have gained prominence. This study presents a real-time American Sign Language (ASL) interpretation system that integrates deep learning with keypoint tracking to enhance accessibility and foster inclusivity. By combining the YOLOv11 model for gesture recognition with MediaPipe for precise hand tracking, the system achieves high accuracy in identifying ASL alphabet letters in real time. The proposed approach addresses challenges such as gesture ambiguity, environmental variations, and computational efficiency. Additionally, this system enables users to spell out names and locations, further improving its practical applications. Experimental results demonstrate that the model attains a mean Average Precision (mAP@0.5) of 98.2%, with an inference speed optimized for real-world deployment. This research underscores the critical role of AI-driven assistive technologies in empowering the DHH community by enabling seamless communication and interaction.\n\n1. Introduction\nFor millions of Deaf and Hard-of-Hearing individuals worldwide, communication barriers hinder their ability to engage effectively in everyday interactions. Traditional methods, such as sign language interpretation services, are often limited in availability, costly, and reliant on human interpreters. In an increasingly digital world, there is a growing need for intelligent assistive technologies that provide real-time, accurate, and accessible solutions to bridge the communication gap [1]. American Sign Language (ASL) is one of the most widely used sign languages, consisting of distinct hand gestures that represent letters, words, and phrases. However, existing ASL recognition systems often struggle with real-time performance, accuracy, and robustness across diverse environments. To address these limitations, this study introduces a real-time ASL interpretation system that integrates deep learning with keypoint tracking. By leveraging YOLOv11 for rapid hand gesture recognition and MediaPipe for precise landmark detection, the proposed approach enhances the efficiency and reliability of ASL interpretation [2,3]. At the core of our system, a built-in webcam functions as a non-contact optical sensor that captures real-time visual data. This vision-based sensor converts light into digital image frames, which serve as the primary input for gesture analysis. MediaPipe extracts 21 keypoints per hand from each frame to generate a skeletal representation, while YOLOv11 detects and classifies specific ASL alphabet letters based on these visual inputs. The webcam’s role as a sensing device enables the system to acquire gesture-based data continuously and reliably. It also ensures that the entire recognition pipeline—from sensing to classification—can operate in real time, even under different lighting conditions and backgrounds, using only standard hardware. This highlights the system’s practical viability as an accessible and scalable assistive technology. In addition to enhancing communication accessibility, AI-driven ASL recognition systems have the potential to revolutionize various industries. In education, they can facilitate more effective learning environments for Deaf students, allowing them to interact seamlessly with teachers and peers. In healthcare, real-time ASL interpretation can bridge the gap between Deaf patients and medical professionals, ensuring better access to quality care [4]. Similarly, workplaces can become more inclusive by integrating ASL recognition tools, enabling Deaf employees to participate fully in discussions and decision-making processes. The integration of AI and deep learning in assistive technologies not only fosters inclusivity but also paves the way for more advancements in human–computer interaction, making communication more intuitive and accessible for individuals with hearing impairments [5,6]. The significance of this research lies in its potential to empower the Deaf community by providing an AI-driven tool that translates ASL gestures into text, facilitating smoother interactions in education, workplaces, healthcare, and social settings. By developing a robust and accessible ASL interpretation system, this study contributes to the advancement of assistive technologies aimed at promoting inclusivity and bridging communication barriers for the DHH population [7]. The key contributions of our research paper can be summarized as follows: • Developed a real-time AI-driven ASL interpretation system capable of accurately spelling out names and locations through hand gesture recognition. • Integrated YOLOv11 for gesture detection and MediaPipe for hand tracking, ensuring both high accuracy and fast inference speeds. • Implemented keypoint-based annotation techniques to enhance the model’s ability to capture subtle variations in finger positioning, improving classification precision. • Utilized a standard built-in webcam as a vision-based optical sensor for gesture data acquisition, demonstrating the system’s practical application in real-time human– computer interaction and AI-driven assistive technology. • Achieved an impressive 98.2% mean Average Precision (mAP@0.5), demonstrating the system’s reliability in recognizing ASL letters. • Curated and processed a large-scale dataset of 130,000 ASL hand gesture images, each annotated with 21 keypoints to enhance model learning and generalization. The research paper is structured as follows: Sections 2 and 3 discuss relevant research and background concepts, including Transfer Learning, YOLO, and MediaPipe, which are fundamental to real-time ASL alphabet recognition. Section 4 elaborates on the methodology used in this study, detailing dataset preparation, preprocessing, augmentation, annotation, and data balancing techniques. Section 5 presents and discusses the results obtained, evaluating model performance through precision–recall analysis, confidence-based evaluations, and confusion matrix analysis, along with the real-time system’s robustness and accuracy. Finally, a general conclusion is provided in Section 6.\n\n2. Background\n2.1. Transfer Learning Transfer learning is a deep learning technique that adapts a model pre-trained on a large dataset to a new, related task with a smaller dataset. Instead of training from scratch, this approach leverages the pre-trained model’s knowledge, improving generalization and reducing training time. It is particularly advantageous in object detection, where pre-trained models learn fundamental visual features—such as edges, textures, and shapes—that can be fine-tuned for specific applications [8,9]. For this task, a pre-trained YOLO11 model, originally trained on the COCO dataset, serves as the base model. This model has already learned essential visual patterns, making it well suited for hand detection. By utilizing its embedded knowledge, transfer learning enables adaptation for ASL-specific detection. During this process, the backbone of YOLO11—responsible for extracting core visual features—remains frozen, preserving its learned representations. Since the lower convolutional layers are already optimized for detecting edges, textures, and object structures, retraining them is redundant. Freezing these layers retains essential knowledge, prevents unwarranted modifications, and reduces computational overhead, allowing the model to focus on refining task-specific features [10]. While the backbone remains unchanged, the Neck and Head layers are fine-tuned to specialize in detecting ASL alphabet signs. These layers play a critical role in refining feature representations, predicting bounding boxes, and accurately classifying hand gestures. Finetuning involves several optimization strategies, including using a lower learning rate for stable weight updates, restricting gradient updates to trainable layers to preserve pre-trained features, and applying data augmentation techniques (e.g., flipping, rotation, and lighting variations) to enhance generalization [11]. Throughout this phase, the model progressively learns to distinguish between ASL letters, refine bounding box predictions, and improve classification accuracy, ensuring the reliable recognition of hand gestures. See Figure 1 for a visual representation of the workflow of the transfer learning methodology using the YOLO model. Figure 1. Workflow of transfer learning methodology for ASL hand gesture detection using YOLO. builds upon previous iterations with improved performance in localization, detection accuracy, and efficiency. It is particularly optimized for detecting small and complex objects, making it ideal for applications such as ASL hand gesture recognition [12]. 2.2.1. Detection Pipeline YOLOv11 follows a single-stage detection pipeline, meaning it processes the entire image in one pass instead of multiple steps like region-based models (e.g., Faster R-CNN) [13]. The input image is divided into grid cells (e.g., a 640 × 640 resolution image is divided into 32 × 32 cells). Each cell is responsible for detecting objects within its region. YOLOv11 utilizes a deep convolutional neural network (CNN) to extract meaningful spatial features from the image, ensuring efficient object detection [14]. 2.2.2. Bounding Box Prediction Each grid cell predicts multiple bounding boxes [15], including • (x, y): Center coordinates of the object. • (w, h): Width and height relative to the image size. • Confidence Score: Probability of an object being present. Since objects may overlap, multiple bounding boxes are generated to handle these cases effectively. 2.2.3. Class Prediction Each bounding box is assigned a class probability score using softmax classification [16]. For ASL hand gesture detection, the class labels range from 0 to 25, corresponding to the A–Z alphabet. 2.2.4. Non-Maximum Suppression (NMS) YOLO follows a technique called Non-Maximum Suppression (NMS), where multiple boxes may predict the same object. NMS removes redundant detections by keeping only the highest-confidence detection while suppressing lower-confidence overlapping boxes [17]. 2.2.5. Final Output The final output consists of localized bounding boxes and class labels. In ASL detection, YOLOv11 detects hands and assigns them to ASL letter categories based on keypoints, ensuring accurate real-time recognition. Figure 2 illustrates the step-by-step process of YOLO, including grid cell division, bounding box regression, classification with confidence scores, and final object detection. Figure 2. YOLO detection workflow for ASL hand gesture recognition. 2.3. MediaPipe provides pre-built solutions for various computer vision tasks, including hand tracking, face detection, object tracking, pose estimation, and gesture recognition [18]. The MediaPipe Hand Tracking system employs a two-stage ML pipeline for precise and efficient hand detection and tracking. 2.3.1. BlazePalm Detector BlazePalm is an optimized deep learning model for real-time palm detection. As a single-shot detector, it processes an entire image in one pass, enabling the fast and efficient localization of hands. It generates a robust bounding box around the detected hand, even in cases of partial occlusion, ensuring stable tracking for applications like gesture recognition and sign language interpretation [19,20]. Its speed and accuracy make it ideal for mobile and embedded systems. 2.3.2. Hand Landmark Model The Hand Landmark Model is a lightweight convolutional neural network (CNN) trained on diverse datasets to detect 21 keypoints across the hand, including the fingers and wrist. It outputs (x, y, z) coordinates for each keypoint, where z provides depth estimation. This enables 3D gesture analysis [21]. 2.3.3. Keypoint Representation The 21 detected keypoints are stored in a normalized form [22] using the following representation: Ki = (xi, yi), for i ∈[0, 20] (1) where • Ki represents the i-th keypoint. • xi, yi are the normalized coordinates within the range [0, 1]. This normalization ensures consistency across different image resolutions and scales, making the keypoints adaptable for various machine learning applications [22,23]. Table 1 shows the keypoint IDs and the hand part associated with each ID. Table 1. MediaPipe hand keypoints and corresponding hand parts. Keypoint ID(s) Hand Part 0 Wrist 1, 2, 3, 4 Thumb 5, 6, 7, 8 Index Finger 9, 10, 11, 12 Middle Finger 13, 14, 15, 16 Ring Finger 17, 18, 19, 20 Pinky Finger Figure 3 illustrates the 21 keypoints tracked by MediaPipe’s hand tracking model, capturing the articulation of the fingers and wrist for gesture recognition and 3D hand analysis. By integrating YOLOv11 with MediaPipe Hands, this study enhances real-time ASL alphabet recognition, improving both its accuracy and its computational efficiency. Figure 3. Illustration of the 21 keypoints tracked by MediaPipe’s hand tracking model.\n\n4. Methodology\nThis study aims to develop a real-time American Sign Language (ASL) alphabet recognition system that accurately identifies and translates ASL hand gestures into text, enabling users to spell names and locations interactively. The proposed approach integrates YOLOv11 for gesture detection and MediaPipe for hand tracking to enhance precision and real-time responsiveness. A major challenge in ASL recognition systems lies in distinguishing visually similar gestures, such as “A” and “T” or “M” and “N”, which often leads to misclassifications. Additionally, the dataset quality presents significant obstacles, including poor image resolution, motion blur, inconsistent lighting, and variations in hand sizes, skin tones, and backgrounds. These factors introduce bias and reduce the model’s ability to generalize across different users and environments [29]. To address these challenges, a large-scale dataset was curated, incorporating custom keypoint-based annotations to enhance the model’s ability to detect subtle variations in finger positioning. This ensures improved accuracy, robustness against variations in hand structure, and better adaptability to real-world conditions. Figure 4 presents an overview of the data processing pipeline for the proposed realtime ASL recognition system. It illustrates the sequential stages, including data preprocessing, augmentation, annotation, and model training, leading to the final real-time recognition system. 4.1. Dataset Description The ASL Alphabet Hand Gesture Dataset is a comprehensive collection of hand gesture images designed to train deep learning models for real-time ASL recognition. This dataset enhances computer vision models by enabling the precise classification of ASL alphabet gestures, thereby supporting real-time sign language interpretation systems . Comprising 130,000 images, the dataset captures diverse hand gestures under varying conditions to improve model generalization. These conditions include • Different lighting environments: bright, dim, and shadowed. • Various backgrounds: natural outdoor settings and indoor scenes. • Multiple angles and hand orientations for robustness. Each image is annotated with 21 keypoints, accurately marking critical hand structures such as fingertips, knuckles, and the wrist (see Figure 5). These annotations provide a skeletal representation of the hand, enabling the model to distinguish between similar gestures with greater precision. Figure 5. Annotated ASL Alphabet Hand Gestures with 21 keypoints for model training. For optimal training and evaluation, the dataset is systematically partitioned as follows: • Training set: 80%. • Validation set: 10%. • Testing set: 10%. This structured division ensures that the trained models achieve high performance in real-world ASL recognition applications, making the dataset a valuable resource for AI-driven sign language accessibility solutions. 4.2. Data Preprocessing • Resizing: Standardizing all images to 640 × 640 pixels for consistency. • Contrast adjustment: Enhancing visual clarity using contrast stretching. • Grayscale conversion: Applied to 10% of images to improve robustness to color variations. 4.3. Data Augmentation To enhance generalization and mitigate overfitting, 30% of the dataset underwent augmentation with the following transformations: • Flipping: Horizontal and vertical flips to introduce diverse orientations. • Rotation: Random variations between −10◦and +10◦to simulate natural hand positioning. • Brightness adjustment: Random modifications within ±15% to accommodate lighting variations. • Grayscale conversion: Selectively applied to certain images to enhance the model’s focus on gesture structure. These augmentations significantly improve the model’s ability to recognize ASL gestures across different environmental conditions, ensuring higher accuracy and reliability in real-time applications. 4.4. Data Annotation To optimize YOLO for ASL hand gesture recognition, images were annotated using 21 keypoints extracted from hand landmarks. Unlike traditional bounding boxes that enclose the entire hand without precisely identifying its central structure, this approach prioritizes the 21 keypoints detected by MediaPipe. From these keypoints, a bounding box is computed based on their spatial distribution, ensuring it accurately encapsulates the gesture while preserving crucial articulation details. By leveraging this method, the annotation process enhances precision, as it focuses on the actual structure of the hand rather than just its outer boundaries, leading to more reliable recognition and classification. The annotation process was automated and underwent the following steps: 1. Hand Landmark Detection: • MediaPipe extracted 21 keypoints corresponding to joints and finger positions of the hand. • The detected keypoints were scaled and normalized to maintain consistency across varying image resolutions. 2. Bounding Box Calculation: A bounding box was generated to enclose only the 21 keypoints that were previously captured on the hand by determining the minimum and maximum coordinates from the extracted keypoints. 4.4.1. Bounding Box Computation Since YOLO requires object detection labels in the format (x_center, y_center, width, height), the bounding box was calculated using the following mathematical expressions: Finding the Bounding Box Extent The bounding box must enclose all 21 detected keypoints. To determine its position, we compute the top-left corner(xmin, ymin) and bottom-right corner (xmax, ymax): xmax = max(x0, x1, . . . , x20), ymax = max(y0, y1, . . . , y20) Here, xmin and ymin represent the smallest x and y values among all detected keypoints, defining the top-left corner of the bounding box. Similarly, xmax and ymax represent the largest x and y values, defining the bottom-right corner. Computing Bounding Box Dimensions The width and height of the bounding box are calculated as width = xmax −xmin, height = ymax −ymin These values represent the total span of the hand in the x and y directions. Determining the Bounding Box Center Since YOLO requires bounding box coordinates in a center-based format, the center of the bounding box is computed as xcenter = xmin + xmax This ensures the bounding box is properly centered around the detected hand. Normalization for YOLO Format Given that keypoints are already normalized between 0 and 1 relative to image dimensions, the bounding box coordinates must also be constrained within the same range: xcenter, ycenter, width, height ∈[0, 1] This prevents values from exceeding the image boundaries, ensuring consistency across different image resolutions. 4.4.2. Annotation Format for YOLO Each detected hand was assigned a class label corresponding to the ASL letter and formatted according to YOLO’s labeling convention: • Class Label: An integer (0–25) representing the ASL letter. • where Ki represents the i-th keypoint, and xi, yi are in the range [0, 1]. • Bounding Box: Stored in the YOLO format as Figure 6 illustrates the effectiveness of the annotation process during batch processing. It shows bounding boxes that focus solely on the 21 keypoints, ensuring precise localization. This structured annotation approach enables YOLO to detect and classify ASL gestures with greater precision, leading to a fine-tuned, real-time ASL recognition system optimized for robust gesture classification. 2 , ycenter = ymin + ymax 2 Ki = (xi, yi), i ∈[0, 20] (xcenter, ycenter, width, height) Figure 6. Effectiveness of annotation in batch processing with precise keypoint localization. 4.4.3. Data Balancing Balancing the dataset is essential for training YOLOv11 to detect and interpret the American Sign Language (ASL) alphabet in real time. Without balance, the model favors more frequent classes, leading to bias where certain letters are detected with high accuracy while others are misclassified or ignored. This results in unreliable performance, making the system unsuitable for users relying on ASL [30]. An imbalanced dataset also affects YOLOv11’s detection confidence. If certain letters appear more often, the model assigns higher confidence to them while lowering confidence for less frequent ones, leading to poor recall and inconsistent real-time detection. Balancing the dataset ensures uniform exposure, resulting in stable confidence scores and improved accuracy across all classes. Overfitting is another critical issue. If the model encounters significantly more samples of some letters, it becomes biased toward those while failing to learn the characteristics of rare ones. This weakens its ability to recognize less frequent letters, reducing overall effectiveness [31]. To address these challenges, the upsampling technique has been utilized through data augmentation to balance the dataset to 5000 images per class. This ensures equal learning across all ASL letters, prevents bias, enhances generalization, avoids overfitting, and improves real-world performance. A well-balanced dataset is crucial for building a reliable, inclusive, and effective real-time ASL interpretation system. Figure 7 illustrates the dataset’s distribution and the spatial characteristics of the bounding boxes, offering insight into the overall structure and organization of the data. Top-left (Bar Chart): This plot confirms that the dataset is balanced, with each of the distribution ensures that no letter is overrepresented or underrepresented, preventing model bias. Top-right (Bounding Box Overlays): This visualization displays multiple bounding boxes superimposed on each other. The tight clustering of bounding boxes suggests that the hand positions across different images are well aligned and consistently annotated, reducing variations that could impact the model’s learning. Bottom-left (Heatmap of x-y Coordinates): This heatmap shows the spatial distribution of the hand’s center positions in normalized coordinates. The density around the center suggests that most hand gestures are captured within a specific region of the frame, making it easier for YOLOv11 to detect and recognize them accurately. Bottom-right (Heatmap of Width–Height Distribution): This plot highlights the distribution of bounding box sizes. The concentration of values in a specific region indicates that the hands in the dataset mostly follow a uniform scale, ensuring that the model generalizes well without being biased toward specific sizes. This visualization confirms that the dataset is structured optimally for YOLOv11 training, ensuring consistency across annotations and reducing variability in hand positioning and bounding box dimensions. Figure 7. Overview of the dataset distribution and spatial properties of the bounding boxes.\n\n5. Results and Discussion\n5.1. Model Performance and Evaluation Metrics The performance evaluation of the proposed American Sign Language (ASL) alphabet recognition system was conducted using standard multiclass classification metrics, including precision, recall, F1-score, and mean Average Precision (mAP). The system is powered by the trained YOLO11 model, a deep learning architecture with 181 layers and 2,594,910 parameters, optimized for efficient real-time inference. With a computational complexity of 6.5 GFLOPs (Giga Floating Point Operations per Second) per inference, the model was rigorously tested on a balanced dataset of ASL alphabet hand gesture images. The training was conducted with 300 epochs, a batch size of 16, and an input image size of 640 × 640. The AdamW optimizer was employed to enhance training stability. The chosen configuration ensures the model’s reliability, accuracy, and scalability for real-world ASL translation applications. The model’s performance is summarized in Table 3. Table 3. Performance metrics of the ASL recognition model. Table 3. Cont. Metric Value F1-Sore (F) 0.991 mAP@0.5 0.982 mAP@0.5–0.95 0.933 Inference Speed 1.3 ms per image Post-processing Speed 0.3 ms per image These results demonstrate the model’s strong ability to accurately detect and classify ASL hand gestures with a high mean Average Precision (mAP@0.5) of 98.2%, confirming its effectiveness in a real-time sign language interpretation system. 5.2. Precision–Recall Analysis in Multiclass Classification The precision–recall (PR) curve provides insight into how the model balances precision and recall across different confidence thresholds for a multiclass classification setting [32]. In a multiclass classification setting, the precision and recall for each class c are computed as where • TPc (True Positives): Correctly classified instances of class c. • FPc (False Positives): Instances incorrectly classified as class c. • FNc (False Negatives): Instances of class c that were misclassified. From Figure 8, we observe that precision remains above 0.98 across most recall values, confirming minimal false positive detections. The slight degradation in precision at extreme recall values suggests that the model maintains a conservative detection strategy. Precisionc = TPc TPc + FPc (2) Recallc = TPc TPc + FNc (3) 5.3. Confidence-Based Evaluations for Multiclass Classification To further analyze model performance, we examine the recall–confidence, precision– confidence, and F1–confidence curves [33]. 5.3.1. Recall–Confidence Curve In a multiclass classification scenario, the recall–confidence curve illustrates how recall changes as the confidence threshold τ increases [34]. The global recall at a given confidence level τ is computed as Recall(τ) = ∑N c=1 TPc(τ) As seen in Figure 9, recall remains at its highest value (close to 0.99) for lower confidence thresholds, ensuring that most detections are retained. However, as the confidence threshold surpasses τ = 0.8, recall starts to decline sharply. This behavior indicates a trade-off where the model prioritizes precision over recall, filtering out lower-confidence predictions to improve accuracy while reducing the number of retrieved positive instances. The sharp drop in recall near τ = 0.99 suggests that only the highest-confidence predictions are retained, which can be useful for applications requiring high precision such ASL interpretation systems. Figure 9. Recall–confidence curve. 5.3.2. Precision–Confidence Curve The global precision at confidence τ is computed as Precision(τ) = ∑N c=1 TPc(τ) From Figure 10, precision remains above 0.90 across nearly all confidence levels. The model achieves perfect precision at confidence τ = 0.985, indicating that highconfidence detections are nearly always correct. ∑N c=1 TPc(τ) + FNc(τ) (4) ∑N c=1 TPc(τ) + FPc(τ) (5) Figure 10. Precision–confidence curve. 5.3.3. F1–Confidence Curve The F1-score at confidence τ balances precision and recall [35]: F1(τ) = 2 × Precision(τ) × Recall(τ) Figure 11 shows the optimal F1-score occurs at τ = 0.99, suggesting the best trade-off between detection accuracy and reliability. Precision(τ) + Recall(τ) (6) 5.4. Confusion Matrix Analysis The confusion matrix provides a class-wise breakdown of model performance [36]. For a multiclass, the confusion matrix element at (i, j) represents CMij = P(ypred = j | ytrue = i) (7) where ypred and ytrue are the predicted and actual labels, respectively. Refer to Figure 12 for a visualization. Figure 12. Normalized confusion matrix for ASL alphabet recognition. Key findings: • The model demonstrates high classification accuracy with minimal misclassifications. • Class-wise performance remains consistent, ensuring robustness across all ASL alphabet classes. • Confusion is minimal, confirming the effectiveness of the YOLO11n architecture in distinguishing ASL gestures. These findings confirm the model’s strong classification ability, making it a promising solution for real-time ASL translation applications. 5.5. Evaluation of a Real-Time ASL Interpretation System Evaluating a real-time American Sign Language (ASL) interpretation system for users to spell names and locations requires a comprehensive assessment of key performance aspects, including accuracy, latency, robustness, and usability. To conduct this evaluation, we utilized a laptop’s built-in webcam for real-time hand tracking and ASL recognition. In our system, the webcam serves as a non-contact vision-based sensor that captures gesture data in the form of image frames. These frames are processed using MediaPipe to extract 21 hand keypoints and passed to YOLOv11 for ASL gesture classification. This sensor-driven pipeline enables real-time processing under various lighting and background conditions using only standard hardware. 5.5.1. System Specifications The system was tested on a laptop with the following specifications: 5.5.2. Performance Analysis Our analysis of image processing per frame revealed the following time distribution: • Preprocessing: 1.1 ms. • Inference (Model Prediction): 70.9 ms. • Loss Calculation: 0.0 ms. • Postprocessing: 0.5 ms. 5.5.3. Robustness Across Different Conditions To further evaluate the system’s effectiveness, we tested it under various conditions, including different hand shapes, skin tones, and background complexities. The system demonstrated strong generalization capabilities, accurately recognizing these variations with minimal error. This robustness can be attributed to the integration of MediaPipe, which focuses solely on detecting 21 keypoints of the hand. Once the hand shape is determined, this information is passed to YOLOv11 for the final recognition process. Despite the two-step approach—first detecting the hand shape and keypoints and then processing the recognition—our model remains fast and highly efficient in detecting hand gestures and translating them into text. 5.5.4. ASL Translation to Names and Locations Figures 13 and 14 illustrate the system’s ability to interpret ASL gestures into meaningful text. (Figure 13. The model translating ASL into a name. Figure 14. The model translating ASL into a location.) These results highlight the system’s efficiency in real-time ASL interpretation, ensuring minimal latency while maintaining high accuracy across diverse conditions\n\n3. Related Work\nResearch on real-time American Sign Language (ASL) interpretation has advanced significantly with deep learning, particularly through object detection frameworks such as YOLO. Recent studies demonstrate that high accuracy can be achieved for static hand gesture recognition (e.g., fingerspelling alphabets) in real time. However, challenges persist in continuous sign language recognition. Below, we summarize key studies, their methodologies, and performance metrics, followed by a comparison table. Early deep learning approaches applied object detection models for classifying hand gestures. A study by [24] trained a YOLOv5 model on the MU Hand Images ASL dataset (2425 images) and achieved a recall of 97% and a precision of 95%, resulting in an F1-score of approximately 96%. The model also attained a mean Average Precision (mAP) of 98% at IoU 0.5 (mAP@0.5). However, this dataset featured only one signer, limiting the model’s generalizability and reducing its practical applicability in diverse user scenarios. Expanding upon this, [25] utilized a larger custom dataset of 8000 images (covering 40 classes, including letters and numbers) to develop a YOLOv4-based recognizer. Their model achieved an mAP@0.5 of 98.0%, with a recall of approximately 96% and an F1-score of 96%. They reported real-time performance at 28.9 FPS. However, their evaluation was conducted using pre-recorded video rather than live webcam integration, limiting the assessment of its robustness in dynamic, real-world environments. More recent advancements in YOLO variants have further improved recognition accuracy. The study by [26] introduced a hybrid approach that combined YOLOv6 for static signs and an LSTM for dynamic signs. Their YOLOv6 model, trained on a custom-collected dataset, achieved a test accuracy of about 95%, with a precision of 96% and a recall of 92% for ASL alphabet gestures (mAP 96%). This outperformed their sequential-sign model, where an LSTM applied to continuous sign sequences achieved an accuracy of 88–92%, underscoring that static gesture recognition is currently more advanced than continuous sign recognition. The study by [27] explored the application of YOLOv9 in real-time ASL interpretation. Their study utilized a dataset of 1334 images for training, validation, and testing. Despite achieving high accuracy across precision and recall metrics, the limited dataset size presents challenges for building a reliable real-time interpretation system. In a more recent study, ref. [28] further refined sign language detection with YOLOv8, focusing on the Indonesian Sign Language System (SIBI). Their study trained YOLOv8 on video and image datasets, employing extensive preprocessing and data mining techniques to improve model efficiency. The model, optimized for 168 layers with over 11 million parameters, achieved an average inference speed of 4.6 ms per image. Detection results demonstrated exceptionally high success rates, particularly for the letters D, F, N, O, and Q, which achieved up to 96% accuracy. Overall, 22 out of 26 letters displayed “excellent” detection results (above 90%), while four letters (H, M, T, Z) showed “good” detection results (86–89%). The study highlights the potential of YOLOv8 in enhancing real-time sign language recognition. Despite these advancements, several research gaps remain. Many studies focus solely on isolated signs (individual letters or digits) using relatively uniform image datasets, often featuring a single signer in a controlled background. While achieving near-perfect accuracy in such settings is impressive, it does not necessarily translate to real-world applications involving multiple users, varying lighting conditions, and continuous signing. Performance significantly declines when extending to word-level or sentence-level ASL, which involves gesture transitions and motion. For instance, the LSTM-based continuous sign recognition model in [26] attained only 88% accuracy, highlighting the difficulty of fluid ASL recognition. Additionally, the lack of a standardized large-scale benchmark dataset for continuous ASL gestures impedes direct comparisons among studies, as most researchers curate their own datasets. Some models also require on-screen hands or specific camera setups, making it challenging to achieve comparable accuracy with arbitrary video feeds in diverse settings. To address these limitations, we propose a novel YOLOv11-based real-time ASL recognition system integrated with MediaPipe keypoint tracking. This approach enhances precision and robustness, overcoming challenges faced in previous studies. Unlike prior works that relied on small or single-signer datasets, our model is trained on a largescale dataset of 130,000 images, each annotated with 21 keypoints. This ensures improved generalization across diverse signers, hand shapes, and lighting conditions. Moving beyond static gesture recognition, our system facilitates real-time ASL translation, allowing users to interactively spell names, locations, and simple phrases with high accuracy. The proposed model achieves an mAP@0.5 of 98.2% with an inference speed of 1.3 ms per image, making it highly suitable for real-world applications. By integrating deep learning with keypoint tracking, our system advances AI-driven assistive technologies, bridging the gap between research and practical ASL interpretation tools. This work sets the stage for more robust, real-time sign language translation in diverse environments. Table 2 below provides a comparative summary of representative studies, detailing real-time capability, dataset size, and key performance metrics. All listed works focus on the deep-learning-based recognition of ASL alphabet gestures and report performance on static gesture classification Table 2. Comparison of our proposed approach with recent advancements in YOLO-based ASL recognition models, highlighting dataset size, performance metrics, and real-time capability. Ref Year Model Dataset (Images) Precision Recall F1-Score mAP@0.5 Real-Time [24] 2021 YOLOv5 2425 95% 97% 96% 98% No [25] 2022 YOLOv4 8000 96.00% 96.00% 96.00% 98.01% Yes Table 2. Cont. Ref Year Model Dataset (Images) Precision Recall F1-Score mAP@0.5 Real-Time [28] 2025 YOLOv8 Video-based dataset 100% 99% 89% 96% Yes Our work 2025 YOLOv11 130,000 98.50% 98.10% 99.10% 98.20% Yes\n\n6. Conclusions\nThe development of assistive technologies for the Deaf and Hard-of-Hearing community is crucial in fostering inclusivity and eliminating communication barriers. This study presents a real-time ASL interpretation system that effectively integrates deep learning with keypoint tracking, demonstrating high accuracy and efficiency in recognizing ASL alphabet gestures. By leveraging YOLOv11 for gesture detection and MediaPipe for precise hand tracking, the proposed system ensures robust performance across diverse conditions. Experimental results validate the system’s effectiveness, achieving a mean Average Precision (mAP@0.5) of 98.2% with minimal latency, making it highly suitable for realtime applications. The ability to translate ASL gestures into text in real time significantly enhances accessibility in various domains, including education, healthcare, and professional settings. Future work will focus on extending the system’s capabilities beyond individual letter recognition to full ASL sentence interpretation, enabling more natural and fluid communication. This will require advancements in sequence modeling techniques, such as recurrent neural networks (RNNs), transformers, or temporal convolutional networks (TCNs), to effectively capture contextual dependencies between consecutive gestures. In conclusion, this research underscores the transformative potential of AI-driven assistive technologies in empowering the Deaf community. By bridging the communication gap through real-time ASL recognition, this system contributes to a more inclusive society where individuals with hearing impairments can interact seamlessly with the world around them, whether introducing themselves or indicating locations.\n\nReferences\n1. Papatsimouli, M.; Sarigiannidis, P.; Fragulis, G.F. A survey of advancements in real-time sign language translators: Integration with IoT technology. Technologies 2023, 11, 83. [CrossRef] 2. Alsharif, B.; Ilyas, M. Internet of things technologies in healthcare for people with hearing impairments. In IoT and Big Data Technologies for Health Care; Springer: Cham, Switzerland, 2022; pp. 299–308. 3. Cui, R.; Liu, H.; Zhang, C. Recurrent Convolutional Neural Networks for Continuous Sign Language Recognition by Staged Optimization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 21–26 July 2017; pp. 7361–7369. 4. Rautaray, S.S.; Agrawal, A. Vision based hand gesture recognition for human computer interaction: A survey. Artif. Intell. Rev. 2015, 43, 1–54. [CrossRef] .\nAuthor Contributions: Conceptualization, B.A.; methodology, B.A.; software, B.A.; validation, B.A., E.A. and A.I.; formal analysis, B.A.; investigation, E.A. and A.I.; resources, B.A. and E.A.; data curation, B.A.; writing—original draft preparation, B.A.; writing—review and editing, E.A. and A.I.; visualization, B.A.; supervision, I.M. and M.I. All authors have read and agreed to the published version of the manuscript. Funding: This research received no external funding. Institutional Review Board Statement: Not applicable. Informed Consent Statement: Not applicable. Data Availability Statement: Data are available upon request. Conflicts of Interest: Author Bader Alsharif is employed by the Technical and Vocational Training Corporation. The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. 5. ZainEldin, H.; Gamel, S.A.; Talaat, F.M.; Aljohani, M.; Baghdadi, N.A.; Malki, A.; Badawy, M.; Elhosseini, M.A. Silent no more: A comprehensive review of artificial intelligence, deep learning, and machine learning in facilitating Deaf and mute communication. Artif. Intell. Rev. 2024, 57, 188. [CrossRef] 6. Cheok, M.J.; Omar, Z.; Jaward, M.H. A review of hand gesture and sign language recognition techniques. Int. J. Mach. Learn. Cybern. 2019, 10, 131–153. [CrossRef] 7. Zdravkova, K.; Krasniqi, V.; Dalipi, F.; Ferati, M. Cutting-edge communication and learning assistive technologies for disabled children: An artificial intelligence perspective. Front. Artif. Intell. 2022, 5, 970430. [CrossRef] 8. Guo, J.; Liu, Y.; Lin, L.; Li, S.; Cai, J.; Chen, J.; Huang, W.; Lin, Y.; Xu, J. Chromatic plasmonic polarizer-based synapse for all-optical convolutional neural network. Nano Lett. 2023, 23, 9651–9656. [CrossRef] 9. Alsharif, B.; Alanazi, M.; Altaher, A.S.; Altaher, A.; Ilyas, M. Deep Learning Technology to Recognize American Sign Language Alphabet Using Mulit-Focus Image Fusion Technique. In Proceedings of the 2023 IEEE 20th International Conference on Smart Communities: Improving Quality of Life Using AI, Robotics and IoT (HONET), Boca Raton, FL, USA, 4–6 December 2023; pp. 1–6. 10. Ali, M.L.; Zhang, Z. The YOLO framework: A comprehensive review of evolution, applications, and benchmarks in object detection. Computers 2024, 13, 336. [CrossRef] 11. Alaftekin, M.; Pacal, I.; Cicek, K. Real-time sign language recognition based on YOLO algorithm. Neural Comput. Appl. 2024, 36, 7609–7624. [CrossRef] 12. Saxena, S.; Paygude, A.; Jain, P.; Memon, A.; Naik, V. Hand Gesture Recognition using YOLO Models for Hearing and Speech Impaired People. In Proceedings of the 2022 IEEE Students Conference on Engineering and Systems (SCES), Prayagraj, India, 1–3 July 2022; pp. 1–6. [CrossRef] 13. Khanam, R.; Hussain, M. Yolov11: An overview of the key architectural enhancements. arXiv 2024, arXiv:2410.17725. 14. Ye, T.; Huang, S.; Qin, W.; Tu, H.; Zhang, P.; Wang, Y.; Gao, C.; Gong, Y. YOLO-FIX: Improved YOLOv11 with Attention and Multi-Scale Feature Fusion for Detecting Glue Line Defects on Mobile Phone Frames. Electronics 2025, 14, 927. [CrossRef] 15. Hoermann, S.; Henzler, P.; Bach, M.; Dietmayer, K. Object detection on dynamic occupancy grid maps using deep learning and automatic label generation. In Proceedings of the 2018 IEEE Intelligent Vehicles Symposium (IV), Suzhou, China, 26–30 June 2018; pp. 826–833. 16. Li, Y.; Wang, T.; Kang, B.; Tang, S.; Wang, C.; Li, J.; Feng, J. Overcoming classifier imbalance for long-tail object detection with balanced group softmax. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 13–19 June 2020; pp. 10991–11000. 17. Dobransk`y, M. Object Detection for Video Surveillance Using the SSD Approach. Master’s Thesis, Technical University of Košice, Košice, Slovakia, 2019. 18. Rajendran, A.K.; Sethuraman, S.C. A survey on yogic posture recognition. IEEE Access 2023, 11, 11183–11223. [CrossRef] 19. Pinochet, D. Computational Gestural Making: A Framework for Exploring the Creative Potential of Gestures, Materials, and Computational 20. Noroozi, F.; Corneanu, C.A.; Kami´nska, D.; Sapi´nski, T.; Escalera, S.; Anbarjafari, G. Survey on emotional body gesture recognition. IEEE Trans. Affect. Comput. 2018, 12, 505–523. 21. Zhang, H.; Tian, Y.; Zhang, Y.; Li, M.; An, L.; Sun, Z.; Liu, Y. Pymaf-x: Towards well-aligned full-body model regression from monocular images. IEEE Trans. Pattern Anal. Mach. Intell. 2023, 45, 12287–12303. 22. Zhao, L.; Li, X.; Zhuang, Y.; Wang, J. Deeply-learned part-aligned representations for person re-identification. In Proceedings of the IEEE International Conference on Computer Vision, Venice, Italy, 22–29 October 2017; pp. 3219–3228. 23. Lowe, D.G. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vis. 2004, 60, 91–110. [CrossRef] 24. Dima, T.F.; Ahmed, M.E. Using YOLOv5 algorithm to detect and recognize American sign language. In Proceedings of the 2021 International Conference on Information Technology (ICIT), Amman, Jordan, 14–15 July 2021; pp. 603–607. 25. Al-Shaheen, A.; Çevik, M.; Alqaraghulı, A. American sign language recognition using yolov4 method. Int. J. Multidiscip. Stud. Innov. Technol. 2022, 6, 61–65. 26. Buttar, A.M.; Ahmad, U.; Gumaei, A.H.; Assiri, A.; Akbar, M.A.; Alkhamees, B.F. Deep learning in sign language recognition: A hybrid approach for the recognition of static and dynamic signs. Mathematics 2023, 11, 3729. [CrossRef] 27. Imran, A.; Hulikal, M.S.; Gardi, H.A. Real Time American Sign Language Detection Using Yolo-v9. arXiv 2024, arXiv:2407.17950. 28. Kurniawan, R.; Wijaya, Y.A. YOLOv8 Algorithm to Improve the Sign Language Letter Detection System Model. J. Artif. Intell. Eng. Appl. (JAIEA) 2025, 4, 1379–1385. 29. Alsharif, B.; Altaher, A.S.; Altaher, A.; Ilyas, M.; Alalwany, E. Deep learning technology to recognize american sign language alphabet. Sensors 2023, 23, 7970. [CrossRef] 30. Alsharif, B.; Alanazi, M.; Ilyas, M. Machine Learning Technology to Recognize American Sign Language Alphabet. In Proceedings of the 2023 IEEE 20th International Conference on Smart Communities: Improving Quality of Life Using AI, Robotics and IoT (HONET), Boca Raton, FL, USA, 4–6 December 2023; pp. 173–178. 31. Raschka, S. Model evaluation, model selection, and algorithm selection in machine learning. arXiv 2018, arXiv:1811.12808. 32. Brownlee, J. Imbalanced Classification with Python: Better Metrics, Balance Skewed Classes, Cost-Sensitive Learning; Machine Learning Mastery: San Francisco, CA, USA, 2020. 33. Sundaresan Geetha, A.; Alif, M.A.R.; Hussain, M.; Allen, P. Comparative Analysis of YOLOv8 and YOLOv10 in Vehicle Detection: Performance Metrics and Model Efficacy. Vehicles 2024, 6, 1364–1382. [CrossRef] 34. Alalwany, E.; Alsharif, B.; Alotaibi, Y.; Alfahaid, A.; Mahgoub, I.; Ilyas, M. Stacking Ensemble Deep Learning for Real-Time Intrusion Detection in IoMT Environments. Sensors 2025, 25, 624. [CrossRef] 35. Aldahoul, N.; Karim, H.A.; Sabri, A.Q.M.; Tan, M.J.T.; Momo, M.A.; Fermin, J.L. A comparison between various human detectors and CNN-based feature extractors for human activity recognition via aerial captured video sequences. IEEE Access 2022, 10, 63532–63553. [CrossRef] 36. Naing, K.M.; Boonsang, S.; Chuwongin, S.; Kittichai, V.; Tongloy, T.; Prommongkol, S.; Dekumyoy, P.; Watthanakulpanich, D. Automatic recognition of parasitic products in stool examination using object detection approach. PeerJ Comput. Sci. 2022, 8, e1065. [CrossRef] [PubMed] Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.\n--- FOOTNOTES Department of Electrical Engineering and Computer Science, Florida Atlantic University, 777 Glades Road, Department of Computer Science and Engineering, College of Telecommunication and Information, Technical 21 Keypoint Coordinates: Normalized values stored as 26 ASL letters having an equal number of instances (5000 images per class). The uniform Tools; Massachusetts Institute of Technology: Cambridge, MA, USA, 2023.", "persona": "Student", "response": "### 1. Overview\nFor millions of deaf and hard-of-hearing people, using sign language to communicate in a world built around spoken words can be exhausting. Whether it’s ordering food, asking for directions, or taking part in a classroom discussion, barriers show up everywhere. That’s why researchers from Florida Atlantic University’s College of Engineering and Computer Science have created a new system that could change the way we think about accessibility. “This project is a great example of how cutting-edge AI can be applied to serve humanity,” said Imad Mahgoub, a co-author on the publication. A growing need The deaf and hard-of-hearing community is large and diverse. In the U.S. alone, about 11 million people – roughly 3.6% of the population – are deaf or have significant hearing loss. Around 37.5 million adults experience some level of hearing difficulty. That’s a lot of people who could benefit from better communication tools. “The significance of this research lies in its potential to transform communication for the deaf community by providing an AI-driven tool that translates American Sign Language gestures into text, enabling smoother interactions across education, workplaces, health care and social settings,” commented Mohammad Ilyas, co-author of the research study. Helping the deaf community say more This would make communication even more natural and fluent, moving from spelling out words to sharing entire ideas.\n\n### 2. Background\nTo solve these issues, the FAU team combined two powerful tools: YOLOv11 for object detection and MediaPipe for detailed hand tracking. The researchers also built a massive dataset – 130,000 images strong – to train the model. These images include hands in a variety of lighting conditions, backgrounds, and angles. This diversity helps the system learn how to generalize across different people and environments, thus reducing the chance of bias. “This research highlights the transformative power of AI-driven assistive technologies in empowering the deaf community,” said Stella Batalama, Dean of the Department of Electrical Engineering at FAU.\n\n### 3. Methods\n“What makes this system especially notable is that the entire recognition pipeline – from capturing the gesture to classifying it operates seamlessly in real time, regardless of varying lighting conditions or backgrounds,” said Bader Alsharif, lead author on the study. MediaPipe then pinpoints 21 key spots on each hand – fingertips, knuckles, and the wrist – to build a kind of skeleton map. These points help the system understand hand structure and motion. Check us out on EarthSnap, a free app brought to you by Eric Ralls and Earth.com. Recognizing sign language is tricky for machines ASL relies on precise hand shapes and movements to represent letters, words, and phrases.\n\n### 4. Results\nBut current recognition tools often miss the mark – especially when signs look alike. Together, they allow the system to detect and classify ASL alphabet letters with an accuracy of 98.2% (mean Average Precision at 0.5). The entire process works in real time and with very little delay. YOLOv11 uses this skeletal data to match hand gestures accurately to ASL letters. —– —–\n\n### 5. Takeaways\nWhile interpreters and captioning services can help, they’re often limited, costly, or unavailable when most needed. For example, “A” and “T” or “M” and “N” can be hard to tell apart for machines. These tools also struggle in poor lighting, with motion blur, and with differences in hand shape or skin tone, all of which affect how accurate the machine interpretations are. “By developing a robust and accessible ASL interpretation system, our study contributes to the advancement of assistive technologies to break down barriers for the deaf and hard-of-hearing population.” While this new tool already shows strong results for recognizing the ASL alphabet, the team isn’t stopping there. They’re now working to expand the system to understand full ASL sentences. Like what you read? Subscribe to our newsletter for engaging articles, exclusive content, and the latest updates.", "paper_id": "37", "story_id": "37-05"}
{"paper": "Abstract\nWe present a method to edit a target portrait footage by taking a sequence of audio as input to synthesize a photorealistic video. This method is unique because it is highly dynamic. It does not assume a person-speciﬁc rendering network yet capable of translating arbitrary source audio into arbitrary video output. Instead of learning a highly heterogeneous and nonlinear mapping from audio to the video directly, we ﬁrst factorize each target video frame into orthogonal parameter spaces, i.e., expression, geometry, and pose, via monocular 3D face reconstruction. Next, a recurrent network is introduced to translate source audio into expression parameters that are primarily related to the audio content. The audio-translated expression parameters are then used to synthesize a photo-realistic human subject in each video frame, with the movement of the mouth regions precisely mapped to the source audio. The geometry and pose parameters of the target human portrait are re- tained, therefore preserving the context of the original video footage. Finally, we introduce a novel video rendering network and a dynamic programming method to construct a temporally coherent and photo-realistic video. Extensive experiments demonstrate the superiority of our method over existing approaches. Our method is end-to-end learnable and robust to voice variations in the source audio. Some results are shown in Fig. 1. Video results are shown on our project page1.\n\n1. Introduction\nI’m going where the sun keeps shining. Through the pouring rain. Going where the weather suits my clothes. Fred Neil, Everybody’s Talkin’ Video portrait editing is a highly sought-after technique in view of its wide applications, such as ﬁlmmaking, video production, and telepresence. Commercial video editing applications, such as Adobe Premiere and Apple iMovie, are resource-intensive tools. Indeed, editing audio-visual content would require one or more footages to be reshot. Moreover, the quality of the edited video is highly dependent on the prowess of editors. Audio-based approach is an attractive technique for automatic video portrait editing. Several methods [6, 62] are proposed to animate the mouth region of a still image to follow an audio speech. The result is an animated static image rather than a video, hence sacriﬁcing realism. Audiodriven 3D head animation [46] is an audio-based approach but aiming at a different goal, namely to drive stylized 3D computer graphic avatars, rather than to generate a photorealistic video. Suwajanakorn et al. [45] attempted to synthesize photo-realistic videos driven by audio. While impressive performance was achieved, the method assumes the source audio and target video to come from the same identity. The method is only demonstrated on the audio tracks and videos of Barack Obama. Besides, it requires long hours of single-identity data (up to 17 hours [45]) for training using relatively controlled and high-quality shots. In this paper, we investigate a learning-based framework that can perform many-to-many audio-to-video translation, i.e., without assuming a single identity of source audio and the target video. We further assume a scarce number of target video available for training, e.g., at most a 15-minute footage of a person is needed. Such assumptions make our problem non-trivial: 1) Without sufﬁcient data, especially in the absence of source video, it is challenging to learn direct mapping from audio to video. 2) To apply the framework on arbitrary source audios and target videos, our method needs to cope with large audio-video variations between different subjects. 3) Without explicitly specifying scene geometry, materials, lighting, and dynamics, as in the case of a standard graphics rendering engine, it is hard for a learning-based framework to generate photo-realistic yet temporally coherent videos. To overcome the aforementioned challenges, we propose to use the expression parameter space, rather than the full pixels, as the target space for audio-to-video mapping. This facilitates the learning of more effective mapping, since the expression is semantically more relevant to the audio source, compared to other orthogonal spaces, such as geometry and pose. In particular, we manipulate the expression of a target face by generating a new set of parameters through a novel LSTM-based Audio-to-Expression Translation Network. The newly generated expression parameters, combined with geometry and pose parameters of the target human portrait, allow us to reconstruct a 3D face mesh with the same identity and head pose of the target but with new expression (i.e., lip movements) that matches the phonemes of the source audio. We further propose an Audio ID-Removing Network that keeps audio-to-expression translation agnostic to the identity of the source audio. Thus, the translation is robust to variations in the voices of different people in different source audio. Finally, we solve the difﬁcult face generation problem as a face completion problem conditioned on facial landmarks. Speciﬁcally, after reconstructing a 3D face mesh with new expression parameters, we extract the associated 2D landmarks from the mouth region and represent them as heatmaps. These heatmaps are combined with target frames where the mouth region is masked. Taking the landmark heatmaps and the masked target frames as inputs, a video rendering network is then used to complete the mouth region of each frame guided by dynamics of the landmarks. We summarize our contributions as follows: 1) We make the ﬁrst attempt at formulating an end-to-end learnable framework that supports audio-based video portrait editing. We demonstrate coherent and photo-realistic results by focusing speciﬁcally on expression parameter space as the target space, from which source audios can be effectively translated into target videos. 2) We present an Audio ID-Removing Network that encourages an identity-agnostic audio-to-expression translation. This network allows our framework to cope with large variations in voices that are present in arbitrary audio sources. 3) We propose a Neural Video Rendering Network based on the notion of face completion with a masked face as input and mesh landmarks as conditions. This approach facilitates the generation of photo-realistic video for arbitrary people within one single network. Ethical Considerations. Our method could contribute greatly towards advancement in video editing. We envisage relevant industries, such as ﬁlmmaking, video production, and telepresence to beneﬁt immensely from this technique. We do acknowledge the potential of such forward-looking technology being misused or abused for various malevolent purposes, e.g., aspersion, media manipulation, or dissemination of malicious propaganda. Therefore, we strongly advocate and support all safeguarding measures against such exploitative practices. We welcome enactment and enforcement of legislation to obligate all edited videos to be distinctly labeled as such, to mandate informed consent be collected from all subjects involved in the edited video, as well as to impose hefty levy on all law defaulters. Being at the forefront of developing creative and innovative technologies, we strive to develop methodologies to detect edited video as a countermeasure. We also encourage the public to serve as sentinels in reporting any suspicious-looking videos to the authority. Working in concert, we shall be able to promote cutting-edge and innovative technologies without compromising the personal interest of the general public.\n\n3. Methodology\nThe architecture of the proposed method is shown in Fig. 2. First, we register a parametric 3D face model [5] in the target video, for every portrait video frame to extract face geometry, pose, and expression parameters. Then, the Audio-to-Expression Translation Network learns the mapping from the source audio feature to face expression parameters. We design an Audio ID-Removing Network to alleviate the issues on large variations caused by multiple speakers. Lastly, we formulate the talking face generation problem as a face completion problem guided by mouth region landmarks, in which the landmarks are projected from the restructured 3D facial mesh. We propose a Neural Video Rendering network to complete the mouth region of each frame, guided by the dynamics of the landmarks to generate a photo-realistic portrait video. 3.1. 3D Face Modeling We leverage a parametric 3D face model [5] on portrait video frame to recover low dimensional geometry, expression, and pose parameters. To reduce parameter dimension, geometry and expression bases are computed based on high-quality head scans [2] and facial blendshapes [5, 1] Figure 2: Architecture. Our network contains an Audio-to-Expression Translation Network that learns facial expression parameters from speech audio and a Neural Video Rendering Network that generates mouth region guided by projected mouth landmarks. via principal component analysis (PCA). The geometry parameters s ∈R199 and the expression parameters e ∈R29 are the coefﬁcients of geometry and expression principle components in the PCA, respectively. The pose of the head p ∈R6 which contains 3 head rotation coefﬁcients, 2 translation coefﬁcients (x and y directions on the screen surface), and 1 scaling coefﬁcient. All the parameters are computed by solving a non-linear optimization problem, constrained by the statistical linear 3D face model [2]. By optimizing the geometry, expression, and pose parameters of a given monocular face image based on its detected facial landmarks, protrait video frames will be automatically annotated with low dimensional vectors [2]. The recovered expression parameters are used as the learning target in the Audio-to-Expression Translation Network. Then, the recovered geometry and pose parameters, together with the expression parameters inferred by the Audio-to-Expression Translation Network, are employed for reconstructing the 3.2. Audio-to-Expression Translation 3.2.1 Audio ID-Removing Network We empirically ﬁnd that identity information embedded in the speech feature degrades the performance of mapping speech to mouth movement. Inspired by recent advances of the speaker adaptation method in the literature of speech recognition [50, 39], we transfer the speech feature lies in different speaker domains onto a “global speaker” domain by applying a linear transformation, in the form [50]: x ′ = Wix + bi = ¯Wi¯x, where ¯ Wi = (Wi, bi), ¯x = (x; 1), ¯ Wi = I + Pk j=1 λj ¯W j (1) Here, x and x ′ represent the raw and transferred speech feature, respectively, while ¯ Wi = I + Pk j=1 λj ¯W j represents the speaker-speciﬁc adaptation parameter that is factorized into an identity matrix I plus weighted sum of k components ¯W j [39]. In speech recognition, these parameters are iteratively optimized by fMLLR [11, 15] and EM Figure 3: Audio ID-Removing Network. We formulate the speaker adaptation method from speech recognition [50, 39] as a neural network. The network removes identity in speech MFCC spectrum by transferring it to the “global speaker” domain. algorithms. We formulate the above method into a neural network to be integrated with our end-to-end deep learning network. From Eq.(1), the parameters λj need to be learned from the input speech feature, while the matrix components ¯W j is general speech features of different speakers. Thus, we design an LSTM+FC network to infer λj from the input and set the matrix components ¯W j as the optimizing parameter of the Audio ID-Removing Network. The matrix components ¯W j of the Audio ID-Removing Network are updated by the gradient descent-based algorithm. The details of the network is depicted in Fig. 3. The output of the Audio IDRemoving Network is a new MFCC (Mel-frequency cepstral coefﬁcients) spectrum. We apply a pre-trained speaker identity network VGGVox [33, 8] on the new MFCC spectrum and constrain the Audio ID-Removing Network by the following cross-entropy loss function: N X 1 N log p(c|x ′), (2) Lnorm = − c=1 where N is the number of speakers, c is the speaker class label. The p(c|x ′) is the probability of assigning MFCC x ′ to speaker c, which is inferred from the pre-trained VGGVox. Eq. (2) enforces the Audio ID-Removing Network to produce an MFCC spectrum that is not distinguishable by the pre-trained VGGVox. 3.2.2 Audio-to-Expression Translation Network We formulate a simple but effective Audio-to-Expression Translation Network that learns the mapping from the IDremoved MFCC feature to the corresponding facial expression parameters. To infer the expression parameters at time t, the translation network observes a sliding window speech clip of 1 second, which contains 0.8 seconds before time t and 0.2 seconds after time t. We empirically ﬁnd it challenging to train a network to solely regress the expression parameters. The underlying reason could be that the expression parameters are deﬁned and related to the 3DMM model that is hard to model by the network. To facilitate the learning, we introduce a shape constraint. In particular, with the predicted expression parameters from audio and the ground truth geometry/pose parameters of the video portrait, we can obtain a predicted reconstructed 3D facial mesh. Then, we project 3D points of mouth area to the 2D space to obtain the predicted 2D mouth landmarks. Using a similar method, we can obtain a set of ground-truth 2D mouth landmarks from the groundtruth expression parameters. The shape constraint can be introduced between the predicted 2D mouth landmarks and ground-truth 2D mouth landmarks. The whole process of generating mouth landmarks from expression parameters only involves linear operations and thus is differential. The loss function is written as follows: Ltrans = Lexp + Lshape = ||ˆe −e||2 + ||ˆl −l||2, (3) where e and l are the ground truth expression and landmark, respectively, and ˆe and ˆl are the output expression and landmark of the translation network, respectively. The Audio ID-Removing and Audio-to-Expression Translation Networks are trained jointly, whose objective function is weighted sum of Lnorm (Eq. (2)) and Ltrans (Eq. (3)). 3.3. Neural Video Rendering Network 3.3.1 Network Architecture Our ﬁnal step is to generate photo-realistic talking face video that is conditioned on dynamic background portrait video and is guided by the mouth region landmark heatmap sequence. We design a completion-based generation network that completes the mouth region guided by mouth landmarks. First, to obtain the masked face images, a tailored dynamic programming based on retiming algorithm inspired by [45] is introduced to select frame sequence whose head shaking and blink of eyes look compatible with the source speech. Then, the mouth area that contains lip, jaw, and nasolabial folds are manually occluded by a square mask ﬁlled with random noise. To make the conversion from the landmark coordinates to heatmap differentiable, we follow [22, 56] to generate heatmaps with Gaussianlike functions centered at landmark locations. We mod- ify a Unet [42, 40]-based network as our generation network. The employed skip-connection enables our network to transfer ﬁne-scale structure information. In this way, the landmark heatmap at the input can directly guide the mouth region generation at the output, and the structure of the generated mouth obeys the heatmaps [40, 53]. We composite the generated mouth region over the target face frame according to the input mouth region mask. To obtain the mouth region mask, we connect the outermost mouth landmarks as a polygon and ﬁll it with white color, then we erode the binary mask and smooth its boundaries with a Gaussian ﬁlter [26]. With the soft mask, we leverage Poisson blending [36] to achieve seamless blending. To improve the temporal continuity of generated video, we apply a sliding window on the input masked video frames and heatmaps [27, 26]. The input of the Neural Video Rendering Network is a tensor stacked by 7 RGB frames and 7 heatmap gray images [26]. It works well in most cases while a little lip motion jitters and appearance ﬂicker might emerge in the ﬁnal video. Then, a video temporal ﬂicker removal algorithm improved from [3] is applied to eliminate these artifacts. Please refer to appendix for more details of the ﬂicker removal algorithm. 3.3.2 Loss Functions The loss function for training the Neural Video Rendering Network is written as follows: Lrender = Lrecon + Ladv + Lvgg + Ltv + Lgp. (4) The reconstruction loss Lrecon is the pixel-wise L1 loss between the ground truth and generated images. To improve the realism of the generated video, we apply the LSGAN [30] adversarial loss Ladv and add the gradient penalty term Lgp [19] for faster and more stable training. We also apply the perception loss Lvgg [24] to improve the quality of generated images by constraining the image features at different scales. The total variation regularization term Ltv is used to reduce spike artifact that usually occurs when Lvgg is applied [24]. The network is trained endto-end with Ltotal = Lnorm + Ltrans + Lrender (Equations (2),(3), and (4)) with different coefﬁcients. Due to the limited space, we report the details of the loss function, network architecture, and experimental settings in our appendix.\n\n4. Results\nWe show qualitative and quantitative results on a variety of videos to demonstrate the superiority of our method over existing techniques and the effectiveness of proposed components. Datasets. We evaluate our method on a talking face benchmark dataset GRID [9] and a speech video dataset we newly Figure 4: Many-to-many results. (a) One-to-many results: we use speech audio of 1 speaker to drive face of 3 different speakers. (b) Many-to-one results: we use speech audio of 2 different speakers to drive 1 same speaker. For video results we refer to the video on our project page. collected. The former contains 1,000 sentences spoken by 18 males and 16 females. We follow Chen et al. [6] to split training and testing sets on the GRID dataset. Since GRID only provides frontal face videos of minor head movement, we record a video dataset that contains multiple head poses and time-varying head motion. The collected dataset contains speech videos of 4 speakers. Each speaker contributes 15 minutes video for training and 2 minutes video for testing, all videos are captured from 7 viewpoints to provide 7 head poses. Resolution of each video is 1920 × 1080. We also take several videos downloaded from YouTube with the same percentage of the training and testing split of recorded data to evaluate our approach. Evaluation Metrics. To evaluate the accuracy of the expression parameters and the projected landmarks under various head poses and motions, we apply the following distance metric: Eexp = 1 Nexp PNexp i=1 ||be(i) −e(i)||2, Eldmk = 1 Nldmk PNldmk i=1 ||bl(i) −l(i)||2, (5) where Nldmk and Nexp are the number of landmarks and expression parameters respectively and i is the index of landmarks or expression parameters. To quantitatively evaluate the generated quality of portrait videos, we apply common image quality metrics like PSNR [54] and SSIM [54]. To qualitatively evaluate the generated quality of portrait videos, we conduct a user study in Section 4.4 and demonstrate some generated video results on our project page. 4.1. Audio-to-Video Translation Many-to-Many Results. To prove that the audio-toexpression network is capable of handling various speakers and the face completion network is generalized on multiple speakers, we present one-to-many results and many-to-one results in Fig. 4 and on our project page. In the one-to-many results, we use the speech audio of one speaker to drive different speakers. Note that different speakers share a single generator instead of multiple person-speciﬁc generators. In the many-to-one results, we use the speech audio of different speakers to drive the same speaker. This is in contrast to recent methods, where the whole pipeline [45] or part of components [26, 13] is designed for a speciﬁc person, which disables these methods in handling different voice timbres and facial appearances. Large Pose Results. The main purpose of leveraging 3D face model is to handle head pose variations in generating talking face videos. As far as we know, majority of the recent audio-driving methods focus on generating frontal face video no matter whether a 3D head model is applied [45, 26] or not [51, 61, 7]. Our method, however, can generate portrait videos under various large poses driven by audio input. Thanks to the decomposition of audio-driving facial animation problem in our framework, which makes the audio only relate to expression parameters of face rather than shape or pose parameters. Results are shown in Fig. 5 and the video on our project page. Note that in previous methods [45, 61, 7], they directly learn a mapping from audio to landmarks, which involves the shape and pose information that is actually independent to the input audio. Audio Editing & Singing Results. Our method can also be used to edit the speech contents of a pre-recorded video by splitting and recombining the words or sentences taken from any source audio. We show our audio editing results in Fig. 6 and video on our project page. In addition, we also ask a person to record singing and the audio is fed into our network. The driving result can be viewed in Fig. 7 and video on our project page. This demonstrates the general- Figure 5: Large pose results. We demonstrate 4 head poses including up, down, right and left. Video results of all of 7 head poses can be viewed the video on our project page. ization capability of our method and its potential in more complex audio-to-video tasks. Figure 6: Audio editing. We select “knowledge is” and “virtue” from “Knowledge is one thing, virtue is another” in the source audio, then recombine them as “Knowledge is virtue” as input. Figure 7: Singing. We evaluate our network on the singing audio clips. Video result can be viewed in the video on our project page. 4.2. Comparison with State-of-the-Art We compare our method with the recent state-of-the-art portrait video generation methods, e.g., Audio2Obama [45], Face2Face [48], DVP (Deep Video Portrait) [27] and Textbased Editing (TBE) [13]. The comparative results are demonstrated in Fig. 8 and video on our project page. First, the Audio2Obama [45] combines a weighted median texture for synthesizing lower face texture and a teeth proxy for capturing teeth sharp details. Our GAN-based rendering network generates better texture details compare to the weighted median texture synthesis [45], e.g., nasolabial folds (Fig. 8 (a)). Then, we compare our method to Face2Face [48] that supports talking face generation driving by source video in Fig. 8 (b). Face2Face [48] directly transfers facial expression of source video in the parameter space while our method infers facial expression from source audio. The similar lip movement of Face2Face and our method in Fig. 8 (b) suggests the effectiveness of our Audio-to-Expression Translation Network in learning accurate lip movement from speech audio. Moreover, our GAN-based rendering network generates better texture details, such as mouth corners and nasolabial folds. We also compare to another video-driving method DVP [27] that supports talking face generation (Fig. 8 (c)). In DVP, a rendering-to-video translation network is designed to synthesize the whole frame other than the face region. It avoids the blending of face region and background that might be easily detectable. The DVP might fail in a complex and dynamic background as shown in Fig. 8 (c). In contrast, our method uses the original background and achieves seamless blending that is hard to distinguish. Finally, we compare our method with the contemporary text-based talking face editing method TBE [13] in Fig. 8 (d). In TBE, the mouth region is searched by phoneme and a semi-parametric inpainting network is proposed to inpaint the seam between the retrieved mouth and the original face background. This method requires training of a person-speciﬁc network per input video while our method can generalize on multiple speakers and head poses. Besides, our generation network produces competitive mouth details as shown in Fig. 8 (d). Figure 8: Comparison to state-of-the-art methods. Comparison between our method with Audio2Obama [45], Face2Face [48], DVP [27], and TBE [13]. 4.3. Ablation Study Evaluation of Parameter Regression. To prove the superiority of incorporating the 3D face model, we compare our network with the one that replaces Audio-to-Expression Translation Network with an Audio-to-Landmark Trans- lation Network as performed in [45]. The Audio-toLandmark Translation Network modiﬁes the last fully connected layer of the Audio-to-Expression Translation Network so that its output dimension is the coordinate number of mouth region landmarks. The visualized comparison can be viewed in the video on our project page and Fig. 9 (a). We also compare the quantitative metric on GRID and collected dataset as shown in Tab. 1. In the collected dataset that contains more head motion and poses, our method achieves better lip synchronization results as the mouth generated by the one that applies Audio-to-Landmark Translation Network does not even open. Table 1: 2D vs 3D quantitative comparison. Eexp, Eldmk, PSNR, and SSIM comparison of 2D and 3D parameter regression. Parameter Regression Eexp Eldmk PSNR SSIM GRID - 3.99 28.06 0.89 0.65 2.24 31.19 0.95 Collected - 3.13 26.76 0.93 0.595 1.82 29.16 0.95 Figure 9: (a) 2D vs 3D qualitative comparison. 3D parameter regression outperforms 2D parameter regression for head motion and poses. (b) ID-removing qualitative comparison. Improvement can be observed on a failure case caused by not applying Audio ID-Removing Network. Table 2: ID-removing quantitative comparison. “BL” is the 3D parameter regression baseline; “IR” is “id-removing”. Evaluation of Completion-Based Generation. We evaluate the effects of the proposed completion-based generation that beneﬁts from jointly training on data of different people. As shown in Tab. 3, jointly training completion-based generators outperform separately training person-speciﬁc generators with much fewer network parameters when the number of speakers increases, regardless of the time length of the training data. Table 3: Training data size and training style. PSNR/SSIM of different amount of training data and training style. The “Joint” means jointly training one generator for all speakers and “Split” means separately training multiple person-speciﬁc generators. One generation network contains 75 million parameters and N(N = 4 in the table) is the speaker number. Time 2 mins 5 mins 10 mins 15 mins PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM Split (N× 75M) 29.621/0.868 29.338/0.875 29.487/0.849 29.650/0.876 Joint (75M) 30.421/0.886 30.664/0.888 30.787/0.892 31.072/0.897 4.4. User Study To quantitatively evaluate the visual quality of generated portrait videos, following [13], we conduct a webbased user study involving 100 participants on the collected dataset. The study includes 3 generated video clips for each of the 7 cameras and for each of the 4 speakers, hence a total of 84 video clips. Similarly, we also collect 84 ground truth video clips and mix them up with the generated video clips to perform the user study. We separately calculate the study results of the generated and ground truth video clips. In the user study, all the 84 × 2 = 168 video clips are randomly shown to the participants and they are asked to evaluate its realism by evaluating if the clips are real on a likert scale of 1-5 (5-absolutely real, 4-real, 3-hard to judge, 2-fake, 1-absolutely fake) [13]. As shown in Tab. 4, the generated and the ground truth video clips are rated as “real”(score 4 and 5) in 55.0% and 70.1% cases, respectively. Since humans are highly tuned to the slight audiovideo misalignment and generation ﬂaws, the user study results demonstrate that our method can generate deceptive audio-video content for large poses in most cases. Table 4: User study. User study results on generated and ground truth video clips for videos of 7 poses. Generated Videos Ground Truth Videos score 1 2 3 4 5 “real” 1 2 3 4 5 “real” front 5.2 8.5 20.6 42.6 23.2 65.8% 0.6 12.1 10.1 29.7 47.5 77.2% up-30◦ 4.6 25.0 14.2 36.9 19.3 56.3% 0.8 13.5 13.2 29.1 43.4 72.5% down-30◦ 4.8 22.0 15.2 39.7 18.3 58.0% 0.9 13.6 14.1 30.2 41.3 71.5% right-30◦ 3.9 22.9 15.8 42.1 15.3 57.4% 1.3 15.6 14.4 29.2 39.6 68.8% right-60◦ 7.3 33.8 11.4 36.9 10.6 47.5% 0.8 17.8 15.1 29.1 37.2 66.3% left-30◦ 3.2 20.9 21.9 40.1 13.9 54.0% 1.1 12.8 16.1 31.5 38.6 70.1% left-60◦ 7.9 33.7 12.1 35.1 11.3 46.3% 0.7 17.5 14.1 27.2 40.5 67.7% all poses 5.3 23.8 15.9 39.0 16.0 55.0% 0.9 14.7 13.9 29.4 41.2 70.6%\n\n2. Related Work\nAudio-based Facial Animation. Driving a facial animation of a target 3D head model by input source audio learns to associate phonemes or speech features of source audio with visemes. Taylor et al. [47] propose to directly map phonemes of source audio to face rig. Afterward, many speech-driven methods have been shown superior to phoneme-driven methods under different 3D models, e.g. face rig [61, 12], face mesh [25], and expression blendshapes [37]. Compared to driving a 3D head model, driving a photorealistic portrait video is much harder since speaker-speciﬁc appearance and head pose are crucial for the quality of the generated portrait video. Taylor et al. [46] present a sliding window neural network that maps speech feature window to visual feature window encoded by active appearance model (AAM) [10] parameters. To improve the visual quality, in several methods [23, 52, 60], a still face image is taken as a reference for video generation. However, the result is an animation of a still image rather than a natural video. Recently, Suwajanakornet et al. [45] obtain the stateof-the-art result in synthesizing the Obama video portrait. However, it assumes the source and target to have the same identity and requires long hour of training data (up to 17 hours). Thus, it is not applicable in audio-based video editing that need to cope with different sources of voice and target actors. In addition, the target video data is relatively scarce. Fried et al. [13] proposed a method to edit a talking-head video based on its transcript to produce a realistic video. While it produce compelling results, a personspeciﬁc face rendering network need to be trained for each target person. Besides, it takes a long time for viseme search (up to 2 hours for 1-hour recording) and relies on phoneme, thus it cannot be scaled to different languages. Video-based Facial Reenactment. It is inherently difﬁcult to synthesize mouth movements based solely on speech audio. Therefore, many methods turn to learning mouth movements from videos comprising the same/intended speech content. [17, 55, 48, 27, 32]. From source portrait video, facial landmarks [17, 40] or expression parameters [55] are estimated to drive the target face image. In all of these methods [17, 40, 55], the generated portrait videos are framewise realistic but they suffer from poor temporal continuity. ReenactGAN [58] is the ﬁrst end-to-end learnable videobased facial reenactment method. It introduces a notion of “boundary latent space” to perform many-to-one face reenactment. However, ReenactGAN needs person-speciﬁc transformers and decoders, which makes the model size increase linearly with persion identities raising. Many model-based methods [48, 27, 32] leverage a 3D head model to disentangle facial geometry, expression, and pose. Face2Face [48] transfers expressions in parameter space from the source to the target actor. To synthesize a realistic target mouth region, the best mouth image of the target actor is retrieved and warped [48]. Kim et al. [27] present a method that transfers expression, pose parameters, and eye movement from the source to the target actor. These methods are person-speciﬁc [27] therefore rigid in practice and suffer audio-visual misalignment [48, 27, 32] therefore creating artifacts leading to unrealistic results. Deep Generative Models. Inspired by the successful application of GAN [18] in image generation [41, 31, 63, 20, 57], many methods [6, 60, 62, 38, 27] leverage GAN to generate photo-realistic talking face images conditioned on coarse rendering image [27], fused audio, and image features [6, 62, 60]. Generative inpainting networks [34, 21, 29, 59] are capable of modifying image content by imposing guided object edges or semantic maps [59, 44]. We convert talking face generation into an inpainting problem of mouth region, since mouth movement is primarily induced by input speeches. Monocular 3D Face Reconstruction. Reconstructing 3D face shape and texture from a single face image has extensive applications in face image manipulation and animation [14, 16, 48, 43, 45]. In general, monocular 3D face reconstruction produces facial shape, expression, texture, and pose parameters by solving a non-linear optimization problem constrained by a statistical linear model of facial shape and texture, such as Basel face model [35], FaceWarehouse model [5], and Flame [28]. Recently, different 3D head models have been increasingly applied in talking portrait video synthesis [48, 45, 27, 32, 13, 26].\n\n5. Conclusion\nIn this work, we present the ﬁrst end-to-end learnable audio-based video editing method. At the core of our approach is the learning from audio to expression space bypassing the highly nonlinearity of directly mapping audio source to target video. Audio ID-Removing Network and Neural Video Rendering Network are introduced to enable generation of photo-realistic videos given arbitrary targets and audio sources. Extensive experiments demonstrate the robustness of our method and the effectiveness of each pivotal component. We believe our approach is a step forward towards solving the important problem of audio-based video editing and we hope it will inspire more researches in this direction.\n\nReferences\n[1] Oleg Alexander, M. Rogers, W. Lambeth, Jen-Yuan Chiang, Wan-Chun Ma, Chuan-Chang Wang, and Paul E. Debevec. The digital emily project: Achieving a photorealistic digital actor. CG& A, 30:20–31, 2010. 3 [2] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In SIGGRAPH, 1999. 3, 4 [3] Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, Deqing Sun, Sylvain Paris, and Hanspeter Pﬁster. Blind video temporal consistency. ACM TOG, 34:196, 2015. 5 [4] Nicolas Bonneel, James Tompkin, Kalyan Sunkavalli, Deqing Sun, Sylvain Paris, and Hanspeter Pﬁster. Blind video temporal consistency. TOG, 34:196, 2015. 11 [5] Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. Facewarehouse: A 3d facial expression database for visual computing. TVCG, 20:413–425, 2014. 3 [6] Lele Chen, Zhiheng Li, Ross K. Maddox, Zhiyao Duan, and Chenliang Xu. Lip movements generation at a glance. In ECCV, 2018. 2, 3, 6, 11, 12 [7] Lele Chen, Ross K. Maddox, Zhiyao Duan, and Chenliang Xu. Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In CVPR, 2019. 6 [8] J. S. Chung, A. Nagrani, and A. Zisserman. Voxceleb2: Deep speaker recognition. In Interspeech, 2018. 4 [9] Martin Cooke, Stuart Cunningham, and Xu Shao. An audiovisual corpus for speech perception and automatic speech recognition. JASA, 2006. 5 [10] Timothy F. Cootes, Gareth J. Edwards, and Christopher J. Taylor. Active appearance models. In ECCV, 1998. 3 [11] Vassilios V. Digalakis, Dimitry Rtischev, and L. G. Neumeyer. Speaker adaptation using constrained estimation of gaussian mixtures. IEEE Transactions on Speech and Audio Processing, 3:357–366, 1995. 4 [12] Peter Edwards, Chris Landreth, Eugene Fiume, and Karan Singh. Jali: an animator-centric viseme model for expressive lip synchronization. ACM TOG, 35:127, 2016. 3 [13] Ohad Fried, Maneesh Agrawala, Ayush Tewari, Michael Zollh¨ofer, Adam Finkelstein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, and Christian Theobalt. Textbased editing of talking-head video. ACM TOG, 38:68, 2019. 3, 6, 7, 8 [14] Graham Fyffe, Andrew Jones, Oleg Alexander, Ryosuke Ichikari, and Paul E. Debevec. Driving high-resolution facial scans with video performance capture. ACM TOG, 34:8,\n2014. 3\n[15] Mark JF Gales. Maximum likelihood linear transformations for hmm-based speech recognition. Computer Speech & Language, 12(2):75–98, 1998. 4 [16] Pablo Garrido, Levi Valgaerts, H. Sarmadi, Ingmar Steiner, Kiran Varanasi, Patrick P´erez, and Christian Theobalt. Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track. CGF, 34:193–204, 2015. 3 [17] Jiahao Geng, Tianjia Shao, Youyi Zheng, Yanlin Weng, and Kun Zhou. Warp-guided gans for single-photo facial animation. ACM TOG, 37:231, 2018. 3 [18] Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron C. Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS,\n2014. 3\n[19] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In NeurIPS, 2017. 5 [20] Rui Huang, Shu Zhang, Tianyu Li, and Ran He. Beyond face rotation: Global and local perception gan for photorealistic and identity preserving frontal view synthesis. In ICCV,\n2017. 3\n[21] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion. ACM TOG, 36:107, 2017. 3 [22] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Conditional image generation for learning the structure of visual objects. In NeurIPS, 2018. 5 [23] Amir Jamaludin, Joon Son Chung, and Andrew Zisserman. You said that?: Synthesising talking faces from audio. IJCV, pages 1–13, 2019. 3, 11, 12 [24] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. 5 [25] Tero Karras, Timo Aila, Samuli Laine, Antti Herva, and Jaakko Lehtinen. Audio-driven facial animation by joint end-to-end learning of pose and emotion. ACM TOG, 36:94,\n2017. 3\n[26] Hyeongwoo Kim, Mohamed Elgharib, Michael Zollh¨ofer, Hans-Peter Seidel, Thabo Beeler, Christian Richardt, and Christian Theobalt. Neural style-preserving visual dubbing. ACM TOG, 38:178, 2019. 3, 5, 6 [27] Hyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies, Matthias Nießner, Patrick P´erez, Christian Richardt, Michael Zollh¨ofer, and Christian Theobalt. Deep video portraits. ACM TOG, 37:163, 2018. 3, 5, 7 [28] Tianye Li, Timo Bolkart, Michael J. Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4d scans. ACM TOG, 36:194, 2017. 3 [29] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In ECCV, 2018. 3 [30] Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, 2017. 5 [31] Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In ICCV, 2016. 3 [32] Koki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral Agarwal, Jens Fursund, and Hao Li. pagan: real-time avatars using dynamic textures. ACM TOG, 37:258, 2018. 3 [33] A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb: a large-scale speaker identiﬁcation dataset. In Interspeech,\n2017. 4\n[34] Deepak Pathak, Philipp Kr¨ahenb¨uhl, Jeff Donahue, Trevor Darrell, and Alexei A. Efros. Context encoders: Feature learning by inpainting. CVPR, pages 2536–2544, 2016. 3 [35] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face model for pose and illumination invariant face recognition. AVSS, pages 296– 301, 2009. 3 [36] Patrick P´erez, Michel Gangnet, and Andrew Blake. Poisson image editing. ACM TOG, 22:313–318, 2003. 5 [37] Hai X Pham, Samuel Cheung, and Vladimir Pavlovic. Speech-driven 3d facial animation with implicit emotional awareness: A deep learning approach. In CVPRW, 2017. 3 [38] Hai Xuan Pham, Yuting Wang, and Vladimir Pavlovic. Generative adversarial talking head: Bringing portraits to life with a weakly supervised neural network. CoRR, abs/1803.07716, 2018. 3 [39] Daniel Povey and Kaisheng Yao. A basis representation of constrained mllr transforms for robust adaptation. Computer Speech & Language, 26:35–51, 2012. 4 [40] Shengju Qian, Kwan-Yee Lin, Wayne Wu, Yangxiaokang Liu, Quan Wang, Fumin Shen, Chen Qian, and Ran He. Make a face: Towards arbitrary high ﬁdelity face manipulation. In ICCV, 2019. 3, 5 [41] Alec Radford, Luke Metz, and Soumith Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks. CoRR, abs/1511.06434, 2015. 3 [42] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. 5 [43] Joseph Roth, Yiying Tong, and Xiaoming Liu. Adaptive 3d face reconstruction from unconstrained photo collections. In CVPR, 2016. 3 [44] Linsen Song, Jie Cao, Linxiao Song, Yibo Hu, and Ran He. Geometry-aware face completion and editing. In AAAI,\n2018. 3\n[45] Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. Synthesizing obama: learning lip sync from audio. ACM TOG, 36:95, 2017. 2, 3, 5, 6, 7, 8, 13 [46] Sarah Taylor, Akihiro Kato, Iain A. Matthews, and Ben P. Milner. Audio-to-visual speech conversion using deep neural networks. In Interspeech, 2016. 2, 3 [47] Sarah L. Taylor, Taehwan Kim, Yisong Yue, Moshe Mahler, James Krahe, Anastasio Garcia Rodriguez, Jessica K. Hodgins, and Iain A. Matthews. A deep learning approach for generalized speech animation. ACM TOG, 36:93, 2017. 3 [48] Justus Thies, Michael Zollh¨ofer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. Face2face: Real-time face capture and reenactment of rgb videos. In CVPR, 2016. 3, 7 [49] Laurens van der Maaten and Geoffrey E. Hinton. Visualizing data using t-sne. JMLR, 9:2579–2605, 2008. 8, 12 [50] Karthik Visweswariah, Vaibhava Goel, and Ramesh Gopinath. Structuring linear transforms for adaptation using training time information. In ICASSP, 2002. 4 [51] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In NeurIPS, 2016. 6, 11, 12 [52] Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. End-to-end speech-driven facial animation with temporal gans. In BMVC, 2018. 3 [53] Miao Wang, Guo-Ye Yang, Ruilong Li, Run-Ze Liang, SongHai Zhang, Peter. M. Hall, and Shi-Min Hu. Example-guided style consistent image synthesis from semantic labeling. In CVPR, 2019. 5 [54] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. Image quality assessment: from error visibility to structural similarity. TIP, 13(4):600–612, 2004. 6 [55] Olivia Wiles, A. Sophia Koepke, and Andrew Zisserman. X2face: A network for controlling face generation using images, audio, and pose codes. In ECCV, 2018. 3 [56] Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, and Chen Change Loy. Disentangling content and style via unsupervised geometry distillation. In ICLRW, 2019. 5 [57] Wayne Wu, Kaidi Cao, Cheng Li, Chen Qian, and Chen Change Loy. Transgaga: Geometry-aware unsupervised image-to-image translation. In CVPR, 2019. 3 [58] Wayne Wu, Yunxuan Zhang, Cheng Li, Chen Qian, and Chen Change Loy. Reenactgan: Learning to reenact faces via boundary transfer. In ECCV, 2018. 3 [59] Jiahui Yu, Zhe L. Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S. Huang. Free-form image inpainting with gated convolution. CoRR, abs/1806.03589, 2018. 3 [60] Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang. Talking face generation by adversarially disentangled audio-visual representation. In AAAI, 2018. 3 [61] Yang Zhou, Zhan Xu, Chris Landreth, Evangelos Kalogerakis, Subhransu Maji, and Karan Singh. Visemenet: Audiodriven animator-centric speech animation. ACM TOG, 37:161, 2018. 3, 6 [62] Hao Zhu, Aihua Zheng, Huaibo Huang, and Ran He. Highresolution talking face generation via mutual information approximation. CoRR, abs/1812.06589, 2018. 2, 3, 11 [63] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In ICCV, 2017. 3\n\nAppendix\nA. Details of Audio-to-Expression Translation\nNetwork\nThe network architecture of our Audio-to-Expression\nTranslation Network can be viewed in Figure 10. In the\ntraining phase, we use paired audio and video frames from\nthe training footage as network input. Ground truth facial\nshape, expression and pose parameters are calculated from\nvideo frame by monocular reconstruction. From the input\naudio, our Audio-to-Expression Translation Network infers\npredicted expression parameters that are supervised by the\nground truth expression parameters. The loss function is\nLexp = ||ˆe −e||2 in Eq. 6. We reconstruct facial 3D mesh\nby predicted expression parameters and ground truth shape\nparameters, then we use the ground truth pose parameters\nto project 2D mouth landmarks that are supervised by the\nground truth 2D mouth landmarks. The loss function is\nLshape = ||ˆl −l||2 in Eq. 6. The ground truth 2D mouth\nlandmarks are projected in a similar way where ground truth\nexpression parameters are used. In the testing phase, the\npredicted expression parameters from the source audio together with ground truth shape and pose parameters from\ntarget video are used to estimate 2D mouth landmarks. The\nembedded mouth identity and head pose of estimated 2D\nmouth landmarks are the same as those of the target video\nwhile the mouth movement in accord with the source audio.\nLtrans = Lexp + Lshape = ||ˆe −e||2 + ||ˆl −l||2\n(6)\nB. Temporal Flicker Removal Algorithm\nIn our approach, talking face video is generated frame by\nframe and temporal information in video frames is only concerned in landmark estimation. During testing, we ﬁnd the\ngenerated talking face videos demonstrate acceptable frame\ncontinuity even in the circumstance that the video temporal\nﬂicker removal algorithm is not applied. It is due to that audio series clips used to generate time-adjacent frames contain vast overlap in time. The remaining temporal ﬂicker in\nthe video can be attributed to two reasons: 1) The inferred\nmouth and jawline landmarks contain slight jitter. 2) Appearance ﬂicker, especially color ﬂicker exists in the video\nframes generated by the inpainting network. Based on the\nabove analysis, our ﬂicker removal algorithm contains two\nparts: mouth landmark motion smoothing and face appearance deﬂicker. Algorithm 1 demonstrates the mouth landmark motion smooth algorithm.\nAlgorithm 1 Mouth Landmark Smoothing Algorithm\nRequire: lt−1: mouth and jawline landmarks at time t −1;\nlt: mouth and jawline landmarks at time t;\ndth:\nmouth movement distance threshold, s:\nmouth\nmovement smooth strength;\nEnsure: l\n′\nt: smoothed mouth and jawline landmarks at time\nt;\nget mouth center position ct at time t from lt\nget mouth center position ct−1 at time t −1 from lt−1\nif ||ct −ct−1||2 > dth then\nl\n′\nt = lt\nelse\nα = exp(−s||ct −ct−1||2)\nl\n′\nt = αlt−1 + (1 −α)lt\nend if\nreturn l\n′\nt\nThe appearance deﬂicker algorithm is modiﬁed from [4].\nWe take mouth movement into consideration. If the mouth\ndoes not move, then the color ﬂicker is more obvious, and\nthen we increase the deﬂicker strength. We denote the generated frame and processed frame at time t as Pt and Ot,\nrespectively. The mouth center moving distance between\ntime t −1 and t is denoted as dt. The processed frame at t\nis written as:\nF(Pt) = 4π2f 2F(Pt) + λtF(warp(Ot−1))\n4π2f 2 + λt\n(7)\nwhere λt = exp(−dt). Here, F is the Fourier transform\nand f means frequency. Function warp(Ot−1) uses optical\nﬂow from Pt−1 to Pt to warp input frame Ot−1. Compared\nwith [4], the weight of previous frame λt is measured by\nthe strength of mouth motion instead of global frame consistency.\nC. Other Experiments\nC.1. Quantitative Comparison on GRID dataset\nOur method mainly focuses on talking face video editing, which is different from the recent methods that generate full face from input audio and reference still face image [51, 23, 6, 62]. Here we quantitatively compare our\nmethod with these methods [51, 23, 6, 62] on image generation metrics. For a fair comparison, in our method, we do\nnot apply any post-process and we also modify the input of\nthe inpainting network to generate the full face other than\nthe mouth region. Speciﬁcally, the original network input\ntensor is stacked by 7 RGB frames and 7 heatmap gray images (from time t −6 to time t), we remove the RGB frame\nand heatmap gray image at time t and require the network\nto generate the complete frame image at time t. Table 5\ndemonstrates that our approach outperforms these methods\nin PSNR and achieves comparable performance in SSIM.\nFigure 10: Architecture of Audio-to-Expression Network. The Audio ID-Removing Network eliminates identity information in speech\naudio. The Audio-to-Expression Translation Network estimate expression parameters from input audio. We constrain the predicted expression parameters and the projected 2D mouth landmark from the reconstructed facial mesh.\nTable 5: SSIM, PSNR, IS and FID score comparison of our\nmethod and recent methods on GRID dataset. for fair comparison, we generate the full face and do not apply any post process.\nMethod\nPSNR\nSSIM\nIS\nFID\nVondrick et al. [51]\n28.45\n0.60\n-\n-\nJamaludin et al. [23]\n29.36\n0.74\n-\n-\nChen et al. [6]\n29.89\n0.73\n-\n-\nOurs\n30.01\n0.94\n23.53\n9.01\nC.2. Ablation Study on Temporal Flicker Removal\nAlgorithm\nThe temporal ﬂicker removal algorithm tries to smooth\nthe output landmark coordinates and eliminate the appearance ﬂicker.\nThe quantitative improvement is slight as\nshown in Table 6 but the temporal continuity improvement\nis obvious as shown in the video on our project page, especially when the mouth does not open. We demonstrate the\nquantitative results of our 3D parameter regression baseline, Audio ID-Removing Network and Temporal Flicker\nRemoval Algorithm in Table 6.\nC.3. Audio ID-removing Effects in tSNE map\nThe tSNE [49] maps in Figure 11 demonstrate the 2D\nvisualized the distribution of the input MFCC spectrum and\nthe identity removed MFCC spectrum produced by our Audio ID-Removing Network. We can see that the speaker\nidentity can not be distinguished after removing identity in\nthe MFCC spectrum.\nD. Runtime Performance\nTable 6: ID-removing & Deﬂicker quantitative comparison.\n“BL” is the 3D parameter regression baseline; “IR” is “idremoving”; “DF” is “deﬂicker”. The metrics validate the effectiveness of the proposed components except for the deﬂicker algorithm. The deﬂicker algorithm mainly focus on removing temporal\ndiscontinuity that can be viewed in the video on our project page.\nMethod\nEexp\nEldmk\nPSNR\nSSIM\nBL\n0.84\n3.09\n30.88\n0.94\n+IR\n0.65\n2.24\n32.23\n0.97\n+DF\n0.84\n3.07\n27.71\n0.92\n+IR+DF\n0.65\n2.24\n31.19\n0.95\nGRID\nBL\n0.68\n1.92\n26.86\n0.95\n+IR\n0.59\n1.83\n31.21\n0.96\n+DF\n0.68\n1.92\n27.46\n0.93\n+IR+DF\n0.59\n1.82\n29.16\n0.95\nCollected\nFigure 11: tSNE before and after id-removing. 2D visualized\ndistributions of input MFCC and normalized MFCC. Different\ncolor represents different speaker. Our audio normalization erases\nthe identity information embedded in the MFCC spectrum.\ni7-8700. The audio to expression network takes 17 ms per\nframe and the inpainting network takes 77 ms per frame.\nThe post processes including deﬂicker and teeth proxy take\n1.3s and 300 ms per frame respectively. The deﬂicker al-\ngorithm involves the calculation of optical ﬂow that dominates the inference time. Thus, it takes about 1.7s/100ms to\ngenerate one video frame with/without the post-process on\naverage.\n\n\n---\nFOOTNOTES\n1NLPR, CASIA\n2SenseTime Research\n3Nanyang Technological University\n3D facial mesh.\n2D\n3D\n2D\n3D", "persona": "Student", "response": "### 1. Overview\nVideos by VICE In this url, at least, the researchers seem to be aware of the risks highly customizable and realistic deepfakes pose to society. On Thursday, the Bulletin of the Atomic Scientists, stewards of the Doomsday Clock, included deepfakes as a reason why we’re closer to the end of the world than ever, saying that the emergence of algorithmically-generated video “threatens to further undermine the ability of citizens and decision makers to separate truth from fiction.” But sussing truth from fiction might be relatively low on the list of today’s AI-related concerns: for example, SenseTime, one of the companies that developed this research, was recelty implicated in developing technology that helped the Chinese government profile a Muslim minority group. Researchers from Nanyang Technological University in Singapore, the National Laboratory of Pattern Recognition in China, and artificial intelligence software company SenseTime developed the method for creating deepfakes from audio sources. Basically, the AI takes an audio clip of someone speaking, and a video of another person (or the same person), and generates realistic footage of the person saying the words from the source audio. The person in the video becomes a puppet for the original voice.\n\n### 2. Background\nThey then reconstruct a 3D face mesh to match lip movements that match source audio phonemes, or individual sounds—similar to how recent text-to-video methods work. The researchers say this method creates “very deceptive” audio-visual results. Compared to past deepfake methods like Face2Face from 2016 and “Synthesizing Obama” from 2017, the results are more crisp with fewer artifacts visible to the naked eye. In an online poll of 100 participants conducted by the researchers, 55 percent of generated videos were rated as “real.” Because this method can use the real voice of the person you’re trying to deepfake, and splice their words up into whatever you want, it’s another leap forward in deepfake realism.\n\n### 3. Methods\nTo do this, the researchers first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face, according to the url, “Everybody’s Talkin’: Let Me Talk as You Want,” published this month to the arxiv preprint server. From there, they draw 2D landmarks of the face, focusing especially on mouth movements. That way, instead of requiring the algorithm to train on the entire scene, it’s only training on the facial features and leaves the background sharp. “We do acknowledge the potential of such forward-looking technology being misused or abused for various malevolent purposes,” including media manipulation and propaganda, the researchers write in their url. “Therefore, we strongly advocate and support all safeguarding measures against such exploitative practices… Working in concert, we shall be able to promote cutting-edge and innovative technologies without compromising the personal interest of the general public.”\n\n### 4. Results\nA woman looks at the camera and says, “Knowledge is one thing, virtue is another.” Then, she says, “Knowledge is virtue.” The same person, with the same voice, says two conflicting statements—but she only said the first in real life. The second statement is the work of an AI system that took audio of her speech and turned it into a video. Play video\n\n### 5. Takeaways\nThe researchers say this is the first end-to-end learnable audio-based video editing method. If you’re going to make your deepfake speak, however, an unconvincing voice can make or break its believability—for example, the deepfakes of Mark Zuckerberg last year, with a voice that’s comically unrealistic. Faked audio has been a focus of AI engineers and deepfake developers for years, and algorithmically-generated voices alone can sound incredibly real. A generated voice mimicking Jordan Peterson was so realistic that Peterson himself threatened to sue its creator.", "paper_id": "05", "story_id": "05-01"}
{"paper": "Abstract\nImage animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the speciﬁc object to animate. Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local afﬁne transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories. Our source code is publicly available1. 1\n\nIntroduction\nGenerating videos by animating objects in still images has countless applications across areas of interest including movie production, photography, and e-commerce. More precisely, image animation refers to the task of automatically synthesizing videos by combining the appearance extracted from a source image with motion patterns derived from a driving video. For instance, a face image of a certain person can be animated following the facial expressions of another individual (see Fig. 1). In the literature, most methods tackle this problem by assuming strong priors on the object representation (e.g. 3D model) [4] and resorting to computer graphics techniques [6, 34]. These approaches can be referred to as object-speciﬁc methods, as they assume knowledge about the model of the speciﬁc object to animate. Recently, deep generative models have emerged as effective techniques for image animation and video retargeting [2, 42, 3, 43, 28, 29, 38, 41, 32, 22]. In particular, Generative Adversarial Networks (GANs) [14] and Variational Auto-Encoders (VAEs) [21] have been used to transfer facial expressions [38] or motion patterns [3] between human subjects in videos. Nevertheless, these approaches usually rely on pre-trained models in order to extract object-speciﬁc representations such as keypoint locations. Unfortunately, these pre-trained models are built using costly ground-truth data annotations [2, 28, 32] and are not available in general for an arbitrary object category. To address this issues, recently Siarohin et al. [29] introduced Monkey-Net, the ﬁrst object-agnostic deep model for image Nicu Sebe DISI, University of Trento Huawei Technologies Ireland niculae.sebe@unitn.it Driving video image Source Driving video image Source Figure 1: Example animations produced by our method trained on different datasets: VoxCeleb [23] (top left), Tai-Chi-HD (top right), Fashion-Videos [42] (bottom left) and MGif [29] (bottom right). We use relative motion transfer for VoxCeleb and Fashion-Videos and absolute transfer for MGif and Tai-Chi-HD see Sec. 3.4. Check our project page for more qualitative results2. animation. Monkey-Net encodes motion information via keypoints learned in a self-supervised fashion. At test time, the source image is animated according to the corresponding keypoint trajectories estimated in the driving video. The major weakness of Monkey-Net is that it poorly models object appearance transformations in the keypoint neighborhoods assuming a zeroth order model (as we show in Sec. 3.1). This leads to poor generation quality in the case of large object pose changes (see Fig. 4). To tackle this issue, we propose to use a set of self-learned keypoints together with local afﬁne transformations to model complex motions. We therefore call our method a ﬁrst-order motion model. Second, we introduce an occlusion-aware generator, which adopts an occlusion mask automatically estimated to indicate object parts that are not visible in the source image and that should be inferred from the context. This is especially needed when the driving video contains large motion patterns and occlusions are typical. Third, we extend the equivariance loss commonly used for keypoints detector training [18, 45], to improve the estimation of local afﬁne transformations. Fourth, we experimentally show that our method signiﬁcantly outperforms state-of-the-art image animation methods and can handle high-resolution datasets where other approaches generally fail. Finally, we release a new high resolution dataset, Thai-Chi-HD, which we believe could become a reference benchmark for evaluating frameworks for image animation and video generation. 2\n\nMethod\nWe are interested in animating an object depicted in a source image S based on the motion of a similar object in a driving video D. Since direct supervision is not available (pairs of videos in which objects move similarly), we follow a self-supervised strategy inspired from Monkey-Net [29]. For training, we employ a large collection of video sequences containing objects of the same object category. Our model is trained to reconstruct the training videos by combining a single frame and a learned latent representation of the motion in the video. Observing frame pairs, each extracted from the same video, it learns to encode motion as a combination of motion-speciﬁc keypoint displacements and local afﬁne transformations. At test time we apply our model to pairs composed of the source image and of each frame of the driving video and perform image animation of the source object. An overview of our approach is presented in Fig. 2. Our framework is composed of two main modules: the motion estimation module and the image generation module. The purpose of the motion estimation module is to predict a dense motion ﬁeld from a frame D ∈R3×H×W of dimension H × W of the driving video D to the source frame S ∈R3×H×W . The dense motion ﬁeld is later used to align the feature maps computed from S with the object pose in D. The motion ﬁeld is modeled by a function TS←D : R2 →R2 that maps each pixel location in D with its corresponding location in S. TS←D is often referred to as backward optical ﬂow. We employ backward optical ﬂow, rather than forward optical ﬂow, since back-warping can be implemented efﬁciently in a differentiable manner using bilinear sampling [17]. We assume there exists an abstract reference frame R. We independently estimate two transformations: from R to S (TS←R) and from R to D (TD←R). Note that unlike X2Face [41] the reference frame is an abstract concept that cancels out in our derivations later. Therefore it is never explicitly computed and cannot be visualized. This choice allows us to independently process D and S. This is desired since, at test time the model receives pairs of the source image and driving frames sampled from a different video, which can be very different visually. Instead of directly predicting TD←R and TS←R, the motion estimator module proceeds in two steps. In the ﬁrst step, we approximate both transformations from sets of sparse trajectories, obtained by using keypoints learned in a self-supervised way. The locations of the keypoints in D and S are separately predicted by an encoder-decoder network. The keypoint representation acts as a bottleneck resulting in a compact motion representation. As shown by Siarohin et al. [29], such sparse motion representation is well-suited for animation as at test time, the keypoints of the source image can be moved using the keypoints trajectories in the driving video. We model motion in the neighbourhood of each keypoint using local afﬁne transformations. Compared to using keypoint displacements only, the local afﬁne transformations allow us to model a larger family of transformations. We use Taylor expansion to represent TD←R by a set of keypoint locations and afﬁne transformations. To this end, the keypoint detector network outputs keypoint locations as well as the parameters of each afﬁne transformation. During the second step, a dense motion network combines the local approximations to obtain the resulting dense motion ﬁeld ˆTS←D. Furthermore, in addition to the dense motion ﬁeld, this network outputs an occlusion mask ˆOS←D that indicates which image parts of D can be reconstructed by warping of the source image and which parts should be inpainted, i.e.inferred from the context. Finally, the generation module renders an image of the source object moving as provided in the driving video. Here, we use a generator network G that warps the source image according to ˆTS←D and inpaints the image parts that are occluded in the source image. In the following sections we detail each of these step and the training procedure. 3.1 Local Afﬁne Transformations for Approximate Motion Description The motion estimation module estimates the backward optical ﬂow TS←D from a driving frame D to the source frame S. As discussed above, we propose to approximate TS←D by its ﬁrst order Taylor expansion in a neighborhood of the keypoint locations. In the rest of this section, we describe the motivation behind this choice, and detail the proposed approximation of TS←D. We assume there exist an abstract reference frame R. Therefore, estimating TS←D consists in estimating TS←R and TR←D. Furthermore, given a frame X, we estimate each transformation TX←R in the neighbourhood of the learned keypoints. Formally, given a transformation TX←R, we consider its ﬁrst order Taylor expansions in K keypoints p1, . . . pK. Here, p1, . . . pK denote the coordinates of the keypoints in the reference frame R. Note that for the sake of simplicity in the following the point locations in the reference pose space are all denoted by p while the point locations in the X, S or D pose spaces are denoted by z. We obtain: TX←R(p) = TX←R(pk) + \u0012 d dpTX←R(p) p=pk In this formulation, the motion function TX←R is represented by its values in each keypoint pk and its Jacobians computed in each pk location: \u0013 (p −pk) + o(∥p −pk∥), (1) Furthermore, in order to estimate TR←X = T −1 X←R, we assume that TX←R is locally bijective in the neighbourhood of each keypoint. We need to estimate TS←D near the keypoint zk in D, given that zk is the pixel location corresponding to the keypoint location pk in R. To do so, we ﬁrst estimate the transformation TR←D near the point zk in the driving frame D, e.g. pk = TR←D(zk). Then we estimate the transformation TS←R near pk in the reference R. Finally TS←D is obtained as follows: TS←D = TS←R ◦TR←D = TS←R ◦T −1 D←R, (3) After computing again the ﬁrst order Taylor expansion of Eq. (3) (see Sup. Mat.), we obtain: TS←D(z) ≈TS←R(pk) + Jk(z −TD←R(pk)) (4) with: Jk = \u0012 d \u0013 \u0012 d dpTS←R(p) p=pk In practice, TS←R(pk) and TD←R(pk) in Eq. (4) are predicted by the keypoint predictor. More precisely, we employ the standard U-Net architecture that estimates K heatmaps, one for each keypoint. The last layer of the decoder uses softmax activations in order to predict heatmaps that can be interpreted as keypoint detection conﬁdence map. Each expected keypoint location is estimated using the average operation as in [29, 25]. Note if we set Jk = 1 (1 is 2 × 2 identity matrix), we get the motion model of Monkey-Net. Therefore Monkey-Net uses a zeroth-order approximation of TS←D(z) −z. For both frames S and D, the keypoint predictor network also outputs four additional channels for each keypoint. From these channels, we obtain the coefﬁcients of the matrices d dpTS←R(p)|p=pk and d dpTS←R(p)|p=pk in Eq. (5) by computing spatial weighted average using as weights the corresponding keypoint conﬁdence map. Combining Local Motions. We employ a convolutional network P to estimate ˆTS←D from the set of Taylor approximations of TS←D(z) in the keypoints and the original source frame S. Importantly, since ˆTS←D maps each pixel location in D with its corresponding location in S, the local patterns in ˆTS←D, such as edges or texture, are pixel-to-pixel aligned with D but not with S. This misalignment issue makes the task harder for the network to predict ˆTS←D from S. In order to provide inputs already roughly aligned with ˆTS←D, we warp the source frame S according to local transformations estimated in Eq. (4). Thus, we obtain K transformed images S1, . . . SK that are each aligned with ˆTS←D in the neighbourhood of a keypoint. Importantly, we also consider an additional image S0 = S for the background. For each keypoint pk we additionally compute heatmaps Hk indicating to the dense motion network where each transformation happens. Each Hk(z) is implemented as the difference of two heatmaps centered in TD←R(pk) and TS←R(pk): (TD←R(pk) −z)2 Hk(z) = exp σ In all our experiments, we employ σ = 0.01 following Jakab et al. [18]. The heatmaps Hk and the transformed images S0, . . . SK are concatenated and processed by a UNet [26]. ˆTS←D is estimated using a part-based model inspired by Monkey-Net [29]. We assume that an object is composed of K rigid parts and that each part is moved according to Eq. (4). Therefore we estimate K+1 masks Mk, k = 0, . . . K that indicate where each local transformation holds. The ﬁnal dense motion prediction ˆTS←D(z) is given by: K X \u0013−1 (5) dpTD←R(p) p=pk ! (TS←R(pk) −z)2 ! −exp . (6) σ 3.2 Occlusion-aware Image Generation As mentioned in Sec.3, the source image S is not pixel-to-pixel aligned with the image to be generated ˆD. In order to handle this misalignment, we use a feature warping strategy similar to [30, 29, 15]. More precisely, after two down-sampling convolutional blocks, we obtain a feature map ξ ∈RH′×W ′ of dimension H′ × W ′. We then warp ξ according to ˆTS←D. In the presence of occlusions in S, optical ﬂow may not be sufﬁcient to generate ˆD. Indeed, the occluded parts in S cannot be recovered by image-warping and thus should be inpainted. Consequently, we introduce an occlusion map ˆOS←D ∈[0, 1]H′×W ′ to mask out the feature map regions that should be inpainted. Thus, the occlusion mask diminishes the impact of the features corresponding to the occluded parts. The transformed feature map is written as: ξ′ = ˆOS←D ⊙fw(ξ, ˆTS←D) (8) where fw(·, ·) denotes the back-warping operation and ⊙denotes the Hadamard product. We estimate the occlusion mask from our sparse keypoint representation, by adding a channel to the ﬁnal layer of the dense motion network. Finally, the transformed feature map ξ′ is fed to subsequent network layers of the generation module (see Sup. Mat.) to render the sought image. 3.3 Training Losses We train our system in an end-to-end fashion combining several losses. First, we use the reconstruction loss based on the perceptual loss of Johnson et al. [19] using the pre-trained VGG-19 network as our main driving loss. The loss is based on implementation of Wang et al. [38]. With the input driving frame D and the corresponding reconstructed frame ˆD, the reconstruction loss is written as: I X Ni( ˆD) −Ni(D) , (9) Lrec( ˆD, D) = i=1 where Ni(·) is the ith channel feature extracted from a speciﬁc VGG-19 layer and I is the number of feature channels in this layer. Additionally we propose to use this loss on a number of resolutions, forming a pyramid obtained by down-sampling ˆD and D, similarly to MS-SSIM [40, 33]. The resolutions are 256 × 256, 128 × 128, 64 × 64 and 32 × 32. There are 20 loss terms in total. Imposing Equivariance Constraint. Our keypoint predictor does not require any keypoint annotations during training. This may lead to unstable performance. Equivariance constraint is one of the most important factors driving the discovery of unsupervised keypoints [18, 44]. It forces the model to predict consistent keypoints with respect to known geometric transformations. We use thin plate splines deformations as they were previously used in unsupervised keypoint detection [18, 44] and are similar to natural image deformations. Since our motion estimator does not only predict the keypoints, but also the Jacobians, we extend the well-known equivariance loss to additionally include constraints on the Jacobians. We assume that an image X undergoes a known spatial deformation TX←Y. In this case TX←Y can be an afﬁne transformation or a thin plane spline deformation. After this deformation we obtain a new image Y. Now by applying our extended motion estimator to both images, we obtain a set of local approximations for TX←R and TY←R. The standard equivariance constraint writes as: TX←R ≡TX←Y ◦TY←R (10) After computing the ﬁrst order Taylor expansions of both sides, we obtain the following constraints (see derivation details in Sup. Mat.): TX←R(pk) ≡TX←Y ◦TY←R(pk), (11) \u0012 d \u0013 ≡ \u0012 d dpTX←R(p) p=pk dpTX←Y(p) p=TY←R(pk) \u0013 \u0012 d dpTY←R(p) p=pk \u0013 , (12) L1 would force the magnitude of the Jacobians to zero and would lead to numerical problems. To this end, we reformulate this constraint in the following way: 1 ≡ \u0012 d \u0013−1 \u0012 d dpTX←R(p) p=pk dpTX←Y(p) p=TY←R(pk) where 1 is 2 × 2 identity matrix. Then, L1 loss is employed similarly to the keypoint location constraint. Finally, in our preliminary experiments, we observed that our model shows low sensitivity to the relative weights of the reconstruction and the two equivariance losses. Therefore, we use equal loss weights in all our experiments. 3.4 Testing Stage: Relative Motion Transfer At this stage our goal is to animate an object in a source frame S1 using the driving video D1, . . . DT . Each frame Dt is independently processed to obtain St. Rather than transferring the motion encoded in TS1←Dt(pk) to S, we transfer the relative motion between D1 and Dt to S1. In other words, we apply a transformation TDt←D1(p) to the neighbourhood of each keypoint pk: TS1←St(z) ≈TS1←R(pk) + Jk(z −TS←R(pk) + TD1←R(pk) −TDt←R(pk)) (14) with Jk = \u0012 d \u0013 \u0012 d dpTD1←R(p) p=pk Detailed mathematical derivations are provided in Sup. Mat.. Intuitively, we transform the neighbourhood of each keypoint pk in S1 according to its local deformation in the driving video. Indeed, transferring relative motion over absolute coordinates allows to transfer only relevant motion patterns, while preserving global object geometry. Conversely, when transferring absolute coordinates, as in X2Face [41], the generated frame inherits the object proportions of the driving video. It’s important to note that one limitation of transferring relative motion is that we need to assume that the objects in S1 and D1 have similar poses (see [29]). Without initial rough alignment, Eq. (14) may lead to absolute keypoint locations physically impossible for the object of interest. 4\n\nExperiments\nDatasets. We train and test our method on four different datasets containing various objects. Our model is capable of rendering videos of much higher resolution compared to [29] in all our experiments. • The VoxCeleb dataset [23] is a face dataset of 22496 videos, extracted from YouTube videos. For pre-processing, we extract an initial bounding box in the ﬁrst video frame. We track this face until it is too far away from the initial position. Then, we crop the video frames using the smallest crop containing all the bounding boxes. The process is repeated until the end of the sequence. We ﬁlter out sequences that have resolution lower than 256 × 256 and the remaining videos are resized to 256 × 256 preserving the aspect ratio. It’s important to note that compared to X2Face [41], we obtain more natural videos where faces move freely within the bounding box. Overall, we obtain 19522 training videos and 525 test videos, with lengths varying from 64 to 1024 frames. • The UvA-Nemo dataset [9] is a facial analysis dataset that consists of 1240 videos. We apply the exact same pre-processing as for VoxCeleb. Each video starts with a neutral expression. Similar to Wang et al. [39], we use 1116 videos for training and 124 for evaluation. • The BAIR robot pushing dataset [10] contains videos collected by a Sawyer robotic arm pushing diverse objects over a table. It consists of 42880 training and 128 test videos. Each video is 30 frame long and has a 256 × 256 resolution. • Following Tulyakov et al. [35], we collected 280 tai-chi videos from YouTube. We use 252 videos for training and 28 for testing. Each video is split in short clips as described in pre-processing of VoxCeleb dataset. We retain only high quality videos and resized all the clips to 256 × 256 pixels (instead of 64 × 64 pixels in [35]). Finally, we obtain 3049 and 285 video chunks for training and testing respectively with video length varying from 128 to 1024 frames. This dataset is referred to as the Tai-Chi-HD dataset. The dataset will be made publicly available. \u0013 \u0012 d dpTY←R(p) p=pk \u0013 , (13) \u0013−1 (15) dpTDt←R(p) p=pk Table 1: Quantitative ablation study for video reconstruction on Tai-Chi-HD. Tai-Chi-HD L1 (AKD, MKR) AED Baseline 0.073 (8.945, 0.099) 0.235 Pyr. 0.069 (9.407, 0.065) 0.213 Pyr.+OS←D 0.069 (8.773, 0.050) 0.205 Jac. w/o Eq. (12) 0.073 (9.887, 0.052) 0.220 Full 0.063 (6.862, 0.036) 0.179 Table 2: Paired user study: user preferences in favour of our approach. X2Face [41] Monkey-Net [29] Tai-Chi-HD 92.0% 80.6% VoxCeleb 95.8% 68.4% Nemo 79.8% 60.6% Bair 95.0% 67.0% quantitatively evaluate each method on the \"proxy\" task of video reconstruction. This task consists of reconstructing the input video from a representation in which appearance and motion are decoupled. In our case, we reconstruct the input video by combining the sparse motion representation in (2) of each frame and the ﬁrst video frame. Second, we evaluate our model on image animation according to a user-study. In all experiments we use K=10 as in [29]. Other implementation details are given in Sup. Mat. Metrics. To evaluate video reconstruction, we adopt the metrics proposed in Monkey-Net [29]: • L1. We report the average L1 distance between the generated and the ground-truth videos. • Average Keypoint Distance (AKD). For the Tai-Chi-HD, VoxCeleb and Nemo datasets, we use 3rd-party pre-trained keypoint detectors in order to evaluate whether the motion of the input video is preserved. For the VoxCeleb and Nemo datasets we use the facial landmark detector of Bulat et al. [5]. For the Tai-Chi-HD dataset, we employ the human-pose estimator of Cao et al. [7]. These keypoints are independently computed for each frame. AKD is obtained by computing the average distance between the detected keypoints of the ground truth and of the generated video. • Missing Keypoint Rate (MKR). In the case of Tai-Chi-HD, the human-pose estimator returns an additional binary label for each keypoint indicating whether or not the keypoints were successfully detected. Therefore, we also report the MKR deﬁned as the percentage of keypoints that are detected in the ground truth frame but not in the generated one. This metric assesses the appearance quality of each generated frame. • Average Euclidean Distance (AED). Considering an externally trained image representation, we report the average euclidean distance between the ground truth and generated frame representation, similarly to Esser et al. [11]. We employ the feature embedding used in Monkey-Net [29]. Ablation Study. We compare the following variants of our model. Baseline: the simplest model trained without using the occlusion mask (OS←D=1 in Eq. (8)), jacobians (Jk = 1 in Eq. (4)) and is supervised with Lrec at the highest resolution only; Pyr.: the pyramid loss is added to Baseline; Pyr.+OS←D: with respect to Pyr., we replace the generator network with the occlusion-aware network; Jac. w/o Eq. (12) our model with local afﬁne transformations but without equivariance constraints on jacobians Eq. (12); Full: the full model including local afﬁne transformations described in Sec. 3.1. In Fig. 3, we report the qualitative ablation. First, the pyramid loss leads to better results according to all the metrics except AKD. Second, adding OS←D to the model consistently improves all the metrics with respect to Pyr.. This illustrates the beneﬁt of explicitly modeling occlusions. We found that without equivariance constraint over the jacobians, Jk becomes unstable which leads to poor motion estimations. Finally, our Full model further improves all the metrics. In particular, we note that, with respect to the Baseline model, the MKR of the full model is smaller by the factor of 2.75. It shows that our rich motion representation helps generate more realistic images. These results are conﬁrmed by our qualitative evaluation in Tab. 1 where we compare the Baseline and the Full models. In these experiments, each frame D of the input video is reconstructed from its ﬁrst frame (ﬁrst column) and the estimated keypoint trajectories. We note that the Baseline model does not locate any Input D Baseline Pyr. Pyr.+OS←D Jac. w/o Eq. (12) Full Figure 3: Qualitative ablation on Tai-Chi-HD. Table 3: Video reconstruction: comparison with the state of the art on four different datasets. Tai-Chi-HD VoxCeleb Nemo Bair L1 (AKD, MKR) AED L1 AKD AED L1 AKD AED L1 X2Face [41] 0.080 (17.654, 0.109) 0.272 0.078 7.687 0.405 0.031 3.539 0.221 0.065 Monkey-Net [29] 0.077 (10.798, 0.059) 0.228 0.049 1.878 0.199 0.018 1.285 0.077 0.034 Ours 0.063 (6.862, 0.036) 0.179 0.043 1.294 0.140 0.016 1.119 0.048 0.027 Driving video image Source X2Face [41] MonkeyNet [29] Ours Figure 4: Qualitative comparison with state of the art for the task of image animation on two sequences and two source images from the Tai-Chi-HD dataset. keypoints in the arms area. Consequently, when the pose difference with the initial pose increases, the model cannot reconstruct the video (columns 3,4 and 5). In contrast, the Full model learns to detect a keypoint on each arm, and therefore, to more accurately reconstruct the input video even in the case of complex motion. Comparison with State of the Art. We now compare our method with state of the art for the video reconstruction task as in [29]. To the best of our knowledge, X2Face [41] and Monkey-Net [29] are the only previous approaches for model-free image animation. Quantitative results are reported in Tab. 3. We observe that our approach consistently improves every single metric for each of the four different datasets. Even on the two face datasets, VoxCeleb and Nemo datasets, our approach clearly outperforms X2Face that was originally proposed for face generation. The better performance of our approach compared to X2Face is especially impressive X2Face exploits a larger motion embedding (128 ﬂoats) than our approach (60=K*(2+4) ﬂoats). Compared to Monkey-Net that uses a motion representation with a similar dimension (50=K*(2+3)), the advantages of our approach are clearly visible on the Tai-Chi-HD dataset that contains highly non-rigid objects (i.e.human body). We now report a qualitative comparison for image animation. Generated sequences are reported in Fig. 4. The results are well in line with the quantitative evaluation in Tab. 3. Indeed, in both examples, X2Face and Monkey-Net are not able to correctly transfer the body notion in the driving video, instead warping the human body in the source image as a blob. Conversely, our approach is able to generate signiﬁcantly better looking videos in which each body part is independently animated. This qualitative evaluation illustrates the potential of our rich motion description. We complete our evaluation with a user study. We ask users to select the most realistic image animation. Each question consists of the source image, the driving video, and the corresponding results of our method and a competitive method. We require each question to be answered by 10 AMT worker. This evaluation is repeated on 50 different input pairs. Results are reported in Tab. 2. We observe that our method is clearly preferred over the competitor methods. Interestingly, the largest difference with the state of the art is obtained on Tai-Chi-HD: the most challenging dataset in our evaluation due to its rich motions.\n\nRelated work\nVideo Generation. Earlier works on deep video generation discussed how spatio-temporal neural networks could render video frames from noise vectors [37, 27]. More recently, several approaches tackled the problem of conditional video generation. For instance, Wang et al. [39] combine a recurrent neural network with a VAE in order to generate face videos. Considering a wider range of applications, Tulyakov et al. [35] introduced MoCoGAN, a recurrent architecture adversarially trained in order to synthesize videos from noise, categorical labels or static images. Another typical case of conditional generation is the problem of future frame prediction, in which the generated video is conditioned on the initial frame [12, 24, 31, 36, 45]. Note that in this task, realistic predictions can be obtained by simply warping the initial video frame [1, 12, 36]. Our approach is closely related Driving video image Source Driving video image Source Figure 2: Overview of our approach. Our method assumes a source image S and a frame of a driving video frame D as inputs. The unsupervised keypoint detector extracts ﬁrst order motion representation consisting of sparse keypoints and local afﬁne transformations with respect to the reference frame R. The dense motion network uses the motion representation to generate dense optical ﬂow ˆTS←D from D to S and occlusion map ˆOS←D. The source image and the outputs of the dense motion network are used by the generator to render the target image. to these previous works since we use a warping formulation to generate video sequences. However, in the case of image animation, the applied spatial deformations are not predicted but given by the driving video. Image Animation. Traditional approaches for image animation and video re-targeting [6, 34, 13] were designed for speciﬁc domains such as faces [46, 43], human silhouettes [8, 38, 28] or gestures [32] and required a strong prior of the animated object. For example, in face animation, method of Zollhofer et al. [46] produced realistic results at expense of relying on a 3D morphable model of the face. In many applications, however, such models are not available. Image animation can also be treated as a translation problem from one visual domain to another. For instance, Wang et al. [38] transferred human motion using the image-to-image translation framework of Isola et al. [16]. Similarly, Bansal et al. [3] extended conditional GANs by incorporating spatio-temporal cues in order to improve video translation between two given domains. Such approaches in order to animate a single person require hours of videos of that person labelled with semantic information, and therefore have to be retrained for each individual. In contrast to these works, we neither rely on labels, prior information about the animated objects, nor on speciﬁc training procedures for each object instance. Furthermore, our approach can be applied to any object within the same category (e.g., faces, human bodies, robot arms etc). Several approaches were proposed that do not require priors about the object. X2Face [41] uses a dense motion ﬁeld in order to generate the output video via image warping. Similarly to us they employ a reference pose that is used to obtain a canonical representation of the object. In our formulation, we do not require an explicit reference pose, leading to signiﬁcantly simpler optimization and improved image quality. Siarohin et al. [29] introduced Monkey-Net, a self-supervised framework for animating arbitrary objects by using sparse keypoint trajectories. In this work, we also employ sparse trajectories induced by self-supervised keypoints. However, we model object motion in the neighbourhood of each predicted keypoint by a local afﬁne transformation. Additionally, we explicitly model occlusions in order to indicate to the generator network the image regions that can be generated by warping the source image and the occluded areas that need to be inpainted. 3\n\n5 Conclusions We presented a novel approach for image animation based on keypoints and local afﬁne transformations. Our novel mathematical formulation describes the motion ﬁeld between two frames and is efﬁciently computed by deriving a ﬁrst order Taylor expansion approximation. In this way, motion is described as a set of keypoints displacements and local afﬁne transformations. A generator network combines the appearance of the source image and the motion representation of the driving video. In addition, we proposed to explicitly model occlusions in order to indicate to the generator network which image parts should be inpainted. We evaluated the proposed method both quantitatively and qualitatively and showed that our approach clearly outperforms state of the art on all the benchmarks.\n\nReferences\n[1] Mohammad Babaeizadeh, Chelsea Finn, Dumitru Erhan, Roy H Campbell, and Sergey Levine. Stochastic variational video prediction. In ICLR, 2017. [2] Guha Balakrishnan, Amy Zhao, Adrian V Dalca, Fredo Durand, and John Guttag. Synthesizing images of humans in unseen poses. In CVPR, 2018. [3] Aayush Bansal, Shugao Ma, Deva Ramanan, and Yaser Sheikh. Recycle-gan: Unsupervised video retargeting. In ECCV, 2018. [4] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In SIGGRAPH, 1999. [5] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? (and a dataset of 230,000 3d facial landmarks). In ICCV, 2017. [6] Chen Cao, Qiming Hou, and Kun Zhou. Displaced dynamic expression regression for real-time facial tracking and animation. TOG, 2014. [7] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Realtime multi-person 2d pose estimation using part afﬁnity ﬁelds. In CVPR, 2017. [8] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and Alexei A Efros. Everybody dance now. In ECCV, 2018. [9] Hamdi Dibeklio˘glu, Albert Ali Salah, and Theo Gevers. Are you really smiling at me? spontaneous versus posed enjoyment smiles. In ECCV, 2012. [10] Frederik Ebert, Chelsea Finn, Alex X Lee, and Sergey Levine. Self-supervised visual planning with temporal skip connections. In CoRL, 2017. [11] Patrick Esser, Ekaterina Sutter, and Björn Ommer. A variational u-net for conditional appearance and shape generation. In CVPR, 2018. [12] Chelsea Finn, Ian Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction through video prediction. In NIPS, 2016. [13] Zhenglin Geng, Chen Cao, and Sergey Tulyakov. 3d guided ﬁne-grained face manipulation. In CVPR, 2019. [14] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NIPS, 2014. [15] Artur Grigorev, Artem Sevastopolsky, Alexander Vakhitov, and Victor Lempitsky. Coordinatebased texture inpainting for pose-guided image generation. In CVPR, 2019. [16] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, 2017. [17] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In NIPS, 2015. [18] Tomas Jakab, Ankush Gupta, Hakan Bilen, and Andrea Vedaldi. Unsupervised learning of object landmarks through conditional image generation. In NIPS, 2018. [19] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016. [20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2014. [21] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. [22] Yahui Liu, Marco De Nadai, Gloria Zen, Nicu Sebe, and Bruno Lepri. Gesture-to-gesture translation in the wild via category-independent conditional maps. ACM MM, 2019. [23] A. Nagrani, J. S. Chung, and A. Zisserman. Voxceleb: a large-scale speaker identiﬁcation dataset. In INTERSPEECH, 2017. [24] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Actionconditional video prediction using deep networks in atari games. In NIPS, 2015. [25] Joseph P Robinson, Yuncheng Li, Ning Zhang, Yun Fu, and Sergey Tulyakov. Laplace landmark localization. In ICCV, 2019. [26] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, 2015. [27] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Temporal generative adversarial nets with singular value clipping. In ICCV, 2017. [28] Aliaksandra Shysheya, Egor Zakharov, Kara-Ali Aliev, Renat Bashirov, Egor Burkov, Karim Iskakov, Aleksei Ivakhnenko, Yury Malkov, Igor Pasechnik, Dmitry Ulyanov, Alexander Vakhitov, and Victor Lempitsky. Textured neural avatars. In CVPR, June 2019. [29] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. Animating arbitrary objects via deep motion transfer. In CVPR, 2019. [30] Aliaksandr Siarohin, Enver Sangineto, Stéphane Lathuilière, and Nicu Sebe. Deformable gans for pose-based human image generation. In CVPR, 2018. [31] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In ICML, 2015. [32] Hao Tang, Wei Wang, Dan Xu, Yan Yan, and Nicu Sebe. Gesturegan for hand gesture-to-gesture translation in the wild. In ACM MM, 2018. [33] Hao Tang, Dan Xu, Wei Wang, Yan Yan, and Nicu Sebe. Dual generator generative adversarial networks for multi-domain image-to-image translation. In ACCV, 2018. [34] Justus Thies, Michael Zollhofer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. Face2face: Real-time face capture and reenactment of rgb videos. In CVPR, 2016. [35] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In CVPR, 2018. [36] Joost Van Amersfoort, Anitha Kannan, Marc’Aurelio Ranzato, Arthur Szlam, Du Tran, and Soumith Chintala. Transformation-based models of video sequences. arXiv preprint arXiv:1701.08435, 2017. [37] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In NIPS, 2016. [38] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-to-video synthesis. In NIPS, 2018. [39] Wei Wang, Xavier Alameda-Pineda, Dan Xu, Pascal Fua, Elisa Ricci, and Nicu Sebe. Every smile is unique: Landmark-guided diverse smile generation. In CVPR, 2018. [40] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assessment. In ACSSC, 2003. [41] Olivia Wiles, A Sophia Koepke, and Andrew Zisserman. X2face: A network for controlling face generation using images, audio, and pose codes. In ECCV, 2018. [42] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for pose-guided human video generation. In BMVC, 2019. [43] Egor Zakharov, Aliaksandra Shysheya, Egor Burkov, and Victor Lempitsky. Few-shot adversarial learning of realistic neural talking head models. In ICCV, 2019. [44] Yuting Zhang, Yijie Guo, Yixin Jin, Yijun Luo, Zhiyuan He, and Honglak Lee. Unsupervised discovery of object landmarks as structural representations. In CVPR, 2018. [45] Long Zhao, Xi Peng, Yu Tian, Mubbasir Kapadia, and Dimitris Metaxas. Learning to forecast and reﬁne residual motion for image-to-video generation. In ECCV, 2018. [46] Michael Zollhöfer, Justus Thies, Pablo Garrido, Derek Bradley, Thabo Beeler, Patrick Pérez, Marc Stamminger, Matthias Nießner, and Christian Theobalt. State of the art on monocular 3d face reconstruction, tracking, and applications. In Computer Graphics Forum, 2018.\n\nA Detailed Derivations A.1 Approximating Motion with Local Afﬁne Transformations Here, we detail the derivation leading to the approximation of TS←D near the keypoint zk in Eq. (4). Using ﬁrst order Taylor expansion we can obtain: TS←D(z) = TS←D(zk) + \u0012 d dz TS←D(z) z=zk TS←D can be written as the composition of two transformations: TS←D = TS←R ◦TR←D (17) In order to compute the zeroth order term, we estimate the transformation TR←D near the point zk in the driving frame D, e.g pk = TR←D(zk). Then we can estimate the transformation TS←R near pk in the reference R. Since pk = TR←D(zk) and T −1 R←D = TD←R, we can write zk = TD←R(pk). Consequently, we obtain: TS←D(zk) = TS←R ◦TR←D(zk) = TS←R ◦T −1 D←R(zk) = TS←R ◦T −1 D←R ◦TD←R(pk) = TS←R(pk). (18) Concerning the ﬁrst order term, we apply the function composition rule in Eq. (17) and obtain: \u0012 d \u0013 = \u0012 d dz TS←D(z) z=zk dpTS←R(p) p=TR←D(zk) Since the matrix inverse of the Jacobian is equal to the Jacobian of the inverse function, and since pk = TR←D(zk), Eq. (19) can be rewritten: \u0012 d \u0013 = \u0012 d dz TS←D(z) z=zk dpTS←R(p) p=pk After injecting Eqs. (18) and (20) into (16), we ﬁnally obtain: TS←D(z) ≈TS←R(pk) + \u0012 d \u0013 \u0012 d dpTS←R(p) p=pk A.2 Equivariance Loss At training time, we use equivariance constraints that enforces: TX←R ≡TX←Y ◦TY←R (22) After applying ﬁrst order Taylor expansion on the left-hand side, we obtain: TX←R(p) = TX←R(pk) + \u0012 d dpTX←R(p) p=pk After applying ﬁrst order Taylor expansion on the right-hand side in Eq. (22), we obtain: TX←Y ◦TY←R(p) = TX←Y ◦TY←R(pk) + \u0012 d dpTX←Y ◦TY←R (24) We can further simplify this expression using derivative of function composition: \u0012 d \u0013 = \u0012 d p=pk dpTX←Y(p) p=TY←R(pk) dpTX←Y ◦TY←R Eq. (22) holds only when every coefﬁcient in Taylor expansion of the right and left sides are equal. Thus, it leads us to the following constaints: \u0013 (z −zk) + o(∥z −zk∥) (16) \u0013 \u0012 d dz T −1 D←R(z) z=zk \u0013 (19) \u0013 \u0012 d \u0013−1 (20) dpTD←R(p) p=pk \u0013−1 (z −TD←R(pk)) (21) dpTD←R(p) p=pk \u0013 (p −pk) + o(∥p −pk∥). (23) p=pk \u0013 (p −pk) + o(∥p −pk∥), \u0013 \u0012 d dpTY←R(p) p=pk \u0013 . (25) A.3 Transferring Relative Motion In order to transfer only relative motion patterns, we propose to estimate TSt←R(p) near the keypoint pk by shifting the motion in the driving video to the location of keypoint pk in the source. To this aim, we introduce VS1←D1(pk) = TS1←R(pk) −TD1←R(pk) ∈R2 that is the 2D vector from the landmark position pk in D1 to its position in S1. We proceed as follows. First, we shift point coordinates according to −VS1←D1(pk) in order to obtain coordinates in D1. Second, we apply the transformation TDt←D1. Finally, we translate the points back in the original coordinate space using VS1←D1(pk). Formally, it can be written: TSt←R(p) = TDt←D1 \u0000 TS1←R(p) −VS1←D1(pk) \u0001 + VS1←D1(pk) Now, we can compute the value and Jacobian in the pk: TSt←R(pk) = TDt←D1 ◦TD1←R(pk) −TD1←R(pk) + TS1←R(pk) and: \u0012 d \u0013 = \u0012 d \u0013 \u0012 d dpTSt←R(p) p=pk dpTDt←R(p) p=pk Now using Eq. (21) and treating S1 as source and St as driving frame, we obtain: TS1←St(z) ≈TS1←R(pk) + Jk(z −TS←R(pk) + TD1←R(pk) −TDt←R(pk)) (28) with \u0013 \u0012 d Jk = \u0012 d dpTD1←R(p) p=pk Note that, here, \u0010 d dpTS1←R(p) p=pk \u0011 canceled out. B Implementation details B.1 Architecture details In order to reduce memory and computational requirements of our model, the keypoint detector and dense motion predictor both work on resolution of 64 × 64 (instead of 256 × 256). For the two networks of the motion module, we employ an architecture based on U-Net [26] with ﬁve conv3×3 - bn - relu - avg −pool2×2 blocks in the encoders and ﬁve upsample2×2 - conv3×3 - bn - relu blocks in the decoders. In the generator network, we use the Johnson architecture [19] with two down-sampling blocks, six residual-blocks and two up-sampling blocks. We train our network using Adam [20] optimizer with learning rate 2e −4 and batch size 20. We employ learning decay by dropping the learning rate at T 2 and 3T 4 iterations, where T is total number of iteration. We chose T ≈100k for Tai-Chi-HD and VoxCeleb, and T ≈40k for Nemo and Bair. The model converges in approximately 2 days using 2 TitanX gpus for Tai-Chi-HD and VoxCeleb. B.2 Equivariance loss implementation As explained above our equivariance losses force the keypoint detector to be equivariant to some transformations TX←Y. In our experiments TX←Y is implemented using randomly sampled thin plate splines. We sample spline parameters from normal distributions with zero mean and variance equal to 0.005 for deformation component and 0.05 for the afﬁne component. For deformation component we use uniform 5 × 5 grid. C Additional experiments \u0013−1 \u0012 d dpTD1←R(p) p=pk dpTS1←R(p) p=pk \u0013 . \u0013−1 . (29) dpTDt←R(p) p=pk We compare our approach with X2face [41] and Monkey-Net [29]. In Fig. 5, we show three animation examples from the VoxCeleb dataset. First, X2face is not capable of generating realistic video sequences as we can see, for instance in the last frame of the last sequence. Then, Monkey-Net generates realistic frames but fails to generate speciﬁc facial expressions as in the third frame of the ﬁrst sequence or in transferring the eye movements as in the last two frames of the second sequence. In Fig. 6, we show three animation examples from the Nemo dataset. First, we observe that this dataset is simpler than VoxCeleb since the persons are facing a uniformly black background. With this simpler dataset, X2Face generates realistic videos. However, it is not capable of inpainting image parts that are not visible in the source image. For instance, X2Face does not generate the teeth. Our approach also perform better than Monkey-Net as we can see by comparing the generate teeth in the ﬁrst sequence or the closed eyes in the fourth frames of the second and third sequences. In Fig. 6, we report additional examples for the Tai-Chi-HD dataset. These examples are well in line with what is reported in the main paper. Both X2Face and Monkey-Net completely fail to generate realistic videos. The source images are warped without respecting human body structure. Conversely, our approach is able to deform the person in foreground without affecting the background. Even though we can see few minor artifacts, our model is able to move each body part independently following the body motion in the driving video. Finally, in Fig. 8 we show three image animation examples on the Bair dataset. Again, we see that X2Face is not able to transfer motion since it constantly returns frames almost identical with the source images. Compared to Monkey-Net, our approach performs slightly better since it preserves better the robot arm as we can see in the second frame of the ﬁrst sequence or in the fourth frame of the last sequence. C.2 Keypoint detection We now illustrate the keypoints that are learned by our self-supervised approach in Fig. 9. On the Tai-Chi-HD dataset, the keypoints are semantically consistent since each of them corresponds to a body part: light green for the right foot, and blue and red for the face for instance. Note that, a light green keypoint is constantly located in the bottom left corner in order to model background or camera motion. On VoxCeleb, we observe that, overall, the obtained keypoints are semantically consistent except for the yellow and green keypoints. For instance, the red and purple keypoints constantly correspond to the nose and the chin respectively. We observe a similar consistency for the Nemo dataset. For the Bair dataset, we note that two keypoints (dark blue and light green) correspond to the robotic arm. C.3 Visualizing occlusion masks In Fig. 10, we visualize the predicted occlusion masks ˆOS←D on the Tai-Chi-HD, VoxCeleb and Nemo datasets. In the ﬁrst sequence, when the person in the driving video is moving backward (second to fourth frames), the occlusion mask becomes black (corresponding to 0) in the background regions that are occluded in the source frame. It indicates that these parts cannot be generated by warping the source image features and must be inpainted. A similar observation can be made on the example sequence of VoxCeleb. Indeed, we see that when the face is rotating, the mask has low values (dark grey) in the neck region and in the right face side (in the left-hand side of the image) that are not visible in the source Frame. Then, since the driving video example from Nemo contains only little motion, the predicted mask is almost completely white. Overall, these three examples show that the occlusion masks truly indicate occluded regions even if no speciﬁc training loss is employed in order to lead to this behaviour. Finally, the predicted occlusion masks are more difﬁcult to interpret in the case of the Bair dataset. Indeed, the robotic arm is masked out in every frame whereas we could expect that the model generates it by warping. A possible explanation is that, since in this particular dataset, the moving object is always the same, the network can generate without warping the source image. We observe also that masks have low values for the regions corresponding to the arm shadow. It is explained by the fact that shadows cannot be obtained by image warping and that they need to be added by the generator. Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Driving video image Source X2face [41] MonkeyNet [29] Ours Tai-Chi-HD VoxCeleb Nemo Bair Driving video image Source Occlusion Output Driving video image Source Occlusion Output Driving video image Source Occlusion Output Driving video image Source Occlusion Output", "persona": "Student", "response": "### 1. Overview\nSeeing used to be believing. Thank AI, we finally have to farewell this cute and naive, but dangerous faith. Because it actually never was. In the XXth century, photos were retouched by repressive regimes. With Deep Learning, we experience new ways to re-illustrate reality. It is not a danger; it's a chance. Using a painting by Arcimboldo, we can see how the motion assignment works — some of the graphical features are still detected as background and are not animated. The most interesting effect happens if we use an image without physiognomical patterns — or even abstract art. I used work by Pollock: Probably if we combine this method with StyleTransfer — or even with Deep Painterly Harmonization, we will achieve even more coherent results.\n\n### 2. Background\nThe first thought coming to your mind if you see such models is surely: DeepFake. We are already too biased (and keep biasing our Neural Networks). Can we still believe in visuals? Can we distinguish between real and fake in the Digital Age? The answer is: \"we cannot anymore\".\n\n### 3. Methods\nThe key points are set along with transformations (similar to a puppet tool in Photoshop, or like sensors on motion capture suit) — and so the trained movement can be transferred to a target image. Shortly put, the unsupervised learning approach analyzes the motion data in source footage, universalizes it and, applies to target footage. None None And it works astonishingly well.\n\n### 4. Results\nI tried my luck on Nefertiti using the footage of AI pioneer Geoffrey Hinton. This footage is delivered with Notebook. You can use another video material. It has to fit into specific requirements and sizes. None None None None None Age of DeepFakes: a chance?\n\n### 5. Takeaways\nAmong various methods, the framework and paper \"First Order Motion Model for Image Animation\" by Aliaksandr Siarohin et al. captivates through its brilliant idea: Motion is described as a set of keypoints displacements and local affine transformations. A generator network combines the appearance of the source image and the motion representation of the driving video. In addition, we proposed to explicitly model occlusions in order to indicate to the generator network which image parts should be inpainted (source). The result was more than convincing: In opposite, applying my own photo delivers some glitches, especially regarding glasses. I suppose, these patterns are missing in the footage and so the allocation of keypoints sometimes fails:", "paper_id": "16", "story_id": "16-01"}
{"paper": "Artificial intelligence-supported screen reading versus standard double reading in the Mammography Screening with Artificial Intelligence trial (MASAI): a clinical safety analysis of a randomised, controlled, non-inferiority, singleblinded, screening accuracy study Kristina Lång, Viktoria Josefsson, Anna-Maria Larsson, Stefan Larsson, Charlotte Högberg, Hanna Sartor, Solveig Hofvind, Ingvar Andersson, Aldana Rosso Summary Background Retrospective studies have shown promising results using artificial intelligence (AI) to improve mammography screening accuracy and reduce screen-reading workload; however, to our knowledge, a randomised trial has not yet been conducted. We aimed to assess the clinical safety of an AI-supported screen-reading protocol compared with standard screen reading by radiologists following mammography. Lancet Oncol 2023; 24: 936–44 See Comment page 830 Division of Diagnostic Translational Medicine, Lund University, Malmö, Sweden (K Lång PhD, V Josefsson MD, H Sartor PhD, I Andersson PhD, Methods In this randomised, controlled, population-based trial, women aged 40–80 years eligible for mammography screening (including general screening with 1·5–2-year intervals and annual screening for those with moderate hereditary risk of breast cancer or a history of breast cancer) at four screening sites in Sweden were informed about the study as part of the screening invitation. Those who did not opt out were randomly allocated (1:1) to AI-supported screening (intervention group) or standard double reading without AI (control group). Screening examinations were automatically randomised by the Picture Archive and Communications System with a pseudo-random number generator after image acquisition. The participants and the radiographers acquiring the screening examinations, but not the radiologists reading the screening examinations, were masked to study group allocation. The AI system (Transpara version 1.7.0) provided an examination-based malignancy risk score on a 10-level scale that was used to triage screening examinations to single reading (score 1–9) or double reading (score 10), with AI risk scores (for all examinations) and computer-aided detection marks (for examinations with risk score 8–10) available to the radiologists doing the screen reading. Here we report the prespecified clinical safety analysis, to be done after 80 000 women were enrolled, to assess the secondary outcome measures of early screening performance (cancer detection rate, recall rate, false positive rate, positive predictive value [PPV] of recall, and type of cancer detected [invasive or in situ]) and screen-reading workload. Analyses were done in the modified intention-to-treat population (ie, all women randomly assigned to a group with one complete screening examination, excluding women recalled due to enlarged lymph nodes diagnosed with lymphoma). The lowest acceptable limit for safety in the intervention group was a cancer detection rate of more than 3 per 1000 participants screened. The trial is registered with ClinicalTrials.gov, NCT04838756, and is closed to accrual; follow-up is ongoing to assess the primary endpoint of the trial, interval cancer rate. Findings Between April 12, 2021, and July 28, 2022, 80 033 women were randomly assigned to AI-supported screening (n=40 003) or double reading without AI (n=40 030). 13 women were excluded from the analysis. The median age was 54·0 years (IQR 46·7–63·9). Race and ethnicity data were not collected. AI-supported screening among 39 996 participants resulted in 244 screen-detected cancers, 861 recalls, and a total of 46 345 screen readings. Standard screening among 40 024 participants resulted in 203 screen-detected cancers, 817 recalls, and a total of 83 231 screen readings. Cancer detection rates were 6·1 (95% CI 5·4–6·9) per 1000 screened participants in the intervention group, above the lowest acceptable limit for safety, and 5·1 (4·4–5·8) per 1000 in the control group—a ratio of 1·2 (95% CI 1·0–1·5; p=0·052). Recall rates were 2·2% (95% CI 2·0–2·3) in the intervention group and 2·0% (1·9–2·2) in the control group. The false positive rate was 1·5% (95% CI 1·4–1·7) in both groups. The PPV of recall was 28·3% (95% CI 25·3–31·5) in the intervention group and 24·8% (21·9–28·0) in the control group. In the intervention group, 184 (75%) of 244 cancers detected were invasive and 60 (25%) were in situ; in the control group, 165 (81%) of 203 cancers were invasive and 38 (19%) were in situ. The screen-reading workload was reduced by 44·3% using AI. Interpretation AI-supported mammography screening resulted in a similar cancer detection rate compared with standard double reading, with a substantially lower screen-reading workload, indicating that the use of AI in mammography screening is safe. The trial was thus not halted and the primary endpoint of interval cancer rate will be assessed in 100 000 enrolled participants after 2-years of follow up.\n\nIntroduction\nEuropean guidelines recommend double reading of screening mammograms to ensure high sensitivity.1 A meta-analysis suggested that double reading resulted in 0·44 more cancers being detected per 1000 people screened than with single reading;2 however, this comes at the expense of a large screen-reading workload and can potentially increase false positives.3,4 Double reading can also be difficult to sustain because of a shortage of breast radiologists in many countries.5 In addition, despite double reading, some cancers might be missed and diagnosed as interval cancers.6 Interval cancers generally have a worse prognosis than screen-detected cancers, and the interval cancer rate is therefore an important indicator of screening efficacy.1,6 In retrospective studies, about 20–30% of interval cancers have been shown to display highly suspicious signs of malignancy at the preceding screening mammogram,6–8 suggesting that mammography alone could have been sufficient for detection—ie, without the need for supplementary imaging methods. Establishing a more efficient and effective mammography screening programme is therefore warranted. Recently developed image analysis tools based on artificial intelligence (AI) have promising applications in mammography screening, such as facilitating triage of screening examinations according to risk of malignancy or supporting detection with computer-aided detection Research in context Evidence before this study We searched MEDLINE for studies published in English between Jan 1, 2015, and Dec 31, 2020, that included “breast cancer screening” or “mammography screening”, and “artificial intelligence” or “machine learning” in the title or abstract. No prospective trials were identified. There were several retrospective accuracy studies using screening data or enriched datasets. We found no systematic reviews on test accuracy. The retrospective studies, using different artificial intelligence (AI) software and mammography devices, indicated that AI could be used to differentiate between screening examinations with low and high probability of malignancy, which could potentially be used to improve the efficacy of screening and reduce the workload, especially the requirement for double reading. (CAD) marks highlighting suspicious findings.9 Retrospective studies suggest that the accuracy of AI is similar to or better than that of breast radiologists.10–13 AI has also been shown to be able to identify examinations that were normal (ie, true negatives), and, since the vast majority of women who attend screening do not have breast cancer, adapting single and double reading to AI risk scores could allow more efficient screen reading.14–17 Additionally, AI has been shown to retrospectively classify screening examinations as high risk before a diagnosis of interval cancer, and could, therefore, help radiologists to reduce false negative screening results when used as detection support.16,18,19 Taken together, the evidence suggests that use of AI could potentially benefit mammography screening by reducing the screenreading workload and the number of interval cancers, but randomised trials are needed to assess the efficacy of AI-supported screening.13 In the randomised, controlled Mammography Screening with Artificial Intelligence trial (MASAI), we investigate an AI-supported screen-reading procedure involving triage of screening examinations to single or double reading, along with detection support. Here we report a prespecified safety analysis, the objective of which was to assess the safety of using AI-supported screening compared with standard double reading by determining the effect on cancer detection, which could be used to inform new trials or programme-based more cancers being detected and exceeded the lowest acceptable limit for safety compared with standard double reading without AI, without affecting the false positive rate. The AI supported screen-reading procedure enabled a 44·3% reduction in the screen-reading workload. The results indicate that the proposed screening strategy is safe. Implications of all the available evidence The results from this randomised trial support the findings of earlier retrospective studies, indicating a general potential of AI to improve screening efficacy and reduce workload. The clinical safety analysis concludes that the AI-supported screen-reading procedure can be considered safe. Implementation of AI in clinical practice to reduce the screen-reading workload could therefore be considered to help address workforce shortages. The assessment of the primary endpoint of interval cancer rate, together with a characterisation of detected cancers in the entire study population, will provide further insight into the efficacy of screening, possible side-effects such as overdiagnosis, and the prognostic implications of using AI in mammography screening, taking cost-effectiveness into account. evaluations. In addition, we compared recalls, false positives, positive predictive value of recalls, and screenreading workload for the two screen-reading procedures.\n\nMethods\nStudy design and participants The MASAI trial was designed as a randomised, parallel, non-inferiority, single-blinded, controlled, screening accuracy study to compare AI-supported mammography screening with standard double reading without AI. The study was done within the Swedish national screening programme and participants were recruited at four screening sites in southwest Sweden (Malmö, Lund, Landskrona, and Trelleborg). Screen reading and further assessment of recalled participants were done at a single site, the Unilabs Mammography Unit at Skåne University Hospital (Malmö, Sweden). The inclusion criterion was women (defined here as people registered with a female Swedish personal identity number indicating female gender, which can include trans women who have changed their legal gender) eligible to participate in population-based mammography screening, which also includes those with moderate hereditary risk of breast cancer and those with a history of breast cancer. No exclusion criteria were applied. The Swedish population-based mammography screening programme invites women aged 40–74 years for screening at intervals of 1·5–2 years. Those younger than 55 years are first screened at 1·5-year intervals, and those aged 55 years or older are screened at 2-year intervals. Annual screening is done for people considered to have a moderate hereditary risk of breast cancer (lifetime risk 18–29%) and for those with a history of breast cancer (for 10 years after surgery, with an upper age limit of 80 years). Information about the study was included in screening invitation letters and in SMS text message reminders before scheduled appointments, with a link to a website containing detailed study information in Swedish and English. People eligible for screening who did not wish to participate in the trial were asked to opt out at the time of the screening visit and received standard of care. Information about the race or ethnicity of participants was not collected. For more on Transpara see https://screenpoint-medical.com The study was approved by the Swedish Ethical Review Authority (2020-04936), which also waived the need for written informed consent. The study protocol (versions 1.1 and 1.2) and the statistical analysis plan are available at the Lund University website. The protocol was updated to improve clarity; there were no changes in the trial procedures nor analyses in the statistical analysis plan from those described in the first and updated protocol versions. For the study protocol and statistical analysis plan see https://portal.research.lu.se/en/ projects/mammography- randomised in the Picture Archive and Communications System (PACS; Sectra, Linköping, Sweden) to AIsupported screening (intervention group) or standard double reading without AI (control group) with a pseudorandom number generator. The people screened and the radiographers acquiring the screening examinations were masked to study group allocation, since the automatic randomisation was not visible on the radiographer’s PACS interface. The screen readers were not masked to the results of the allocation. Procedures A single-vendor mammography system was used for the screening examinations (Senographe Pristina, GE Healthcare, Freiburg, Germany). Standard screening examination included two views per breast with the addition of implant-displacement views for people with breast implants. The examinations randomised to the intervention group were analysed using Transpara  version 1.7.0 (ScreenPoint Medical, Nijmegen, Netherlands). This system uses deep learning to identify and interpret mammographic regions suspicious for cancer. It was developed with more than 200 000 examinations for training and testing, which were obtained from multiple institutions in more than ten countries covering a range of populations, modality manufacturers, and variations in screening and diagnostic workflows. Annotations of more than 10 000 cancers in the database are based on biopsy results and include regions marked in previous mammograms in which cancers were visible but not detected by radiologists. The AI system provided an examination-based malignancy risk score on a continuous scale ranging from 1 to 10. The risk scores were also presented on a discrete 10-level scale, calibrated to assign approximately a tenth of screening examinations to each risk score. Examinations were considered to be low risk (risk score 1–7), intermediate risk (risk scores 8 and 9), or high risk (risk score 10). Cancer prevalence increases sharply in the group with a risk score of 10, and retrospective studies using the same AI version as in this trial have reported 87–90% of screen-detected cancers and 45% of interval cancers to be in this group.16,17 The AI system also provided CAD marks at suspicious regional findings of calcifications and soft-tissue lesions, with a regional risk score on a discrete scale from 1 to 98. To limit the number of CAD marks that could potentially disturb the screen reading or lead to an increase in false positives, the AI system was preconfigured for CAD marks to be available only for examinations with risk scores of 8, 9, and 10, accounting for approximately 30% of all examinations (regional risk score threshold >42). The AI system was also configured to analyse implant-displacement views in screening examinations of people with breast implants. The PACS was customised with separate worklists for single and double reading. Examinations with the highest 1% risk, classified as extra high risk, were flagged in the high-risk worklist as 10H. A risk score threshold of 9·8, which was determined from the observed risk score distribution in the screening population, was used to select this group. Screening examinations in the control group were not analysed with AI at any timepoint. In the intervention group, examinations with risk scores of 1–9 (low and intermediate risk) underwent single reading and examinations with risk scores of 10 (high risk) underwent double reading (figure 1). Double reading was done by two different breast radiologists. The second reader had access to the first reader’s assessment (unblinded double reading), which is the standard of care in the regional screening programme in the Skåne region. Readers were aware of the examination risk score (for all examinations), presented both in the PACS worklists and on the image monitor. Readers first read the examination without CAD marks and then with CAD marks, if available (ie, for examinations with risk scores of 8–10). The readers were instructed to recall cases with the highest 1% risk, except for obvious false positives. In the control group, screening examinations were read with standard unblinded double reading without AI. The outcomes of the screen reading were either no suspicion of malignancy or recall. Participants could be recalled due to mammographic findings or selfreported symptoms. Current practice in the screening programme is to recall participants with self-reported symptoms, such as a lump, when the mammogram cannot safely be classified as normal. Before the final decision, readers had the option of referral to a consensus meeting or to a technical recall (eg, due to poor image quality), or both. Consensus meetings are common practice in screening programmes with double reading; in these meetings, difficult or equivocal findings are reassessed by two radiologists, with a joint decision made to recall or clear of suspicion of malignancy.3 The images acquired at technical recall were by default randomised de novo due to the technical setup; however, participants were assessed according to their originally assigned group. Screening examinations allocated to the intervention group that failed to be processed by AI underwent standard-of-care reading. 16 breast radiologists at the Unilabs Mammography Unit at Skåne University Hospital were involved in the screen reading, of whom 15 had more than 2 years of experience in breast imaging and 14 had more than 5 years of experience. 12 of the radiologists had a yearly reading volume of at least 5000 cases. Three radiologists had a yearly reading volume of 1000–3000 cases, and one radiologist read on average 700 cases per year. Based on the group composition, only readers with more than 2 years of experience were allowed to read from the single-reading worklist. Before each screen-reading session, the radiologist rolled a six-sided die to randomly allocate themselves to either of the two groups: numbers 1–3 allocated them to the control group and 4–6 to the intervention group. Participants could withdraw from the study at any time, at which point all personal data would be removed and they would be excluded from analyses. True positive cases were initially identified through linkage with the Regional Cancer Registry (on Sept 12, 2022); to compensate for a delay in registry reporting, all recalled participants were manually assessed with use of patient records, and true positives were validated by histopathology reports on surgical samples or core-needle biopsies. Outcomes The primary outcome measure of the MASAI trial is interval cancer rate, which will be assessed after the full study population of 100 000 screened participants have had at least a 2-year follow-up (estimated December, 2024). Secondary outcome measures are early screening performance (cancer detection rate, recall rate, false positive rate, and positive predictive value [PPV] of recall), screen-reading workload (number of screen-readings and consensus meetings), detection in relation to tumour type and stage, proportion of interval cancers by cancer type and stage, sensitivity and specificity, and incremental cost-effectiveness ratio. In the current clinical safety analysis, the secondary outcome measures of early screening performance of cancer detection rate (number of cancers detected per 1000 participants screened), recall rate (proportion of screened participants who were recalled), false positive rate, PPV of recall, type of detected cancer (invasive or in situ), and screen-reading workload were assessed. The screen-reading workload was reported as the sum of all screen readings, including those made at consensus meetings. The number and proportion of screenings that resulted in a consensus meetings (consensus meeting rate) were also reported separately. Statistical analysis The intention-to-treat population comprised all participants who underwent breast screening. The modified intentionto-treat (mITT) population comprised participants with a complete screening examination, excluding those who were asked to attend a technical recall but did not attend. Participants recalled due to bilateral enlarged lymph nodes and diagnosed with lymphoma were also excluded from the mITT population, since they were not recalled due to AI triage Risk score 10 Double reading plus AI Intervention group Risk score 1–9 suspicion of breast cancer. Participants were analysed in their allocated group regardless of the actual reading procedure (treatment policy strategy). The hypothesis for the primary analysis was the noninferiority of AI-supported mammography screening compared with standard double reading, in terms of interval cancer rate, with a secondary hypothesis of superiority. Considering the interplay of screen-reading workload and the number of interval cancers, the noninferiority margin for the primary endpoint was set at the intervention yielding at most 20% more interval cancers than in the control group. The sample size calculations were done with use of Fisher’s exact test to compare the risk ratio based on the observed interval cancer rate in the current screening programme. A total sample size of 100 000 (intention-to-treat population) was expected to have at least 80% statistical power to show that the ratio of the interval cancer rate is at most 1·2 in the intervention group compared with the control group. The mITT population among the 100 000 enrolled participants will be used in the primary analysis. The sample size calculation for the clinical safety analysis was based on a worst case scenario of the intervention yielding a cancer detection rate of 3 per 1000 screened participants (based on the assumption that single reading could lead to reduced detection), at which rate the study could be halted, compared with a detection rate of 5 per 1000 screened participants in the control group (reflecting the observed rate in the current screening programme). According to Fisher’s exact test, a sample size of 80 000 (intention-to-treat population) was needed to show with a power greater than 80% that the proportion of detected cancer did not reach the worst case scenario. The mITT population among the 80 000 enrolled participants was used in the clinical safety analysis. Throughout the study, the overall recall rate was monitored as part of the institutional quality assurance reports to ensure that the recall rate did not drop below what was observed in the clinic 6 months before the start of the trial (average recall rate 2·1%), which could indicate a reduction in cancer detection. The number of enrolled participants was monitored monthly. Trial data were extracted from PACS on Sept 12, 2022, which was 1·5 months after roughly 80 000 participants had been enrolled, to ensure sufficient time for the screen reading and initial investigations of recalled participants. Participants were matched with the Regional Cancer Registry on the same day as data extraction and a rapid preliminary analysis of cancer detection was available 1 week later, which was used to inform the decision to continue the trial. Descriptive statistics were used to summarise study population characteristics. Frequencies and percentages were calculated for categorical data. 95% CIs were calculated with the Clopper-Pearson method. The cancer detection rate, recall rate, false positive rate, and PPV of recall were calculated separately for the intervention and control groups. The cancer detection rate was compared with Fisher’s exact test and the ratio of the proportions with corresponding 95% CIs were reported. A two-sided p value of less than 0·05 was considered to indicate statistical significance. The numbers of screen readings and consensus meetings were calculated separately for the intervention and the control groups. In a post-hoc analysis, the distribution of AI risk scores by screening examinations, screen-detected cancers, recalls, and PPV of recall were reported with descriptive statistics. This analysis was included to describe AI performance and no inferential statistical analyses were done. All statistical analyses were done with Stata IC 17.0 software and Python 3.8.5. The trial is registered with ClincialTrials.gov, NCT04838756. Role of the funding source The funders of the study had no role in study design, data collection, data analysis, data interpretation, or writing of the report.\n\nResults\nBetween April 12, 2021, and July 28, 2022, 80 160 women presented for screening and 127 (0·2%) opted out of the trial. 80 033 participants were randomly assigned: 40 003 (50·0%) to undergo AI-supported screening (intervention group) and 40 030 (50·0%) to undergo double reading without AI (control group). 39 996 partici­ pants in the intervention group and 40 024 in the control group were included in the clinical safety analysis (mITT Figure 2: Trial profile AI=artificial intelligence. population; figure 2). The median age of participants in the mITT population was 54·0 years (IQR 46·7–63·9). The age distribution and indication for screening was similar between groups (table 1). The AI system did not to provide a risk score for 306 (0·8%) of 39 996 participants in the intervention group. There were 38 (0·1%) technical recalls among 40 003 participants in the intervention group and 46 (0·1%) among 40 030 participants in the control group. Early screening performance and workload measures are presented in table 2. Based on the rapid preliminary analysis of cancer detection internally reported on Sept 20, 2022, among the 39 996 participants screened with AI, 244 cancers were detected and 861 participants were recalled. Among the 40 024 participants in the control group, 203 cancers were detected and 817 partici­ pants were recalled. The cancer detection rate was 6·1 (95% CI 5·4–6·9) per 1000 participants for AI-supported screening (ie, above the lower safety limit) and 5·1 (4·4–5·8) per 1000 for double reading without AI, a ratio of 1·2 (95% CI 1·0–1·5; p=0·052). The absolute difference in cancer detection per 1000 screened participants was 1·0 (95% CI 0·0–2·1). The false positive rate was the same in both groups. 36 886 fewer screen readings were done in the intervention group than in the control group, representing in a 44·3% reduction in the screen-reading workload. Of the 244 cancers detected in the intervention group, 184 (75%) were invasive, among which 152 (83%) were stage T1 (tumour diameter ≤20 mm). In the control group, 165 (81%) of 203 cancers were invasive, of which 129 (78%) were stage T1. In situ cancers constituted 60 (25%) detected cancers in the intervention group and 38 (19%) in the control group. The distribution of AI risk scores in the intervention group and early screening performance measures per risk score are presented in table 3 (post hoc). The cancer detection rate in the high-risk group (ie, those with a risk score of 10 that underwent double reading) was 72·3 per 1000 participants screened (208 of 2875 participants), a frequency of one cancer per 14 screening examinations. In the high-risk group, 11 (2·6%) of 416 recalls were due to self-reported symptoms. Of the 490 screening examinations flagged as extra high risk by AI (highest 1% risk), 189 (38·6%) were recalled—ie, 22·0% of all 861 recalls in the intervention group. Of the 189 recalled participants classified as being extra high risk, 136 had cancer (PPV of recall 72·0%), resulting in a cancer detection rate of 277·6 per 1000 screening examinations in the extrahigh-risk category. Thus, the 1·2% of screening examinations flagged as extra high risk contained 55·7% of all screen-detected cancers in this group. 36 815 (92·0%) of 39 996 screening examinations were those with risk scores of 1–9 (which underwent single reading), among which there were 440 (1·2%) recalls (51·1% of all 861 recalls), including 114 (25·9%) recalls Intervention group (n=39 996) Control group (n=40 024) Age, years Mean (SD) 55·3 (10·2) 55·3 (10·2) Range 39·6–80·1 39·5–79·9 <45 7568 (18·9%) 7607 (19·0%) 45–49 7155 (17·9%) 7209 (18·0%) 50–54 6505 (16·3%) 6559 (16·4%) 55–59 5021 (12·6%) 4822 (12·0%) 60–64 5007 (12·5%) 5214 (13·0%) 65–69 4345 (10·9%) 4265 (10·7%) ≥70 4395 (11·0%) 4348 (10·9%) Screening indication General screening 38 969 (97·4%) 38 951 (97·3%) History of breast cancer 984 (2·5%) 1017 (2·5%) Moderate hereditary risk 43 (0·1%) 56 (0·1%) Data are mean (SD), range, or n (%). Table 1: Baseline population characteristics, modified intention-to-treat population Intervention group (n=39 996) Control group (n=40 024) Early screening performance Number of recalls 861 817 Recall rate, % 2·2% (2·0–2·3) 2·0% (1·9–2·2) Number of screendetected cancers 244 203 6·1 (5·4–6·9) 5·1 (4·4–5·8) Cancer-detection rate, per 1000 participants screened False positive rate, % 1·5% (1·4–1·7) 1·5% (1·4–1·7) Positive predictive value of recall, % 28·3% (25·3–31·5) 24·8% (21·9–28·0) Workload Number of screen readings 46 345 83 231 Number of consensus meetings 1584 1576 Consensus meeting rate 4·0% (3·8–4·2) 3·9% (3·8–4·1) Data are n or point estimate (95% CI). Table 2: Early screening performance and workload measures, modified intention-to-treat population based on self-reported symptoms. 36 cancers were detected by screening in the single-reading with AI group (14·8% of all 244 screen-detected cancers), with an overall cancer detection rate of 1·0 per 1000 participants screened. There was a considerable difference in cancer detection rate between those with risk scores of 1–7 (0·2 per 1000 participants screened; six cancers detected among 30 464 participants) and those with risk scores of 8–9 (4·7 per 1000; 30 cancers detected among 6351 partici­pants), meaning that, to detect one cancer, radiologists had to read Positive predictive value of recalls Screendetected cancers (n=244) Participants screened (N=39 996) Participants recalled (n=861) 10 2875 (7·2%) 416 (48·3%) 208 (85·2%) 50·0% 9 3212 (8·0%) 116 (13·5%) 23 (9·4%) 19·8% 8 3139 (7·8%) 65 (7·5%) 7 (2·9%) 10·8% 7 3075 (7·7%) 36 (4·2%) 1 (0·4%) 2·8% 6 3193 (8·0%) 41 (4·8%) 1 (0·4%) 2·4% 5 3503 (8·8%) 52 (6·0%) 0 0% 4 3697 (9·2%) 35 (4·1%) 1 (0·4%) 2·9% 3 4247 (10·6%) 30 (3·5%) 1 (0·4%) 3·3% 2 4368 (10·9%) 31 (3·6%) 1 (0·4%) 3·2% 1 8381 (21·0%) 34 (3·9%) 1 (0·4%) 2·9% Missing score 306 (0·8%) 5 (0·6%) 0 0% Data are n (%) or %. Table 3: Distribution of artificial intelligence examination risk scores and early screening performance measures, intervention group (post-hoc analysis) 5000 examinations in the group with scores of 1–7 and 212 examinations in the group with scores of 8–9.\n\nDiscussion\nThis clinical safety analysis showed that a screen-reading procedure using an AI tool to triage screening examinations to single or double reading and with use of AI as detection support in mammography screening was safe, because the cancer detection rate (6·1 per 1000 participants screened) was above the prespecified lower limit for safety, and was similar to that of double reading without AI (5·1 per 1000). The use of AI did not influence the rates of recalls, false positives, or consensus meetings, while the screen-reading workload was reduced by almost half. The MASAI trial aims to answer two key questions on the use of AI in mammography screening. The first question regards whether AI can be safely used to reduce the screen-reading workload with sustained performance, which we addressed in the current report. The second key question regards the effect on screening outcome, with a primary outcome of interval cancer rate, a central indicator of screening efficacy.1,6 The full study population of 100 000 screened participants and a 2-year follow-up is needed to investigate this endpoint. The MASAI trial proposes one of several possible strategies of integrating AI in the screen-reading pathway.13,20 In European screening programmes, in which double reading is standard, AI has been suggested to replace one of the readers, to be used as a standalone reader for examinations with low AI risk scores, to force examinations with high AI risk scores to a consensus meeting or to arbitration, or to automatically recall cases above a specific threshold. Different strategies can also be considered regarding reader access to AI information: having it available at the time of screening or, for example, only at the consensus meeting to limit automation bias. Our strategy was to use AI to triage examinations to single or double reading and to let radiologists have access to AI information in the form of risk scores and CAD marks at the time of screen reading. The rationale underlying this design was to take advantage of the bias introduced by AI. We hypothesised that, in addition to the benefit of CAD marks as detection support, knowledge of disease prevalence would influence radiologists’ operator point and thereby reduce false positives when reading low-risk examinations (in addition to single reading itself leading to fewer false positives4) and reduce false negatives when reading high-risk examinations.21 Access to risk scores and CAD marks did not seem to introduce a detrimental automation bias, since the false positive rate remained unchanged. This finding emphasises the importance of radiologists having the final decision to recall, which, besides reducing false positives, constitutes a practical approach to meeting established medicolegal requirements, as opposed to the current ethical and legal uncertainties of using AI as a standalone reader. However, if results from prospective studies show that use of AI in screen reading is safe, it could potentially lead to overreliance on AI and cause an increased risk of detrimental automation bias over time. In our study, access to AI information also enabled the fast and safe handling of screening examinations with a very high probability of cancer. These examinations were flagged in the PACS worklist and could therefore be prioritised for a timely and scrutinised screen reading. This approach was effective as indicated by the findings that the 1·2% of screening examinations flagged as extra high risk contained 55·7% of all screen-detected cancers in this group. AI-supported screening resulted in 20% more cancers (244 vs 203) being detected than with standard screening. 152 stage T1 invasive cancers were detected in the intervention group compared with 129 in the control group, which might indicate an increase in early detection without the need for supplementary imaging methods. The incremental increase was, however, not as large as that observed with digital breast tomosynthesis screening in a previous study.22 Still, the higher cancer detection with tomosynthesis compared with mammography in screening has not convincingly been shown to translate into a reduction of interval cancers,22 which could question its clinical importance since it is also a more resource-demanding technique. The clinical significance of the additional detected invasive cancers in our study remains to evaluated. The evolution of AI over time could affect all available tests for breast cancer screening, but the use of AI in tomosynthesis screening has not yet been evaluated in a prospective study. cancer is more likely with low-grade cancers, since they might never progress into a clinically relevant event during the patient’s lifetime.23 Hence, the planned characterisation of detected cancers in the full study population will bring some clarity to possible overdiagnosis with AI-supported screening. Fenton and colleagues showed a 34% increase in the detection of in situ cancers (from 1·17 to 1·57 per 1000 screening mammograms, p=0·09) after the implementation of conventional CAD in screening but without a parallel increase in the detection of invasive cancer.24 Conventional CAD was also shown to increase false positives and related costs, and its use in screening could ultimately not be justified.2,24–27 AI thus seems to have improved performance compared with that of conventional CAD, but could still have hypersensitivity to calcifications, a typical presentation of in situ cancers.13 Subsequent screening will show whether the relatively higher detection observed in our trial is a result of screening with a more sensitive technique for the first time (ie, a prevalence effect), causing an initial high incidence that levels out during subsequent screening rounds.28 We found that the benefit of AI-supported screening in terms of screen-reading workload reduction was considerable. The actual time saved was not measured, but, if we assume that a radiologist reads on average 50 screening examinations per hour, it would have taken one radiologist 4·6 months less to read the 46 345 screening examinations in the intervention group compared with the 83 231 in the control group. There was concern about whether AI would lead to an increase in cases referred to consensus meetings, considering the eventual need to discuss CAD findings and the possible reader anxiety arising from single reading. Consensus meetings constitute an important step to increase the specificity, but are resource demanding.3 Contrary to expectations, the proportion of screenings that led to a consensus meeting was not affected by the use of AI. These results are promising and can be used to inform new trials and programme-based evaluations to address the radiologist shortage. However, we still need to improve our understanding of what the implications are for patient outcome—most importantly, the effect on interval cancer rates. We also need to investigate whether the higher detection of small invasive cancers will lead to a subsequent reduction of prognostically significant cancers and whether the frequency of in situ cancers detected will be reduced at subsequent screenings. An analysis of the prognostic characteristics of cancers detected in the full study population of the MASAI trial is underway. Furthermore, AI systems come at a financial cost, and, while the market and its business models might develop, the willingness to pay to reduce the workload must be determined. Costeffectiveness can be determined only when the downstream cost of the intervention has been assessed. screening and can thus provide evidence for clinical implementation. A strength of the study is the close resemblance to a real screening setting, since no exclusion criteria were applied and few people invited for screening opted out. The main limitation of this study is that it was conducted at a single centre in the Swedish screening programme. The study was also limited to the combination of one type of mammography device and one AI system. AI system performance will inevitably vary with technical factors such as AI algorithms and image processing,12 but will probably be of less importance than the variability of radiologists. Our screening strategy emphasises the central role of the radiologist to make the final decision to recall a patient, and the present results are dependent on the performance of the participating radiologists. The radiologists participating in this trial were, overall, moderately to highly experienced in breast imaging, which could also limit the generalisability of our findings to some settings (eg, those with predominantly lessexperienced screen readers, which have been shown to have a higher rate of false positives29), and the qualification for single reading with AI is likely to require revision. Only readers with more than 2 years of experience were allowed to conduct single reading in the intervention group, which could have introduced a bias in reader performance in relation to the control group. Because only one of the 16 participating radiologists had less than 2 years of experience, we do not believe this factor would have had a major influence on the results. Another limitation of the reported results was that the true false positive rate requires a follow-up period in case of later interval cancer diagnosis. Because this is an uncommon event, we do not expect the false positive rate to change substantially. Results from this study must be considered in the context of AI being a constantly evolving field with continuously updated algorithms and machine learning models, and the related challenge of the transparency of these updates.30 To mitigate the implications of these updates and estimate the effect on screening performance, mammograms from the MASAI trial will be stored in a data warehouse to enable a reanalysis with updated algorithms using the trial outcome data as a reference. More importantly, implementing AI systems would require having a monitoring system of algorithm performance in place. In summary, this clinical safety analysis of the MASAI trial, in which an AI system was used to triage screening examinations to single or double reading and as detection support, showed that AI-supported mammography screening can be considered safe, since it resulted in a similar rate of screen-detected cancer—exceeding the lowest acceptable limit for safety—without increasing rates of recalls, false positives, or consensus meetings, and while substantially reducing the screen-reading workload compared with screening by means of standard double reading.\n\nAcknowledgments\nWe thank the funders of the trial: the Swedish Cancer Society (21 1631Pj, 22 0611FE), the Confederation of Regional Cancer Centres (21/00060), and Swedish governmental funding of clinical research (ALF; 2020-Projekt0079, 2022-Projekt0100). We also thank the staff at the Unilabs Mammography Unit at Skåne University Hospital for making this study possible; Unilabs, Sectra, and ScreenPoint Medical for the technical support; and the participants involved in the trial.\n\nContributors KL and AR conceptualised and designed the trial with input from IA and SH. AR did the statistical analysis. KL, AR, VJ, and HS directly accessed and verified the underlying data reported in the manuscript. All authors were involved in data interpretation. KL wrote the first draft of the report with input from AR. All authors revised the report and provided important intellectual content. All authors had full access to all the data in the study and had final responsibility for the decision to submit for publication. Declaration of interests KL has been an advisory board member for Siemens Healthineers and has received lecture honorarium from AstraZeneca. SH is head of BreastScreen Norway at the Cancer Registry of Norway which has a research agreement with Screenpoint Medical. All other authors declare no competing interests. Data sharing De-identified data will be made available upon reasonable request, with investigator support and a signed data access agreement. A proposal should be submitted to be reviewed by the study steering committee. The data are not publicly available due to data protection regulations. The study protocol and statistical analysis plan are available online at https://portal.research.lu.se/en/projects/mammography-screening-withartificial-intelligence. References\n1 Schünemann HJ, Lerda D, Quinn C, et al. Breast cancer screening and diagnosis: a synopsis of the European Breast Guidelines. Ann Intern Med 2020; 172: 46–56. 2 Taylor P, Potts HWW. Computer aids and human second reading as interventions in screening mammography: two systematic reviews to compare effects on cancer detection and recall rate. Eur J Cancer 2008; 44: 798–807. 3 Taylor-Phillips S, Stinton C. Double reading in breast cancer screening: considerations for policy-making. Br J Radiol 2020; 93: 20190610. 4 Posso M, Carles M, Rué M, Puig T, Bonfill X. Cost-effectiveness of double reading versus single reading of mammograms in a breast cancer screening programme. PLoS One 2016; 11: e0159806. 5 Gulland A. Staff shortages are putting UK breast cancer screening “at risk,” survey finds. BMJ 2016; 353: i2350. 6 Houssami N, Hunter K. The epidemiology, radiology and biological characteristics of interval breast cancers in population mammography screening. NPJ Breast Cancer 2017; 3: 12. 7 Hovda T, Hoff SR, Larsen M, Romundstad L, Sahlberg KK, Hofvind S. True and missed interval cancer in organized mammographic screening: a retrospective review study of diagnostic and prior screening mammograms. Acad Radiol 2022; 29 (suppl 1): S180–91. 8 Hofvind S, Skaane P, Vitak B, et al. Influence of review design on percentages of missed interval breast cancers: retrospective study of interval cancers in a population-based screening program. Radiology 2005; 237: 437–43. 9 Sechopoulos I, Teuwen J, Mann R. Artificial intelligence for breast cancer detection in mammography and digital breast tomosynthesis: state of the art. Semin Cancer Biol 2021; 72: 214–25. 10 Rodriguez-Ruiz A, Lång K, Gubern-Merida A, et al. Stand-alone artificial intelligence for breast cancer detection in mammography: comparison with 101 radiologists. J Natl Cancer Inst 2019; 111: 916–22. 11 McKinney SM, Sieniek M, Godbole V, et al. International evaluation of an AI system for breast cancer screening. Nature 2020; 577: 89–94. 12 Salim M, Wåhlin E, Dembrower K, et al. External evaluation of 3 commercial artificial intelligence algorithms for independent assessment of screening mammograms. JAMA Oncol 2020; 6: 1581–88. 13 Freeman K, Geppert J, Stinton C, et al. Use of artificial intelligence for image analysis in breast cancer screening programmes: systematic review of test accuracy. BMJ 2021; 374: n1872. 14 Rodriguez-Ruiz A, Lång K, Gubern-Merida A, et al. Can we reduce the workload of mammographic screening by automatic identification of normal exams with artificial intelligence? A feasibility study. Eur Radiol 2019; 29: 4825–32. 15 Dembrower K, Wåhlin E, Liu Y, et al. Effect of artificial intelligencebased triaging of breast cancer screening mammograms on cancer detection and radiologist workload: a retrospective simulation study. Lancet Digit Health 2020; 2: e468–74. 16 Larsen M, Aglen CF, Lee CI, et al. Artificial intelligence evaluation of 122 969 mammography examinations from a population-based screening program. Radiology 2022; 303: 502–11. 17 Lauritzen AD, Rodríguez-Ruiz A, von Euler-Chelpin MC, et al. An artificial intelligence-based mammography screening protocol for breast cancer: outcome and radiologist workload. Radiology 2022; 304: 41–49. 18 Lång K, Hofvind S, Rodríguez-Ruiz A, Andersson I. Can artificial intelligence reduce the interval cancer rate in mammography screening? Eur Radiol 2021; 31: 5940–47. 19 Byng D, Strauch B, Gnas L, et al. AI-based prevention of interval cancers in a national mammography screening program. Eur J Radiol 2022; 152: 110321. 20\t Larsen M, Aglen CF, Hoff SR, Lund-Hanssen H, Hofvind S. Possible strategies for use of artificial intelligence in screen-reading of mammograms, based on retrospective data from 122,969 screening examinations. Eur Radiol 2022; 32: 8238–46. 21 Evans KK, Birdwell RL, Wolfe JM. If you don’t find it often, you often don’t find it: why some cancers are missed in breast cancer screening. PLoS One 2013; 8: e64366. 22\t Houssami N, Hofvind S, Soerensen AL, et al. Interval breast cancer rates for digital breast tomosynthesis versus digital mammography population screening: an individual participant data meta-analysis. eClinicalMedicine 2021; 34: 100804. 23\t van Luijt PA, Heijnsdijk EAM, Fracheboud J, et al. The distribution of ductal carcinoma in situ (DCIS) grade in 4232 women and its impact on overdiagnosis in breast cancer screening. Breast Cancer Res 2016; 18: 47. 24 Fenton JJ, Taplin SH, Carney PA, et al. Influence of computer-aided detection on performance of screening mammography. N Engl J Med 2007; 356: 1399–409. 25\t Lehman CD, Wellman RD, Buist DSM, Kerlikowske K, Tosteson ANA, Miglioretti DL. Diagnostic accuracy of digital screening mammography with and without computer-aided detection. JAMA Intern Med 2015; 175: 1828–37. 26\t Elmore JG, Lee CI. Artificial Intelligence in medical imaging: learning from past mistakes in mammography. JAMA Health Forum 2022; 3: e215207. 27 Fenton JJ. Is it time to stop paying for computer-aided mammography? JAMA Intern Med 2015; 175: 1837–38. 28\t Gur D, Nishikawa RM, Sumkin JH. New screening technologies and practices: a different approach to estimation of performance improvement by using data from the transition period. Radiology 2015; 275: 9–12. 29\t Alberdi RZ, Llanes AB, Ortega RA, et al. Effect of radiologist experience on the risk of false-positive results in breast cancer screening programs. Eur Radiol 2011; 21: 2083–90. 30\t Larsson SHF, Heintz F. Transparency in artificial intelligence. Internet Policy Rev 2020; 9: 1469.\n--- FOOTNOTES Radiology, Department of of Oncology, Department of (A-M Larsson PhD); Department Correspondence to: Department of Translational", "persona": "Student", "response": "### 1. Overview\nAn interim safety analysis of the first randomized controlled trial of its kind involving over 80,000 Swedish women published in The Lancet Oncology journal, finds artificial intelligence (AI)-supported mammography analysis is as good as two breast radiologists working together to detect breast cancer, without increasing false positives and almost halving the screen-reading workload. However, the final trial results looking at whether the use of AI in interpreting mammography images translates into a reduction in interval cancers (cancers detected between screenings that generally have a poorer prognosis than screen-detected cancers) in 100,000 women followed over two years—and ultimately whether AI's use in mammography screening is justified—are not expected for several years. \"These promising interim safety results should be used to inform new trials and program-based evaluations to address the pronounced radiologist shortage in many countries. But they are not enough on their own to confirm that AI is ready to be implemented in mammography screening,\" cautions lead author Dr. Kristina Lång from Lund University, Sweden. In the AI-supported analysis, the AI system first analyzed the mammography image and predicted the risk of cancer on a scale of one to 10, with one representing the lowest risk and 10 the highest. If the risk score was less than 10 the image was further analyzed by one radiologist, whereas if the AI system predicted a risk score of 10 then two radiologists analyzed the image.\n\n### 2. Background\nImportantly, there were 36,886 fewer screen readings by radiologists in the AI-supported group than in the control group (46,345 vs. 83,231), resulting in a 44% reduction in the screen-reading workload of radiologists. Although the actual time saved by using AI was not measured in the trial, the researchers calculate that if a radiologist reads on average 50 mammograms an hour, it would have taken one radiologist 4.6 months less to read the roughly 40,000 screening examinations with the help of AI compared with the roughly 40,000 in the control arm that were double read. \"The greatest potential of AI right now is that it could allow radiologists to be less burdened by the excessive amount of reading,\" says Lång. \"While our AI-supported screening system requires at least one radiologist in charge of detection, it could potentially do away with the need for double reading of the majority of mammograms easing the pressure on workloads and enabling radiologists to focus on more advanced diagnostics while shortening waiting times for patients.\" Despite the promising findings, the authors note several limitations including that the analysis was conducted at a single center and was limited to one type of mammography device and one AI system which might limit the generalizability of the results.\n\n### 3. Methods\nEuropean guidelines recommend double reading of screening mammograms by two radiologists to ensure high sensitivity (to correctly identify those with disease). But there is a shortage of breast radiologists in many countries, including a shortfall of around 41 (8%) in the UK in 2020 and about 50 in Sweden, and it takes over a decade to train a radiologist capable of interpreting mammograms. Between April 2021 and July 2022, 80,033 women aged 40-80 years who had undergone mammogram screening at four sites in southwest Sweden were randomly assigned in a 1:1 ratio to either AI-supported analysis, where a commercially available AI-supported mammogram reading system analyzed the mammograms before they were also read by one or two radiologists (intervention arm), or standard analysis performed by two radiologists without AI (control arm). The system also provided CAD marks to assist radiologists in accurately interpreting mammography images. Women were recalled for additional testing based on suspicious findings. Radiologists had the final decision to recall women and were instructed to recall cases with the highest 1% risk, except for obvious false positives. Writing in a urled Comment, Dr. Nereo Segnan, former Head of the Unit of Cancer Epidemiology and past Director of Department of Screening at CPO Piemonte in Italy (who was not involved in the study) notes that the AI risk score for breast cancer seems very accurate at being able to separate high risk from low-risk women, adding that, \"In risk stratified screening protocols, the potential for appropriately modulating the criteria for recall in low-risk and high-risk groups is remarkable.\"\n\n### 4. Results\nAI failed to provide a risk score in 0.8% of cases (306/39,996) that were referred to standard care (double reading). The recall rates averaged 2.2% (861 women) for AI-supported screening and 2.0% (817 women) for standard double reading without AI. These were similar to the average 2.1% recall rate in the clinic six months prior to the trial starting, indicating that cancer detection rates had not fallen. In total, 244 women (28%) recalled from AI-supported screening were found to have cancer compared with 203 women (25%) recalled from standard screening—resulting in 41 more cancers detected with the support of AI (of which 19 were invasive and 22 were in situ cancers). The false-positive rate was 1.5% in both arms.\n\n### 5. Takeaways\n\"We still need to understand the implications on patients' outcomes, especially whether combining radiologists' expertise with AI can help detect interval cancers that are often missed by traditional screening, as well as the cost-effectiveness of the technology.\" This interim analysis of the Mammography Screening with Artificial Intelligence (MASAI) trial compared early screening performance (e.g., cancer detection, recalls, false positives) and screen-reading workload in the two arms. The MASAI trial will continue to establish primary outcome results of whether AI-supported mammography screening reduces interval cancers. The lowest acceptable limit for clinical safety in the intervention group was set at a cancer detection rate above three cancers per 1,000 screened women. This was based on the premise that the cancer detection rate might decline because the majority of screening examinations would undergo single reading instead of double reading. The baseline detection rate in the current screening program with double reading is five cancers per 1,000 screened women. Overall, AI-supported screening resulted in a cancer detection rate of six per 1,000 screened women compared to five per 1,000 for standard double reading without AI—equivalent to detecting one additional cancer for every 1,000 women screened. However, he cautions, \"In the AI-supported screening group of the MASAI trial, the possible presence of over diagnosis (i.e., the system identifying non-cancers) or over-detection of indolent lesions, such as a relevant portion of ductal carcinomas in situ, should prompt caution in the interpretation of results that otherwise seem straightforward in favoring the use of AI...It is, therefore, important to acquire biological information on the detected lesions. The final results of the MASAI trial are expected to do so, as the characteristics of identified cancers and the rate of interval cancers—not just the detection rate—are indicated as main outcomes. An important research question thus remains: is AI, when appropriately trained, able to capture relevant biological features—or, in other words, the natural history of the disease—such as the capacity of tumors to grow and disseminate?\"", "paper_id": "54", "story_id": "54-01"}
{"paper": "Abstract\nWe show for the ﬁrst time that learning powerful representations from speech audio alone followed by ﬁne-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task deﬁned over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.1 1\n\nIntroduction\nNeural networks beneﬁt from large quantities of labeled training data. However, in many settings labeled data is much harder to come by than unlabeled data: current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide [31]. Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them - a process that requires learning good representations of speech. In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to ﬁne-tune the model on labeled data. This has been particularly successful for natural language processing [43, 45, 9] and is an active research area for computer vision [20, 2, 36, 19, 6]. In this paper, we present a framework for self-supervised learning of representations from raw audio data. Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations [26, 56], similar to masked language modeling [9]. The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors [54, 49, 48, 28] (§ 2). As part of training, we learn discrete speech units [53, 32, 7, 18] via a gumbel softmax [24, 5] to represent the latent representations in the contrastive task (Figure 1) which we ﬁnd to be more effective than non-quantized targets. After pre-training on unlabeled speech, the model is ﬁne-tuned Context representations C … … Quantized representations Q q q q q q Latent speech representations Z X raw waveform Figure 1: Illustration of our framework which jointly learns contextualized speech representations and an inventory of discretized speech units. on labeled data with a Connectionist Temporal Classiﬁcation (CTC) loss [14, 4] to be used for downstream speech recognition tasks (§ 3) Previous work learned a quantization of the data followed by a contextualized representations with a self-attention model [5, 4], whereas our approach solves both problems end-to-end. Masking parts of the input with Transformer networks for speech has been explored [4, 26], but prior work relies either on a two-step pipeline or their model is trained by reconstructing the ﬁlter bank input features. Other related work includes learning representations from auto-encoding the input data [52, 11] or directly predicting future timesteps [8]. Our results show that jointly learning discrete speech units with contextualized representations achieves substantially better results than ﬁxed units learned in a prior step [4]. We also demonstrate the feasibility of ultra-low resource speech recognition: when using only 10 minutes of labeled data, our approach achieves word error rate (WER) 4.8/8.2 on the clean/other test sets of Librispeech. We set a new state of the art on TIMIT phoneme recognition as well as the 100 hour clean subset of Librispeech. Moreover, when we lower the amount of labeled data to just one hour, we still outperform the previous state of the art self-training method of [42] while using 100 times less labeled data and the same amount of unlabeled data. When we use all 960 hours of labeled data from Librispeech, then our model achieves 1.8/3.3 WER (§ 4, § 5).\n\n2 Model Our model is composed of a multi-layer convolutional feature encoder f : X 7→Z which takes as input raw audio X and outputs latent speech representations z1, . . . , zT for T time-steps. They are then fed to a Transformer g : Z 7→C to build representations c1, . . . , cT capturing information from the entire sequence [9, 5, 4]. The output of the feature encoder is discretized to qt with a quantization module Z 7→Q to represent the targets (Figure 1) in the self-supervised objective (§ 3.2). Compared to vq-wav2vec [5], our model builds context representations over continuous speech representations and self-attention captures dependencies over the entire sequence of latent representations end-to-end. Feature encoder. The encoder consists of several blocks containing a temporal convolution followed by layer normalization [1] and a GELU activation function [21]. The raw waveform input to the encoder is normalized to zero mean and unit variance. The total stride of the encoder determines the number of time-steps T which are input to the Transformer (§ 4.2). Contextualized representations with Transformers. The output of the feature encoder is fed to a context network which follows the Transformer architecture [55, 9, 33]. Instead of ﬁxed positional embeddings which encode absolute positional information, we use a convolutional layer similar to [37, 4, 57] which acts as relative positional embedding. We add the output of the convolution followed by a GELU to the inputs and then apply layer normalization. ` Contrastive loss L Transformer Masked CNN results in prior work which learned discrete units in a ﬁrst step followed by learning contextualized representations [5]. Product quantization amounts to choosing quantized representations from multiple codebooks and concatenating them. Given G codebooks, or groups, with V entries e ∈ RV ×d/G, we choose one entry from each codebook and concatenate the resulting vectors e1, . . . , eG and apply a linear transformation Rd 7→Rf to obtain q ∈Rf. The Gumbel softmax enables choosing discrete codebook entries in a fully differentiable way [16, 24, 35]. We use the straight-through estimator [26] and setup G hard Gumbel softmax operations [24]. The feature encoder output z is mapped to l ∈RG×V logits and the probabilities for choosing the v-th codebook entry for group g are pg,v = exp(lg,v + nv)/τ PV k=1 exp(lg,k + nk)/τ , (1) where τ is a non-negative temperature, n = −log(−log(u)) and u are uniform samples from U(0, 1). During the forward pass, codeword i is chosen by i = argmaxjpg,j and in the backward pass, the true gradient of the Gumbel softmax outputs is used. 3 Training To pre-train the model we mask a certain proportion of time steps in the latent feature encoder space (§ 3.1), similar to masked language modeling in BERT [9]. The training objective requires identifying the correct quantized latent audio representation in a set of distractors for each masked time step (§ 3.2) and the ﬁnal model is ﬁne-tuned on the labeled data (§ 3.3). 3.1 Masking We mask a proportion of the feature encoder outputs, or time steps before feeding them to the context network and replace them with a trained feature vector shared between all masked time steps; we do not mask inputs to the quantization module. To mask the latent speech representations output by the encoder, we randomly sample without replacement a certain proportion p of all time steps to be starting indices and then mask the subsequent M consecutive time steps from every sampled index; spans may overlap. 3.2 Objective During pre-training, we learn representations of speech audio by solving a contrastive task Lm which requires to identify the true quantized latent speech representation for a masked time step within a set of distractors. This is augmented by a codebook diversity loss Ld to encourage the model to use the codebook entries equally often. L = Lm + αLd (2) where α is a tuned hyperparameter. Contrastive Loss. Given context network output ct centered over masked time step t, the model needs to identify the true quantized latent speech representation qt in a set of K + 1 quantized candidate representations ˜q ∈Qt which includes qt and K distractors [23, 54]. Distractors are uniformly sampled from other masked time steps of the same utterance. The loss is deﬁned as Lm = −log exp(sim(ct, qt)/κ) P ˜q∼Qt exp(sim(ct, ˜q)/κ) (3) where we compute the cosine similarity sim(a, b) = aT b/∥a∥∥b∥between context representations and quantized latent speech representations [19, 6]. codebook ¯pg across a batch of utterances; the softmax disribution does not contain the gumbel noise nor a temperature:2 G X Ld = 1 GV g=1 −H(¯pg) = 1 GV 3.3 Fine-tuning Pre-trained models are ﬁne-tuned for speech recognition by adding a randomly initialized linear projection on top of the context network into C classes representing the vocabulary of the task [4]. For Librispeech, we have 29 tokens for character targets plus a word boundary token. Models are optimized by minimizing a CTC loss [14] and we apply a modiﬁed version of SpecAugment [41] by masking to time-steps and channels during training which delays overﬁtting and signiﬁcantly improves the ﬁnal error rates, especially on the Libri-light subsets with few labeled examples. 4 Experimental Setup 4.1 Datasets As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio. We ﬁne-tune on ﬁve labeled data settings: 960 hours of transcribed Librispeech, the train-clean-100 subset comprising 100 hours (100 hours labeled), as well as the Libri-light limited resource training subsets originally extracted from Librispeech, these are train-10h (10 hours labeled), train-1h (1 hour labeled), train-10min (10 min labeled). We follow the evaluation protocol of Libri-light for these splits and evaluate on the standard Librispech dev-other/clean and test-clean/other sets. We ﬁne-tune the pre-trained models for phoneme recognition on the TIMIT dataset [13]. It contains ﬁve hours of audio recordings with detailed phoneme labels. We use the standard train, dev and test split and follow the standard protocol of collapsing phone labels to 39 classes. 4.2 Pre-training Models are implemented in fairseq [39]. For masking, we sample p = 0.065 of all time-steps to be starting indices and mask the subsequent M = 10 time-steps. This results in approximately 49% of all time steps to be masked with a mean span length of 14.7, or 299ms (see Appendix A for more details on masking). The feature encoder contains seven blocks and the temporal convolutions in each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2). This results in an encoder output frequency of 49 hz with a stride of about 20ms between each sample, and a receptive ﬁeld of 400 input samples or 25ms of audio. The convolutional layer modeling relative positional embeddings has kernel size 128 and 16 groups. We experiment with two model conﬁgurations which use the same encoder architecture but differ in the Transformer setup: BASE contains 12 transformer blocks, model dimension 768, inner dimension (FFN) 3,072 and 8 attention heads. Batches are built by cropping 250k audio samples, or 15.6sec, from each example. Crops are batched together to not exceed 1.4m samples per GPU and we train on a total of 64 V100 GPUs for 1.6 days [38]; the total batch size is 1.6h. The LARGE model contains 24 transformer blocks with model dimension 1,024, inner dimension 4,096 and 16 attention heads. We crop 320k audio samples, or 20sec, with a limit of 1.2m samples per GPU and train on 128 V100 GPUs over 2.3 days for Librispeech and 5.2 days for LibriVox; the total batch size is 2.7h. We use dropout 0.1 in the Transformer, at the output of the feature encoder and the input to the quantization module. Layers are dropped at a rate of 0.05 for BASE and 0.2 for LARGE [22, 12]; there is no layer drop for LV-60k. G X V X v=1 ¯pg,v log ¯pg,v (4) g=1 We optimize with Adam [29], warming up the learning rate for the ﬁrst 8% of updates to a peak of 5 × 10−4 for BASE and 3 × 10−4 for LARGE, and then linearly decay it. LARGE trains for 250k updates, BASE for 400k updates, and LARGE on LV-60k for 600k updates. We use weight α = 0.1 for the diversity loss Equation 2. For the quantization module we use G = 2 and V = 320 for both models, resulting in a theoretical maximum of 102.4k codewords. Entries are of size d/G = 128 for BASE amd d/G = 384 for LARGE. The Gumbel softmax temperature τ is annealed from 2 to a minimum of 0.5 for BASE and 0.1 for LARGE by a factor of 0.999995 at every update. The temperature in the contrastive loss (Equation 3) is set to κ = 0.1. For the smaller Librispeech dataset, we regularize the model by applying an L2 penalty to the activations of the ﬁnal layer of the feature encoder and scale down the gradients for the encoder by a factor of 10. We also use a slightly different encoder architecture where we do not use layer normalization, and instead of normalizing the raw waveform, the output of the ﬁrst encoder layer is normalized. In the contrastive loss we use K = 100 distractors. We choose the training checkpoint with the lowest Lm on the validation set. 4.3 Fine-tuning After pre-training we ﬁne-tune the learned representations on labeled data and add a randomly initialized output layer on top of the Transformer to predict characters (Librispeech/Libri-light) or phonemes (TIMIT). For Libri-light, we train three seeds with two different learning rates (2e-5 and 3e-5) for all subsets and choose the conﬁguration with lowest WER on dev-other subset decoded with the ofﬁcial 4-gram language model (LM) with beam 50 and ﬁxed model weights (LM weight 2, word insertion penalty -1). For BASE on the labeled 960h subset we use a learning rate of 1e-4. We optimize with Adam and a tri-state rate schedule where the learning rate is warmed up for the ﬁrst 10% of updates, held constant for the next 40% and then linearly decayed for the remainder. BASE uses a batch size of 3.2m samples per GPU and we ﬁne-tune on 8 GPUs, giving a total batch size of 1,600sec. LARGE batches 1.28m samples on each GPU and we ﬁne-tune on 24 GPUs, resulting in an effective batch size of 1,920sec. For the ﬁrst 10k updates only the output classiﬁer is trained, after which the Transformer is also updated. The feature encoder is not trained during ﬁne-tuning. We mask the feature encoder representations with a strategy similar to SpecAugment [41] detailed in Appendix B. 4.4 Language Models and Decoding We consider two types of language models (LM): a 4-gram model and a Transformer [3] trained on the Librispeech LM corpus. The Transformer LM is identical to [51] and contains 20 blocks, model dimension 1,280, inner dimension 6,144 and 16 attention heads. We tune the weights of the language model (interval [0, 5]) and a word insertion penalty ([−5, 5]) via Bayesian optimization3: we run 128 trials with beam 500 for the 4-gram LM and beam 50 for the Transformer LM and choose the best set of weights according to performance on dev-other. Test performance is measured with beam 1,500 for the n-gram LM and beam 500 for the Transformer LM. We use the beam search decoder of [44]. 5\n\nResults\n5.1 Low-Resource Labeled Data Evaluation We ﬁrst evaluate our pre-trained models in settings where the amount of labeled data is limited to get a sense of how the representations learned on unlabeled data can improve low resource settings. If a pre-trained model captures the structure of speech, then it should require few labeled examples to ﬁne-tune it for speech recognition. The models are pre-trained on the audio data of either Librispeech (LS-960) or LibriVox (LV-60k) and most results are obtained by decoding with a Transformer language model (Transf.); Appendix C shows results with no language model at all as well as with an n-gram language model. The LARGE model pre-trained on LV-60k and ﬁne-tuned on only 10 minutes of labeled data achieves a word error rate of 5.2/8.6 on the Librispeech clean/other test sets. Ten minutes of labeled data corresponds to just 48 recordings with an average length of 12.5 seconds. This demonstrates that ultra-low resource speech recognition is possible with self-supervised learning on unlabeled data. Table 1: WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech. Models use either the audio of Librispeech (LS-960) or the larger LibriVox (LV-60k) as unlabeled data. We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters). Prior work used 860 unlabeled hours (LS-860) but the total with labeled data is 960 hours and comparable to our setup. Model Unlabeled LM dev test data clean other clean other 10 min labeled Discrete BERT [4] LS-960 4-gram 15.7 24.1 16.3 25.2 BASE LS-960 4-gram 8.9 15.7 9.1 15.6 Transf. 6.6 13.2 6.9 12.9 LARGE LS-960 Transf. 6.6 10.6 6.8 10.8 LV-60k Transf. 4.6 7.9 4.8 8.2 1h labeled Discrete BERT [4] LS-960 4-gram 8.5 16.4 9.0 17.6 BASE LS-960 4-gram 5.0 10.8 5.5 11.3 Transf. 3.8 9.0 4.0 9.3 LARGE LS-960 Transf. 3.8 7.1 3.9 7.6 LV-60k Transf. 2.9 5.4 2.9 5.8 10h labeled Discrete BERT [4] LS-960 4-gram 5.3 13.2 5.9 14.1 Iter. pseudo-labeling [58] LS-960 4-gram+Transf. 23.51 25.48 24.37 26.02 LV-60k 4-gram+Transf. 17.00 19.34 18.03 19.92 BASE LS-960 4-gram 3.8 9.1 4.3 9.5 Transf. 2.9 7.4 3.2 7.8 LARGE LS-960 Transf. 2.9 5.7 3.2 6.1 LV-60k Transf. 2.4 4.8 2.6 4.9 100h labeled Hybrid DNN/HMM [34] - 4-gram 5.0 19.5 5.8 18.6 TTS data augm. [30] - LSTM 4.3 13.5 Discrete BERT [4] LS-960 4-gram 4.0 10.9 4.5 12.1 Iter. pseudo-labeling [58] LS-860 4-gram+Transf. 4.98 7.97 5.59 8.95 LV-60k 4-gram+Transf. 3.19 6.14 3.72 7.11 Noisy student [42] LS-860 LSTM 3.9 8.8 4.2 8.6 BASE LS-960 4-gram 2.7 7.9 3.4 8.0 Transf. 2.2 6.3 2.6 6.3 LARGE LS-960 Transf. 2.1 4.8 2.3 5.0 LV-60k Transf. 1.9 4.0 2.0 4.0 Our approach of jointly learning discrete units and contextualized representations clearly improves over previous work which learned quantized audio units in a separate step [4], reducing WER by a about a third. A recent iterative self-training approach [42] represents the state of the art on the clean 100 hour subset of Librispeech but it requires multiple iterations of labeling, ﬁltering, and re-training. Our approach is simpler: we pre-train on the unlabeled data and ﬁne-tune on the labeled data. On the 100 hour subset of Librispeech, their method achieves WER 4.2/8.6 on test-clean/other which compares to WER 2.3/5.0 with the LARGE model in a like for like setup, a relative WER reduction of 45%/42%. Table 2: WER on Librispeech when using all 960 hours of labeled data (cf. Table 1). Model Unlabeled LM dev test data clean other clean other Supervised CTC Transf [51] - CLM+Transf. 2.20 4.94 2.47 5.45 S2S Transf. [51] - CLM+Transf. 2.10 4.79 2.33 5.17 Transf. Transducer [60] - Transf. - - 2.0 4.6 ContextNet [17] - LSTM 1.9 3.9 1.9 4.1 Conformer [15] - LSTM 2.1 4.3 1.9 3.9 Semi-supervised CTC Transf. + PL [51] LV-60k CLM+Transf. 2.10 4.79 2.33 4.54 S2S Transf. + PL [51] LV-60k CLM+Transf. 2.00 3.65 2.09 4.11 Iter. pseudo-labeling [58] LV-60k 4-gram+Transf. 1.85 3.26 2.10 4.01 Noisy student [42] LV-60k LSTM 1.6 3.4 1.7 3.4 This work LARGE - from scratch - Transf. 1.7 4.3 2.1 4.6 BASE LS-960 Transf. 1.8 4.7 2.1 4.8 LARGE LS-960 Transf. 1.7 3.9 2.0 4.1 LV-60k Transf. 1.6 3.0 1.8 3.3 light data splits contain both clean and noisy data leading to better accuracy on test-other compared to test-clean. Increasing model size reduces WER on all setups with the largest improvements on test-other (BASE vs. LARGE both on LS-960) and increasing the amount of unlabeled training data also leads to large improvements (LARGE LS-960 vs. LV-60k). 5.2 High-Resource Labeled Data Evaluation on Librispeech In this section we evaluate the performance when large quantities of labeled speech are available to assess the effectiveness of our approach in a high resource setup. Speciﬁcally, we ﬁne-tune the same models as before on the full 960 hours of labeled Librispeech: BASE and LARGE pre-trained on LS-960 as well as LARGE pre-trained on LV-60k. Table 2 shows that our approach achieves WER 1.8/3.3 on test-clean/other on the full Librispeech benchmark. This is despite a weaker baseline architecture: supervised training of our architecture achieves WER 2.1/4.6 (LARGE - from scratch) compared to WER 1.9/4.1 for ContextNet [17], the baseline architecture of the state of the art [42]. We use a simple Transformer with CTC which does not perform as well as seq2seq models [51]. Note that the vocabulary of our acoustic model (characters) does not match the vocabulary of the LM (words) which delays feedback from the LM and is likely to be detrimental. Most recent work [51, 58, 17, 42] uses the better performing word pieces [50] for both models. Moreover, our result is achieved without any data balancing such as [42]. Finally, self-training is likely complimentary to pre-training and their combination may yield even better results. Appendix E presents a detailed error analysis of our pre-trained models in various labeled data setups. 5.3 Phoneme Recognition on TIMIT Next, we evaluate accuracy on TIMIT phoneme recognition by ﬁne-tuning the pre-trained models on the labeled TIMIT training data. We ﬁne-tune as for the 10 hour subset of Libri-light but do not use a language model. Table 3 shows that our approach can achieve a new state of the art on this dataset, reducing PER by a relative 23%/29% over the next best result on the dev/test sets. Appendix D shows an analysis of how the discrete latent speech representations related to phonemes. Other recent work on pre-training which evaluates on TIMIT includes [47] who solve multiple tasks to learn good representations of speech. Table 3: TIMIT phoneme recognition accuracy in terms of phoneme error rate (PER). CNN + TD-ﬁlterbanks [59] 15.6 18.0 PASE+ [47] - 17.2 Li-GRU + fMLLR [46] – 14.9 wav2vec [49] 12.9 14.7 vq-wav2vec [5] 9.6 11.6 This work (no LM) LARGE (LS-960) 7.4 8.3 Table 4: Average WER and standard deviation on combined dev-clean/other of Librispeech for three training seeds. We ablate quantizing the context network input and the targets in the contrastive loss. Continuous inputs, quantized targets (Baseline) 7.97 0.02 Quantized inputs, quantized targets 12.18 0.41 Quantized inputs, continuous targets 11.18 0.16 Continuous inputs, continuous targets 8.58 0.08 5.4 Ablations A difference to previous work [5, 4] is that we quantize the latent audio representations only for the contrastive loss, i.e., when latents are used as targets, but not when the latents are input to the Transformer network. We motivate this choice by an ablating for which we adopt a reduced training setup to increase experimental turn around: we pre-train BASE on LS-960 for 250k updates with masking probability p = 0.075, ﬁne-tune on train-10h for 60k updates on a single GPU with 640k samples per batch, or 40 sec of speech audio. We report the average WER and standard deviation on the concatenation of dev-clean and dev-other (dev PER) for three seeds of ﬁne-tuning. Table 4 shows that our strategy of continuous inputs with quantized targets (Baseline) performs best. Continuous latent speech representations retain more information to enable better context representations and quantizing the target representations leads to more robust training. Quantizing the latents both in the input and the targets performs least well, and explains the lower performance of prior work [5, 4]. Continuous targets reduce the effectiveness of self-supervised training since targets can capture detailed artifacts of the current sequence, e.g. speaker and background information, which make the task easier and prevent the model from learning general representations beneﬁcial to speech recognition. The training accuracy of identifying the correct latent audio representation increases from 62% to 78.0% when switching from quantized to continuous targets. Continuous inputs and continuous targets perform second best but various attempts to improve it did not lead to better results (see Appendix F for this experiment and other ablations on various hyperparameters). 6\n\nConclusion\nWe presented wav2vec 2.0, a framework for self-supervised learning of speech representations which masks latent representations of the raw waveform and solves a contrastive task over quantized speech representations. Our experiments show the large potential of pre-training on unlabeled data for speech processing: when using only 10 minutes of labeled training data, or 48 recordings of 12.5 seconds on average, we achieve a WER of 4.8/8.2 on test-clean/other of Librispeech. Our model achieves results which achieve a new state of the art on the full Librispeech benchmark for noisy speech. On the clean 100 hour Librispeech setup, wav2vec 2.0 outperforms the previous best result while using 100 times less labeled data. The approach is also effective when large amounts of labeled data are available. We expect performance gains by switching to a seq2seq architecture and a word piece vocabulary. dev PER test PER avg. WER std. Broader Impact There are around 7,000 languages in the world and many more dialects. However, for most of them no speech recognition technology exists since current systems require hundreds or thousands of hours of labeled data which is hard to collect for most languages. We have shown that speech recognition models can be built with very small amounts of annotated data at very good accuracy. We hope our work will make speech recognition technology more broadly available to many more languages and dialects.\n\nAcknowledgments\nWe thank Tatiana Likhomanenko and Qiantong Xu for helpful discussion and their help with wav2letter integration.\n\nReferences\n[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv, 2016. [2] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual information across views. In Proc. of NeurIPS, 2019. [3] A. Baevski and M. Auli. Adaptive input representations for neural language modeling. In Proc. of ICLR, 2018. [4] A. Baevski, M. Auli, and A. Mohamed. Effectiveness of self-supervised pre-training for speech recognition. arXiv, abs/1911.03912, 2019. [5] A. Baevski, S. Schneider, and M. Auli. vq-wav2vec: Self-supervised learning of discrete speech representations. In Proc. of ICLR, 2020. [6] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. arXiv, abs/2002.05709, 2020. [7] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord. Unsupervised speech representation learning using wavenet autoencoders. arXiv, abs/1901.08810, 2019. [8] Y. Chung, W. Hsu, H. Tang, and J. R. Glass. An unsupervised autoregressive model for speech representation learning. arXiv, abs/1904.03240, 2019. [9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv, abs/1810.04805, 2018. [10] S. Dieleman, A. van den Oord, and K. Simonyan. The challenge of realistic music generation: modelling raw audio at scale. arXiv, 2018. [11] R. Eloff, A. Nortje, B. van Niekerk, A. Govender, L. Nortje, A. Pretorius, E. Van Biljon,\nE. van der Westhuizen, L. van Staden, and H. Kamper. Unsupervised acoustic unit discovery for\nspeech synthesis using discrete latent-variable neural networks. arXiv, abs/1904.07556, 2019. [12] A. Fan, E. Grave, and A. Joulin. Reducing transformer depth on demand with structured dropout. In Proc. of ICLR, 2020. [13] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM. Linguistic Data Consortium, 1993. [14] A. Graves, S. Fernández, and F. Gomez. Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In Proc. of ICML, 2006. [15] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang. Conformer: Convolution-augmented transformer for speech recognition. arXiv, 2020. [16] E. J. Gumbel. Statistical theory of extreme values and some practical applications: a series of lectures, volume 33. US Government Printing Ofﬁce, 1954. [17] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati, R. Pang, and Y. Wu. Contextnet: Improving convolutional neural networks for automatic speech recognition with global context. arXiv, 2020. [18] D. Harwath, W.-N. Hsu, and J. Glass. Learning hierarchical discrete linguistic units from visually-grounded speech. In Proc. of ICLR, 2020. [19] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. arXiv, abs/1911.05722, 2019. [20] O. J. Hénaff, A. Razavi, C. Doersch, S. M. A. Eslami, and A. van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv, abs/1905.09272, 2019. [21] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv, 2016. [22] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger. Deep networks with stochastic depth. arXiv, 2016. [23] M. G. A. Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proc. of AISTATS, 2010. [24] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv, abs/1611.01144, 2016. [25] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117–128, Jan. 2011. [26] D. Jiang, X. Lei, W. Li, N. Luo, Y. Hu, W. Zou, and X. Li. Improving transformer-based speech recognition using unsupervised pre-training. arXiv, abs/1910.09932, 2019. [27] J. Kahn et al. Libri-light: A benchmark for asr with limited or no supervision. In Proc. of ICASSP, 2020. [28] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. van den Oord. Learning robust and multilingual speech representations. arXiv, 2020. [29] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, 2015. [30] A. Laptev, R. Korostik, A. Svischev, A. Andrusenko, I. Medennikov, and S. Rybin. You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation. arXiv, abs/2005.07157, 2020. [31] M. P. Lewis, G. F. Simon, and C. D. Fennig. Ethnologue: Languages of the world, nineteenth edition. Online version: http://www.ethnologue.com, 2016. [32] A. H. Liu, T. Tu, H. yi Lee, and L. shan Lee. Towards unsupervised speech recognition and synthesis with quantized speech representation learning. arXiv, 2019. [33] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [34] C. Lüscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schlüter, and H. Ney. Rwth asr systems for librispeech: Hybrid vs attention. In Interspeech 2019, 2019. [35] C. J. Maddison, D. Tarlow, and T. Minka. A* sampling. In Advances in Neural Information Processing Systems, pages 3086–3094, 2014. [36] I. Misra and L. van der Maaten. Self-supervised learning of pretext-invariant representations. arXiv, 2019. [37] A. Mohamed, D. Okhonko, and L. Zettlemoyer. Transformers with convolutional context for ASR. arXiv, abs/1904.11660, 2019. [38] M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling neural machine translation. In Proc. of WMT, 2018. [39] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL System Demonstrations, 2019. [40] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: an asr corpus based on public domain audio books. In Proc. of ICASSP, pages 5206–5210. IEEE, 2015. [42] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V. Le. Improved noisy student training for automatic speech recognition. arXiv, abs/2005.09629, 2020. [43] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In Proc. of ACL, 2018. [44] V. Pratap, A. Hannun, Q. Xu, J. Cai, J. Kahn, G. Synnaeve, V. Liptchinsky, and R. Collobert. Wav2letter++: A fast open-source speech recognition system. In Proc. of ICASSP, 2019. [45] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/ research-covers/language-unsupervised/language_understanding_paper.pdf, 2018. [46] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio. Light gated recurrent units for speech recognition. IEEE Transactions on Emerging Topics in Computational Intelligence, 2(2):92–102, 2018. [47] M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski, J. Monteiro, J. Trmal, and Y. Bengio. Multi-task self-supervised learning for robust speech recognition. arXiv, 2020. [48] M. Rivière, A. Joulin, P.-E. Mazaré, and E. Dupoux. Unsupervised pretraining transfers well across languages. arXiv, abs/2002.02848, 2020. [49] S. Schneider, A. Baevski, R. Collobert, and M. Auli. wav2vec: Unsupervised pre-training for speech recognition. In Proc. of Interspeech, 2019. [50] M. Schuster and K. Nakajima. Japanese and korean voice search. In Proc. of ICASSP, 2012. [51] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert. End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures. arXiv, abs/1911.08460, 2020. [52] A. Tjandra, B. Sisman, M. Zhang, S. Sakti, H. Li, and S. Nakamura. Vqvae unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge 2019. arXiv, 1905.11449, 2019. [53] A. van den Oord, O. Vinyals, et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems, pages 6306–6315, 2017. [54] A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv, abs/1807.03748, 2018. [55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In Proc. of NIPS, 2017.\n[56] W. Wang, Q. Tang, and K. Livescu. Unsupervised pre-training of bidirectional speech encoders via masked reconstruction. arXiv, 2020. [57] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. In Proc. of ICLR, 2019. [58] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert. Iterative pseudo-labeling for speech recognition. arXiv, 2020. [59] N. Zeghidour, N. Usunier, I. Kokkinos, T. Schaiz, G. Synnaeve, and E. Dupoux. Learning ﬁlterbanks from raw speech for phone recognition. In Proc. of ICASSP, 2018. [60] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar. Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss. arXiv, 2020.\n\nAppendices A Masking distribution When choosing which time-steps to mask, each latent speech representation in an utterance is considered a candidate starting time-step with probability p where M is the length of each masked span starting from the respective time step; both are hyper-parameters. Sampled starting time steps are expanded to length M and spans can overlap. For a 15 sec long audio sample, the average mask length is 14.7 time-steps, corresponding to 299ms of audio, with a median of 10 time-steps, and a maximum of about 100 time steps; about 49% of all time-steps in the sample will be masked. A plot of the corresponding mask length distribution is shown in Figure 2 and an ablation of M and p as well as the effect of other masking strategies is shown in Table 5. Reducing M results in increased prediction accuracy for the self-supervised but the task becomes trivial when spans with length one are masked, leading to poor performance on downstream speech recognition tasks. We also consider other masking strategies: w/o overlap uniform(a,b) samples for each starting index a span length M s from interval a to b and masks the subsequent M s time-steps taking care not to overlap with existing spans; poisson(λ) and normal(µ, σ) sample M s from Poisson and normal distributions. 0.25 0.20 Percent of all spans 0.15 0.10 0.05 0.00 20 40 60 80 100 Span length Figure 2: Mask length distribution for a 15 second sample with p = 0.065 and M = 10. Table 5: Ablations on settings for the masking strategy during pre-training. When masking without overlap, we choose starting time steps with p = 0.037 which results in the total number of masked tokens to match the baseline. Baseline (p = 0.075) 7.97 0.02 Mask length M = 8 8.33 0.05 Mask length M = 12 8.19 0.08 Mask length M = 15 8.43 0.19 Mask probability p = 0.065 7.95 0.08 Mask probability p = 0.06 8.14 0.22 Mask w/o overlap, uniform(1,31) 8.39 0.02 Mask w/o overlap, uniform(10,30) 9.17 0.05 Mask w/o overlap, poisson(15) 8.13 0.04 Mask w/o overlap, normal(15, 10) 8.37 0.03 Mask w/o overlap, length 10 9.15 0.02 Mask w/o overlap, length 15 9.43 0.26 avg WER std B Fine-tuning Setup During ﬁne-tuning we apply a masking strategy to the feature encoder outputs similar to SpecAugment [41]: we randomly choose a number of starting time steps for which a span of ten subsequent time-steps is replaced with a mask embedding; spans may overlap and we use the same masked time step embedding as during pre-training. We also mask channels by choosing a number of channels as starting indices and then expand each one to cover the subsequent 64 channels. Spans may overlap and the selected channel spans are set to zero value. We use LayerDrop [22, 12] at a rate of 0.05 for BASE and 0.1 for LARGE during ﬁne-tuning. Table 6 summarizes the ﬁne-tuning hyper-parameter settings used for the different labeled data setup. Table 7 shows the decoding parameters used for ﬁnal evaluations of the various labeled data setups for Librispeech pre-trained models and Table 8 shows decoding parameters for LibriVox. Table 6: Fine-tuning hyperparameters timestep mask prob. channel mask prob. updates 10 min 0.075 0.008 12k 1 hour 0.075 0.004 13k 10 hours 0.065 0.004 20k 100 hours 0.05 0.008 50k 960 hours 0.05 0.0016 320k TIMIT 0.065 0.012 40k Table 7: Decoding parameters for Librispeech subsets for models pre-trained on Librispeech 4gram LM weight 4gram word insert. TransLM weight TransLM word insert. 10 min 3.23 -0.26 1.20 -1.39 1 hour 2.90 -1.62 1.15 -2.08 10 hours 2.46 -0.59 1.06 -2.32 100 hours 2.15 -0.52 0.87 -1.00 960 hours 1.74 0.52 0.92 -0.86 Table 8: Decoding parameters for Librispeech subsets for models pre-trained on Librivox. 4gram LM weight 4gram word insert. TransLM weight TransLM word insert. 10 min 3.86 -1.18 1.47 -2.82 1 hour 3.09 -2.33 1.33 -0.69 10 hours 2.12 -0.90 0.94 -1.05 100 hours 2.15 -0.52 0.87 -1.00 960 hours 1.57 -0.64 0.90 -0.31 C Full results for Libri-light and Librispeech Table 9: WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups (cf. Table 1). Model Unlabeled LM dev test data clean other clean other 10 min labeled BASE LS-960 None 46.1 51.5 46.9 50.9 4-gram 8.9 15.7 9.1 15.6 Transf. 6.6 13.2 6.9 12.9 LARGE LS-960 None 43.0 46.3 43.5 45.3 4-gram 8.6 12.9 8.9 13.1 Transf. 6.6 10.6 6.8 10.8 LARGE LV-60k None 38.3 41.0 40.2 38.7 4-gram 6.3 9.8 6.6 10.3 Transf. 4.6 7.9 4.8 8.2 1h labeled BASE LS-960 None 24.1 29.6 24.5 29.7 4-gram 5.0 10.8 5.5 11.3 Transf. 3.8 9.0 4.0 9.3 LARGE LS-960 None 21.6 25.3 22.1 25.3 4-gram 4.8 8.5 5.1 9.4 Transf. 3.8 7.1 3.9 7.6 LARGE LV-60k None 17.3 20.6 17.2 20.3 4-gram 3.6 6.5 3.8 7.1 Transf. 2.9 5.4 2.9 5.8 10h labeled BASE LS-960 None 10.9 17.4 11.1 17.6 4-gram 3.8 9.1 4.3 9.5 Transf. 2.9 7.4 3.2 7.8 LARGE LS-960 None 8.1 12.0 8.0 12.1 4-gram 3.4 6.9 3.8 7.3 Transf. 2.9 5.7 3.2 6.1 LARGE LV-60k None 6.3 9.8 6.3 10.0 4-gram 2.6 5.5 3.0 5.8 Transf. 2.4 4.8 2.6 4.9 100h labeled BASE LS-960 None 6.1 13.5 6.1 13.3 4-gram 2.7 7.9 3.4 8.0 Transf. 2.2 6.3 2.6 6.3 LARGE LS-960 None 4.6 9.3 4.7 9.0 4-gram 2.3 5.7 2.8 6.0 Transf. 2.1 4.8 2.3 5.0 LARGE LV-60k None 3.3 6.5 3.1 6.3 4-gram 1.8 4.5 2.3 4.6 Transf. 1.9 4.0 2.0 4.0 Table 10: WER on Librispeech when using all 960 hours of Librispeech as labeled data (cf. Table 2). Model Unlabeled LM dev test data clean other clean other LARGE - from scratch - None 2.8 7.6 3.0 7.7 - 4-gram 1.8 5.4 2.6 5.8 - Transf. 1.7 4.3 2.1 4.6 BASE LS-960 None 3.2 8.9 3.4 8.5 4-gram 2.0 5.9 2.6 6.1 Transf. 1.8 4.7 2.1 4.8 LARGE LS-960 None 2.6 6.5 2.8 6.3 4-gram 1.7 4.6 2.3 5.0 Transf. 1.7 3.9 2.0 4.1 LARGE LV-60k None 2.1 4.5 2.2 4.5 4-gram 1.4 3.5 2.0 3.6 Transf. 1.6 3.0 1.8 3.3 D Analysis of Discrete Latent Speech Representations Next, we investigate whether the discrete latent speech representations qt learned by the quantizer relate to phonetic information: Using LARGE pre-trained on LV-60k and without any ﬁne-tuning, we compute the discrete latents for the training data of TIMIT and compute the co-occurrence between human annotated phonemes and the latents. Ties are broken by choosing the phoneme which is most represented in the receptive ﬁeld of qt. The training data contains 3696 utterances of average length 13.6 sec, or 563k discrete latents. Figure 3 plots P(phoneme|qt) and shows that many discrete latents appear to specialize in speciﬁc phonetic sounds. The silence phoneme (bcl) represents 22% of all human annotated speech data and is therefore also modeled by many different latents. aa ae ah aw ay b ch d dh dx eh axr ey f g bcl hh ih iy jh k el em en eng ow oy p r s sh t th uh uw v w y z Figure 3: Visualization of the co-occurrence between discrete latent speech representations and phonemes. We plot the conditional probability P(phoneme|qt) on TIMIT train data. The y-axis shows the collapsed 39 classes of phonemes and the x-axis is over the different discrete latents. E Speech Recognition Error Analysis In this section we study the most common errors our models make when ﬁne-tuned on different amounts of labeled data (Table 11). We also show transcriptions of a few relatively challenging utterances from the dev-clean subset of Librispeech (Table 12). We consider models with no lexicon or no language model decoding, marked None in Table 9: Larger capacity decreases error rates: LARGE on LS-960 improves the word error rate on dev-clean from 46.1 to 43 compared to BASE. Increasing the amount of unlabeled training data further decreases the error rate to 33.8 for LARGE on LS-960. In the ten minute labeled data setup, the model is still able to recognize basic units of speech: Table 11 shows that most errors are around spelling of words, e.g., omitting silent characters such as could →coud, know →now, or ignoring repeated letters such as still →stil, little →litle. The LARGE LV-60k model achieves WER 38.3 on dev-clean and adding a Transformer language model enables to choose more likely pronunciations during the search and gives a large WER improvement to 5.0. The ten minute models without lexicon and language model tend to spell words phonetically and omit repeated letters, e.g., will →wil (Table 11). Spelling errors decrease with more labeled data: with one hour of labeled data, slightly less common words move into the list of the most frequent errors, e.g., heaven and food are spelled phonetically. At ten hours, top errors include articles, e.g., a, the which are a common source of errors in speech recognition in general. There are also alternative spellings, color vs. colour as well as relatively rare words including person names, still spelled phonetically, e.g., phoebe →feeby. At 100 hours, person names dominate the most frequent errors: phoebe →phebe, along with incorrect spacing anyone →any one, awhile →a while. Finally at 960 hours the word error rate falls to 2% and top errors are mostly articles, incorrect splits, and some very rare words or names such as deucalion or gryce. The “from scratch” 960 hour model has a similar word error rate as the 100 hour pre-trained model and displays a similar pattern of errors. The pre-trained speech representations can be easily adapted to recognize speciﬁc sounds while ﬁne-tuning grounds these representations to the actual spelling. Table 11: Top word errors for models trained on 10m, 1h and 10h, 100h, 960h of labeled data and decoded on the Librispeech dev-clean subset without a language model or lexicon (see Table 9 and Table 10 - None). In brackets is the total number of occurrences of each error. 10m LARGE LV-60k 1h LARGE LV-60k 10h LARGE LV-60k all →al (181) too →to (26) in →and (15) are →ar (115) until →untill (24) a →the (11) will →wil (100) new →knew (22) o →oh (10) you →yo (90) door →dor (18) and →in (9) one →on (89) says →sais (18) mode →mod (9) two →to (81) soul →sol (17) ursus →ersus (9) well →wel (80) bread →bred (16) tom →tome (8) been →ben (73) poor →pore (16) randal →randol (7) upon →apon (73) a →the (13) the →a (7) good →god (67) either →ither (13) color →colour (6) see →se (66) food →fud (13) ﬂour →ﬂower (6) we →whe (60) doubt →dout (12) phoebe →feeby (6) little →litle (54) earth →erth (12) an →and (5) great →grate (53) led →lead (12) cucumbers →cucombers (5) your →yor (53) sea →see (12) egg →eg (5) could →coud (51) thee →the (12) macklewain →macklewaine (5) here →hear (51) tom →tome (12) magpie →magpi (5) know →now (45) add →ad (11) milner →millner (5) there →ther (45) good →god (11) stacy →staci (5) three →thre (45) heaven →heven (11) trevelyan →trevellion (5) still →stil (42) mary →marry (11) verloc →verlock (5) off →of (40) randal →randel (11) ann →an (4) don’t →dont (37) answered →ansered (10) anyone →one (4) shall →shal (36) blood →blod (10) apartment →appartment (4) little →litl (35) bozzle →bosel (10) basin →bason (4) 100h LARGE LV-60k 960h LARGE LV-60k 960h LARGE from scratch a →the (13) a →the (12) and →in (20) and →in (10) and →in (9) a →the (16) in →and (10) macklewain →mackelwaine (7) in →and (13) o →oh (8) in →and (6) the →a (10) minnetaki →minnitaki (7) o →oh (6) in →an (8) randal →randall (7) bozzle →bosell (5) and →an (5) christie →cristy (6) criss →chris (5) clarke →clark (4) macklewain →mackelwane (6) bozzle →bosel (4) grethel →gretel (4) randal →randoll (6) clarke →clark (4) macklewain →mackelwaine (4) bozzle →bosall (5) colored →coloured (4) this →the (4) kaliko →calico (5) grethel →gretel (4) an →and (3) trevelyan →trevelian (5) lige →lyge (4) anyone →one (3) an →and (4) the →a (4) bozzle →basell (3) and →an (4) and →an (3) buns →bunds (3) anyone →one (4) ann →marianne (3) carrie →carry (3) bozzle →bozall (4) butte →bute (3) criss →chris (3) clarke →clark (4) color →colour (3) he’s →is (3) gryce →grice (4) deucalion →ducalion (3) his →is (3) i’m →am (4) forcemeat →meat (3) honor →honour (3) in →ind (4) gryce →grice (3) lattimer →latimer (3) letty →lettie (4) honor →honour (3) millet →mellet (3) phoebe →phebe (4) kearny →kirney (3) pyncheon →pension (3) the →a (4) nuova →noiva (3) tad →ted (3) ann →anne (3) thing →anything (3) thing →anything (3) awhile →while (3) this →the (3) trevelyan →trevelian (3) Table 12: Examples of transcription of selected utterances from the dev-clean subset by various models without a language model or lexicon. Capitalized words indicate errors. Model Transcription Reference i’m mister christopher from london 10m LV-60k IM mister CRESTIFER FROME LUNDEN 1h LV-60k IM mister CRISTIFFHER from LOUNDEN 10h LV-60k i’m mister CHRYSTEPHER from london 100h LV-60k i’m mister christopher from london 960h LV-60k i’m mister christopher from london 960h scratch I MISSTER christopher from london Reference il popolo e una bestia 10m LV-60k ILPOPULAR ONABESTIA 1h LV-60k O POPOLAONABASTIA 10h LV-60k U POPULAONABASTIAR 100h LV-60k O POPALOON A BASTYA 960h LV-60k YOU’LL POP A LAWYE ON A BAISTYE 960h scratch OL POPALOY ON ABESTIA Reference he smelt the nutty aroma of the spirit 10m LV-60k he SMELTD the NUDY aroma of the spirit 1h LV-60k he SMELTD the NUDDY ARROMA of the spirit 10h LV-60k he smelt the NUDDY ERROMA of the spirit 100h LV-60k he smelt the NUDDY aroma of the spirit 960h LV-60k he smelt the NUTTIE aroma of the spirit 960h scratch he smelt the nutty EROMA of the spirit Reference phoebe merely glanced at it and gave it back 10m LV-60k FEABY MEARLY glanced at it and gave it BAK 1h LV-60k FIEABY merely glanced at it and gave it back 10h LV-60k FEEBY merely glanced at it and gave it back 100h LV-60k BEBE merely glanced at it and gave it back 960h LV-60k phoebe merely glanced at it and gave it back 960h scratch phoebe merely glanced at it and gave it back Reference sauterne is a white bordeaux a strong luscious wine the best known varieties being 10m LV-60k SULTERIN is a white BORDOE a strong LUCHOUS WIN the best NOWN VERIATYS being 1h LV-60k CLTEREN is a white BORDO a strong LUCHIOUS wine the best known VERIETIES being 10h LV-60k SOTERN is a white BOURDO a strong LUCIOUS wine the best known VORIETIES being 100h LV-60k SOTERN is a white BORDAUX a strong LUCIOUS wine the best known varieties being 960h LV-60k SOTERN is a white bordeaux a strong luscious wine the best known varieties being 960h scratch SOTERAN is a white bordeaux a strong luscious wine the best known varieties being Reference i happen to have mac connell’s box for tonight or there’d be no chance of our getting places 10m LV-60k i HAPEND to have MECONALES BOXS for TONIT ORE THIRLD be no chance of OR GETING places 1h LV-60k i happen to have MACCONNEL’S BOCXS for tonight or TE’ELD be no chance of our getting places 10h LV-60k i HAPPENED to have MUKONNEL’S box for tonight or THERED be no chance of our getting places 100h LV-60k i HAPPENED to have MC CONNEL’S box for TO NIGHT or there’d be no chance of our getting places 960h LV-60k i happen to have MC CONALL’S box for TO NIGHT or there’d be no chance of our getting places 960h scratch i HAPPENE to have MACONEL’S box for TO NIGHT or there’d be no chance of our getting places F Ablations Table 13 ablates various hyperparameter choices of our architecture. The setup for the baseline model is described in § 5.4. First, we tried to improve the continuous input and continuous target model (§ 5.4) by adding an MLP on top of the continuous target representation and we also tried to use a separate set of encoder parameters for the representations used as input and targets (Separate encoders). Both did not lead to meaningful improvements. Increasing the receptive ﬁeld size from 25ms to 30ms had little effect. Setting the diversity penalty weight (α) too low results in lower codebook usage and lower performance. Setting it too high leads to slight instability. Doubling the number of relative positional embeddings to 256 also did not help. Stopping gradients from the quantizer to the encoder shows that the encoder requires training signal from the quantizer as well. Next, increasing the number of negatives did not result in better performance (K = 200) and sampling negatives from the entire batch of utterances hurt performance, likely because candidates from other utterances are easy to distinguish. Sampling negatives from any time step in the utterance, masked or unmasked, does not help and is more computationally expensive. Gumbel noise is important and increasing the number of codebooks did not result in better performance. Table 13: Ablation of various hyper-parmeter choices. We report average WER and standard deviation on combined dev-clean/other of Librispeech for three seeds of training. Baseline (p = 0.075, α = 0.1) 7.97 0.02 Continuous inputs, continuous targets 8.58 0.08 + MLP on targets 8.51 0.05 + Separate encoders 8.90 0.01 receptive ﬁeld 30ms 7.99 0.06 diversity penalty α = 0 8.48 0.08 α = 0.05 8.34 0.08 α = 0.2 8.58 0.45 Conv pos emb, kernel 256 8.14 0.05 No gradient to encoder from quantizer 8.41 0.08 Negatives K = 200 same utterance 8.12 0.05 K = 50 same utterance + K = 50 from batch 8.79 0.06 Sample negatives from any time step 8.07 0.02 No Gumbel noise 8.73 0.42 Codebook G=4, V=18 9.02 0.38 G=8, V=8 8.13 0.07 Predict exactly U time steps from edges U = 1 9.53 0.91 U = 5 8.19 0.07 U = 10 8.07 0.07 U = 15 7.89 0.10 U = 20 7.90 0.01 We also investigated predicting only time steps immediately next to the last unmasked time step for each span. This enables to better control the difﬁculty of the pre-training task. Given the leftmost or rightmost unmasked time step next to a masked span, we compute the contrastive loss only for the ﬁrst U masked time steps next to these unsmasked spans. Predicting only up to one time step performs poorly because there is little training signal from each utterance and predicting more time steps performs better but does not signiﬁcantly outperform predicting all masked time steps. Increasing the number of training updates helps but this increases training time. avg. WER std.", "persona": "Student", "response": "### 1. Overview\nThis opens the door for speech recognition models in many more languages, dialects, and domains that previously required much more transcribed audio data to provide acceptable accuracy. Similar to the Bidirectional Encoder Representations from Transformers (BERT), our model is trained by predicting speech units for masked parts of the audio. A major difference is that speech audio is a continuous signal that captures many aspects of the recording with no clear segmentation into words or other units. Wav2vec 2.0 tackles this issue by learning basic units that are 25ms long to enable learning of high-level contextualized representations. These units are then used to describe many different speech audio recordings and make wav2vec more robust. This enables us to build speech recognition systems that can outperform the best semisupervised methods, even with 100x less labeled training data. Learning discrete latent speech units What happens if we increase the amount of unannotated data?\n\n### 2. Background\nTo answer this question, we trained on 53K hours of unannotated data from the LibriVox dataset (a large collection of public audiobooks) and fine-tuned with only 10 minutes of labeled data. The result was a model that still achieved a WER of 8.6 percent. This demonstrates that wav2vec 2.0 can enable speech recognition models for settings where there is very little labeled training data. Traditional speech recognition models are primarily trained on annotated speech audio with transcriptions. Good systems require large amounts of annotated data, which is only available for a small number of languages.\n\n### 3. Methods\nThe model first processes the raw waveform of the speech audio with a multilayer convolutional neural network to get latent audio representations of 25ms each. These representations are then fed into a quantizer as well as a transformer. The quantizer chooses a speech unit for the latent audio representation from an inventory of learned units. About half the audio representations are masked before being fed into the transformer. The transformer adds information from the entire audio sequence. Finally, the output of the transformer is used to solve a contrastive task. This task requires the model to identify the correct quantized speech units for the masked positions.\n\n### 4. Results\nWe also developed a cross-lingual approach, dubbed XLSR, that can learn speech units common to several languages. This approach helps when we have even small amounts of unlabeled speech, since languages for which we have little data can benefit from languages for which more data is available. Une erreur s’est produite Nous rencontrons des difficultés pour lire cette vidéo. En savoir plus Cross-lingual training For some languages, even unannotated data is limited. To address this issue, we explore the idea of cross-lingual training. The idea is to pretrain a single model on multiple languages at the same time, which results in representations that are better than training on a single language. This has worked particularly well for natural language processing with XLM-R. Performance for low-resource languages can improve significantly with this method, since they benefit from related languages. Results on the Common Voice benchmark in terms of phoneme error rate (PER), comparing training on each language individually (XLSR-Mono) with training on all 10 languages simultaneously (XLSR-10).\n\n### 5. Takeaways\nWe are releasing pretrained models and code for wav2vec 2.0, the successor to wav2vec. This new model learns basic speech units used to tackle a self-supervised task. The model is trained to predict the correct speech unit for masked parts of the audio, while at the same time learning what the speech units should be. With just 10 minutes of transcribed speech and 53K hours of unlabeled speech, wav2vec 2.0 enables speech recognition models at a word error rate (WER) of 8.6 percent on noisy speech and 5.2 percent on clean speech on the standard LibriSpeech benchmark. There are thousands of languages spoken around the world, many with several different dialects, which presents a huge challenge for building high-quality speech recognition technology. It’s simply not feasible to obtain resources for each dialect and every language across the many possible domains (read speech, telephone speech, etc.). Our new model, wav2vec 2.0, uses self-supervision to push the boundaries by learning from unlabeled training data to enable speech recognition systems for many more languages, dialects, and domains. With just one hour of labeled training data, wav2vec 2.0 outperforms the previous state of the art on the 100-hour subset of the LibriSpeech benchmark — using 100 times less labeled data. Wav2vec 2.0 is part of our vision for machine learning models that rely less on labeled data, thanks to self-supervised learning. Self-supervision has helped us advance image classification, video understanding, and our content understanding systems. We hope that the algorithm will enable improved speech technology for many more languages, dialects, and domains, and lead to improvements for existing systems. Our approach learns a set of speech units, which are shorter than phonemes, to describe the speech audio sequence.", "paper_id": "24", "story_id": "24-03"}
{"paper": "Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.\n\nThe development of computational methods to predict three-dimensional (3D) protein structures from the protein sequence has proceeded along two complementary paths that focus on either the physical interactions or the evolutionary history. The physical interaction programme heavily integrates our understanding of molecular driving forces into either thermodynamic or kinetic simulation of protein physics16 or statistical approximations thereof17. Although theoretically very appealing, this approach has proved highly challenging for even moderate-sized proteins due to the computational intractability of molecular simulation, the context dependence of protein stability and the difficulty of producing sufficiently accurate models of protein physics. The evolutionary programme has provided an alternative in recent years, in which the constraints on protein structure are derived from bioinformatics analysis of the evolutionary history of proteins, homology to solved structures18,19 and pairwise evolutionary correlations20–24. This bioinformatics approach has benefited greatly from the steady growth of experimental protein structures deposited in the Protein Data Bank (PDB)5, the explosion of genomic sequencing and the rapid development of deep learning techniques to interpret these correlations. Despite these advances, contemporary physical and evolutionary-history-based approaches produce predictions that are far short of experimental accuracy in the majority of cases in which a close homologue has not been solved experimentally and this has limited their utility for many biological applications. In this study, we develop the first, to our knowledge, computational approach capable of predicting protein structures to near experimental accuracy in a majority of cases. The neural network AlphaFold that we developed was entered into the CASP14 assessment (May–July 2020; entered under the team name ‘AlphaFold2’ and a completely different model from our CASP13 AlphaFold system10). The CASP assessment is carried out biennially using recently solved structures that have not been deposited in the PDB or publicly disclosed so that it is a blind test for the participating methods, and has long served as the gold-standard assessment for the accuracy of structure prediction25,26. In CASP14, AlphaFold structures were vastly more accurate than competing methods. AlphaFold structures had a median backbone accuracy of 0.96 Å r.m.s.d.95 (Cα root-mean-square deviation at 95% residue coverage) (95% confidence interval = 0.85–1.16 Å) whereas the next best performing method had a median backbone accuracy of 2.8 Å r.m.s.d.95 (95% confidence interval = 2.7–4.0 Å) (measured on CASP domains; see Fig. 1a for backbone accuracy and Supplementary Fig. 14 for all-atom accuracy). As a comparison point for this accuracy, the width of a carbon atom is approximately 1.4 Å. In addition to very accurate domain structures (Fig. 1b), AlphaFold is able to produce highly accurate side chains (Fig. 1c) when the backbone is highly accurate and considerably improves over template-based methods even when strong templates are available. The all-atom accuracy of AlphaFold was 1.5 Å r.m.s.d.95 (95% confidence interval = 1.2–1.6 Å) compared with the 3.5 Å r.m.s.d.95 (95% confidence interval = 3.1–4.2 Å) of the best alternative method. Our methods are scalable to very long proteins with accurate domains and domain-packing (see Fig. 1d for the prediction of a 2,180-residue protein with no structural homologues). Finally, the model is able to provide precise, per-residue estimates of its reliability that should enable the confident use of these predictions. We demonstrate in Fig. 2a that the high accuracy that AlphaFold demonstrated in CASP14 extends to a large sample of recently released PDB structures; in this dataset, all structures were deposited in the PDB after our training data cut-off and are analysed as full chains (see Methods, Supplementary Fig. 15 and Supplementary Table 6 for more details). Furthermore, we observe high side-chain accuracy when the backbone prediction is accurate (Fig. 2b) and we show that our confidence measure, the predicted local-distance difference test (pLDDT), reliably predicts the Cα local-distance difference test (lDDT-Cα) accuracy of the corresponding prediction (Fig. 2c). We also find that the global superposition metric template modelling score (TM-score)27 can be accurately estimated (Fig. 2d). Overall, these analyses validate that the high accuracy and reliability of AlphaFold on CASP14 proteins also transfers to an uncurated collection of recent PDB submissions, as would be expected (see Supplementary Methods 1.15 and Supplementary Fig. 11 for confirmation that this high accuracy extends to new folds).\n\nThe AlphaFold network AlphaFold greatly improves the accuracy of structure prediction by incorporating novel neural network architectures and training procedures based on the evolutionary, physical and geometric constraints of protein structures. In particular, we demonstrate a new architecture to jointly embed multiple sequence alignments (MSAs) and pairwise features, a new output representation and associated loss that enable accurate end-to-end structure prediction, a new equivariant attention architecture, use of intermediate losses to achieve iterative refinement of predictions, masked MSA loss to jointly train with the structure, learning from unlabelled protein sequences using self-distillation and self-estimates of accuracy. The AlphaFold network directly predicts the 3D coordinates of all heavy atoms for a given protein using the primary amino acid sequence and aligned sequences of homologues as inputs (Fig. 1e; see Methods for details of inputs including databases, MSA construction and use of templates). A description of the most important ideas and components is provided below. The full network architecture and training procedure are provided in the Supplementary Methods. The network comprises two main stages. First, the trunk of the network processes the inputs through repeated layers of a novel neural network block that we term Evoformer to produce an Nseq × Nres array (Nseq, number of sequences; Nres, number of residues) that represents a processed MSA and an Nres × Nres array that represents residue pairs. The MSA representation is initialized with the raw MSA (although see Supplementary Methods 1.2.7 for details of handling very deep MSAs). The Evoformer blocks contain a number of attention-based and non-attention-based components. We show evidence in ‘Interpreting the neural network’ that a concrete structural hypothesis arises early within the Evoformer blocks and is continuously refined. The key innovations in the Evoformer block are new mechanisms to exchange information within the MSA and pair representations that enable direct reasoning about the spatial and evolutionary relationships. The trunk of the network is followed by the structure module that introduces an explicit 3D structure in the form of a rotation and translation for each residue of the protein (global rigid body frames). These representations are initialized in a trivial state with all rotations set to the identity and all positions set to the origin, but rapidly develop and refine a highly accurate protein structure with precise atomic details. Key innovations in this section of the network include breaking the chain structure to allow simultaneous local refinement of all parts of the structure, a novel equivariant transformer to allow the network to implicitly reason about the unrepresented side-chain atoms and a loss term that places substantial weight on the orientational correctness of the residues. Both within the structure module and throughout the whole network, we reinforce the notion of iterative refinement by repeatedly applying the final loss to outputs and then feeding the outputs recursively into the same modules. The iterative refinement using the whole network (which we term ‘recycling’ and is related to approaches in computer vision28,29) contributes markedly to accuracy with minor extra training time (see Supplementary Methods 1.8 for details). Fraction of correct F1 rotamers 0.25 0.9 Fraction of proteins 0.20 0.8 0.15 0.7 0.10 0.6 0.05 0.5 0 0–0.5 0.5–1 1–2 2–4 4–8 >8 20 40 60 80 100 lDDT-Cα of a residue Full chain Cα r.m.s.d.95 (Å) c 100 100 80 60 lDDT-Cα 90 40 20 80 90 100 80 0 0 20 40 60 80 100 Average pLDDT on the resolved region d 1.0 1.0 0.8 0.6 TM-score 0.9 0.4 0.2 0.8 0.9 1.0 0.8 0 0 0.2 0.4 0.6 0.8 1.0 pTM on the resolved region Fig. 2 | Accuracy of AlphaFold on recent PDB structures. The analysed structures are newer than any structure in the training set. Further filtering is applied to reduce redundancy (see Methods). a, Histogram of backbone r.m.s.d. for full chains (Cα r.m.s.d. at 95% coverage). Error bars are 95% confidence intervals (Poisson). This dataset excludes proteins with a template (identified by hmmsearch) from the training set with more than 40% sequence identity covering more than 1% of the chain (n = 3,144 protein chains). The overall median is 1.46 Å (95% confidence interval = 1.40–1.56 Å). Note that this measure will be highly sensitive to domain packing and domain accuracy; a high r.m.s.d. is expected for some chains with uncertain packing or packing errors. b, Correlation between backbone accuracy and side-chain accuracy. Filtered to structures with any observed side chains and resolution better than 2.5 Å (n = 5,317 protein chains); side chains were further filtered to B-factor <30 Å2. A rotamer is classified as correct if the predicted torsion angle is within 40°. Each point aggregates a range of lDDT-Cα, with a bin size of 2 units above 70 lDDT-Cα and 5 units otherwise. Points correspond to the mean accuracy; error bars are 95% confidence intervals (Student t-test) of the mean on a per-residue basis. c, Confidence score compared to the true accuracy on chains. Least-squares linear fit lDDT-Cα = 0.997 × pLDDT − 1.17 (Pearson’s r = 0.76). n = 10,795 protein chains. The shaded region of the linear fit represents a 95% confidence interval estimated from 10,000 bootstrap samples. In the companion paper39, additional quantification of the reliability of pLDDT as a confidence measure is provided. d, Correlation between pTM and full chain TM-score. Least-squares linear fit TM-score = 0.98 × pTM + 0.07 (Pearson’s r = 0.85). n = 10,795 protein chains. The shaded region of the linear fit represents a 95% confidence interval estimated from 10,000 bootstrap samples. Evoformer The key principle of the building block of the network—named Evoformer (Figs. 1e, 3a)—is to view the prediction of protein structures as a graph inference problem in 3D space in which the edges of the graph are defined by residues in proximity. The elements of the pair representation encode information about the relation between the residues (Fig. 3b). The columns of the MSA representation encode the individual residues of the input sequence while the rows represent the sequences in which those residues appear. Within this framework, we define a number of update operations that are applied in each block in which the different update operations are applied in series. The MSA representation updates the pair representation through an element-wise outer product that is summed over the MSA sequence dimension. In contrast to previous work30, this operation is applied within every block rather than once in the network, which enables the continuous communication from the evolving MSA representation to the pair representation. Within the pair representation, there are two different update patterns. Both are inspired by the necessity of consistency of the pair representation—for a pairwise description of amino acids to be representable as a single 3D structure, many constraints must be satisfied including the triangle inequality on distances. On the basis of this intuition, we arrange the update operations on the pair representation in terms of triangles of edges involving three different nodes (Fig. 3c). In particular, we add an extra logit bias to axial attention31 to include the ‘missing edge’ of the triangle and we define a non-attention update operation ‘triangle multiplicative update’ that uses two edges to update the missing third edge (see Supplementary Methods 1.6.5 for details). The triangle multiplicative update was developed originally as a more symmetric and cheaper replacement for the attention, and networks that use only the attention or multiplicative update are both able to produce high-accuracy structures. However, the combination of the two updates is more accurate. We also use a variant of axial attention within the MSA representation. During the per-sequence attention in the MSA, we project additional logits from the pair stack to bias the MSA attention. This closes the loop by providing information flow from the pair representation back into the MSA representation, ensuring that the overall Evoformer block is able to fully mix information between the pair and MSA representations and prepare for structure generation within the structure module. End-to-end structure prediction The structure module (Fig. 3d) operates on a concrete 3D backbone structure using the pair representation and the original sequence row (single representation) of the MSA representation from the trunk. The and translations, each with respect to the global frame (residue gas) (Fig. 3e). These rotations and translations—representing the geometry of the N-Cα-C atoms—prioritize the orientation of the protein backbone so that the location of the side chain of each residue is highly constrained within that frame. Conversely, the peptide bond geometry is completely unconstrained and the network is observed to frequently violate the chain constraint during the application of the structure module as breaking this constraint enables the local refinement of all parts of the chain without solving complex loop closure problems. Satisfaction of the peptide bond geometry is encouraged during fine-tuning by a violation loss term. Exact enforcement of peptide bond geometry is only achieved in the post-prediction relaxation of the structure by gradient descent in the Amber32 force field. Empirically, this final relaxation does not improve the accuracy of the model as measured by the global distance test (GDT)33 or lDDT-Cα34 but does remove distracting stereochemical violations without the loss of accuracy. The residue gas representation is updated iteratively in two stages (Fig. 3d). First, a geometry-aware attention operation that we term ‘invariant point attention’ (IPA) is used to update an Nres set of neural activations (single representation) without changing the 3D positions, then an equivariant update operation is performed on the residue gas using the updated activations. The IPA augments each of the usual attention queries, keys and values with 3D points that are produced in the local frame of each residue such that the final value is invariant to global rotations and translations (see Methods ‘IPA’ for details). The 3D queries and keys also impose a strong spatial/locality bias on the attention, which is well-suited to the iterative refinement of the protein structure. After each attention operation and element-wise transition block, the module computes an update to the rotation and translation of each backbone frame. The application of these updates within the local frame of each residue makes the overall attention and update block an equivariant operation on the residue gas. Predictions of side-chain χ angles as well as the final, per-residue accuracy of the structure (pLDDT) are computed with small per-residue networks on the final activations at the end of the network. The estimate of the TM-score (pTM) is obtained from a pairwise error prediction that is computed as a linear projection from the final pair representation. The final loss (which we term the frame-aligned point error (FAPE) (Fig. 3f)) compares the predicted atom positions to the true positions under many different alignments. For each alignment, defined by aligning the predicted frame (Rk, tk) to the corresponding true frame, we compute the distance of all predicted atom positions xi  from the true atom positions. The resulting Nframes × Natoms distances are penalized with a clamped L1  loss. This creates a strong bias for atoms to be correct relative to the local frame of each residue and hence correct with respect to its side-chain interactions, as well as providing the main source of chirality for AlphaFold (Supplementary Methods 1.9.3 and Supplementary Fig. 9). Training with labelled and unlabelled data The AlphaFold architecture is able to train to high accuracy using only supervised learning on PDB data, but we are able to enhance accuracy (Fig. 4a) using an approach similar to noisy student self-distillation35. In this procedure, we use a trained network to predict the structure of around 350,000 diverse sequences from Uniclust3036 and make a new dataset of predicted structures filtered to a high-confidence subset. We then train the same architecture again from scratch using a mixture of PDB data and this new dataset of predicted structures as the training data, in which the various training data augmentations such as cropping and MSA subsampling make it challenging for the network to recapitulate the previously predicted structures. This self-distillation procedure makes effective use of the unlabelled sequence data and considerably improves the accuracy of the resulting network. Additionally, we randomly mask out or mutate individual residues within the MSA and have a Bidirectional Encoder Representations from Transformers (BERT)-style37 objective to predict the masked elements of the MSA sequences. This objective encourages the network to learn to interpret phylogenetic and covariation relationships without hardcoding a particular correlation statistic into the features. The BERT objective is trained jointly with the normal PDB structure loss on the same training examples and is not pre-trained, in contrast to recent independent work38. (Fig. 4 | Interpreting the neural network. a, Ablation results on two target sets: the CASP14 set of domains (n = 87 protein domains) and the PDB test set of chains with template coverage of ≤30% at 30% identity (n = 2,261 protein chains). Domains are scored with GDT and chains are scored with lDDT-Cα. The ablations are reported as a difference compared with the average of the three baseline seeds. Means (points) and 95% bootstrap percentile intervals (error bars) are computed using bootstrap estimates of 10,000 samples. b, Domain GDT trajectory over 4 recycling iterations and 48 Evoformer blocks on CASP14 targets LmrP (T1024) and Orf8 (T1064) where D1 and D2 refer to the individual domains as defined by the CASP assessment. Both T1024 domains obtain the correct structure early in the network, whereas the structure of T1064 changes multiple times and requires nearly the full depth of the network to reach the final structure. Note, 48 Evoformer blocks comprise one recycling iteration.) Evoformer block—in which each intermediate represents the belief of the network of the most likely structure at that block. The resulting trajectories are surprisingly smooth after the first few blocks, showing that AlphaFold makes constant incremental improvements to the structure until it can no longer improve (see Fig. 4b for a trajectory of accuracy). These trajectories also illustrate the role of network depth. For very challenging proteins such as ORF8 of SARS-CoV-2 (T1064), the network searches and rearranges secondary structure elements for many layers before settling on a good structure. For other proteins such as LmrP (T1024), the network finds the final structure within the first few layers. Structure trajectories of CASP14 targets T1024, T1044, T1064 and T1091 that demonstrate a clear iterative building process for a range of protein sizes and difficulties are shown in Supplementary Videos 1–4. In Supplementary Methods 1.16 and Supplementary Figs. 12, 13, we interpret the attention maps produced by AlphaFold layers. Figure 4a contains detailed ablations of the components of AlphaFold that demonstrate that a variety of different mechanisms contribute to AlphaFold accuracy. Detailed descriptions of each ablation model, their training details, extended discussion of ablation results and the effect of MSA depth on each ablation are provided in Supplementary Methods 1.13 and Supplementary Fig. 10.(Fig. 5 | Effect of MSA depth and cross-chain contacts. a, Backbone accuracy (lDDT-Cα) for the redundancy-reduced set of the PDB after our training data cut-off, restricting to proteins in which at most 25% of the long-range contacts are between different heteromer chains. We further consider two groups of proteins based on template coverage at 30% sequence identity: covering more than 60% of the chain (n = 6,743 protein chains) and covering less than 30% of the chain (n = 1,596 protein chains). MSA depth is computed by counting the number of non-gap residues for each position in the MSA (using the Neff weighting scheme; see Methods for details) and taking the median across residues. The curves are obtained through Gaussian kernel average smoothing (window size is 0.2 units in log10(Neff)); the shaded area is the 95% confidence interval estimated using bootstrap of 10,000 samples. b, An intertwined homotrimer (PDB 6SK0) is correctly predicted without input stoichiometry and only a weak template (blue is predicted and green is experimental)  MSA depth and cross-chain contacts Although AlphaFold has a high accuracy across the vast majority of deposited PDB structures, we note that there are still factors that affect accuracy or limit the applicability of the model. The model uses MSAs and the accuracy decreases substantially when the median alignment depth is less than around 30 sequences (see Fig. 5a for details). We observe a threshold effect where improvements in MSA depth over around 100 sequences lead to small gains. We hypothesize that the MSA information is needed to coarsely find the correct structure within the early stages of the network, but refinement of that prediction into a high-accuracy model does not depend crucially on the MSA information. The other substantial limitation that we have observed is that AlphaFold is much weaker for proteins that have few intra-chain or homotypic contacts compared to the number of heterotypic contacts (further details are provided in a companion paper39). This typically occurs for bridging domains within larger complexes in which the shape of the protein is created almost entirely by interactions with other chains in the complex. Conversely, AlphaFold is often able to give high-accuracy predictions for homomers, even when the chains are substantially intertwined (Fig. 5b). We expect that the ideas of AlphaFold are readily applicable to predicting full hetero-complexes in a future system and that this will remove the difficulty with protein chains that have a large number of hetero-contacts.\n\nDiscussion\nThe methodology that we have taken in designing AlphaFold is a combination of the bioinformatics and physical approaches: we use a physical and geometric inductive bias to build components that learn from PDB data with minimal imposition of handcrafted features (for example, AlphaFold builds hydrogen bonds effectively without a hydrogen bond score function). This results in a network that learns far more efficiently from the limited data in the PDB but is able to cope with the complexity and variety of structural data. In particular, AlphaFold is able to handle missing the physical context and produce accurate models in challenging cases such as intertwined homomers or proteins that only fold in the presence of an unknown haem group. The ability to handle underspecified structural conditions is essential to learning from PDB structures as the PDB represents the full range of conditions in which structures have been solved. In general, AlphaFold is trained to produce the protein structure most likely to appear as part of a PDB structure. For example, in cases in which a particular stochiometry, ligand or ion is predictable from the sequence alone, AlphaFold is likely to produce a structure that respects those constraints implicitly. AlphaFold has already demonstrated its utility to the experimental community, both for molecular replacement57 and for interpreting cryogenic electron microscopy maps58. Moreover, because AlphaFold outputs protein coordinates directly, AlphaFold produces predictions in graphics processing unit (GPU) minutes to GPU hours depending on the length of the protein sequence (for example, around one GPU minute per model for 384 residues; see Methods for details). This opens up the exciting possibility of predicting structures at the proteome-scale and beyond—in a companion paper39, we demonstrate the application of AlphaFold to the entire human proteome39. The explosion in available genomic sequencing techniques and data has revolutionized bioinformatics but the intrinsic challenge of experimental structure determination has prevented a similar expansion in our structural knowledge. By developing an accurate protein structure prediction algorithm, coupled with existing large and well-curated structure and sequence databases assembled by the experimental community, we hope to accelerate the advancement of structural bioinformatics that can keep pace with the genomics revolution. We hope that AlphaFold—and computational approaches that apply its techniques for other biophysical problems—will become essential tools of modern biology\n\nRelated work\nThe prediction of protein structures has had a long and varied development, which is extensively covered in a number of reviews14,40–43. Despite the long history of applying neural networks to structure prediction14,42,43, they have only recently come to improve structure prediction10,11,44,45. These approaches effectively leverage the rapid improvement in computer vision systems46 by treating the problem of protein structure prediction as converting an ‘image’ of evolutionary couplings22–24 to an ‘image’ of the protein distance matrix and then integrating the distance predictions into a heuristic system that produces the final 3D coordinate prediction. A few recent studies have been developed to predict the 3D coordinates directly47–50, but the accuracy of these approaches does not match traditional, hand-crafted structure prediction pipelines51. In parallel, the success of attention-based networks for language processing52 and, more recently, computer vision31,53 has inspired the exploration of attention-based methods for interpreting protein sequences54–56.\n\nOnline content Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code 1. Thompson, M. C., Yeates, T. O. & Rodriguez, J. A. Advances in methods for atomic resolution macromolecular structure determination. F1000Res. 9, 667 (2020). 2. Bai, X.-C., McMullan, G. & Scheres, S. H. W. How cryo-EM is revolutionizing structural biology. Trends Biochem. Sci. 40, 49–57 (2015). 3. Jaskolski, M., Dauter, Z. & Wlodawer, A. A brief history of macromolecular crystallography, illustrated by a family tree and its Nobel fruits. FEBS J. 281, 3985–4009 (2014). 4. Wüthrich, K. The way to NMR structures of proteins. Nat. Struct. Biol. 8, 923–925 (2001). 5. wwPDB Consortium. Protein Data Bank: the single global archive for 3D macromolecular structure data. Nucleic Acids Res. 47, D520–D528 (2018). 6. Mitchell, A. L. et al. MGnify: the microbiome analysis resource in 2020. Nucleic Acids Res. 48, D570–D578 (2020). 7. Steinegger, M., Mirdita, M. & Söding, J. Protein-level assembly increases protein sequence recovery from metagenomic samples manyfold. Nat. Methods 16, 603–606 (2019). 8. Dill, K. A., Ozkan, S. B., Shell, M. S. & Weikl, T. R. The protein folding problem. Annu. Rev. Biophys. 37, 289–316 (2008). 9. Anfinsen, C. B. Principles that govern the folding of protein chains. Science 181, 223–230 (1973). 10. Senior, A. W. et al. Improved protein structure prediction using potentials from deep learning. Nature 577, 706–710 (2020). 11. Wang, S., Sun, S., Li, Z., Zhang, R. & Xu, J. Accurate de novo prediction of protein contact map by ultra-deep learning model. PLOS Comput. Biol. 13, e1005324 (2017). 12. Zheng, W. et al. Deep-learning contact-map guided protein structure prediction in CASP13. Proteins 87, 1149–1164 (2019). 13. Abriata, L. A., Tamò, G. E. & Dal Peraro, M. A further leap of improvement in tertiary structure prediction in CASP13 prompts new routes for future assessments. Proteins 87, 1100–1112 (2019). 14. Pearce, R. & Zhang, Y. Deep learning techniques have significantly impacted protein structure prediction and protein design. Curr. Opin. Struct. Biol. 68, 194–207 (2021). 15. Moult, J., Fidelis, K., Kryshtafovych, A., Schwede, T. & Topf, M. Critical assessment of techniques for protein structure prediction, fourteenth round. CASP 14 Abstract Book https://www.predictioncenter.org/casp14/doc/CASP14_Abstracts.pdf (2020). 16. Brini, E., Simmerling, C. & Dill, K. Protein storytelling through physics. Science 370, eaaz3041 (2020). 17. Sippl, M. J. Calculation of conformational ensembles from potentials of mean force. An approach to the knowledge-based prediction of local structures in globular proteins.18. Šali, A. & Blundell, T. L. Comparative protein modelling by satisfaction of spatial restraints. J. Mol. Biol. 234, 779–815 (1993). 19. Roy, A., Kucukural, A. & Zhang, Y. I-TASSER: a unified platform for automated protein structure and function prediction. Nat. Protocols 5, 725–738 (2010).\n\n20.\t Altschuh, D., Lesk, A. M., Bloomer, A. C. & Klug, A. Correlation of co-ordinated amino acid\nsubstitutions with function in viruses related to tobacco mosaic virus. J. Mol. Biol. 193, 693–707 (1987). 21. Shindyalov, I. N., Kolchanov, N. A. & Sander, C. Can three-dimensional contacts in protein structures be predicted by analysis of correlated mutations? Protein Eng. 7, 349–358 (1994).\n\n22.\t Weigt, M., White, R. A., Szurmant, H., Hoch, J. A. & Hwa, T. Identification of direct residue\ncontacts in protein–protein interaction by message passing. Proc. Natl Acad. Sci. USA 106, 67–72 (2009).\n\n24.\t Jones, D. T., Buchan, D. W. A., Cozzetto, D. & Pontil, M. PSICOV: precise structural contact\nprediction using sparse inverse covariance estimation on large multiple sequence alignments. Bioinformatics 28, 184–190 (2012).\n\n25.\t Moult, J., Pedersen, J. T., Judson, R. & Fidelis, K. A large-scale experiment to assess protein\nstructure prediction methods. Proteins 23, ii–iv (1995).\n\n26.\t Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K. & Moult, J. Critical assessment of\nmethods of protein structure prediction (CASP)-round XIII. Proteins 87, 1011–1020 (2019). 27. Zhang, Y. & Skolnick, J. Scoring function for automated assessment of protein structure template quality. Proteins 57, 702–710 (2004).\n\n28.\t Tu, Z. & Bai, X. Auto-context and its application to high-level vision tasks and 3D brain\nimage segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 32, 1744–1757 (2010).\n\n29.\t Carreira, J., Agrawal, P., Fragkiadaki, K. & Malik, J. Human pose estimation with iterative\nerror feedback. In Proc. IEEE Conference on Computer Vision and Pattern Recognition 4733–4742 (2016). Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons license, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.\n\nFull algorithm details Extensive explanations of the components and their motivations are available in Supplementary Methods 1.1–1.10, in addition, pseudocode is available in Supplementary Information Algorithms 1–32, network diagrams in Supplementary Figs. 1–8, input features in Supplementary Table 1 and additional details are provided in Supplementary Tables 2, 3. Training and inference details are provided in Supplementary Methods 1.11–1.12 and Supplementary Tables 4, 5. IPA The IPA module combines the pair representation, the single representation and the geometric representation to update the single representation (Supplementary Fig. 8). Each of these representations contributes affinities to the shared attention weights and then uses these weights to map its values to the output. The IPA operates in 3D space. Each residue produces query points, key points and value points in its local frame. These points are projected into the global frame using the backbone frame of the residue in which they interact with each other. The resulting points are then projected back into the local frame. The affinity computation in the 3D space uses squared distances and the coordinate transformations ensure the invariance of this module with respect to the global frame (see Supplementary Methods 1.8.2 ‘Invariant point attention (IPA)’ for the algorithm, proof of invariance and a description of the full multi-head version). A related construction that uses classic geometric invariants to construct pairwise features in place of the learned 3D points has been applied to protein design59. In addition to the IPA, standard dot product attention is computed on the abstract single representation and a special attention on the pair representation. The pair representation augments both the logits and the values of the attention process, which is the primary way in which the pair representation controls the structure generation. Inputs and data sources Inputs to the network are the primary sequence, sequences from evolutionarily related proteins in the form of a MSA created by standard tools including jackhmmer60 and HHBlits61, and 3D atom coordinates of a small number of homologous structures (templates) where available. For both the MSA and templates, the search processes are tuned for high recall; spurious matches will probably appear in the raw MSA but this matches the training condition of the network. One of the sequence databases used, Big Fantastic Database (BFD), was custom-made and released publicly (see ‘Data availability’) and was used by several CASP teams. BFD is one of the largest publicly available collections of protein families. It consists of 65,983,866 families represented as MSAs and hidden Markov models (HMMs) covering 2,204,359,010 protein sequences from reference databases, metagenomes and metatranscriptomes. BFD was built in three steps. First, 2,423,213,294 protein sequences were collected from UniProt (Swiss-Prot&TrEMBL, 2017-11)62, a soil reference protein catalogue and the marine eukaryotic reference catalogue7, and clustered to 30% sequence identity, while enforcing a 90% alignment coverage of the shorter sequences using MMseqs2/Linclust63. This resulted in 345,159,030 clusters. For computational efficiency, we removed all clusters with less than three members, resulting in 61,083,719 clusters. Second, we added 166,510,624 representative protein sequences from Metaclust NR (2017-05; discarding all sequences shorter than 150 residues)63 by aligning them against the cluster representatives using MMseqs264. Sequences that fulfilled the sequence identity and coverage criteria were assigned to the best scoring cluster. The remaining 25,347,429 sequences that could not be assigned were clustered separately and added as new clusters, resulting in the final clustering. Third, for each of the clusters, we computed an MSA using Training regimen To train, we use structures from the PDB with a maximum release date of 30 April 2018. Chains are sampled in inverse proportion to cluster size of a 40% sequence identity clustering. We then randomly crop them to 256 residues and assemble into batches of size 128. We train the model on Tensor Processing Unit (TPU) v3 with a batch size of 1 per TPU core, hence the model uses 128 TPU v3 cores. The model is trained until convergence (around 10 million samples) and further fine-tuned using longer crops of 384 residues, larger MSA stack and reduced learning rate (see Supplementary Methods 1.11 for the exact configuration). The initial training stage takes approximately 1 week, and the fine-tuning stage takes approximately 4 additional days. The network is supervised by the FAPE loss and a number of auxiliary losses. First, the final pair representation is linearly projected to a binned distance distribution (distogram) prediction, scored with a cross-entropy loss. Second, we use random masking on the input MSAs and require the network to reconstruct the masked regions from the output MSA representation using a BERT-like loss37. Third, the output single representations of the structure module are used to predict binned per-residue lDDT-Cα values. Finally, we use an auxiliary side-chain loss during training, and an auxiliary structure violation loss during fine-tuning. Detailed descriptions and weighting are provided in the Supplementary Information. An initial model trained with the above objectives was used to make structure predictions for a Uniclust dataset of 355,993 sequences with the full MSAs. These predictions were then used to train a final model with identical hyperparameters, except for sampling examples 75% of the time from the Uniclust prediction set, with sub-sampled MSAs, and 25% of the time from the clustered PDB set. We train five different models using different random seeds, some with templates and some without, to encourage diversity in the predictions (see Supplementary Table 5 and Supplementary Methods 1.12.1 Inference regimen We inference the five trained models and use the predicted confidence score to select the best model per target. Using our CASP14 configuration for AlphaFold, the trunk of the network is run multiple times with different random choices for the MSA cluster centres (see Supplementary Methods 1.11.2 for details of the ensembling procedure). The full time to make a structure prediction varies considerably depending on the length of the protein. Representative timings for the neural network using a single model on V100 GPU are 4.8 min with 256 residues, 9.2 min with 384 residues and 18 h at 2,500 residues. These timings are measured using our open-source code, and the open-source code is notably faster than the version we ran in CASP14 as we now use the XLA compiler75. Since CASP14, we have found that the accuracy of the network without ensembling is very close or equal to the accuracy with ensembling and we turn off ensembling for most inference. Without ensembling, the network is 8× faster and the representative timings for a single model are 0.6 min with 256 residues, 1.1 min with 384 residues and 2.1 h with 2,500 residues. Inferencing large proteins can easily exceed the memory of a single GPU. For a V100 with 16 GB of memory, we can predict the structure of proteins up to around 1,300 residues without ensembling and the 256- and 384-residue inference times are using the memory of a single GPU. The memory usage is approximately quadratic in the number of residues, so a 2,500-residue protein involves using unified memory so that we can greatly exceed the memory of a single V100. In our cloud setup, a single V100 is used for computation on a 2,500-residue protein but we requested four GPUs to have sufficient memory. Searching genetic sequence databases to prepare inputs and final relaxation of the structures take additional central processing unit (CPU) time but do not require a GPU or TPU. Metrics The predicted structure is compared to the true structure from the PDB in terms of lDDT metric34, as this metric reports the domain accuracy without requiring a domain segmentation of chain structures. The distances are either computed between all heavy atoms (lDDT) or only the Cα atoms to measure the backbone accuracy (lDDT-Cα). As lDDT-Cα only focuses on the Cα atoms, it does not include the penalty for structural violations and clashes. Domain accuracies in CASP are reported as GDT33 and the TM-score27 is used as a full chain global superposition metric. We also report accuracies using the r.m.s.d.95 (Cα r.m.s.d. at 95% coverage). We perform five iterations of (1) a least-squares alignment of the predicted structure and the PDB structure on the currently chosen Cα atoms (using all Cα atoms in the first iteration); (2) selecting the 95% of Cα atoms with the lowest alignment error. The r.m.s.d. of the atoms chosen for the final iterations is the r.m.s.d.95. This metric is more robust to apparent errors that can originate from crystal structure artefacts, although in some cases the removed 5% of residues will contain genuine modelling errors. Test set of recent PDB sequences For evaluation on recent PDB sequences (Figs. 2a–d, 4a, 5a), we used a copy of the PDB downloaded 15 February 2021. Structures were filtered to those with a release date after 30 April 2018 (the date limit for inclusion in the training set for AlphaFold). Chains were further filtered to remove sequences that consisted of a single amino acid as well as sequences with an ambiguous chemical component at any residue position. Exact duplicates were removed, with the chain with the most resolved Cα atoms used as the representative sequence. Subsequently, Reporting summary Further information on research design is available in the Nature Research Reporting Summary linked to this paper. Data availability All input data are freely available from public sources. Structures from the PDB were used for training and as templates (https://www.wwpdb.org/ftp/pdb-ftp-sites; for the associated sequence data and 40% sequence clustering see also https://ftp.wwpdb. org/pub/pdb/derived_data/ and https://cdn.rcsb.org/resources/ sequence/clusters/bc-40.out). Training used a version of the PDB downloaded 28 August 2019, while the CASP14 template search used a version downloaded 14 May 2020. The template search also used the PDB70 database, downloaded 13 May 2020 (https://wwwuser.gwdg. de/~compbiol/data/hhsuite/databases/hhsuite_dbs/). We show experimental structures from the PDB with accession numbers 6Y4F77, 6YJ178, 6VR479, 6SK080, 6FES81, 6W6W82, 6T1Z83 and 7JTL84. For MSA lookup at both the training and prediction time, we used UniRef90 v.2020_01 (https://ftp.ebi.ac.uk/pub/databases/uniprot/ previous_releases/release-2020_01/uniref/), BFD (https://bfd.mmseqs. com), Uniclust30 v.2018_08 (https://wwwuser.gwdg.de/~compbiol/ uniclust/2018_08/) and MGnify clusters v.2018_12 (https://ftp.ebi.ac.uk/ pub/databases/metagenomics/peptide_database/2018_12/). Uniclust30 v.2018_08 was also used as input for constructing a distillation structure dataset. Code availability Source code for the AlphaFold model, trained weights and inference script are available under an open-source license at https://github. com/deepmind/alphafold. Neural networks were developed with TensorFlow v.1 (https://github. com/tensorflow/tensorflow), Sonnet v.1 (https://github.com/deepmind/sonnet), JAX v.0.1.69 (https://github.com/google/jax/) and Haiku v.0.0.4 (https://github.com/deepmind/dm-haiku). The XLA compiler is bundled with JAX and does not have a separate version number. For MSA search on BFD+Uniclust30, and for template search against PDB70, we used HHBlits and HHSearch from hh-suite v.3.0-beta.3 release 14/07/2017 (https://github.com/soedinglab/hh-suite). For MSA search on UniRef90 and clustered MGnify, we used jackhmmer from Acknowledgements We thank A. Rrustemi, A. Gu, A. Guseynov, B. Hechtman, C. Beattie,\n\nC. Jones, C. Donner, E. Parisotto, E. Elsen, F. Popovici, G. Necula, H. Maclean, J. Menick,\n\nJ. Kirkpatrick, J. Molloy, J. Yim, J. Stanway, K. Simonyan, L. Sifre, L. Martens, M. Johnson,\n\nM. O’Neill, N. Antropova, R. Hadsell, S. Blackwell, S. Das, S. Hou, S. Gouws, S. Wheelwright,\n\nT. Hennigan, T. Ward, Z. Wu, Ž. Avsec and the Research Platform Team for their contributions;\n\nM. Mirdita for his help with the datasets; M. Piovesan-Forster, A. Nelson and R. Kemp for their\nhelp managing the project; the JAX, TensorFlow and XLA teams for detailed support and enabling machine learning models of the complexity of AlphaFold; our colleagues at DeepMind, Google and Alphabet for their encouragement and support; and J. Moult and the CASP14 organizers, and the experimentalists whose structures enabled the assessment. M.S. acknowledges support from the National Research Foundation of Korea grant (2019R1A6A1A10073437, 2020M3A9G7103933) and the Creative-Pioneering Researchers Program through Seoul National University. Author contributions J.J. and D.H. led the research. J.J., R.E., A. Pritzel, M.F., O.R., R.B.,\n\nA. Potapenko, S.A.A.K., B.R.-P., J.A., M.P., T. Berghammer and O.V. developed the neural network\narchitecture and training. T.G., A.Ž., K.T., R.B., A.B., R.E., A.J.B., A.C., S.N., R.J., D.R., M.Z. and S.B. developed the data, analytics and inference systems. D.H., K.K., P.K., C.M. and E.C. managed the research. T.G. led the technical platform. P.K., A.W.S., K.K., O.V., D.S., S.P. and T. Back contributed technical advice and ideas. M.S. created the BFD genomics database and provided technical assistance on HHBlits. D.H., R.E., A.W.S. and K.K. conceived the AlphaFold project. J.J., R.E. and A.W.S. conceived the end-to-end approach. J.J., A. Pritzel, O.R.,\n\nA. Potapenko, R.E., M.F., T.G., K.T., C.M. and D.H. wrote the paper.\nCompeting interests J.J., R.E., A. Pritzel, T.G., M.F., O.R., R.B., A.B., S.A.A.K., D.R. and A.W.S. have filed non-provisional patent applications 16/701,070 and PCT/EP2020/084238, and provisional patent applications 63/107,362, 63/118,917, 63/118,918, 63/118,921 and 63/118,919, each in the name of DeepMind Technologies Limited, each pending, relating to machine learning for predicting protein structures. The other authors declare no competing interests. Additional information Supplementary information The online version contains supplementary material available at Correspondence and requests for materials should be addressed to J.J. or D.H. Peer review information Nature thanks Mohammed AlQuraishi, Charlotte Deane and Yang Zhang for their contribution to the peer review of this work. Reprints and permissions information is available at http://www.nature.com/reprints.", "persona": "General Public", "response": "### 1. Overview\nDeepMind and several research partners have released a database containing the 3D structures of nearly every protein in the human body, as computationally determined by the breakthrough protein folding system demonstrated last year, AlphaFold. The freely available database represents an enormous advance and convenience for scientists across hundreds of disciplines and domains, and may very well form the foundation of a new phase in biology and medicine. The problem with the proteome is that it’s much, much more complex. Proteins, like DNA, are sequences of known molecules; in DNA these are the handful of familiar bases (adenine, guanine, etc.), but in proteins they are the 20 amino acids (each of which is coded by multiple bases in genes). This in itself creates a great deal more complexity, but it’s only the start. The sequences aren’t simply “code” but actually twist and fold into tiny molecular origami machines that accomplish all kinds of tasks within our body. It’s like going from binary code to a complex language that manifests objects in the real world. Practically speaking this means that the proteome is made up of not just 20,000 sequences of hundreds of acids each, but that each one of those sequences has a physical structure and function. And one of the hardest parts of understanding them is figuring out what shape is made from a given sequence. This is generally done experimentally using something like x-ray crystallography, a long, complex process that may take months or longer to figure out a single protein — if you happen to have the best labs and techniques at your disposal. The structure can also be predicted computationally, though the process has never been good enough to actually rely on — until AlphaFold came along. Without going into the whole history of computational proteomics (as much as I’d like to), we essentially went from distributed brute-force tactics 15 years ago — remember Folding@home?\n\n### 2. Background\n“We believe that this work represents the most significant contribution AI has made to advancing the state of scientific knowledge to date, and is a great example of the kind of benefits AI can bring to society,” said DeepMind founder and CEO Demis Hassabis. The specifics of DeepMind’s advances and how it achieved them I will leave to specialists in the fields of computational biology and proteomics, who will no doubt be picking apart and iterating on this work over the coming months and years. It’s the practical results that concern us today, as the company employed its time since the publication of AlphaFold 2 (the version shown in 2020) not just tweaking the model, but running it… on every single protein sequence they could get their hands on. “The database as you’ll see it tomorrow, it’s a search bar, it’s almost like Google search for protein structures,” said Hassabis in an interview with TechCrunch. “You can view it in the 3D visualizer, zoom around it, interrogate the genetic sequence… and the nice thing about doing it with EMBL-EBI is it’s urled to all their other databases. So you can immediately go and see related genes, And it’s urled to all these other databases, you can see related genes, related in other organisms, other proteins that have related functions, and so on.” Although the prospect of structural bioinformaticians attaining their fondest dreams is heartwarming, it is important to note that there are in fact immediate and real benefits to the work DeepMind and EMBL-EBI have done. It is perhaps easiest to see in their partnership with the Drugs for Neglected Diseases Initiative. “For this particular case, I think explainability was not just a nice-to-have, which often is the case in machine learning, but it was a must-have, given the seriousness of what we wanted it to be used for,” he said. “So I think we’ve done the most we’ve ever done on a particular system to make the case with explainability. So there’s both explainability on a granular level on the algorithm, and then explainability in terms of the outputs, as well the predictions and the structures, and how much you should or shouldn’t trust them, and which of the regions are the reliable areas of prediction.” Nevertheless, his description of the system as “miraculous” attracted my special sense for potential headline words. Hassabis said that there’s nothing miraculous about the process itself, but rather that he’s a bit amazed that all their work has produced something so powerful.\n\n### 3. Methods\nFrom genome to proteome Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter.\n\n### 4. Results\nScale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW The AlphaFold Protein Structure Database is a collaboration between DeepMind, the European Bioinformatics Institute and others, and consists of hundreds of thousands of protein sequences with their structures predicted by AlphaFold — and the plan is to add millions more to create a “protein almanac of the world.” If you’re not familiar with proteomics in general — and it’s quite natural if that’s the case — the best way to think about this is perhaps in terms of another major effort: that of sequencing the human genome.\n\n### 5. Takeaways\nAs you may recall from the late ’90s and early ’00s, this was a huge endeavor undertaken by a large group of scientists and organizations across the globe and over many years. The genome, finished at last, has been instrumental to the diagnosis and understanding of countless conditions, and in the development of drugs and treatments for them. Taking a discipline by surprise So new is the very idea of being able to do this that Hassabis said he fully expects the entire field to change — and change the database along with it. “Structural biologists are not yet used to the idea that they can just look up anything in a matter of seconds, rather than take years to experimentally determine these things,” he said. “And I think that should lead to whole new types of approaches to questions that can be asked and experiments that can be done. Once we start getting wind of that, we may start building other tools that cater to this sort of serendipity: What if I want to look at 10,000 proteins related in a particular way? There isn’t really a normal way of doing that, because that isn’t really a normal question anyone would ask currently. So I imagine we’ll have to start producing new tools, and there’ll be demand for that once we start seeing how people interact with this.” That includes derivative and incrementally improved versions of the software itself, which has been released in open source along with a great deal of development history. Already we have seen an independently developed system, RoseTTAFold, from researchers at the University of Washington’s Baker Lab, which extrapolated from AlphaFold’s performance last year to create something similar yet more efficient — though DeepMind seems to have taken the lead again with its latest version. But the point was made that the secret sauce is out there for all to use. The DNDI focuses, as you might guess, on diseases that don’t warrant the kind of attention or investment from major pharmaceutical companies and medical research outfits that would potentially result in discovering a treatment.", "paper_id": "43", "story_id": "43-01"}
{"paper": "Face2Face: Real-time Face Capture and Reenactment of RGB Videos Justus Thies1 Michael Zollh¨ofer2 Marc Stamminger1 Christian Theobalt2 Matthias Nießner3 Proposed online reenactment setup: a monocular target video sequence (e.g., from Youtube) is reenacted based on the expressions of a source actor who is recorded live with a commodity webcam.\n\nAbstract\nWe present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we ﬁrst address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efﬁcient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate ﬁt. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.\n\n1. Introduction\non RGB [8, 6] as well as RGB-D data [31, 10, 21, 4, 16]. These techniques have become increasingly popular for the animation of virtual CG avatars in video games and movies. It is now feasible to run these face capture and tracking algorithms from home, which is the foundation for many VR and AR applications, such as teleconferencing. In this paper, we employ a new dense markerless facial performance capture method based on monocular RGB data, similar to state-of-the-art methods. However, instead of transferring facial expressions to virtual CG characters, our main contribution is monocular facial reenactment in real-time. In contrast to previous reenactment approaches that run ofﬂine [5, 11, 13], our goal is the online transfer of facial expressions of a source actor captured by an RGB sensor to a target actor. The target sequence can be any monocular video; e.g., legacy video footage downloaded from Youtube with a facial performance. We aim to modify the target video in a photo-realistic fashion, such that it is virtually impossible to notice the manipulations. Faithful photo-realistic facial reenactment is the foundation for a variety of applications; for instance, in video conferencing, the video feed can be adapted to match the face motion of a translator, or face videos can be convincingly dubbed to a foreign language. In our method, we ﬁrst reconstruct the shape identity of the target actor using a new global non-rigid modelbased bundling approach based on a prerecorded training sequence. As this preprocess is performed globally on a set of training frames, we can resolve geometric ambiguities common to monocular reconstruction. At runtime, we track both the expressions of the source and target actor’s video by a dense analysis-by-synthesis approach based on a statistical facial prior. We demonstrate that our RGB tracking accuracy is on par with the state of the art, even with online tracking methods relying on depth data. In order to transfer expressions from the source to the target actor in realtime, we propose a novel transfer functions that efﬁciently applies deformation transfer [27] directly in the used lowdimensional expression space. For ﬁnal image synthesis, we re-render the target’s face with transferred expression coefﬁcients and composite it with the target video’s background under consideration of the estimated environment lighting. Finally, we introduce a new image-based mouth synthesis approach that generates a realistic mouth interior by retrieving and warping best matching mouth shapes from the ofﬂine sample sequence. It is important to note that we maintain the appearance of the target mouth shape; in contrast, existing methods either copy the source mouth region onto the target [30, 11] or a generic teeth proxy is rendered [14, 29], both of which leads to inconsistent results. Fig. 1 shows an overview of our method. We demonstrate highly-convincing transfer of facial expressions from a source to a target video in real time. We show results with a live setup where a source video stream, which is captured by a webcam, is used to manipulate a target Youtube video. In addition, we compare against stateof-the-art reenactment methods, which we outperform both in terms of resulting video quality and runtime (we are the ﬁrst real-time RGB reenactment method). In summary, our key contributions are: • dense, global non-rigid model-based bundling, • accurate tracking, appearance, and lighting estimation in unconstrained live RGB video, • person-dependent expression transfer using subspace deformations, • and a novel mouth synthesis approach.\n\n3. Synthesis of Facial Imagery\nWe use a multi-linear PCA model based on [3, 1, 9]. The ﬁrst two dimensions represent facial identity – i.e., geometric shape and skin reﬂectance – and the third dimension controls the facial expression. Hence, we parametrize a face as: This prior assumes a multivariate normal probability distribution of shape and reﬂectance around the average shape aid ∈R3n and reﬂectance aalb ∈R3n. The shape Eid ∈R3n×80, reﬂectance Ealb ∈R3n×80, and expression Eexp ∈R3n×76 basis and the corresponding standard deviations σid ∈R80, σalb ∈R80, and σexp ∈R76 are given. The model has 53K vertices and 106K faces. A synthesized image CS is generated through rasterization of the model under a rigid model transformation Φ(v) and the full perspective transformation Π(v). Illumination is approximated by the ﬁrst three bands of Spherical Harmonics (SH) [23] basis functions, assuming Labertian surfaces and smooth distant illumination, neglecting self-shadowing. Synthesis is dependent on the face model parameters α, β, δ, the illumination parameters γ, the rigid transformation R, t, and the camera parameters κ deﬁning Π. The vector of unknowns P is the union of these parameters. \n\n 4. Energy Formulation\nGiven a monocular input sequence, we reconstruct all unknown parameters P jointly with a robust variational optimization. The proposed objective is highly non-linear in the unknowns and has the following components: + wregEreg(P) | {z } prior E(P)=wcolEcol(P) + wlanElan(P) | {z } data . (3) The data term measures the similarity between the synthesized imagery and the input data in terms of photoconsistency Ecol and facial feature alignment Elan. The likelihood of a given parameter vector P is taken into account by the statistical regularizer Ereg. The weights wcol, wlan, and wreg balance the three different sub-objectives. In all of our experiments, we set wcol = 1, wlan = 10, and wreg = 2.5 · 10−5. In the following, we introduce the different sub-objectives. Photo-Consistency In order to quantify how well the input data is explained by a synthesized image, we measure the photo-metric alignment error on pixel level: Ecol(P) = 1 X p∈V ∥CS(p) −CI(p)∥2 , (4) |V| where CS is the synthesized image, CI is the input RGB image, and p ∈V denote all visible pixel positions in CS. We use the ℓ2,1-norm [12] instead of a least-squares formulation to be robust against outliers. In our scenario, distance in color space is based on ℓ2, while in the summation over all pixels an ℓ1-norm is used to enforce sparsity. tected in the RGB stream: Elan(P) = 1 |F| X f j∈F wconf,j ∥f j −Π(Φ(vj)∥2 2 . (5) To this end, we employ a state-of-the-art facial landmark tracking algorithm by [24]. Each feature point f j ∈F ⊂ R2 comes with a detection conﬁdence wconf,j and corresponds to a unique vertex vj = Mgeo(α, δ) ∈R3 of our face prior. This helps avoiding local minima in the highlycomplex energy landscape of Ecol(P). Statistical Regularization We enforce plausibility of the synthesized faces based on the assumption of a normal distributed population. To this end, we enforce the parameters to stay statistically close to the mean: \u00132 + \u0012 βi σalb,i \u00132\u0015 + \u0012 δi σexp,i \u00132 . (6) \u0014\u0012 αi 76 X 80 X Ereg(P) = σid,i i=1 i=1 This commonly-used regularization strategy prevents degenerations of the facial geometry and reﬂectance, and guides the optimization strategy out of local minima [3].\n\n5. Data-parallel Optimization Strategy\nThe proposed robust tracking objective is a general unconstrained non-linear optimization problem. We minimize this objective in real-time using a novel data-parallel GPUbased Iteratively Reweighted Least Squares (IRLS) solver. The key idea of IRLS is to transform the problem, in each iteration, to a non-linear least-squares problem by splitting the norm in two components: ||r(P)||2 = (||r(Pold)||2)−1 | {z } constant · ||r(P)||2 2 . Here, r(·) is a general residual and Pold is the solution computed in the last iteration. Thus, the ﬁrst part is kept constant during one iteration and updated afterwards. Close in spirit to [29], each single iteration step is implemented using the Gauss-Newton approach. We take a single GN step in every IRLS iteration and solve the corresponding system of normal equations JT Jδ∗= −JT F based on PCG to obtain an optimal linear parameter update δ∗. The Jacobian J and the systems’ right hand side −JT F are precomputed and stored in device memory for later processing as proposed by Thies et al. [29]. As suggested by [33, 29], we split up the multiplication of the old descent direction d with the system matrix JT J in the PCG solver into two successive matrixvector products. Additional details regarding the optimization framework are provided in the supplemental material.\n\n6. Non-Rigid Model-Based Bundling\non the proposed objective, we jointly estimate all parameters over k key-frames of the input video sequence. The estimated unknowns are the global identity {α, β} and intrinsics κ as well as the unknown per-frame pose {δk, Rk, tk}k and illumination parameters {γk}k. We use a similar data-parallel optimization strategy as proposed for model-to-frame tracking, but jointly solve the normal equations for the entire keyframe set. For our non-rigid modelbased bundling problem, the non-zero structure of the corresponding Jacobian is block dense. Our PCG solver exploits the non-zero structure for increased performance (see additional document). Since all keyframes observe the same face identity under potentially varying illumination, expression, and viewing angle, we can robustly separate identity from all other problem dimensions. Note that we also solve for the intrinsic camera parameters of Π, thus being able to process uncalibrated video footage.\n\n7. Expression Transfer\nTo transfer the expression changes from the source to the target actor while preserving person-speciﬁcness in each actor’s expressions, we propose a sub-space deformation transfer technique. We are inspired by the deformation transfer energy of Sumner et al. [27], but operate directly in the space spanned by the expression blendshapes. This not only allows for the precomputation of the pseudo-inverse of the system matrix, but also drastically reduces the dimensionality of the optimization problem allowing for fast real-time transfer rates. Assuming source identity αS and target identity αT ﬁxed, transfer takes as input the neutral δS N, deformed source δS, and the neutral target δT N expression. Output is the transferred facial expression δT directly in the reduced sub-space of the parametric prior. As proposed by [27], we ﬁrst compute the source deformation gradients Ai ∈R3×3 that transform the source triangles from neutral to deformed. The deformed target ˆvi = M i(αT , δT ) is then found based on the undeformed state vi = M i(αT , δT N) by solving a linear least-squares problem. Let (i0, i1, i2) be the vertex indices of the i-th triangle, V = [vi1 −vi0, vi2 −vi0] and ˆV = [vi1 −ˆvi0, ˆvi2 −ˆvi0], then the optimal unknown target deformation δT is the minimizer of: |F | X AiV −ˆV 2 E(δT ) = F . (7) i=1 This problem can be rewritten in the canonical least-squares form by substitution: E(δT ) = AδT −b 2 2 . (8) Figure 2: Mouth Retrieval: we use an appearance graph to retrieve new mouth frames. In order to select a frame, we enforce similarity to the previously-retrieved frame while minimizing the distance to the target expression. expression is included in the right-hand side b ∈R6|F |. b varies with δS and is computed on the GPU for each new input frame. The minimizer of the quadratic energy can be computed by solving the corresponding normal equations. Since the system matrix is constant, we can precompute its Pseudo Inverse using a Singular Value Decomposition (SVD). Later, the small 76 × 76 linear system is solved in real-time. No additional smoothness term as in [27, 4] is needed, since the blendshape model implicitly restricts the result to plausible shapes and guarantees smoothness.\n\n8. Mouth Retrieval\nFor a given transferred facial expression, we need to synthesize a realistic target mouth region. To this end, we retrieve and warp the best matching mouth image from the target actor sequence. We assume that sufﬁcient mouth variation is available in the target video. It is also important to note that we maintain the appearance of the target mouth. This leads to much more realistic results than either copying the source mouth region [30, 11] or using a generic 3D teeth proxy [14, 29]. Our approach ﬁrst ﬁnds the best ﬁtting target mouth frame based on a frame-to-cluster matching strategy with a novel feature similarity metric. To enforce temporal coherence, we use a dense appearance graph to ﬁnd a compromise between the last retrieved mouth frame and the target mouth frame (cf. Fig. 2). We detail all steps in the following. Similarity Metric Our similarity metric is based on geometric and photometric features. The used descriptor K = {R, δ, F, L} of a frame is composed of the rotation R, expression parameters δ, landmarks F, and a Local Binary Pattern (LBP) L. We compute these descriptors KS for every frame in the training sequence. The target descriptor KT consists of the result of the expression transfer and the LBP of the frame of the driving actor. We measure the distance between a source and a target descriptor as follows: The ﬁrst term Dp measures the distance in parameter space: Dp(KT , KS t ) = ∥δT −δS t ∥2 2 + ∥RT −RS t ∥2 F . The second term Dm measures the differential compatibility of the sparse facial landmarks: \u0000 ∥FT i −FT j ∥2 −∥FS t,i −FS t,j∥2 \u00012 . Dm(KT , KS t ) = X (i,j)∈Ω Here, Ωis a set of predeﬁned landmark pairs, deﬁning distances such as between the upper and lower lip or between the left and right corner of the mouth. The last term Da is an appearance measurement term composed of two parts: Da(KT , KS t , t) = Dl(KT , KS t ) + wc(KT , KS t )Dc(τ, t) . τ is the last retrieved frame index used for the reenactment in the previous frame. Dl(KT , KS t ) measures the similarity based on LBPs that are compared via a Chi Squared Distance (for details see [13]). Dc(τ, t) measures the similarity between the last retrieved frame τ and the video frame t based on RGB cross-correlation of the normalized mouth frames. Note that the mouth frames are normalized based on the models texture parameterization (cf. Fig. 2). To facilitate fast frame jumps for expression changes, we incorporate the weight wc(KT , KS t ) = e−(Dm(KT ,KS t ))2. We apply this frame-to-frame distance measure in a frame-tocluster matching strategy, which enables real-time rates and mitigates high-frequency jumps between mouth frames. Frame-to-Cluster Matching Utilizing the proposed similarity metric, we cluster the target actor sequence into k = 10 clusters using a modiﬁed k-means algorithm that is based on the pairwise distance function D. For every cluster, we select the frame with the minimal distance to all other frames within that cluster as a representative. During runtime, we measure the distances between the target descriptor KT and the descriptors of cluster representatives, and choose the cluster whose representative frame has the minimal distance as the new target frame. Appearance Graph We improve temporal coherence by building a fully-connected appearance graph of all video frames. The edge weights are based on the RGB crosscorrelation between the normalized mouth frames, the distance in parameter space Dp, and the distance of the landmarks Dm. The graph enables us to ﬁnd an inbetween frame that is both similar to the last retrieved frame and the retrieved target frame (see Fig. 2). We compute this perfect match by ﬁnding the frame of the training sequence that minimizes the sum of the edge weights to the last retrieved and current target frame. We blend between the previouslyretrieved frame and the newly-retrieved frame in texture CPU GPU FPS SparseFT MouthRT DenseFT DeformTF Synth 5.97ms 1.90ms 22.06ms 3.98ms 10.19ms 27.6Hz 4.85ms 1.50ms 21.27ms 4.01ms 10.31ms 28.1Hz 5.57ms 1.78ms 20.97ms 3.95ms 10.32ms 28.4Hz Table 1: Avg. run times for the three sequences of Fig. 8, from top to bottom. Standard deviations w.r.t. the ﬁnal frame rate are 0.51, 0.56, and 0.59 fps, respectively. Note that CPU and GPU stages run in parallel. space on a pixel level after optic ﬂow alignment. Before blending, we apply an illumination correction that considers the estimated Spherical Harmonic illumination parameters of the retrieved frames and the current video frame. Finally, we composite the new output frame by alpha blending between the original video frame, the illumination-corrected, projected mouth frame, and the rendered face model.\n\n9. Results\nLive Reenactment Setup Our live reenactment setup consists of standard consumer-level hardware. We capture a live video with a commodity webcam (source), and download monocular video clips from Youtube (target). In our experiments, we use a Logitech HD Pro C920 camera running at 30Hz in a resolution of 640 × 480; although our approach is applicable to any consumer RGB camera. Overall, we show highly-realistic reenactment examples of our algorithm on a variety of target Youtube videos at a resolution of 1280×720. The videos show different subjects in different scenes ﬁlmed from varying camera angles; each video is reenacted by several volunteers as source actors. Reenactment results are generated at a resolution of 1280 × 720. We show real-time reenactment results in Fig. 8 and in the accompanying video. Runtime For all experiments, we use three hierarchy levels for tracking (source and target). In pose optimization, we only consider the second and third level, where we run one and seven Gauss-Newton steps, respectively. Within a Gauss-Newton step, we always run four PCG steps. In addition to tracking, our reenactment pipeline has additional stages whose timings are listed in Table 1. Our method runs in real-time on a commodity desktop computer with an NVIDIA Titan X and an Intel Core i7-4770. Tracking Comparison to Previous Work Face tracking alone is not the main focus of our work, but the following comparisons show that our tracking is on par with or exceeds the state of the art. Shi et al. 2014 [26]: They capture face performances ofﬂine from monocular unconstrained RGB video. The closeups in Fig. 4 show that our online approach yields a closer Figure 3: Comparison of our RGB tracking to Cao et al. [7], and to RGB-D tracking by Thies et al. [29]. Figure 4: Comparison of our tracking to Shi et al. [26]. From left to right: RGB input, reconstructed model, overlay with input, close-ups on eye and cheek. Note that Shi et al. perform shape-from-shading in a post process. face ﬁt, particularly visible at the silhouette of the input face. We believe that our new dense non-rigid bundle adjustment leads to a better shape identity estimate than their sparse approach. Cao et al. 2014 [7]: They capture face performance from monocular RGB in real-time. In most cases, our and their method produce similar high-quality results (see Fig. 3); our identity and expression estimates are slightly more accurate though. Thies et al. 2015 [29]: Their approach captures face performance in real-time from RGB-D, Fig. 3. Results of both approaches are similarly accurate; but our approach does not require depth data. Figure 5: Comparison against FaceShift RGB-D tracking. Figure 6: Dubbing: Comparison to Garrido et al. [14]. FaceShift 2014: We compare our tracker to the commercial real-time RGB-D tracker from FaceShift, which is based on the work of Weise et al. [31]. Fig. 5 shows that we obtain similar results from RGB only. Reenactment Evaluation In Fig. 6, we compare our approach against state-of-the art reenactment by Garrido et al. [14]. Both methods provide highly-realistic reenactment results; however, their method is fundamentally ofﬂine, as they require all frames of a sequence to be present at any time. In addition, they rely on a generic geometric teeth proxy which in some frames makes reenactment less convincing. In Fig. 7, we compare against the work by Thies et al. [29]. Runtime and visual quality are similar for both approaches; however, their geometric teeth proxy leads to undesired appearance changes in the reenacted mouth. Moreover, Thies et al. use an RGB-D camera, which limits the application range; they cannot reenact Youtube videos. We show additional comparisons in the supplemental material against Dale et al. [11] and Garrido et al. [13].\n\n2. Related Work\nOfﬂine RGB Performance Capture Recent ofﬂine performance capture techniques approach the hard monocular reconstruction problem by ﬁtting a blendshape [15] or a multi-linear face [26] model to the input video sequence. Even geometric ﬁne-scale surface detail is extracted via inverse shading-based surface reﬁnement. Ichim et al. [17] build a personalized face rig from just monocular input. They perform a structure-from-motion reconstruction of the static head from a speciﬁcally captured video, to which they ﬁt an identity and expression model. Person-speciﬁc expressions are learned from a training sequence. Suwajanakorn et al. [28] learn an identity model from a collection of images and track the facial animation based on a model-to-image ﬂow ﬁeld. Shi et al. [26] achieve impressive results based on global energy optimization of a set of selected keyframes. Our model-based bundling formulation to recover actor identities is similar to their approach; however, we use robust and dense global photometric alignment, which we enforce with an efﬁcient data-parallel optimization strategy on the GPU. Online RGB-D Performance Capture Weise et al. [32] capture facial performances in real-time by ﬁtting a parametric blendshape model to RGB-D data, but they require a professional, custom capture setup. The ﬁrst real-time facial performance capture system based on a commodity depth sensor has been demonstrated by Weise et al. [31]. Follow up work [21, 4, 10, 16] focused on corrective shapes [4], dynamically adapting the blendshape basis [21], nonrigid mesh deformation [10], and robustness against occlusions [16]. These works achieve impressive results, but rely on depth data which is typically unavailable in most video footage. Online RGB Performance Capture While many sparse real-time face trackers exist, e.g., [25], real-time dense monocular tracking is the basis of realistic online facial reenactment. Cao et al. [8] propose a real-time regressionbased approach to infer 3D positions of facial landmarks which constrain a user-speciﬁc blendshape model. Followup work [6] also regresses ﬁne-scale face wrinkles. These methods achieve impressive results, but are not directly applicable as a component in facial reenactment, since they do not facilitate dense, pixel-accurate tracking. Ofﬂine Reenactment Vlasic et al. [30] perform facial reenactment by tracking a face template, which is rerendered under different expression parameters on top of the target; the mouth interior is directly copied from the source video. Dale et al. [11] achieve impressive results using a parametric model, but they target face replacement and compose the source face over the target. Image-based ofﬂine mouth re-animation was shown in [5]. Garrido et al. [13] propose an automatic purely image-based approach to replace the entire face. These approaches merely enable self-reenactment; i.e., when source and target are the same person; in contrast, we perform reenactment of a different target actor. Recent work presents virtual dubbing [14], a problem similar to ours; however, the method runs at slow ofﬂine rates and relies on a generic teeth proxy for the mouth interior. Kemelmacher et al. [20] generate face animations from large image collections, but the obtained results lack temporal coherence. Li et al. [22] retrieve frames from a database based on a similarity metric. They use optical ﬂow as appearance and velocity measure and search for the k-nearest neighbors based on time stamps and ﬂow Figure 1: Method overview. distance. Saragih et al. [25] present a real-time avatar animation system from a single image. Their approach is based on sparse landmark tracking, and the mouth of the source is copied to the target using texture warping. Berthouzoz et al. [2] ﬁnd a ﬂexible number of in-between frames for a video sequence using shortest path search on a graph that encodes frame similarity. Kawai et al. [18] re-synthesize the inner mouth for a given frontal 2D animation using a tooth and tongue image database; they are limited to frontal poses, and do not produce as realistic renderings as ours under general head motion. Online Reenactment Recently, ﬁrst online facial reenactment approaches based on RGB-(D) data have been proposed. Kemelmacher-Shlizerman et al. [19] enable imagebased puppetry by querying similar images from a database. They employ an appearance cost metric and consider rotation angular distance, which is similar to Kemelmacher et al. [20]. While they achieve impressive results, the retrieved stream of faces is not temporally coherent. Thies et al. [29] show the ﬁrst online reenactment system; however, they rely on depth data and use a generic teeth proxy for the mouth region. In this paper, we address both shortcomings: 1) our method is the ﬁrst real-time RGB-only reenactment technique; 2) we synthesize the mouth regions exclusively from the target sequence (no need for a teeth proxy or direct source-to-target copy).\n\n11. Conclusion\nThe presented approach is the ﬁrst real-time facial reenactment system that requires just monocular RGB input. Our live setup enables the animation of legacy video footage – e.g., from Youtube – in real time. Overall, we believe our system will pave the way for many new and exciting applications in the ﬁelds of VR/AR, teleconferencing, or on-theﬂy dubbing of videos with translated audio.\n\nAcknowledgements We would like to thank Chen Cao and Kun Zhou for the blendshape models and comparison data, as well as Volker Blanz, Thomas Vetter, and Oleg Alexander for the provided face data. The facial landmark tracker was kindly provided by TrueVisionSolution. We thank Angela Dai for the video voice over and Daniel Ritchie for video reenactment. This research is funded by the German Research Foundation (DFG), grant GRK-1773 Heterogeneous Image Systems, the ERC Starting Grant 335545 CapReal, and the Max Planck Center for Visual Computing and Communications (MPC-VCC). We also gratefully acknowledge the support from NVIDIA Corporation for hardware donations.\n\nReferences\n[1] O. Alexander, M. Rogers, W. Lambeth, M. Chiang, and\nP. Debevec. The Digital Emily Project: photoreal facial modeling and animation. In ACM SIGGRAPH Courses, pages\n12:1–12:15. ACM, 2009. [2] F. Berthouzoz, W. Li, and M. Agrawala. Tools for placing cuts and transitions in interview video. ACM TOG, 31(4):67, 2012. [3] V. Blanz and T. Vetter. A morphable model for the synthesis of 3d faces. In Proc. SIGGRAPH, pages 187–194. ACM Press/Addison-Wesley Publishing Co., 1999. [4] S. Bouaziz, Y. Wang, and M. Pauly. Online modeling for realtime facial animation. ACM TOG, 32(4):40, 2013. [5] C. Bregler, M. Covell, and M. Slaney. Video rewrite: Driving visual speech with audio. In Proc. SIGGRAPH, pages 353–\n360. ACM Press/Addison-Wesley Publishing Co., 1997.\n[6] C. Cao, D. Bradley, K. Zhou, and T. Beeler. Real-time highﬁdelity facial performance capture. ACM TOG, 34(4):46:1– 46:9, 2015. [7] C. Cao, Q. Hou, and K. Zhou. Displaced dynamic expression regression for real-time facial tracking and animation. ACM TOG, 33(4):43, 2014. [8] C. Cao, Y. Weng, S. Lin, and K. Zhou. 3D shape regression for real-time facial animation. ACM TOG, 32(4):41, 2013. [9] C. Cao, Y. Weng, S. Zhou, Y. Tong, and K. Zhou. Facewarehouse: A 3D facial expression database for visual computing. IEEE TVCG, 20(3):413–425, 2014. [10] Y.-L. Chen, H.-T. Wu, F. Shi, X. Tong, and J. Chai. Accurate and robust 3d facial capture using a single rgbd camera. Proc. ICCV, pages 3615–3622, 2013. [11] K. Dale, K. Sunkavalli, M. K. Johnson, D. Vlasic, W. Matusik, and H. Pﬁster. Video face replacement. ACM TOG, 30(6):130, 2011. [12] C. H. Q. Ding, D. Zhou, X. He, and H. Zha. R1-pca: rotational invariant l1-norm principal component analysis for robust subspace factorization. In W. W. Cohen and A. Moore, editors, ICML, volume 148 of ACM International Conference Proceeding Series, pages 281–288. ACM, 2006. [13] P. Garrido, L. Valgaerts, O. Rehmsen, T. Thormaehlen,\nP. Perez, and C. Theobalt. Automatic face reenactment. In\nProc. CVPR, 2014. [14] P. Garrido, L. Valgaerts, H. Sarmadi, I. Steiner, K. Varanasi,\nP. Perez, and C. Theobalt. Vdub: Modifying face video of\nactors for plausible visual alignment to a dubbed audio track. In Computer Graphics Forum. Wiley-Blackwell, 2015. [15] P. Garrido, L. Valgaerts, C. Wu, and C. Theobalt. Reconstructing detailed dynamic face geometry from monocular video. ACM TOG, 32(6):158, 2013. [16] P.-L. Hsieh, C. Ma, J. Yu, and H. Li. Unconstrained realtime facial performance capture. In Computer Vision and Pattern Recognition (CVPR), 2015. [17] A. E. Ichim, S. Bouaziz, and M. Pauly. Dynamic 3d avatar creation from hand-held video input. ACM TOG, 34(4):45:1– 45:14, 2015. [18] M. Kawai, T. Iwao, D. Mima, A. Maejima, and S. Morishima. Data-driven speech animation synthesis focusing on realistic inside of the mouth. Journal of Information Processing, 22(2):401–409, 2014. [19] I. Kemelmacher-Shlizerman, A. Sankar, E. Shechtman, and\nS. M. Seitz.\nBeing john malkovich. In Computer Vision - ECCV 2010, 11th European Conference on Computer Vision, Heraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part I, pages 341–353, 2010. [20] I. Kemelmacher-Shlizerman, E. Shechtman, R. Garg, and\nS. M. Seitz.\nExploring photobios. ACM TOG, 30(4):61, 2011. [21] H. Li, J. Yu, Y. Ye, and C. Bregler. Realtime facial animation with on-the-ﬂy correctives. ACM TOG, 32(4):42, 2013. [22] K. Li, F. Xu, J. Wang, Q. Dai, and Y. Liu. A data-driven approach for facial expression synthesis in video. In Proc. CVPR, pages 57–64, 2012. [23] R. Ramamoorthi and P. Hanrahan. A signal-processing framework for inverse rendering. In Proc. SIGGRAPH, pages 117–128. ACM, 2001. [24] J. M. Saragih, S. Lucey, and J. F. Cohn. Deformable model ﬁtting by regularized landmark mean-shift. IJCV, 91(2):200– 215, 2011. [25] J. M. Saragih, S. Lucey, and J. F. Cohn. Real-time avatar animation from a single image. In Automatic Face and Gesture Recognition Workshops, pages 213–220, 2011. [26] F. Shi, H.-T. Wu, X. Tong, and J. Chai. Automatic acquisition of high-ﬁdelity facial performances using monocular videos. ACM TOG, 33(6):222, 2014. [27] R. W. Sumner and J. Popovi´c. Deformation transfer for triangle meshes. ACM TOG, 23(3):399–405, 2004. [28] S. Suwajanakorn, I. Kemelmacher-Shlizerman, and S. M. Seitz. Total moving face reconstruction. In Proc. ECCV, pages 796–812, 2014. [29] J. Thies, M. Zollh¨ofer, M. Nießner, L. Valgaerts, M. Stamminger, and C. Theobalt. Real-time expression transfer for facial reenactment. ACM Transactions on Graphics (TOG), 34(6), 2015. [30] D. Vlasic, M. Brand, H. Pﬁster, and J. Popovi´c. Face transfer with multilinear models. ACM TOG, 24(3):426–433, 2005. [31] T. Weise, S. Bouaziz, H. Li, and M. Pauly. Realtime performance-based facial animation. 30(4):77, 2011. [32] T. Weise, H. Li, L. V. Gool, and M. Pauly. Face/off: Live facial puppetry. In Proceedings of the 2009 ACM SIGGRAPH/Eurographics Symposium on Computer animation (Proc. SCA’09), ETH Zurich, August 2009. Eurographics Association. [33] M. Zollh¨ofer, M. Nießner, S. Izadi, C. Rehmann, C. Zach,\nM. Fisher, C. Wu, A. Fitzgibbon, C. Loop, C. Theobalt, and\nM. Stamminger. Real-time Non-rigid Reconstruction using\nan RGB-D Camera. ACM TOG, 33(4):156, 2014.\n--- FOOTNOTES 1University of Erlangen-Nuremberg 2Max-Planck-Institute for Informatics 3Stanford University", "persona": "General Public", "response": "### 1. Overview\nOnce upon a time, a photo of something was enough to believe it was real. Sure, you’d have to deal with the occasional Big Foot hoax, but for the most part, those who had the time or talent to create believable fakes were in the minority. Then came the age of Photoshop. Edits and fakes are prolific enough that “FAKE!” has become the default; a dubious photo is presumed fake unless proven otherwise. It’s not pixel-perfect yet — even in the relatively low-res clips we’re shown, there’s an uncanny valley effect of something being not quite right. But hot damn is it impressive (and, well, more than a little spooky) even in this early stage. The short version: take a YouTube video of someone speaking like, say, George W. Bush.\n\n### 2. Background\nThat might not be the case for long. Use a standard RGB webcam to capture a video of someone else emoting and saying something entirely different. Throw both videos into the Face2Face system and, bam, you’ve now got a relatively believable video of George W. Bush’s face — now almost entirely synthesized — doing whatever the actor in the second video wanted the target’s face to do. It even tries to work out what the interior of their mouth should look like as they’re speaking.\n\n### 3. Methods\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections.\n\n### 4. Results\nSave $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\n### 5. Takeaways\nWe’re not quite to that point with video. Fake videos exist in droves, obviously, but editing a video to be something it’s not introduces a bevy of challenges not found in the editing of a single still frame, and generally requires considerably more time and talent to do right. People will still yell “FAKE!” but it’ll be a quieter yell. As your Facebook feed probably proves, moderately well-faked videos have a much easier time finding believers. Why spooky? Technology like this will serve to make video less inherently believable. The video’s use of politicians as the editing target is pretty self-aware. In that regard, political hoaxes will hit a lot harder when it’s a video instead of a ‘shopped picture being forwarded around. Don’t freak out too hard, though. Hoaxes have existed in every medium throughout history. This tech isn’t widely available beyond its researchers just yet; the uncanny valley challenges in stepping from “somethings-kinda-off” to pixel-perfect infallibility aren’t small ones. Just remember that, just like photos before it, being on video doesn’t mean it’s real — and that gets a lil’ bit truer each day.", "paper_id": "13", "story_id": "13-02"}
