{"paper": "Robust Speech Recognition via Large-Scale Weak Supervision Alec Radford * 1 Jong Wook Kim * 1 Tao Xu 1 Greg Brockman 1 Christine McLeavey 1 Ilya Sutskever 1\n\nAbstract\nWe study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any ﬁnetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing. arXiv:2212.04356v1  [eess.AS]  6 Dec 2022\n\n1. Introduction\nProgress in speech recognition has been energized by the development of unsupervised pre-training techniques exempliﬁed by Wav2Vec 2.0 (Baevski et al., 2020). Since these methods learn directly from raw audio without the need for human labels, they can productively use large datasets of unlabeled speech and have been quickly scaled up to 1,000,000 hours of training data (Zhang et al., 2021), far more than the 1,000 or so hours typical of an academic supervised dataset. When ﬁne-tuned on standard benchmarks, this approach has improved the state of the art, especially in a low-data setting. These pre-trained audio encoders learn high-quality representations of speech, but because they are purely unsupervised they lack an equivalently performant decoder mapping those representations to usable outputs, necessitating a ﬁnetuning stage in order to actually perform a task such as speech recognition1. This unfortunately limits their usefulness and impact as ﬁne-tuning can still be a complex process requiring a skilled practitioner. There is an additional risk with requiring ﬁne-tuning. Machine learning Wook Kim <jongwook@openai.com>. methods are exceedingly adept at ﬁnding patterns within a training dataset which boost performance on held-out data from the same dataset. However, some of these patterns are brittle and spurious and don’t generalize to other datasets and distributions. In a particularly disturbing example, Radford et al. (2021) documented a 9.2% increase in object classiﬁcation accuracy when ﬁne-tuning a computer vision model on the ImageNet dataset (Russakovsky et al., 2015) without observing any improvement in average accuracy when classifying the same objects on seven other natural image datasets. A model that achieves “superhuman” performance when trained on a dataset can still make many basic errors when evaluated on another, possibly precisely because it is exploiting those dataset-speciﬁc quirks that humans are oblivious to (Geirhos et al., 2020). This suggests that while unsupervised pre-training has improved the quality of audio encoders dramatically, the lack of an equivalently high-quality pre-trained decoder, combined with a recommended protocol of dataset-speciﬁc ﬁnetuning, is a crucial weakness which limits their usefulness and robustness. The goal of a speech recognition system should be to work reliably “out of the box” in a broad range of environments without requiring supervised ﬁne-tuning of a decoder for every deployment distribution. As demonstrated by Narayanan et al. (2018), Likhomanenko et al. (2020), and Chan et al. (2021) speech recognition systems that are pre-trained in a supervised fashion across many datasets/domains exhibit higher robustness and generalize much more effectively to held-out datasets than models trained on a single source. These works achieve this by combining as many existing high-quality speech recognition datasets as possible. However, there is still only a moderate amount of this data easily available. SpeechStew (Chan et al., 2021) mixes together 7 pre-existing datasets totalling 5,140 hours of supervision. While not insigniﬁcant, this is still tiny compared to the previously mentioned 1,000,000 hours of unlabeled speech data utilized in Zhang et al. (2021). Recognizing the limiting size of existing high-quality supervised datasets, recent efforts have created larger datasets for speech recognition. By relaxing the requirement of goldstandard human-validated transcripts, Chen et al. (2021) and Galvez et al. (2021) make use of sophisticated automated pipelines to scale weakly supervised speech recognition to 10,000 and 30,000 hours of noisier training data. This trade-off between quality and quantity is often the right call. Although understudied so far for speech recognition, recent work in computer vision has demonstrated that moving beyond gold-standard crowdsourced datasets such as ImageNet (Russakovsky et al., 2015) to much larger but weakly supervised datasets signiﬁcantly improves the robustness and generalization of models (Mahajan et al., 2018; Kolesnikov et al., 2020). Yet these new datasets are only a few times larger than the sum of existing high-quality datasets and still much smaller than prior unsupervised work. In this work we close that gap, scaling weakly supervised speech recognition the next order of magnitude to 680,000 hours of labeled audio data. We call our approach Whisper2. We demonstrate models trained at this scale transfer well to existing datasets zeroshot, removing the need for any dataset-speciﬁc ﬁne-tuning to achieve high-quality results. In addition to scale, our work also focuses on broadening the scope of weakly supervised pre-training beyond English-only speech recognition to be both multilingual and multitask. Of those 680,000 hours of audio, 117,000 hours cover 96 other languages. The dataset also includes 125,000 hours of X→en translation data. We ﬁnd that for sufﬁciently large models there is no drawback and even beneﬁts to joint multilingual and multitask training. Our work suggests that simple scaling of weakly supervised pre-training has been underappreciated so far for speech recognition. We achieve these results without the need for the self-supervision or self-training techniques that have been a mainstay of recent large-scale speech recognition work. To serve as a foundation for further research on robust speech recognition, we release inference code and models at the following URL: https://github.com/openai/whisper.\n\n2. Approach\n2.1. Data Processing Following the trend of recent work leveraging web-scale text from the internet for training machine learning systems, we take a minimalist approach to data pre-processing. In contrast to a lot of work on speech recognition, we train Whisper models to predict the raw text of transcripts without any signiﬁcant standardization, relying on the expressiveness of sequence-to-sequence models to learn to map between utterances and their transcribed form. This simpliﬁes the speech recognition pipeline since it removes the need for a separate inverse text normalization step in order to produce naturalistic transcriptions. We construct the dataset from audio that is paired with transcripts on the Internet. This results in a very diverse dataset covering a broad distribution of audio from many different environments, recording setups, speakers, and languages. While diversity in audio quality can help train a model to be robust, diversity in transcript quality is not similarly beneﬁcial. Initial inspection showed a large amount of subpar transcripts in the raw dataset. To address this, we developed several automated ﬁltering methods to improve transcript quality. Many transcripts on the internet are not actually humangenerated but the output of existing ASR systems. Recent research has shown that training on datasets of mixed human and machine-generated data can signiﬁcantly impair the performance of translation systems (Ghorbani et al., 2021). In order to avoid learning “transcript-ese”, we developed many heuristics to detect and remove machine-generated transcripts from the training dataset. Many existing ASR systems output only a limited subset of written language which removes or normalizes away aspects that are difﬁcult to predict from only audio signals such as complex punctuation (exclamation points, commas, and question marks), formatting whitespace such as paragraphs, or stylistic aspects such as capitalization. An all-uppercase or all-lowercase transcript is very unlikely to be human generated. While many ASR systems include some level of inverse text normalization, it is often simple or rule-based and still detectable from other unhandled aspects such as never including commas. We also use an audio language detector, which was created by ﬁne-tuning a prototype model trained on a prototype version of the dataset on VoxLingua107 (Valk & Alum¨ae, 2021) to ensure that the spoken language matches the language of the transcript according to CLD2. If the two do not match, we don’t include the (audio, transcript) pair as a speech recognition training example in the dataset. We make an exception if the transcript language is English and add these pairs to the dataset as X→en speech translation training examples instead. We use fuzzy de-duping of transcript texts to reduce the amount of duplication and automatically generated content in the training dataset. We break audio ﬁles into 30-second segments paired with the subset of the transcript that occurs within that time segment. We train on all audio, including segments where there is no speech (though with sub-sampled probability) and use these segments as training data for voice activity detection. data sources and performed manual inspection of these data sources sorting by a combination of both high error rate and data source size in order to identify and remove low-quality ones efﬁciently. This inspection showed a large amount of only partially transcribed or poorly aligned/misaligned transcripts as well as remaining low-quality machine-generated captions that ﬁltering heuristics did not detect. To avoid contamination, we perform de-duplication at a transcript level between the training dataset and the evaluation datasets we thought were at higher risk of overlap, namely TED-LIUM 3 (Hernandez et al., 2018). 2.2. Model Since the focus of our work is on studying the capabilities of large-scale supervised pre-training for speech recognition, we use an off-the-shelf architecture to avoid confounding our ﬁndings with model improvements. We chose an encoder-decoder Transformer (Vaswani et al., 2017) as this architecture has been well validated to scale reliably. All audio is re-sampled to 16,000 Hz, and an 80-channel logmagnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds. For feature normalization, we globally scale the input to be between -1 and 1 with approximately zero mean across the pre-training dataset. The encoder processes this input representation with a small stem consisting of two convolution layers with a ﬁlter width of 3 and the GELU activation function (Hendrycks & Gimpel, 2016) where the second convolution layer has a stride of two. Sinusoidal position embeddings are then added to the output of the stem after which the encoder Transformer blocks are applied. The transformer uses pre-activation residual blocks (Child et al., 2019), and a ﬁnal layer normalization is applied to the encoder output. The decoder uses learned position embeddings and tied input-output token representations (Press & Wolf, 2017). The encoder and decoder have the same width and number of transformer blocks. Figure 1 summarizes the model architecture. We use the same byte-level BPE text tokenizer used in GPT2 (Sennrich et al., 2015; Radford et al., 2019) for the Englishonly models and reﬁt the vocabulary (but keep the same size) for the multilingual models to avoid excessive fragmentation on other languages since the GPT-2 BPE vocabulary is English only. 2.3. Multitask Format Although predicting which words were spoken in a given audio snippet is a core part of the full speech recognition problem and extensively studied in research, it is not the only part. A fully featured speech recognition system can involve many additional components such as voice activity detection, speaker diarization, and inverse text normal- ization. These components are often handled separately, resulting in a relatively complex system around the core speech recognition model. To reduce this complexity, we would like to have a single model perform the entire speech processing pipeline, not just the core recognition part. An important consideration here is the interface for the model. There are many different tasks that can be performed on the same input audio signal: transcription, translation, voice activity detection, alignment, and language identiﬁcation are some examples. For this kind of one-to-many mapping to work with a single model, some form of task speciﬁcation is necessary. We use a simple format to specify all tasks and conditioning information as a sequence of input tokens to the decoder. Since our decoder is an audio-conditional language model, we also train it to condition on the history of text of the transcript in the hope that it will learn to use longer-range text context to resolve ambiguous audio. Speciﬁcally, with some probability we add the transcript text preceding the current audio segment to the decoder’s context. We indicate the beginning of prediction with a <|startoftranscript|> token. First, we predict the language being spoken which is represented by a unique token for each language in our training set (99 total). These language targets are sourced from the aforementioned VoxLingua107 model. In the case where there is no speech in an audio segment, the model is trained to predict a <|nospeech|> token indicating this. The next token speciﬁes the task (either transcription or translation) with an <|transcribe|> or <|translate|> token. After this, we specify whether to predict timestamps or not by including a <|notimestamps|> token for that case. At this point, the task and desired format is fully speciﬁed, and the output begins. For timestamp prediction, we predict time relative to the current audio segment, quantizing all times to the nearest 20 milliseconds which matches the native time resolution of Whisper models, and add additional tokens to our vocabulary for each of these. We interleave their prediction with the caption tokens: the start time token is predicted before each caption’s text, and the end time token is predicted after. When a ﬁnal transcript segment is only partially included in the current 30second audio chunk, we predict only its start time token for the segment when in timestamp mode, to indicate that the subsequent decoding should be performed on an audio window aligned with that time, otherwise we truncate the audio to not include the segment. Lastly, we add a <|endoftranscript|> token. We only mask out the training loss over the previous context text, and train the model to predict all other tokens. Please see Figure 1 for an overview of our format and training setup. Multitask training data (680k hours) Sequence-to-sequence learning English transcription   “Ask not what your country can do for ⋯”   Ask not what your country can do for ⋯ Any-to-English speech translation   “El rápido zorro marrón salta sobre ⋯”   The quick brown fox jumps over ⋯ Transformer Encoder Blocks Transformer Decoder Blocks Non-English transcription  “언덕 위에 올라 내려다보면 너무나 넓고 넓은 ⋯”   언덕 위에 올라 내려다보면 너무나 넓고 넓은 ⋯ ~ Sinusoidal Positional Encoding No speech  (background music playing)   ∅ Multitask training format X → X Transcription Language identification LANGUAGE TRANSCRIBE TAG previous text tokens START OF TRANSCRIPT PREV NO SPEECH TRANSLATE Custom vocabulary / prompting Voice activity X → English detection Translation special tokens text tokens timestamp (VAD) tokens Figure 1. Overview of our approach. A sequence-to-sequence Transformer model is trained on many different speech processing tasks, including multilingual speech recognition, speech translation, spoken language identiﬁcation, and voice activity detection. All of these tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task speciﬁers or classiﬁcation targets, as further explained in Section 2.3. 2.4. Training Details We train a suite of models of various sizes in order to study the scaling properties of Whisper. Please see Table 1 for an overview. We train with data parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing (Griewank & Walther, 2000; Chen et al., 2016). Models were trained with AdamW (Loshchilov & Hutter, 2017) and gradient norm clipping (Pascanu et al., 2013) with a linear learning rate decay to zero after a warmup over the ﬁrst 2048 updates. A batch size of 256 segments was used, and the models are trained for 220 updates which is between two and three passes over the dataset. Due to only training for a few epochs, over-ﬁtting is not a large concern, and we do not use any data augmentation or regularization and instead rely on the diversity contained within such a TRANS- ⋯ EN 0.0 The quick brown CRIBE next-token prediction MLP MLP cross attention self attention self attention cross attention ⋮ ⋮ ⋮ MLP MLP self attention cross attention self attention MLP self attention MLP cross attention self attention Learned Positional Encoding 2 × Conv1D + GELU TRANS- ⋯ SOT EN CRIBE 0.0 The quick Log-Mel Spectrogram Tokens in Multitask Training Format Time-aligned transcription begin ⋯ end time text tokens begin end time text tokens time time EOT NO TIMESTAMPS text tokens Text-only transcription (allows dataset-specific fine-tuning) large dataset to encourage generalization and robustness. Please see Appendix F for full training hyperparameters.3 During early development and evaluation we observed that Whisper models had a tendency to transcribe plausible but almost always incorrect guesses for the names of speakers. This happens because many transcripts in the pre-training dataset include the name of the person who is speaking, encouraging the model to try to predict them, but this information is only rarely inferable from only the most recent 30 Large model (denoted V2) for 2.5X more epochs while adding SpecAugment (Park et al., 2019), Stochastic Depth (Huang et al., 2016), and BPE Dropout (Provilkov et al., 2019) for regularization. Reported results have been updated to this improved model unless otherwise speciﬁed. Model Layers Width Heads Parameters Tiny 4 384 6 Base 6 512 8 Small 12 768 12 Medium 24 1024 16 Large 32 1280 20 Table 1. Architecture details of the Whisper model family. seconds of audio context. To avoid this, we ﬁne-tune Whisper models brieﬂy on the subset of transcripts that do not include speaker annotations which removes this behavior.\n\n3. Experiments\n3.1. Zero-shot Evaluation The goal of Whisper is to develop a single robust speech processing system that works reliably without the need for dataset speciﬁc ﬁne-tuning to achieve high-quality results on speciﬁc distributions. To study this capability, we reuse a wide set of existing speech processing datasets to check whether Whisper is able to generalize well across domains, tasks, and languages. Instead of using the standard evaluation protocol for these datasets, which include both a train and test split, we evaluate Whisper in a zero-shot setting without using any of the training data for each of these datasets so that we are measuring broad generalization. 3.2. Evaluation Metrics Speech recognition research typically evaluates and compares systems based on the word error rate (WER) metric. However, WER, which is based on string edit distance, penalizes all differences between the model’s output and the reference transcript including innocuous differences in transcript style. As a result, systems that output transcripts that would be judged as correct by humans can still have a large WER due to minor formatting differences. While this poses a problem for all transcribers, it is particularly acute for zero-shot models like Whisper, which do not observe any examples of speciﬁc datasets transcript formats. This is not a novel observation; the development of evaluation metrics that better correlate with human judgement is an active area of research, and while there are some promising methods, none have seen widespread adoption for speech recognition yet. We opt to address this problem with extensive standardization of text before the WER calculation to minimize penalization of non-semantic differences. Our text normalizer was developed through iterative manual inspection to identify common patterns where naive WER penalized Whisper models for an innocuous difference. Appendix C includes full details. For several datasets, we observe WER drops of up to 50 percent usually due to a quirk such as a dataset’s reference transcripts seperating contractions from words with whitespace. We caution this development procedure comes at a risk of overﬁtting to the transcription style of Whisper models which we investigate in Section 4.4. We are releasing the code for our text normalizer to allow for easy comparison and to help others study the performance of speech recognition systems in out-of-distribution settings. 3.3. English Speech Recognition In 2015, Deep Speech 2 (Amodei et al., 2015) reported a speech recognition system matched human-level performance when transcribing the LibriSpeech test-clean split. As part of their analysis they concluded: “Given this result, we suspect that there is little room for a generic speech system to further improve on clean read speech without further domain adaptation.” Yet seven years later the SOTA WER on LibriSpeech test-clean has dropped another 73% from their 5.3% to 1.4% (Zhang et al., 2021), far below their reported human-level error rate of 5.8%. Despite this massive and unanticipated further improvement in performance on held-out but in-distribution data, speech recognition models trained on LibriSpeech remain far above human error rates when used in other settings. What explains this gap between reportedly superhuman performance in-distribution and subhuman performance out-of-distribution? We suspect a large part of this gap between human and machine behavior is due to conﬂating different capabilities being measured by human and machine performance on a test set. This claim may seem confusing at ﬁrst; if both humans and machines are taking the same test, how can it be that different skills are being tested? The difference arises not in the testing but in how they trained for it. Humans are often asked to perform a task given little to no supervision on the speciﬁc data distribution being studied. Thus human performance is a measure of out-of-distribution generalization. But machine learning models are usually evaluated after training on a large amount of supervision from the evaluation distribution, meaning that machine performance is instead a measure of in-distribution generalization. While both humans and machines are being evaluated on the same test data, two quite different abilities are being measured due to a difference in train data. Whisper models, which are trained on a broad and diverse distribution of audio and evaluated in a zero-shot setting, could potentially match human behavior much better than existing systems. To study whether this is the case (or whether the difference between machine and human performance is due to yet-to-be-understood factors) we can compare Whisper models with both human performance and standard ﬁne-tuned machine learning models and check which they more closely match. 50 Average WER on [Common Voice, CHiME-6, TED-LIUM] (%) 40 30 20 10 0 0 1 2 3 4 5 6 7 8 WER on LibriSpeech dev-clean (%) Figure 2. Zero-shot Whisper models close the gap to human robustness. Despite matching or outperforming a human on LibriSpeech dev-clean, supervised LibriSpeech models make roughly twice as many errors as a human on other datasets demonstrating their brittleness and lack of robustness. The estimated robustness frontier of zero-shot Whisper models, however, includes the 95% conﬁdence interval for this particular human. To quantify this difference, we examine both overall robustness, that is average performance across many distributions/datasets, and effective robustness, introduced by Taori et al. (2020), which measures the difference in expected performance between a reference dataset, which is usually in-distribution, and one or more out-of-distribution datasets. A model with high effective robustness does better than expected on out-of-distribution datasets as a function of its performance on the reference dataset and approaches the ideal of equal performance on all datasets. For our analysis, we use LibriSpeech as the reference dataset due to its central role in modern speech recognition research and the availability of many released models trained on it, which allows for characterizing robustness behaviors. We use a suite of 12 other academic speech recognition datasets to study out-of-distribution behaviors. Full details about these datasets can be found in Appendix A. Our main ﬁndings are summarized in Figure 2 and Table 2. Although the best zero-shot Whisper model has a relatively unremarkable LibriSpeech clean-test WER of 2.5, which is roughly the performance of modern supervised baseline or the mid-2019 state of the art, zero-shot Whisper models have very different robustness properties than supervised LibriSpeech models and out-perform all benchmarked LibriSpeech models by large amounts on other datasets. Even wav2vec 2.0 Whisper RER Dataset Large (no LM) Large V2 (%) LibriSpeech Clean 2.7 2.7 0.0 Supervised LibriSpeech models Zero-shot Whisper models Zero-shot Human (Alec) Ideal robustness (y = x) Artie 24.5 6.2 74.7 Common Voice 29.9 9.0 69.9 Fleurs En 14.6 4.4 69.9 Tedlium 10.5 4.0 61.9 CHiME6 65.8 25.5 61.2 VoxPopuli En 17.9 7.3 59.2 CORAAL 35.6 16.2 54.5 AMI IHM 37.0 16.9 54.3 Switchboard 28.3 13.8 51.2 CallHome 34.8 17.6 49.4 WSJ 7.7 3.9 49.4 AMI SDM1 67.6 36.4 46.2 LibriSpeech Other 6.2 5.2 16.1 Average 29.3 12.8 55.2 Table 2. Detailed comparison of effective robustness across various datasets. Although both models perform within 0.1% of each other on LibriSpeech, a zero-shot Whisper model performs much better on other datasets than expected for its LibriSpeech performance and makes 55.2% less errors on average. Results reported in word error rate (WER) for both models after applying our text normalizer. the smallest zero-shot Whisper model, which has only 39 million parameters and a 6.7 WER on LibriSpeech test-clean is roughly competitive with the best supervised LibriSpeech model when evaluated on other datasets. When compared to a human in Figure 2, the best zero-shot Whisper models roughly match their accuracy and robustness. For a detailed breakdown of this large improvement in robustness, Table 2 compares the performance of the best zero-shot Whisper model with a supervised LibriSpeech model that has the closest performance to it on LibriSpeech test-clean. Despite their very close performance on the reference distribution, the zero-shot Whisper model achieves an average relative error reduction of 55.2% when evaluated on other speech recognition datasets. This ﬁnding suggests emphasizing zero-shot and out-ofdistribution evaluations of models, particularly when attempting to compare to human performance, to avoid overstating the capabilities of machine learning systems due to misleading comparisons. 3.4. Multi-lingual Speech Recognition In order to compare to prior work on multilingual speech recognition, we report results on two low-data benchmarks: Multilingual LibriSpeech (MLS) (Pratap et al., 2020b) and VoxPopuli (Wang et al., 2021) in Table 3. 160 MY KA BN GU PA LO TE ML KM UZ 80 TG MT NE BE HY 40 SW MR IS KN Word Error Rate (WER) KK SR AF CY FA LT HE AZ LV UR SL HI ET 20 TA HU MK AR RO GL ZH BG FIL BS KO DA EL HR CS SK TH VI 10 FI NB MS UK SV TR FR CA ID NL RU PL JA 5 DE PT EN IT r2 = 0.83 ES 2.5 0.1 1 10 100 Hours of transcribed audio Figure 3. Correlation of pre-training supervision amount with downstream speech recognition performance. The amount of pre-training speech recognition data for a given language is very predictive of zero-shot performance on that language in Fleurs. Model MLS VoxPopuli VP-10K + FT - 15.3 XLS-R (1B) 10.9 10.6 mSLAM-CTC (2B) 9.7 9.1 Maestro - 8.1 Zero-Shot Whisper 7.3 13.6 Table 3. Multilingual speech recognition performance. Zeroshot Whisper improves performance on Multilingual LibriSpeech (MLS) but is still signiﬁcantly behind both Maestro, XLS-R, and mSLAM on VoxPopuli. et al., 2022), and Maestro (Chen et al., 2022b) in a zero-shot setting. We caution that we do use a simple text standardizer for this result which prevents direct comparison or claims of SOTA performance. On VoxPopuli, however, Whisper signiﬁcantly underperforms prior work and only beats the VP-10K+FT baseline from the original paper. We suspect the underperformance of Whisper models on VoxPopuli could be due to other models including this distribution as a major source for their unsupervised pre-training data and the dataset having signiﬁcantly more supervised data, which beneﬁts ﬁne-tuning. While MLS has 10 hours of training data per language, the average amount of training data per language is roughly 10× higher for VoxPopuli. 40 PT CA DE SV 35 FR AF DA SR RO 30 NB BS GL RU HR SK UK CS ID BG MK MS IT 25 TR EL AR ES PL FIL FI OC NL HE KO HI HU BLEU VI 20 FA ET ZH SL ML HY UR TH GU JA LB LV 15 NE PA TE BN LT TG CY MR MT KN BE AZ 10 TA MI LO IS SW SD UZ KM AS 5 KK KA PS r2 = 0.24 MN SN LN AM YO SO MY HA 0 1 10 100 Hours of translated audio Figure 4. Correlation of pre-training supervision amount with downstream translation performance. The amount of pretraining translation data for a given language is only moderately predictive of Whisper’s zero-shot performance on that language in Fleurs. the Indo-European language family and many of which are high-resource languages. These benchmarks only provide limited coverage and room to study Whisper models multilingual capabilities which include training data for speech recognition in 75 languages. To study the performance of Whisper more broadly we also report performance on the Fleurs dataset (Conneau et al., 2022). In particular, we were interested in studying the relationship between the amount of training data we have for a given language and the resulting downstream zero-shot performance for that language. We visualize this relation in Figure 3. We ﬁnd a strong squared correlation coefﬁcient of 0.83 between the log of the word error rate and the log of the amount of training data per language. Checking the regression coefﬁcient for a linear ﬁt to these log-log values results in an estimate that WER halves for every 16× increase in training data. We also observed that many of the largest outliers in terms of worse than expected performance according to this trend are languages that have unique scripts and are more distantly related to the Indo-European languages making up the majority of the training dataset such as Hebrew (HE), Telugu (TE), Chinese (ZH), and Korean (KO). These differences could be due to a lack of transfer due to linguistic distance, our byte level BPE tokenizer being a poor match for these languages, or variations in data quality. X →English High Mid Low All XMEF-X 34.2 20.2 5.9 14.7 XLS-R (2B) 36.1 27.7 15.1 22.1 mSLAM-CTC (2B) 37.8 29.6 18.5 24.8 Maestro 38.2 31.3 18.4 25.2 Zero-Shot Whisper 36.2 32.6 25.2 29.1 Table 4. X→en Speech translation performance. Zero-shot Whisper outperforms existing models on CoVoST2 in the overall, medium, and low resource settings but still moderately underperforms on high-resource languages compared to prior directly supervised work. Language ID Fleurs w2v-bert-51 (0.6B) 71.4 mSLAM-CTC (2B) 77.7 Zero-shot Whisper 64.5 Table 5. Language identiﬁcation performance. Zero-shot Whisper’s accuracy at language identiﬁcation is not competitive with prior supervised results on Fleurs. This is partially due to Whisper being heavily penalized for having no training data for 20 of Fleurs languages. 3.5. Translation We study the translation capabilities of Whisper models by measuring their performance on the X→en subset of CoVoST2 (Wang et al., 2020b). We compare with Maestro, mSLAM, and XLS-R, the highest-performing prior work. We achieve a new state of the art of 29.1 BLEU zero-shot without using any of the CoVoST2 training data. We attribute this to the 68,000 hours of X→en translation data for these languages in our pre-training dataset which, although noisy, is vastly larger than the 861 hours of training data for X→en translation in CoVoST2. Since Whisper evaluation is zero-shot, it does particularly well on the lowest resource grouping of CoVoST2, improving over mSLAM by 6.7 BLEU. Conversely, the best Whisper model does not actually improve over Maestro and mSLAM on average for the highest resource languages. For an additional analysis on an even wider set of languages, we also re-purpose Fleurs, which is a speech recognition dataset, as a translation dataset. Since the same sentences are transcribed for every language we use the English transcripts as reference translations. In Figure 4 we visualize the correlation between the amount of translation training data per language and the resulting zero-shot BLEU score on Fleurs. While there is a clear trend of improvement with increasing training data, the squared correlation coefﬁcient is much lower than the 0.83 observed for speech recognition white noise pub noise 100 WER on LibriSpeech test-clean (%) 50 20 10 5 2 1 40 30 20 10 0 -10 signal-to-noise ratio (dB) 40 30 20 10 0 -10 signal-to-noise ratio (dB) unispeech-sat-base-100h-libri-ft wav2vec2-base-100h wav2vec2-base-960h wav2vec2-large-960h wav2vec2-large-robust-ft-libri-960h wav2vec2-large-960h-lv60-self asr-crdnn-rnnlm-librispeech asr-transformer-transformerlm-librispeech hubert-large-ls960-ft hubert-xlarge-ls960-ft s2t-medium-librispeech-asr s2t-large-librispeech-asr stt_en_conformer_ctc_large stt_en_conformer_transducer_xlarge Whisper Figure 5. WER on LibriSpeech test-clean as a function of SNR under additive white noise (left) and pub noise (right). The accuracy of LibriSpeech-trained models degrade faster than the best Whisper model (⋆). NVIDIA STT models (•) perform best under low noise but are outperformed by Whisper under high noise (SNR < 10 dB). The second-best model under low noise (▼) is ﬁne-tuned on LibriSpeech only and degrades even more quickly. and only 0.24. We suspect this is partly caused by the noisier training data due to errors in audio language identiﬁcation. As an example, Welsh (CY) is an outlier with much worse than expected performance at only 13 BLEU despite supposedly having 9,000 hours of translation data. This large amount of Welsh translation data is surprising, ranking 4th overall for translation data and ahead of some of the most spoken languages in the world like French, Spanish, and Russian. Inspection shows the majority of supposedly Welsh translation data is actually English audio with English captions where the English audio was mis-classiﬁed as Welsh by the language identiﬁcation system, resulting in it being included as translation training data rather transcription data according to our dataset creation rules. 3.6. Language Identiﬁcation To evaluate language identiﬁcation, we use the Fleurs dataset (Conneau et al., 2022). The zero-shot performance of Whisper is not competitive with prior supervised work here and underperforms the supervised SOTA by 13.6%. However, Whisper is heavily disadvantaged for language identiﬁcation on Fleurs, since the Whisper dataset contains no training data for 20 of the 102 languages in Fleurs, upperbounding accuracy at 80.4%. On the 82 overlapping languages the best Whisper model achieves 80.3% accuracy. 3.7. Robustness to Additive Noise We tested the noise robustness of Whisper models and 14 LibriSpeech-trained models by measuring the WER when either white noise or pub noise from the Audio Degradation Toolbox (Mauch & Ewert, 2013) was added to the audio. The pub noise represents a more natural noisy environment with ambient noise and indistinct chatter typical in a crowded restaurant or a pub. Among the 14 models, twelve are pre-trained and/or ﬁne-tuned on LibriSpeech, and the other two are NVIDIA STT models trained on a mixture dataset similar to prior work like SpeechStew that includes LibriSpeech. The level of additive noise corresponding to a given signal-to-noise ratio (SNR) is calculated based on the signal power of individual examples. Figure 5 shows how the ASR performance degrades as the additive noise becomes more intensive. There are many models that outperform our zero-shot performance under low noise (40 dB SNR), which is unsurprising given those models are trained primarily on LibriSpeech, but all models quickly degrade as the noise becomes more intensive, performing worse than the Whisper model under additive pub noise of SNR below 10 dB. This showcases Whisper’s robustness to noise, especially under more natural distribution shifts like the pub noise. 3.8. Long-form Transcription Whisper models are trained on 30-second audio chunks and cannot consume longer audio inputs at once. This is not a 40 35 30 Word Error Rate (%) 25 20 15 10 5 TED-LIUM3 Meanwhile Kincaid46 Rev16 Earnings-21 Earnings-22 CORAAL 0 Whisper Company A Company B Company C Company D NVIDIA STT (CTC large) Figure 6. Whisper is competitive with state-of-the-art commercial and open-source ASR systems in long-form transcription. The distribution of word error rates from six ASR systems on seven long-form datasets are compared, where the input lengths range from a few minutes to a few hours. The boxes show the quartiles of per-example WERs, and the per-dataset aggregate WERs are annotated on each box. Our model outperforms the best open source model (NVIDIA STT) on all datasets, and in most cases, commercial ASR systems as well. problem with most academic datasets comprised of short utterances but presents challenges in real-world applications which often require transcribing minutes- or hours-long audio. We developed a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model. We observed that it is crucial to have beam search and temperature scheduling based on the repetitiveness and the log probability of the model predictions in order to reliably transcribe long audio. The full procedure is described in Section 4.5. We evaluate the long-form transcription performance on seven datasets consisting of speech recordings of various lengths and recording conditions, to cover as diverse a data distribution as possible. These include a long-form adaptation of TED-LIUM3 (Hernandez et al., 2018) concatenated so that each example is a full-length TED talk, a collection of jargon-laden segments taken from The Late Show with Stephen Colbert (Meanwhile), sets of videos/podcasts that has been used as ASR benchmarks in online blogs (Rev16 and Kincaid46), recordings of earnings calls (Del Rio et al., 2021), and the full-length interviews from the Corpus of Regional African American Language (CORAAL) (Gunter et al., 2021). Full details about the long-form datasets can be found in Appendix A. We compare the performance with open-source models as well as 4 commercial ASR services. The results are summarized in Figure 6, showing the distribution of word error rates from Whisper and the 4 commercial ASR services, as well as the NVIDIA STT Conformer-CTC Large model from the NeMo toolkit (Kuchaiev et al., 2019) which performed the best among the open-source models. All commercial ASR services are queried using their default English transcription settings as of September 1st, 2022, and for the NVIDIA STT model we used their buffered inference implementation in the FrameBatchASR class to enable long-form transcription. The results show that Whisper performs better than the compared models on most datasets, especially on the Meanwhile dataset which is heavy with uncommon words. Additionally, we note the possibility that some of the commercial ASR systems have been trained on some of these publicly available datasets, and therefore these results may not be accurately reﬂecting the relative robustness of the systems. 3.9. Comparison with Human Performance Because of ambiguous or indistinct speech as well as labeling errors, there are different levels of irreducible error in each dataset, and with WER metrics from ASR systems alone it is difﬁcult to make sense of how much room for improvement exists in each dataset. To quantify how close Whisper’s performance is to the human performance, we selected 25 recordings from the Kincaid46 dataset and used 5 services to obtain transcripts produced by professional transcribers, among which one provides computer-assisted transcription and the other four are entirely human-transcribed. The audio selection covers various recording conditions such as scripted and unscripted broadcast, telephone and VoIP calls, and meetings. Figure 7 shows the distribution of per-example WERs and aggregate WER across the 25 recordings, where the computer-assisted service has the lowest aggregate WER that is 1.15% point better than Whisper’s, and the pure-human performance is only a fraction of a percentage point better than Whisper’s. These results indicate that Whisper’s English ASR performance is not perfect but very close to human-level accuracy.\n\n4. Analysis and Ablations\n4.1. Model Scaling A large amount of the promise in weakly supervised training approaches is their potential to use datasets much larger than those in traditional supervised learning. However, this comes with the cost of using data that is possibly much noisier and lower quality than gold-standard supervision. A concern with this approach is that although it may look promising to begin with, the performance of models trained on this kind of data may saturate at the inherent quality level of the dataset, which could be far below human level. A related concern is that as capacity and compute spent training on the dataset increases, models may learn to exploit the 30 25 Word Error Rate (%) 20 15 10 5 0 Whisper A B C D E F G H I ASR human transcription computer-assisted Figure 7. Whisper’s performance is close to that of professional human transcribers. This plot shows the WER distributions of 25 recordings from the Kincaid46 dataset transcribed by Whisper, the same 4 commercial ASR systems from Figure 6 (A-D), one computer-assisted human transcription service (E) and 4 human transcription services (F-I). The box plot is superimposed with dots indicating the WERs on individual recordings, and the aggregate WER over the 25 recordings are annotated on each box. idiosyncrasies of the dataset, and their ability to generalize robustly to out-of-distribution data could even degrade. To check whether this is the case, we study the zero-shot generalization of Whisper models as a function of the model size. Our analysis is summarized in Figure 8. With the exception of English speech recognition, performance continues to increase with model size across multilingual speech recognition, speech translation, and language identiﬁcation. The diminishing returns for English speech recognition could be due to saturation effects from approaching humanlevel performance as analysis in Section 3.9 suggests. 4.2. Dataset Scaling At 680,000 hours of labeled audio, the Whisper dataset is one of the largest ever created in supervised speech recognition. Exactly how important is the raw dataset size to Whisper’s performance? To study this, we trained a series of medium-sized models on subsampled versions of the dataset which are 0.5%, 1%, 2%, 4%, and 8% of the full dataset size and compared their performance with the same medium-sized model trained on the whole dataset. Early stopping based on the validation loss was used to select model checkpoints for each dataset size. Evaluation was performed on an exponential moving average estimate of the parameters (Polyak & Juditsky, 1992) using a smoothing rate of 0.9999 to help reduce the effect of the learning rate not fully decaying to zero for the models trained on the subsampled datasets due to early stopping. Performance on English and multilingual speech recognition and X→en translation is reported in Table 6. English Speech Recognition Multilingual Speech Recognition (Fleurs) 20.0 100 50 Average Large V2 Average Large V2 17.5 80 40 15.0 WER on 67 languages (%) WER on 12 datasets (%) BLEU on 21 languages 12.5 60 30 10.0 40 20 7.5 5.0 20 10 2.5 0.0 0 Model parameters Model parameters Figure 8. Zero-shot Whisper performance scales reliably across tasks and languages with increasing model size. Lightly shaded lines represent individual datasets or languages, showing that performance is more varied than the smooth trends in aggregate performance. Large V2 distinguished with a dashed orange line since it includes several changes that are not present for the smaller models in this analysis. Dataset English Multilingual X→En size WER (↓) WER (↓) BLEU (↑) 3405 30.5 92.4 0.2 6811 19.6 72.7 1.7 13621 14.4 56.6 7.9 27243 12.3 45.0 13.9 54486 10.9 36.4 19.2 681070 9.9 29.2 24.8 Table 6. Performance improves with increasing dataset size. English speech recognition performance refers to an average over 12 datasets while the Multilingual speech recognition reports performance on the overlapping subset of languages in Fleurs and X→en translation reports average BLEU on CoVoST2. Dataset size reported in hours. All increases in the dataset size result in improved performance on all tasks, although we see signiﬁcant variability in improvement rates across tasks and sizes. Performance improves rapidly on English speech recognition from 3,000 to 13,000 hours and then slows down noticeably between 13,000 and 54,000 hours. Using the full dataset, which corresponds to another 12.5× increase in size results in only a further 1 point drop in WER. This mirrors the diminishing returns observed with model size scaling for English speech recognition and could similarly be explained by saturation effects when approaching human-level performance. Improvements in WER follow a power-law trend for multilingual speech recognition till 54,000 hours and then deviate from this trend, improving only a further 7 points when increasing to the full dataset size. For X→en translation, performance is practically zero when training on 7,000 hours of audio or less, and then follows a roughly log-linear improvement trend till 54,000 hours before also showing X->En Translation (CoVoST2) Language Identification (Fleurs) 80 Average Large V2 Average Large V2 70 Accuracy on 102 languages (%) 60 50 40 0 30 Model parameters Model parameters diminishing returns when further scaling to the full dataset size. The general trend across tasks of diminishing returns when moving from 54,000 hours to our full dataset size of 680,000 hours could suggest that the current best Whisper models are under-trained relative to dataset size and performance could be further improved by a combination of longer training and larger models. It could also suggest that we are nearing the end of performance improvements from dataset size scaling for speech recognition. Further analysis is needed to characterize “scaling laws” for speech recognition in order to decided between these explanations. 4.3. Multitask and Multilingual Transfer A potential concern with jointly training a single model on many tasks and languages is the possibility of negative transfer where interference between the learning of several tasks results in performance worse than would be achieved by training on only a single task or language. To investigate whether this is occurring, we compared the performance of models trained on just English speech recognition with our standard multitask and multilingual training setup and measured their average performance across our suite of zeroshot English speech recognition benchmarks. We adjust for the amount of FLOPs spent training on the task of English speech recognition as only 65% of compute is spent on this task in a joint training setup; analysis would otherwise be confounded by under-training on the task when compared to a same-sized English-only model. Our results visualized in Figure 9 show that for small models trained with moderate amounts of compute, there is indeed negative transfer between tasks and languages: joint models underperform English-only models trained for the same amount of compute. However, multitask and multilingual 20 English Only Multilingual and Multitask Average WER on 11 english speech recognition datasets 18 16 14 12 10 8 10e+19 10e+20 10e+21 10e+22 FLOPs training on english speech recognition Figure 9. Multitask and multilingual transfer improves with scale. For small models, performance on English speech recognition degrades when trained jointly in a multitask and multilingual setup. However, multilingual and multitask models beneﬁt more from scale and eventually outperform models trained on English data only. 95% bootstrap estimate conﬁdence intervals are shown. models scale better and for our largest experiments outperform their English-only counterparts demonstrating positive transfer from other tasks. For our largest experiments, joint models also slightly outperform English-only models even when not adjusting for compute spent per task. 4.4. Text Normalization Since we developed our text normalization jointly with Whisper to discount innocuous word errors, there is a risk that our normalizer is overﬁtted to ﬁxing Whisper’s peculiarities rather than addressing general variation in transcription. To check this, we compared the performance of Whisper using our normalizer versus an independently developed one from the FairSpeech project (Koenecke et al., 2020). In Figure 10, we visualize the differences. On most datasets the two normalizers perform similarly, without signiﬁcant differences in WER reduction between Whisper and compared open-source models, while on some datasets, namely WSJ, CallHome, and Switchboard, our normalizer reduces the WER of Whisper models’ signiﬁcantly more. The differences in reduction can be traced down to different formats used by the ground truth and how the two normalizers are penalizing them. For example, in CallHome and Switchboard, our standardizer did not penalize differences in common English contractions such as “you’re” versus “you are”, and in WSJ, our normalizer standardized the written and spo- CORAAL Open-source models Whisper models CommonVoice9.en AMI-SDM1 CommonVoice5.1 Fleurs.en_us AMI-IHM Artie LibriSpeech TED-LIUM3 VoxPopuli.en WSJ CallHome Switchboard 0 10 20 30 40 50 Relative WER reduction compared to FairSpeech's normalizer (%) Figure 10. On most datasets, our text normalizer has similar effect on reducing WERs between Whisper models and other open-source models, compared to FairSpeech’s normalizer. For each dataset, the boxplot shows the distribution of relative WER reduction across different models in our eval suite, showing that using our text normalizer generally results in lower WERs than FairSpeech’s. On a few datasets our normalizer reduces WER signiﬁcantly and more so for Whisper models, such as CallHome and Switchboard which have many contractions in the ground truth and WSJ which contains many numerical expressions. ken forms of numerical and monetary expressions, such as “sixty-eight million dollars” versus “$68 million”. 4.5. Strategies for Reliable Long-form Transcription Transcribing long-form audio using Whisper relies on accurate prediction of the timestamp tokens to determine the amount to shift the model’s 30-second audio context window by, and inaccurate transcription in one window may negatively impact transcription in the subsequent windows. We have developed a set of heuristics that help avoid failure cases of long-form transcription, which is applied in the results reported in sections 3.8 and 3.9. First, we use beam search with 5 beams using the log probability as the score function, to reduce repetition looping which happens more frequently in greedy decoding. We start with temperature 0, i.e. always selecting the tokens with the highest probability, and increase the temperature by 0.2 up to 1.0 when either the average log probability over the generated tokens is lower than −1 or the generated text has a gzip compression rate higher than 2.4. Providing the transcribed text from the preceding window as previous-text conditioning when the applied temperature is below 0.5 further improves the performance. We found that the probability of the <|nospeech|> token alone is not sufﬁcient TED-LIUM3 Earnings-22 Earnings-21 Meanwhile Kincaid46 CORAAL Average Rev16 Greedy decoding only 3.95 5.16 9.69 11.7 10.7 14.0 22.0 11.0 + Beam search 4.16 5.71 9.42 11.5 10.2 13.4 20.0 10.6 + Temperature fallback 4.16 5.71 9.42 11.5 10.2 13.4 20.0 10.6 + Voice activity detection 3.56 4.61 9.45 11.4 10.1 13.2 19.4 10.2 + Previous text conditioning 3.42 6.16 8.72 11.0 9.63 13.3 18.1 10.0 + Initial timestamp constraint 3.51 5.26 8.41 11.5 9.73 12.6 19.1 10.0 Table 7. Long-form transcription performance improves incrementally as additional decoding heuristics are employed. Details on each intervention are described in Section 4.5. to distinguish a segment with no speech, but combining the no-speech probability threshold of 0.6 and the average log-probability threshold of −1 makes the voice activity detection of Whisper more reliable. Finally, to avoid a failure mode where the model ignores the ﬁrst few words in the input, we constrained the initial timestamp token to be between 0.0 and 1.0 second. Table 7 shows that adding each of the interventions above incrementally reduces the WER overall, but not evenly across the dataset. These heuristics serve as a workaround for the noisy predictions of the model, and more research would be needed to further improve the reliability of long-form decoding.\n\n5. Related Work\nScaling Speech Recognition A consistent theme across speech recognition research has been documenting the beneﬁts of scaling compute, models, and datasets. Early work applying deep learning to speech recognition found improved performance with model depth and size and leveraged GPU acceleration to make training these larger models tractable (Mohamed et al., 2009). Further research demonstrated that the beneﬁt of deep learning approaches to speech recognition increased with dataset size, improving from being only competitive with prior GMM-HMM systems when using just 3 hours of TIMIT training data for phone recognition to achieving a 30% word error rate reduction when trained on the 2,000 hour Switchboard dataset (Seide et al., 2011). Liao et al. (2013) is an early example of leveraging weakly supervised learning to increase the size of a deep learning based speech recognition dataset by over 1,000 hours. These trends continued with Deep Speech 2 (Amodei et al., 2015) being a notable system developing high-throughput distributed training across 16 GPUs and scaling to 12,000 hours of training data while demonstrating continuing improvements at that scale. By leveraging semi-supervised pre-training, Narayanan et al. (2018) were able to grow dataset size much further and study training on 162,000 hours of labeled audio. More recent work has explored billion-parameter models (Zhang et al., 2020) and using up to 1,000,000 hours of training data (Zhang et al., 2021). Multitask Learning Multitask learning (Caruana, 1997) has been studied for a long time. In speech recognition, multi-lingual models have been explored for well over a decade (Schultz & Kirchhoff, 2006). An inspirational and foundational work in NLP exploring multi-task learning with a single model is Collobert et al. (2011). Multitask learning in the sequence-to-sequence framework (Sutskever et al., 2014) using multiple encoders and decoders was investigated in Luong et al. (2015). The use of language codes with a shared encoder/decoder architecture was ﬁrst demonstrated for machine translation by Johnson et al. (2017), removing the need for separate encoders and decoders. This approach was simpliﬁed further into the “text-to-text” framework of McCann et al. (2018) and popularized by its success with large transformer language models in the work of Radford et al. (2019) and Raffel et al. (2020). Toshniwal et al. (2018) demonstrated jointly training a modern deep learning speech recognition system on several languages with a single model, and Pratap et al. (2020a) scaled this line of work signiﬁcantly to 50 languages with a billion-parameter model. MUTE (Wang et al., 2020c) and mSLAM (Bapna et al., 2022) studied joint training over both text and speech language tasks, demonstrating transfer between them. Robustness The question of how effectively models transfer and how robust they are to distribution shift and other types of perturbations has long been studied and is actively being researched across many ﬁelds of machine learning. Torralba & Efros (2011) highlighted the lack of generalization of machine learning models between datasets over a decade ago. Many other works have shown and continually reiterated how despite high performance on IID test sets, machine learning models can still make many mistakes when evaluated in even slightly different settings (Lake et al., 2017; Jia & Liang, 2017; Alcorn et al., 2019; Barbu et al., 2019; Recht et al., 2019). More recently, Taori et al. (2020) studied the robustness of image classiﬁcation models, and Miller et al. (2020) investigated this for question-answering models. A key ﬁnding has been that multi-domain training increases robustness and generalization as discussed in the Introduction. This ﬁnding has been replicated across many ﬁelds in addition to speech recognition including NLP (Hendrycks et al., 2020) and computer vision (Radford et al., 2021).\n\n7. Conclusion\nWhisper suggests that scaling weakly supervised pretraining has been underappreciated so far in speech recognition research. We achieve our results without the need for the self-supervision and self-training techniques that have been a mainstay of recent large-scale speech recognition work and demonstrate how simply training on a large and diverse supervised dataset and focusing on zero-shot transfer can signiﬁcantly improve the robustness of a speech recognition system.\n\nACKNOWLEDGMENTS\nWe’d like to thank the millions of people who were involved in creating the data used by Whisper. We’d also like to thank Nick Ryder, Will Zhuk, and Andrew Carr for the conversation on the waterfall hike that inspired this project. We are also grateful to the Acceleration and Supercomputing teams at OpenAI for their critical work on software and hardware infrastructure this project used. We’d also like to thank Pamela Mishkin for advising the project from a policy perspective. Finally, we are grateful to the developers of the many software packages used throughout this project including, but not limited, to Numpy (Harris et al., 2020), SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), PyTorch (Paszke et al., 2019), pandas (pandas development team, 2020), and scikit-learn (Pedregosa et al., 2011).\n\nReferences\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.- S., and Nguyen, A. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4845–4854, 2019. Amodei, D., Anubhai, R., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Chen, J., Chrzanowski, M., Coates, A., Diamos, G., et al. Deep speech 2: end-to-end speech recognition in english and mandarin. arxiv. arXiv preprint arXiv:1512.02595, 2015. and Weber, G. Common voice: A massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y., Pino, J., et al. XLS-R: Self-supervised cross-lingual speech representation learning at scale. arXiv preprint arXiv:2111.09296, 2021. Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. arXiv preprint arXiv:2006.11477, 2020. Baevski, A., Hsu, W.-N., Conneau, A., and Auli, M. Unsupervised speech recognition. Advances in Neural Information Processing Systems, 34:27826–27839, 2021. Bapna, A., Cherry, C., Zhang, Y., Jia, Y., Johnson, M., Cheng, Y., Khanuja, S., Riesa, J., and Conneau, A. mslam: Massively multilingual joint pre-training for speech and text. arXiv preprint arXiv:2202.01374, 2022. Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chan, W., Park, D., Lee, C., Zhang, Y., Le, Q., and Norouzi,\nM. SpeechStew: Simply mix all available speech recognition data to train one large neural network. arXiv preprint\narXiv:2104.02133, 2021. Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. Chen, S., Wu, Y., Wang, C., Chen, Z., Chen, Z., Liu, S., Wu, J., Qian, Y., Wei, F., Li, J., et al. Unispeech-sat: Universal speech representation learning with speaker aware pre-training. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6152–6156. IEEE, 2022a. Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Chen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Moreno, P., Bapna, A., and Zen, H. Maestro: Matched speech text representations through modality matching. arXiv preprint arXiv:2204.03409, 2022b. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of machine learning research, 12(ARTICLE):2493–2537, 2011. Conneau, A., Ma, M., Khanuja, S., Zhang, Y., Axelrod, V., Dalmia, S., Riesa, J., Rivera, C., and Bapna, A. Fleurs: Few-shot learning evaluation of universal representations of speech. arXiv preprint arXiv:2205.12446, 2022. Del Rio, M., Delworth, N., Westerman, R., Huang, M., Bhandari, N., Palakapilly, J., McNamara, Q., Dong, J., Zelasko, P., and Jett´e, M. Earnings-21: a practical benchmark for asr in the wild. arXiv preprint arXiv:2104.11348, 2021. Galvez, D., Diamos, G., Torres, J. M. C., Achorn, K., Gopi, A., Kanter, D., Lam, M., Mazumder, M., and Reddi, V. J. The people’s speech: A large-scale diverse english speech recognition dataset for commercial usage. arXiv preprint arXiv:2111.09344, 2021. Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665–673, 2020. Ghorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun, M., Garcia, X., Chelba, C., and Cherry, C. Scaling laws for neural machine translation. arXiv preprint arXiv:2109.07740, 2021. Griewank, A. and Walther, A. Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1):19–45, 2000. Gunter, K., Vaughn, C., and Kendall, T. Contextualizing/s/retraction: Sibilant variation and change in washington dc african american language. Language Variation and Change, 33(3):331–357, 2021. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., Fern´andez del R´ıo, J., Wiebe, M., Peterson, P., G´erard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585:357–362, 2020. doi: 10.1038/ s41586-020-2649-2. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., and Song, D. Pretrained transformers improve out-ofdistribution robustness. arXiv preprint arXiv:2004.06100, 2020. Hernandez, F., Nguyen, V., Ghannay, S., Tomashenko, N. A., and Est`eve, Y. Ted-lium 3: twice as much data and corpus repartition for experiments on speaker adaptation. In SPECOM, 2018. Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert: Selfsupervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451–3460, 2021a. Hsu, W.-N., Sriram, A., Baevski, A., Likhomanenko, T., Xu, Q., Pratap, V., Kahn, J., Lee, A., Collobert, R., Synnaeve, G., et al. Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training. arXiv preprint arXiv:2104.01027, 2021b. Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger,\nK. Q. Deep networks with stochastic depth. In European\nconference on computer vision, pp. 646–661. Springer, 2016. Jia, R. and Liang, P. Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328, 2017. Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., Thorat, N., Vi´egas, F., Wattenberg, M., Corrado, G., et al. Google’s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339– 351, 2017. Kendall, T. and Farrington, C. The corpus of regional african american language. Version 2021.07. Eugene, OR: The Online Resources for African American Language Project. http://oraal.uoregon.edu/coraal,\n2021. Accessed: 2022-09-01.\nKoenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M., Mengesha, Z., Toups, C., Rickford, J. R., Jurafsky, D., and Goel, S. Racial disparities in automated speech recognition. Proceedings of the National Academy of Sciences, 117(14):7684–7689, 2020. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In European conference on computer vision, pp. 491–507. Springer, 2020. Kuchaiev, O., Li, J., Nguyen, H., Hrinchuk, O., Leary, R., Ginsburg, B., Kriman, S., Beliaev, S., Lavrukhin, V., Cook, J., et al. Nemo: a toolkit for building ai applications using neural modules. arXiv preprint arXiv:1909.09577, 2019. Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017. Liao, H., McDermott, E., and Senior, A. Large scale deep neural network acoustic modeling with semi-supervised training data for youtube video transcription. In 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 368–373. IEEE, 2013. Likhomanenko, T., Xu, Q., Pratap, V., Tomasello, P., Kahn, J., Avidov, G., Collobert, R., and Synnaeve, G. Rethinking evaluation in asr: Are our models robust enough? arXiv preprint arXiv:2010.11745, 2020. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., and Kaiser, L. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015. Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., and Van Der Maaten, L. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181–196, 2018. Mauch, M. and Ewert, S. The audio degradation toolbox and its application to robustness evaluation. In Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR 2013), Curitiba, Brazil, 2013. accepted. McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. Meyer, J., Rauchenstein, L., Eisenberg, J. D., and Howell,\nN. Artie bias corpus: An open dataset for detecting demographic bias in speech applications. In Proceedings of\nthe 12th Language Resources and Evaluation Conference, pp. 6462–6468, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-9554634-4. URL https://aclanthology.org/2020. lrec-1.796. Mohamed, A.-r., Dahl, G., Hinton, G., et al. Deep belief networks for phone recognition. In Nips workshop on deep learning for speech recognition and related applications, volume 1, pp. 39, 2009. Narayanan, A., Misra, A., Sim, K. C., Pundak, G., Tripathi, A., Elfeky, M., Haghani, P., Strohman, T., and Bacchiani, M. Toward domain-invariant speech recognition via large scale training. In 2018 IEEE Spoken Language Technology Workshop (SLT), pp. 441–447. IEEE, 2018. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5206–5210. IEEE, 2015. pandas development team, T. pandas-dev/pandas: Pan- 5281/zenodo.3509134. Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. SpecAugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. Pascanu, R., Mikolov, T., and Bengio, Y. On the difﬁculty of training recurrent neural networks. In International conference on machine learning, pp. 1310–1318. PMLR, 2013. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024– 8035, 2019. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. Polyak, B. T. and Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838–855, 1992. Pratap, V., Sriram, A., Tomasello, P., Hannun, A. Y., Liptchinsky, V., Synnaeve, G., and Collobert, R. Massively multilingual asr: 50 languages, 1 model, 1 billion parameters. ArXiv, abs/2007.03001, 2020a. Press, O. and Wolf, L. Using the output embedding to improve language models. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 157–163, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https: //aclanthology.org/E17-2025. Provilkov, I., Emelianenko, D., and Voita, E. Bpe-dropout: Simple and effective subword regularization. arXiv preprint arXiv:1910.13267, 2019. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.-W., Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na, H., Gao, Y., Mori, R. D., and Bengio, Y. SpeechBrain: A general-purpose speech toolkit, 2021. arXiv:2106.04624. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do ImageNet classiﬁers generalize to ImageNet? In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3): 211–252, 2015. Schultz, T. and Kirchhoff, K. Multilingual speech processing. Elsevier, 2006. Seide, F., Li, G., Chen, X., and Yu, D. Feature engineering in context-dependent deep neural networks for conversational speech transcription. In 2011 IEEE Workshop on Automatic Speech Recognition & Understanding, pp. 24–29. IEEE, 2011. Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. 10.5281/zenodo.2591652. Version 5.5. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014. Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L. Measuring robustness to natural distribution shifts in image classiﬁcation. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,\nH. (eds.), Advances in Neural Information Processing\nSystems, volume 33, pp. 18583–18599. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ d8330f857a17c53d217014ee776bfd50-Paper. pdf. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. CVPR 2011, pp. 1521–1528, 2011. Toshniwal, S., Sainath, T. N., Weiss, R. J., Li, B., Moreno,\nP. J., Weinstein, E., and Rao, K. Multilingual speech\nrecognition with a single end-to-end model. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4904–4908, 2018. Valk, J. and Alum¨ae, T. Voxlingua107: a dataset for spoken language recognition. In 2021 IEEE Spoken Language Technology Workshop (SLT), pp. 652–658. IEEE, 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2. Wang, C., Wu, A., and Pino, J. Covost 2 and massively multilingual speech-to-text translation. arXiv preprint arXiv:2007.10310, 2020b. Wang, C., Riviere, M., Lee, A., Wu, A., Talnikar, C., Haziza, D., Williamson, M., Pino, J., and Dupoux, E. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390, 2021. Wang, P., Sainath, T. N., and Weiss, R. J. Multitask training with text data for end-to-end speech recognition. arXiv preprint arXiv:2010.14318, 2020c. Watanabe, S., Mandel, M., Barker, J., Vincent, E., Arora, A., Chang, X., Khudanpur, S., Manohar, V., Povey, D., Raj, D., et al. Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings. arXiv preprint arXiv:2004.09249, 2020. Xu, Q., Baevski, A., Likhomanenko, T., Tomasello, P., Conneau, A., Collobert, R., Synnaeve, G., and Auli, M. Selftraining and pre-training are complementary for speech recognition. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3030–3034. IEEE, 2021. Zhang, Y., Qin, J., Park, D. S., Han, W., Chiu, C.-C., Pang, R., Le, Q. V., and Wu, Y. Pushing the limits of semisupervised learning for automatic speech recognition. arXiv preprint arXiv:2010.10504, 2020. Zhang, Y., Park, D. S., Han, W., Qin, J., Gulati, A., Shor, J., Jansen, A., Xu, Y., Huang, Y., Wang, S., et al. BigSSL: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition. arXiv preprint arXiv:2109.13226, 2021.\n\nA. Evaluation Datasets.\nA.1. Short-form English-only datasets • LibriSpeech (Panayotov et al., 2015): We used the test-clean and test-other splits from the LibriSpeech ASR corpus. • TED-LIUM 3 (Hernandez et al., 2018): We used the test split of TED-LIUM Release 3, using the segmented manual transcripts included in the release. • Common Voice 5.1 (Ardila et al., 2019): We downloaded the English subset of Common Voice Corpus 5.1 from the ofﬁcial website. • Artie bias corpus (Meyer et al., 2020): We used the Artie bias corpus. This is a subset of the Common Voice dataset. • CallHome and Switchboard: We used the two corpora from LDC2002S09 and LDC2002T43. • WSJ: We used LDC93S6B and LDC94S13B and followed the s5 recipe to preprocess the dataset. • CORAAL: We used the 231 interviews from CORAAL (Kendall & Farrington, 2021) and used the preprocessing script from the FairSpeech project. • CHiME-6: For CHiME-6 (Watanabe et al., 2020), we downloaded the CHiME-5 dataset and followed the stage 0 of the s5 track1 recipe to create the CHiME-6 dataset which ﬁxes synchronization. We then used the binaural recordings (* P??.wav) and the corresponding transcripts. • AMI-IHM and AMI-SDM1: We preprocessed the AMI Corpus by following the stage 0 ad 2 of the s5b recipe. A.2. Long-form English-only datasets • TED-LIUM 3 (Hernandez et al., 2018): We used the 11 full-length TED talks from the test split of TED-LIUM Release 3, slicing the source audio ﬁles between the beginning of the ﬁrst labeled segment and the end of the last labeled segment of each talk, and we used the concatenated text as the label. • Meanwhile: This dataset consists of 64 segments from The Late Show with Stephen Colbert. The YouTube video ID and the corresponding start and end timestamps are available as part of the code release. The labels are collected from the closed-caption data for each video and corrected with manual inspection. • Rev16: We use a subset of 16 ﬁles from the 30 podcast episodes in Rev.AI’s Podcast Transcription Benchmark, after ﬁnding that there are multiple cases where a signiﬁcant portion of the audio and the labels did not match, mostly on the parts introducing the sponsors. We selected 16 episodes that do not have this error, whose “ﬁle number”s are: 3 4 9 10 11 14 17 18 20 21 23 24 26 27 29 32 • Kincaid46: This dataset consists of 46 audio ﬁles and the corresponding transcripts compiled in the blog article ¡Which automatic transcription service is the most accurate - 2018¿ by Jason Kincaid. We used the 46 audio ﬁles and reference transcripts from the Airtable widget in the article. For the human transcription benchmark in the paper, we use a subset of 25 examples from this data, whose “Ref ID”s are: 2 4 5 8 9 10 12 13 14 16 19 21 23 25 26 28 29 30 33 35 36 37 42 43 45 • Earnings-21 (Del Rio et al., 2021) and Earnings-22: We used the ﬁles available in the speech-datasets repository, as of their 202206 version. A.3. Multilingual datasets • Multilingual LibriSpeech (Pratap et al., 2020b): We used the test splits from each language in the Multilingual LibriSpeech (MLS) corpus. • Fleurs (Conneau et al., 2022): We collected audio ﬁles and transcripts using the implementation available as HuggingFace datasets. To use as a translation dataset, we matched the numerical utterance IDs to ﬁnd the corresponding transcript in English. • VoxPopuli (Wang et al., 2021): We used the get asr data.py script from the ofﬁcial repository to collect the ASR data in 16 languages, including English. • Common Voice 9 (Ardila et al., 2019): We downloaded the Common Voice Corpus 9 from the ofﬁcial website. • CoVOST 2 (Wang et al., 2020b): We collected the X into English data collected using the ofﬁcial repository.\nB. Compared Models\nFor comparison, we use the following models from HuggingFace, downloaded as of September 2022 using version 4.21.0 of the transformers library: • facebook/wav2vec2-large-960h-lv60-self (Xu et al., 2021) • facebook/wav2vec2-large-robust-ft-libri-960h (Hsu et al., 2021b) • facebook/wav2vec2-base-100h (Baevski et al., 2020) • facebook/wav2vec2-base-960h (Baevski et al., 2020) • facebook/wav2vec2-large-960h (Baevski et al., 2020) • facebook/hubert-large-ls960-ft (Hsu et al., 2021a) • facebook/hubert-xlarge-ls960-ft (Hsu et al., 2021a) • facebook/s2t-medium-librispeech-asr (Wang et al., 2020a) • facebook/s2t-large-librispeech-asr (Wang et al., 2020a) • microsoft/unispeech-sat-base-100h-libri-ft (Chen et al., 2022a) • nvidia/stt en conformer ctc large (Kuchaiev et al., 2019) • nvidia/stt en conformer transducer xlarge (Kuchaiev et al., 2019) • speechbrain/asr-crdnn-rnnlm-librispeech (Ravanelli et al., 2021) • speechbrain/asr-transformer-transformerlm-librispeech (Ravanelli et al., 2021) We note that all of the models above are entirely or partly trained on LibriSpeech.\nC. Text Standardization\nSince Whisper may output any UTF-8 string rather than a restricted set of graphemes, the rules for text standardization need to be more intricate and comprehensive than those deﬁned on e.g. ASCII characters. We perform the following steps to normalize English texts in different styles into a standardized form, which is a best-effort attempt to penalize only when a word error is caused by actually mistranscribing a word, and not by formatting or punctuation differences.\n1. Remove any phrases between matching brackets ([, ]).\n2. Remove any phrases between matching parentheses ((, )).\n3. Remove any of the following words: hmm, mm, mhm, mmm, uh, um\n4. Remove whitespace characters that comes before an apostrophe ’\n5. Convert standard or informal contracted forms of English into the original form.\n6. Remove commas (,) between digits\n7. Remove periods (.) not followed by numbers\n8. Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category\nstarting with M, S, or P, except period, percent, and currency symbols that may be detected in the next step.\n9. Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers, e.g. “Ten\nthousand dollars” →“$10000”.\n10. Convert British spellings into American spellings.\n11. Remove remaining symbols that are not part of any numeric expressions.\n12. Replace any successive whitespace characters with a space.\nA different, language-speciﬁc set of transformations would be needed to equivalently normalize non-English text, but due to our lack of linguistic knowledge to build such normalizers for all languages, we resort to the following basic standardization for non-English text:\n1. Remove any phrases between matching brackets ([, ]).\n2. Remove any phrases between matching parentheses ((, )).\n3. Replace any markers, symbols, and punctuation characters with a space, i.e. when the Unicode category of each\ncharacter in the NFKC-normalized string starts with M, S, or P.\n4. make the text lowercase.\n5. replace any successive whitespace characters with a space.\nAdditionally, we put a space between every letter for the languages that do not use spaces to separate words, namely Chinese, Japanese, Thai, Lao, and Burmese, effectively measuring the character error rate instead. We note that the above is an imperfect solution, and it will sometimes produce unintended and unexpected outputs. We do not claim that the text format resulting from the above is more “correct” in any measure. Rather, the procedures above are designed to better distinguish between innocuous differences in wording and genuine mistranscriptions. Python code for the standardization procedures above is available as part of our code and model release to facilitate future iterations and improvements on text standardization.\nD. Raw Performance Tables\nD.1. English Transcription D.1.1. GREEDY DECODING LibriSpeech.test-clean LibriSpeech.test-other TED-LIUM3 CallHome WSJ Model Whisper tiny.en 5.6 14.6 6.0 5.0 24.1 17.8 26.3 20.0 23.9 41.3 23.7 50.3 11.7 11.6 Whisper tiny 7.6 16.9 7.0 6.7 30.0 22.8 29.6 23.9 31.0 49.6 27.6 58.1 12.7 13.7 Whisper base.en 4.2 10.2 4.9 4.6 20.9 15.2 19.0 13.4 22.6 36.4 20.5 46.7 10.0 7.6 Whisper base 5.0 12.4 5.5 5.1 23.0 16.8 21.6 16.9 26.0 40.2 22.0 49.9 10.0 10.1 Whisper small.en 3.1 7.4 4.0 3.3 18.2 15.7 13.1 9.7 20.2 27.6 17.5 38.0 8.1 6.0 Whisper small 3.4 7.6 4.3 4.0 17.5 14.5 13.5 10.3 18.1 29.3 19.0 39.6 8.3 6.6 Whisper medium.en 3.1 6.3 4.1 3.3 16.2 14.1 10.6 7.6 17.5 25.3 16.4 37.2 7.4 5.0 Whisper medium 2.9 5.9 3.8 2.9 16.4 14.0 10.3 7.2 16.6 26.4 16.6 36.0 7.4 5.4 Whisper large 2.7 5.6 4.0 3.1 15.8 13.1 9.5 6.7 19.4 25.6 16.4 36.9 7.3 4.6 Whisper large-v2 2.7 5.2 4.0 3.9 17.6 13.8 9.0 6.2 16.2 25.5 16.9 36.4 7.3 4.4 wav2vec2-base-100h 6.0 13.4 17.8 13.9 46.9 40.2 47.4 40.8 47.0 79.9 48.1 81.2 28.9 23.1 wav2vec2-base-960h 3.3 8.5 12.8 8.9 40.6 32.9 36.4 30.9 39.9 68.5 40.2 71.9 21.4 17.4 wav2vec2-large-960h-lv60-self 1.8 3.8 7.4 4.4 29.1 22.2 19.9 15.8 29.2 56.3 30.8 57.0 13.0 10.2 wav2vec2-large-960h 2.7 6.2 10.5 7.7 34.8 28.3 29.9 24.5 35.6 65.8 37.0 67.6 17.9 14.6 wav2vec2-large-robust-ft-libri-960h 2.6 5.3 9.2 6.1 23.4 19.8 20.3 16.2 29.4 58.1 31.7 61.6 15.1 11.8 asr-crdnn-rnnlm-librispeech 3.0 9.7 17.7 10.7 59.7 56.1 43.7 33.3 83.8 81.0 57.2 85.8 30.6 32.4 asr-transformer-transformerlm-librispeech 2.1 5.4 11.9 7.4 38.9 33.0 30.6 23.5 44.9 79.5 44.5 75.4 17.8 17.0 hubert-large-ls960-ft 2.0 4.1 8.4 5.4 29.6 22.8 20.8 16.0 32.0 60.0 33.7 59.1 14.4 10.9 hubert-xlarge-ls960-ft 1.9 3.5 8.3 5.4 29.3 22.2 19.8 14.8 31.5 58.5 33.3 58.9 14.2 10.5 s2t-large-librispeech-asr 3.3 8.1 14.9 9.4 54.5 40.3 38.1 30.7 50.2 79.2 53.4 79.5 21.6 18.0 s2t-medium-librispeech-asr 3.6 8.2 15.7 9.7 58.1 42.4 39.3 31.3 52.6 79.8 60.3 85.3 22.9 19.7 stt en conformer ctc large 2.1 4.2 4.4 2.1 11.3 8.2 7.4 4.0 13.5 30.5 15.9 39.9 6.7 8.2 stt en conformer transducer xlarge 1.5 2.8 4.3 1.2 12.0 7.4 4.3 1.5 19.9 36.8 20.5 48.6 6.0 6.3 unispeech-sat-base-100h-libri-ft 5.7 13.8 17.7 13.6 46.5 40.0 45.3 38.6 44.7 74.8 47.8 77.7 29.8 22.4 Table 8. English transcription WER (%) with greedy decoding D.1.2. BEAM SEARCH WITH TEMPERATURE FALLBACK LibriSpeech.test-clean LibriSpeech.test-other TED-LIUM3 Switchboard CallHome WSJ Model Whisper tiny.en 5.4 12.8 5.4 4.6 21.4 16.0 23.5 18.4 21.4 42.0 22.7 54.2 10.9 10.0 Whisper tiny 6.7 15.0 6.3 5.9 24.8 18.3 26.1 20.8 25.1 48.0 25.6 57.3 11.6 12.4 Whisper base.en 4.1 9.6 4.6 4.0 18.3 14.2 17.5 13.2 18.5 35.2 21.1 49.0 9.3 7.1 Whisper base 4.9 11.0 5.0 4.4 20.5 15.6 19.4 15.3 20.5 40.0 21.5 50.0 9.5 8.9 Whisper small.en 3.2 6.7 4.3 3.0 17.2 13.4 12.6 9.2 17.5 29.5 17.9 42.5 8.1 5.3 Whisper small 3.3 7.2 4.3 3.9 17.1 13.3 12.8 9.3 16.4 30.9 19.2 43.5 8.2 6.1 Whisper medium.en 3.0 5.7 4.3 2.8 14.7 12.4 10.3 7.4 15.3 27.0 17.1 39.4 7.8 4.5 Whisper medium 2.7 5.6 4.0 2.7 15.3 13.2 9.7 6.7 14.9 27.6 17.6 43.0 7.6 4.4 Whisper large 2.8 5.7 4.3 3.5 16.2 14.2 8.9 6.4 15.1 25.2 17.6 37.1 7.2 4.5 Whisper large-v2 2.5 4.9 3.7 2.6 16.4 13.6 8.2 5.7 14.2 24.9 17.4 39.9 7.0 4.2 CommonVoice5.1 VoxPopuli.en Switchboard Fleurs.en us AMI-SDM1 AMI-IHM CORAAL CHiME6 Artie CommonVoice5.1 VoxPopuli.en Fleurs.en us AMI-SDM1 AMI-IHM CORAAL CHiME6 Artie D.2. Multilingual Transcription D.2.1. MULTILINGUAL LIBRISPEECH English French Dutch Model Whisper tiny 39.4 15.7 36.8 24.9 41.7 34.2 31.3 19.2 Whisper base 28.4 11.7 26.6 17.7 31.1 22.8 21.9 12.8 Whisper small 17.2 8.3 16.2 10.5 21.4 11.2 13.0 7.8 Whisper medium 11.7 6.8 8.9 7.4 16.0 6.5 9.0 5.3 Whisper large 10.2 6.3 8.9 6.6 14.3 6.6 9.2 5.4 Whisper large-v2 9.3 6.2 7.3 5.5 13.8 5.0 6.8 4.2 Table 10. WER (%) on MLS D.2.2. COMMON VOICE 9 Bulgarian Catalan Bengali Arabic Czech Welsh Model Whisper tiny 90.9 79.3 104.1 51.0 79.7 101.8 77.2 34.5 61.9 28.8 30.3 102.1 120.3 Whisper base 84.4 68.1 103.7 39.9 63.1 93.8 57.5 24.5 51.5 21.9 19.6 88.1 99.0 Whisper small 66.4 44.8 118.6 23.8 34.1 65.4 32.1 13.0 31.7 14.5 10.3 67.2 71.9 Whisper medium 60.3 26.7 124.7 16.4 18.8 43.6 19.3 8.5 20.0 11.2 6.9 45.6 49.9 Whisper large 56.0 24.1 106.0 15.3 17.1 40.3 18.3 7.7 18.3 10.1 6.4 41.4 44.8 Whisper large-v2 53.8 19.9 103.4 14.1 13.5 34.2 14.4 6.4 16.0 9.4 5.6 35.1 39.4 Indonesian Hungarian Finnish French Italian Hindi Model Whisper tiny 68.5 49.7 108.3 87.0 49.6 44.5 36.1 103.5 87.8 102.7 123.0 43.6 45.3 Whisper base 52.9 37.3 106.5 71.9 36.1 30.5 24.2 91.3 78.0 122.9 137.0 29.5 32.8 Whisper small 30.5 22.7 43.6 44.4 18.4 16.0 14.0 72.8 54.6 104.8 225.8 14.2 16.9 Whisper medium 18.8 16.0 31.5 26.9 11.6 9.4 10.5 49.4 37.2 137.8 113.4 8.0 10.1 Whisper large 17.0 14.7 25.0 23.5 10.6 8.1 9.4 43.9 34.8 107.1 117.4 7.1 9.0 Whisper large-v2 14.4 13.9 21.9 19.7 8.5 7.1 9.1 35.2 25.5 103.2 128.4 5.8 7.6 Portuguese Romanian Slovenian Russian Serbian Slovak Model Whisper tiny 35.2 68.2 40.6 104.0 82.0 106.1 58.2 105.7 55.9 53.6 74.7 69.3 52.4 Whisper base 23.7 55.9 28.8 87.2 70.3 103.0 42.4 49.5 32.1 38.6 58.6 51.6 44.9 Whisper small 12.5 33.2 15.0 60.4 45.5 101.3 22.1 28.7 18.1 23.7 39.1 33.3 29.4 Whisper medium 8.1 21.5 9.3 42.0 29.8 85.6 13.7 19.6 10.5 17.7 29.9 24.4 23.2 Whisper large 7.1 19.8 8.2 37.9 25.1 87.4 12.4 17.6 8.8 16.6 28.1 19.9 29.1 Whisper large-v2 6.3 15.8 7.1 31.9 20.6 70.5 10.6 16.1 8.0 14.5 24.2 18.2 26.8 Table 11. WER (%) on CommonVoice9 D.2.3. VOXPOPULI en accented Estonian German Spanish Finnish English French Czech Model Whisper tiny 73.5 27.4 11.6 18.8 19.7 99.2 54.1 32.9 72.4 74.5 40.5 93.1 41.9 31.4 65.9 78.7 81.9 Whisper base 54.7 20.6 9.5 17.5 14.4 83.0 39.7 24.9 53.6 52.6 30.8 82.1 29.4 22.1 49.3 63.7 70.5 Whisper small 28.8 14.8 8.2 19.2 11.1 59.2 24.9 15.7 33.7 31.3 22.9 60.1 18.8 13.3 28.6 37.3 50.8 Whisper medium 18.4 12.4 7.6 19.1 9.6 38.2 16.6 12.2 23.9 19.3 19.7 39.3 14.9 10.1 18.4 23.0 36.3 Whisper large 15.9 11.9 7.2 20.8 8.8 33.3 15.5 11.0 19.0 16.8 18.4 35.0 14.0 9.0 17.0 19.1 31.3 Whisper large-v2 12.6 11.2 7.0 18.6 8.2 28.7 12.4 11.4 16.1 13.8 19.0 33.2 12.9 7.8 14.4 15.4 27.9 Portuguese German Spanish Italian Polish Estonian German Spanish English Persian Danish Greek Malayalam Lithuanian Mongolian Japanese Latvian Polish Dutch Vietnamese Swedish Chinese Turkish Tamil Urdu Thai Lithuanian Hungarian Romanian Slovenian Croatian Slovak Italian Polish Dutch D.2.4. FLEURS Azerbaijani Belarusian Assamese Afrikaans Amharic Arabic Model Whisper tiny 91.2 122.9 63.4 102.0 93.1 94.0 81.0 101.6 82.1 42.8 40.5 82.8 101.3 82.0 Whisper base 81.5 196.8 48.8 102.0 76.4 91.3 65.1 100.6 66.7 29.0 34.1 66.0 85.3 57.6 Whisper small 61.1 120.2 30.6 108.0 49.1 75.1 37.3 104.4 39.4 16.2 20.8 37.6 59.3 32.8 Whisper medium 44.9 229.3 20.4 102.3 33.1 60.4 21.4 100.6 23.9 9.6 12.1 21.3 40.8 19.5 Whisper large 42.6 129.3 18.1 105.6 28.7 56.6 18.4 104.9 20.7 8.0 19.6 17.4 36.6 16.8 Whisper large-v2 36.7 140.3 16.0 106.2 23.4 45.4 14.6 104.1 15.7 7.3 14.7 13.3 33.0 13.8 Estonian German Spanish English Persian Greek Model Whisper tiny 27.8 67.4 12.4 15.9 94.8 101.8 59.5 65.6 41.4 54.8 101.2 100.2 71.6 102.3 Whisper base 17.9 53.5 8.9 9.9 77.9 86.1 43.1 45.8 28.5 47.4 101.4 98.6 61.7 101.1 Whisper small 10.2 30.8 6.1 5.6 51.3 55.8 24.0 27.7 15.0 30.2 106.4 90.1 44.4 38.4 Whisper medium 6.5 19.0 4.4 3.6 29.8 41.0 13.9 19.1 8.7 21.2 104.8 106.6 33.1 26.8 Whisper large 5.5 18.7 4.5 3.5 25.5 36.1 12.2 15.8 7.7 19.0 103.9 87.0 30.2 26.9 Whisper large-v2 4.5 12.5 4.2 3.0 21.9 32.9 9.7 13.8 8.3 15.4 102.7 88.9 27.1 21.5 Indonesian Hungarian Armenian Icelandic Croatian Italian Model Whisper tiny 79.0 83.8 118.6 51.7 113.3 29.8 37.0 107.3 123.0 165.2 100.6 100.7 36.1 99.1 Whisper base 59.1 65.0 126.3 33.1 95.5 17.9 22.8 89.5 114.7 109.2 101.6 107.2 27.8 100.7 Whisper small 33.4 38.9 86.6 16.3 72.6 9.8 12.0 88.6 118.3 70.3 104.4 100.4 19.6 100.1 Whisper medium 19.3 24.3 60.1 10.2 49.9 5.2 7.1 67.9 117.3 48.8 98.9 77.7 16.4 90.0 Whisper large 16.7 21.0 53.7 8.5 43.0 4.2 6.4 87.0 100.5 43.8 96.0 69.8 15.2 86.5 Whisper large-v2 13.4 17.0 44.6 7.1 38.2 4.0 5.3 nan 105.0 37.7 99.7 37.0 14.3 88.0 Macedonian Lithuanian Latvian Lingala Maori Lao Model Whisper tiny 105.4 115.1 98.5 91.6 94.5 73.3 101.5 113.7 100.3 51.2 100.8 124.8 62.0 101.8 Whisper base 96.7 105.1 87.3 79.8 77.5 59.9 107.4 125.7 100.3 35.1 97.6 122.6 44.0 102.4 Whisper small 91.3 102.2 65.6 53.2 59.5 36.9 100.9 144.2 60.2 18.9 92.2 110.1 24.2 69.5 Whisper medium 83.2 101.4 41.1 32.0 77.8 22.0 101.1 103.7 63.2 12.2 83.2 123.0 12.9 54.4 Whisper large 76.8 101.6 35.2 28.3 45.7 20.6 101.4 106.2 43.7 10.2 80.5 124.5 11.4 52.2 Whisper large-v2 75.6 101.5 28.1 23.1 38.5 16.5 100.7 110.5 38.3 8.7 76.6 115.7 9.5 47.1 Portuguese Occitan Punjabi Pashto Polish Dutch Model Whisper tiny 49.0 95.9 102.6 45.6 105.6 20.1 74.7 31.1 105.8 77.2 87.2 128.1 105.6 83.7 Whisper base 33.0 82.9 101.5 30.8 99.0 13.0 56.0 20.5 103.9 60.6 74.6 126.0 109.6 64.3 Whisper small 16.4 87.3 103.6 14.7 92.9 7.3 29.8 11.4 131.7 33.3 49.3 140.0 105.3 42.2 Whisper medium 9.9 79.5 102.0 8.0 119.4 5.0 20.0 7.2 147.0 17.3 31.9 143.9 104.0 44.9 Whisper large 8.3 75.9 102.8 7.2 92.7 4.8 15.4 6.4 177.9 15.7 27.8 130.0 103.5 29.2 Whisper large-v2 6.7 75.3 102.4 5.4 93.7 4.3 14.4 5.6 156.5 11.7 23.1 121.0 102.9 33.9 Swedish Swahili Telugu Tamil Tajik Model Whisper tiny 52.7 100.9 99.9 105.1 101.7 58.8 42.5 51.2 65.2 105.2 60.0 106.4 Whisper base 37.4 92.5 58.7 105.2 109.3 38.2 27.5 37.7 52.0 114.0 40.5 101.8 Whisper small 20.8 73.7 35.2 98.2 84.3 21.9 15.9 19.3 37.3 107.7 21.2 116.4 Whisper medium 11.2 52.8 23.1 82.8 74.0 15.4 10.4 11.6 28.2 109.6 12.7 105.1 Whisper large 10.5 47.9 20.6 100.6 74.5 13.2 9.4 10.3 25.0 93.3 10.7 111.7 Whisper large-v2 8.5 39.3 17.5 99.0 85.8 11.5 8.4 8.6 22.6 90.2 10.3 94.8 Bulgarian Bosnian Chinese Catalan Bengali Danish Czech Welsh Galician Gujarati Hebrew Tagalog Finnish French Hausa Hindi Luxembourgish Georgian Kannada Javanese Japanese Kazakh Korean Khmer Malayalam Norwegian Mongolian Myanmar Marathi Maltese Nepali Malay Romanian Slovenian Russian Serbian Somali Slovak Sindhi Shona Vietnamese Ukrainian Turkish Yoruba Uzbek Urdu Thai D.3. Speech Translation D.3.1. FLEURS Azerbaijani Belarusian Assamese Afrikaans Amharic Arabic Model Whisper tiny 1.6 0.1 0.1 0.4 0.1 0.8 0.4 0.4 0.4 5.2 0.6 0.6 0.6 0.7 Whisper base 4.4 0.3 1.0 0.4 0.8 3.3 2.7 0.7 4.1 13.1 1.9 2.7 0.7 5.0 Whisper small 18.1 0.2 10.6 1.2 5.8 7.1 14.8 2.7 16.8 25.1 9.3 14.2 1.3 18.1 Whisper medium 29.5 0.9 19.9 3.5 11.7 9.8 23.9 10.6 26.0 31.9 15.1 23.6 8.4 28.6 Whisper large 31.6 1.1 23.8 3.9 13.1 11.0 26.2 12.0 28.0 33.7 16.8 25.6 11.2 31.6 Whisper large-v2 34.1 1.9 25.5 5.4 13.7 11.7 28.5 13.2 29.7 34.2 18.4 27.8 13.0 32.7 Estonian German Spanish English Persian Greek Model Whisper tiny 5.2 0.1 68.6 7.7 0.1 0.1 0.2 0.8 4.7 4.0 0.7 0.1 0.2 1.0 Whisper base 13.7 0.7 73.3 12.4 0.3 0.2 0.5 2.1 13.1 10.5 1.5 0.0 0.6 3.4 Whisper small 25.9 11.6 77.3 18.2 3.6 5.8 7.3 12.0 23.5 17.5 3.9 0.3 5.4 11.1 Whisper medium 31.4 19.9 79.2 21.4 13.5 15.0 18.5 20.5 28.6 24.7 12.8 0.5 15.9 19.4 Whisper large 34.3 21.7 77.8 22.8 15.9 17.6 20.6 22.7 31.6 26.0 14.8 0.5 19.6 20.7 Whisper large-v2 34.6 23.7 80.2 23.3 18.7 19.6 22.1 24.4 32.2 27.9 16.2 0.4 21.8 22.0 Indonesian Hungarian Armenian Icelandic Croatian Italian Model Whisper tiny 0.6 0.1 0.1 0.3 0.4 5.3 0.2 0.2 0.1 0.1 0.1 0.8 0.5 0.8 Whisper base 3.7 0.2 0.1 2.6 0.4 11.3 1.5 0.2 0.2 0.2 0.1 0.9 3.7 1.7 Whisper small 14.6 4.8 0.7 16.4 1.8 17.8 9.6 1.4 0.2 0.8 0.5 2.3 12.2 5.7 Whisper medium 23.0 15.5 10.4 24.1 6.8 21.6 14.9 5.0 1.3 4.3 3.3 8.5 19.2 13.6 Whisper large 25.4 18.3 13.2 27.2 6.6 23.5 17.0 5.1 2.7 6.3 5.2 9.9 20.0 15.4 Whisper large-v2 27.0 21.2 16.0 29.1 9.1 23.6 18.9 6.2 2.4 5.4 6.1 11.6 21.3 16.8 Macedonian Lithuanian Latvian Lingala Maori Lao Model Whisper tiny 0.1 0.2 0.1 0.2 0.3 1.0 0.8 0.1 0.2 0.3 0.6 0.1 1.4 0.1 Whisper base 0.1 0.3 0.3 0.4 1.0 5.4 1.4 0.1 0.9 2.1 1.4 0.1 8.4 0.3 Whisper small 0.5 2.0 1.9 1.5 3.9 15.3 5.7 0.1 3.8 14.1 4.9 0.0 22.0 2.9 Whisper medium 0.9 8.1 9.6 10.0 8.5 23.5 13.8 0.5 10.9 23.2 11.2 0.2 29.1 12.7 Whisper large 1.2 9.3 12.0 12.5 9.4 26.4 16.5 1.0 13.1 25.5 12.8 0.5 30.5 12.9 Whisper large-v2 1.0 11.0 14.0 14.3 10.2 27.7 16.7 1.0 12.9 27.3 13.5 0.4 31.4 16.1 Portuguese Occitan Punjabi Pashto Polish Dutch Model Whisper tiny 2.7 1.7 0.3 0.8 0.3 12.1 1.0 3.1 0.5 0.7 0.3 0.1 0.0 0.6 Whisper base 7.5 4.2 1.1 5.1 0.4 22.4 4.9 12.1 0.7 4.6 1.3 0.3 0.1 5.4 Whisper small 15.9 9.5 4.4 14.0 0.8 31.2 18.3 19.7 2.0 14.4 6.9 0.6 0.1 19.3 Whisper medium 21.6 15.9 12.8 19.0 2.1 35.9 26.6 24.8 5.5 22.7 14.0 1.4 0.4 27.7 Whisper large 22.8 16.8 14.6 21.4 3.7 37.4 29.1 26.7 5.9 25.1 16.9 1.8 0.5 30.5 Whisper large-v2 24.0 20.2 15.7 22.3 3.4 38.1 31.5 27.8 5.7 26.1 17.0 1.8 0.7 32.5 Swedish Swahili Telugu Tamil Tajik Model Whisper tiny 1.8 0.1 0.2 0.3 0.2 0.2 0.2 1.2 0.4 0.0 0.1 0.2 Whisper base 9.1 0.1 0.4 0.4 0.2 0.7 2.4 6.9 1.5 0.2 0.9 0.5 Whisper small 22.9 0.1 2.1 4.0 4.4 5.8 15.7 18.7 8.8 0.5 8.5 0.5 Whisper medium 32.1 3.1 7.0 10.8 11.4 12.8 22.9 25.8 14.9 3.8 16.6 0.9 Whisper large 33.1 5.3 8.5 10.9 13.0 15.2 25.7 28.0 16.3 5.8 19.5 1.2 Whisper large-v2 35.3 7.2 9.2 12.5 14.5 16.1 26.6 29.4 17.2 6.0 20.4 1.4 Bulgarian Bosnian Chinese Catalan Bengali Danish Czech Welsh Galician Gujarati Hebrew Tagalog Finnish French Hausa Hindi Luxembourgish Georgian Kannada Javanese Japanese Kazakh Korean Khmer Malayalam Norwegian Mongolian Myanmar Marathi Maltese Nepali Malay Romanian Slovenian Russian Serbian Somali Slovak Sindhi Shona Vietnamese Ukrainian Turkish Yoruba Uzbek Urdu Thai D.3.2. COVOST 2 Estonian German Spanish Catalan Arabic Welsh Model Whisper tiny 0.2 4.9 0.4 4.0 10.5 0.2 0.1 6.1 0.3 5.1 0.3 0.1 0.1 Whisper base 1.2 11.0 0.5 11.7 21.3 0.3 0.1 15.4 4.9 13.0 4.9 0.5 0.1 Whisper small 17.7 22.3 1.0 25.3 33.0 2.4 4.9 27.3 27.6 24.0 17.3 1.4 0.2 Whisper medium 30.6 29.2 12.1 33.2 38.4 11.4 15.5 33.6 42.3 29.5 24.6 9.7 0.2 Whisper large 35.5 30.3 16.1 34.3 38.0 13.4 17.5 34.4 45.4 29.1 24.2 10.5 0.3 Whisper large-v2 39.7 31.8 21.5 36.3 40.1 15.0 19.3 36.4 48.1 30.9 26.1 13.9 0.1 Portuguese Russian Dutch Model Whisper tiny 4.3 9.5 5.7 0.4 2.0 0.1 0.2 0.4 Whisper base 12.4 23.2 16.1 1.4 10.5 0.4 2.8 1.4 Whisper small 28.1 40.6 30.9 9.2 29.9 1.7 16.8 6.8 Whisper medium 38.1 48.7 39.4 17.7 39.5 2.9 27.0 14.0 Whisper large 39.3 48.6 41.6 23.9 40.3 3.7 26.7 17.1 Whisper large-v2 41.2 51.6 43.3 21.6 42.9 4.2 28.3 18.0 Table 15. BLEU scores on CoVoST2 D.4. Long-form Transcription TED-LIUM3 Meanwhile Model Whisper tiny.en 5.5 12.8 13.8 15.1 17.0 22.0 30.3 Whisper tiny 6.8 15.5 16.7 17.0 18.7 24.4 33.1 Whisper base.en 4.6 9.4 11.2 13.2 12.5 16.6 25.2 Whisper base 4.8 12.2 12.2 14.5 13.5 18.4 26.9 Whisper small.en 4.6 6.0 9.4 12.0 10.8 14.0 21.9 Whisper small 4.2 6.9 10.1 12.1 11.1 14.3 22.3 Whisper medium.en 3.6 5.2 8.9 11.9 10.2 13.3 20.6 Whisper medium 3.8 5.4 8.6 11.4 10.3 13.2 20.3 Whisper large 3.8 5.3 8.8 11.0 10.3 13.4 20.4 Whisper large-v2 3.5 5.1 8.8 11.3 9.7 12.6 19.6 wav2vec2-base-100h 17.6 27.7 39.3 35.2 45.7 57.1 55.4 wav2vec2-base-960h 12.8 19.7 32.9 29.8 37.3 46.8 49.1 wav2vec2-large-960h-lv60-self 7.2 11.4 21.1 21.3 21.7 28.0 36.7 wav2vec2-large-960h 10.1 16.4 27.4 26.4 30.4 40.1 43.5 wav2vec2-large-robust-ft-libri-960h 8.8 15.2 22.9 23.4 23.0 31.0 36.8 hubert-large-ls960-ft 8.1 12.9 22.4 23.4 23.0 30.6 37.9 hubert-xlarge-ls960-ft 8.1 12.5 22.9 23.2 23.1 31.3 38.1 stt en conformer ctc large 4.0 9.8 13.1 14.5 12.6 17.6 25.1 stt en conformer transducer xlarge 5.3 10.6 17.1 19.8 16.2 19.7 38.9 Table 16. Long-form English transcription WER (%) Indonesian Mongolian Japanese Latvian Persian French Italian Slovenian Swedish Chinese Turkish Tamil Earnings-21 Earnings-22 Kincaid46 CORAAL Rev16\nE. Training Dataset Statistics\nMultilingual Speech Recognition Dataset Components Chinese 23446 German 13344 Spanish 11100 Russian 9761 French 9752 17% Multilingual Speech Recognition Portuguese 8573 Korean 7993 (117,113 hours) Japanese 7054 Turkish 4333 Polish 4278 Italian 2585 Swedish 2119 Dutch 2077 Catalan 1883 Finnish 1066 Indonesian 1014 Arabic 739 Ukrainian 697 Vietnamese 691 18% Translation (125,739 hours) Hebrew 688 Greek 529 Danish 473 Malay 382 Hungarian 379 Romanian 356 Norwegian 266 Thai 226 Czech 192 Tamil 136 Urdu 104 Croatian 91 Slovak 90 Bulgarian 86 Tagalog 75 Welsh 73 Lithuanian 67 Latvian 65 Azerbaijani 47 Estonian 41 Slovenian 41 Serbian 28 Persian 24 Basque 21 Icelandic 16 Macedonian 16 Armenian 13 Kazakh 12 Hindi 12 Bosnian 11 65% English Speech Recognition Galician 8.9 Albanian 5.7 (438,218 hours) Sinhala 5.4 Swahili 5.4 Telugu 4.3 Afrikaans 4.1 Kannada 3.8 Belarusian 2.4 Khmer 1.3 Bengali 1.3 Maltese 1.1 Haitian Creole 1.0 Punjabi 0.8 Marathi 0.6 Nepali 0.6 Georgian 0.6 Malayalam 0.5 Yiddish 0.4 Uzbek 0.3 Gujarati 0.3 Tajik 0.3 Malagasy 0.2 Burmese 0.1 Sundanese 0.1 Lao 0.1 0.1 1 10 100 Hours of audio Figure 11. Training dataset statistics Translation Turkmen 1 Bashkir 1 Malagasy 2 Uzbek 4 Sundanese 7 Hausa 8 Luxembourgish 10 Tatar 14 Tajik 15 Lingala 20 Lao 20 Somali 21 Macedonian 30 Kazakh 31 Amharic 32 Georgian 40 Maltese 41 Sindhi 46 Faroese 46 Occitan 49 Burmese 59 Pashto 63 Latvian 68 Albanian 72 Haitian Creole 74 Estonian 79 Mongolian 79 Icelandic 84 Yiddish 85 Azerbaijani 86 Kannada 90 Lithuanian 99 Armenian 116 Punjabi 117 Belarusian 133 Nepali 133 Assamese 136 Serbian 136 Slovak 144 Basque 168 Tibetan 186 Sanskrit 195 Bulgarian 202 Gujarati 208 Sinhala 211 Bosnian 219 Catalan 236 Croatian 239 Breton 269 Shona 279 Swahili 282 Marathi 288 Norwegian 322 Afrikaans 330 Hawaiian 338 Galician 368 Danish 386 Persian 392 Slovenian 395 Czech 401 Hebrew 418 Yoruba 432 Ukrainian 509 Hungarian 554 Romanian 555 Javanese 622 Khmer 672 Finnish 750 Malayalam 892 Tagalog 894 Greek 968 Telugu 987 Swedish 1055 Indonesian 1174 Maori 1381 Tamil 1484 Latin 1614 Thai 1635 Malay 1691 Vietnamese 1719 Dutch 1767 Norwegian Nynorsk 1889 Bengali 1988 Urdu 1990 Italian 2145 Polish 2200 Turkish 2241 Arabic 2286 Portuguese 3620 German 4309 French 4481 Hindi 5438 Spanish 6693 Russian 7687 Welsh 8263 Japanese 8860 Chinese 11731 Korean 19938 1 10 100 Hours of audio\nF. Hyperparameters\nHyperparameter Value Updates 1048576 Batch Size 256 Warmup Updates 2048 Max grad norm 1.0 Optimizer AdamW β1 0.9 β2 0.98 ϵ 10−6 Weight Decay 0.1 Weight Init Gaussian Fan-In Learning Rate Schedule Linear Decay Speechless audio subsample factor 10× Condition on prior text rate 50% Table 17. Whisper training hyperparameters. Hyperparameter Value Updates 655360 Batch Size 1024 BPE Dropout 0.1 Stochastic Depth 0.1 SpecAugment Policy LibriSpeech Basic Table 18. Hyperparameters changed for Whisper Large V2. Model Max Learning Rate Tiny 1.5 × 10−3 Base 1 × 10−3 Small 5 × 10−4 Medium 2.5 × 10−4 Large 1.75 × 10−4 Large V2 2.0 × 10−4 Table 19. Whisper model learning rates.\n--- FOOTNOTES *Equal contribution 1OpenAI, San Francisco, CA 94110, USA. Correspondence to: Alec Radford <alec@openai.com>, Jong 3After the original release of Whisper, we trained an additional 39M 74M 244M 769M 1550M 1K 10K 100K 1M 1K 10K 100K 38M 73M 244M 768M 1549M 1549M 38M 73M 244M 768M 1549M 1549M 38M 73M 244M 768M 1549M 1549M 38M 73M 244M 768M 1549M 1549M 1K 10K 1K 10K", "persona": "Student", "response": "### 1. Overview\n680,000 hours of multilingual and multitask supervision results in models which are able to generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zero-shot transfer setting without fine-tuning. A fully featured speech recognition system can involve many additional components such as voice activity detection, speaker diarization, and inverse text normalization. If there is no speech in an audio segment, the model is trained to predict a <|nospeech|> token. Multilingual Speech Recognition 2.5. Robustness to Additive Noise Robustness to Additive Noise The input uses 128 Mel frequency bins instead of 80; For an additional filtering pass, after training an initial model information about its error rate is aggregated. Low-quality audios are identified and removed. 1.2. Pre-processing All audio is re-sampled to 16,000 Hz, and an 80-channel log-magnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds. An encoder-decoder Transformer is used. The encoder processes this input representation with a small stem consisting of two convolution layers with a filter width of 3 and the GELU activation where the second convolution layer has a stride of two. The Transformer uses pre-activation residual blocks and a final layer normalization is applied to the encoder output.\n\n### 2. Background\nWhisper-v2 A new language token for Cantonese; Whisper (Original Whisper-v1 in the url) 1. Whisper (Original Whisper-v1 in the url) An audio language detector is used, which was created by fine-tuning a prototype model trained on a prototype version of the dataset on VoxLingua107 to ensure that the spoken language matches the language of the transcript according to CLD2. If the two do not match, the (audio, transcript) pair is not included. Audio files are broken into 30-second segments. Of those 680,000 hours of audio, 117,000 hours cover 96 other languages. The dataset also includes 125,000 hours of X > en translation data. The decoder uses learned position embeddings and tied input-output token representations. A single model perform the entire speech processing pipeline is preferred. First, the language being spoken is predicted, which is represented by a unique token for each language in the training set (99 total). These language targets are sourced from the aforementioned VoxLingua107 model. The next token specifies the task (either transcription or translation) with an <|transcribe|> or <|translate|> token. Lastly, a <|endoftranscript|> token is added.\n\n### 3. Methods\nThe dataset is constructed from audio that is paired with transcripts on the Internet. This results in a very diverse dataset covering a broad distribution of audio from many different environments, recording setups, speakers, and languages. Diversity in audio quality can help train a model. Several automated filtering methods are used to improve transcript quality. The beginning of prediction is started with a <|startoftranscript|> token. Enter your email Subscribe\n\n### 4. Results\nWhisper Overview 1.4. Multitask Format Whisper Variants 2.1. Zero-shot Evaluation Zero-shot Evaluation Zero-shot Evaluation Zero-shot Whisper model has a relatively unremarkable LibriSpeech clean-test WER of 2.5, which is roughly the performance of modern supervised baseline or the mid-2019 state of the art. Zero-shot Whisper models have very different robustness properties than supervised LibriSpeech models and out-perform all benchmarked LibriSpeech models by large amounts on other datasets. When compared to a human in Figure 2, the best zero-shot Whisper models roughly match their accuracy and robustness. zero-shot and out-of-distribution evaluations of models, particularly when attempting to compare to human performance, to avoid overstating the capabilities of machine learning systems due to misleading comparisons. Whisper performs well on Multilingual LibriSpeech (MLS), outperforming XLS-R, mSLAM and Maestro in a zero-shot setting. The zero-shot performance of Whisper is not competitive with prior supervised work here and underperforms the supervised SOTA by 13.6% on Fleurs, since the Whisper dataset contains no training data for 20 of the 102 languages in Fleurs. The pure-human performance is only a fraction of a percentage point better than Whisper’s. Whisper’s English ASR performance is not perfect but very close to human-level accuracy.\n\n### 5. Takeaways\nResults 1.1. Dataset hours of X!en translation data. We Ttheir prediction are interleaved with the caption tokens: the start time token is predicted before each caption’s text, and the end time token is predicted after. 1.5. Training Details 2. Results Figure 3: Strong squared correlation coefficient of 0.83 between the log of the word error rate and the log of the amount of training data per language. Checking 2.6. Long-form Transcription Yet, it is noted that the possibility that some of the commercial ASR systems have been trained on some of these publicly available datasets, and therefore these results may not be accurately reflecting the relative robustness of the systems. 2.7. Comparison with Human Performance Comparison with Human Performance For feature normalization, the input is globally scaled to be between -1 and 1 with approximately zero mean across the pre-training dataset. , the Whisper dataset is one of the largest ever created in supervised speech recognition.", "paper_id": "25", "story_id": "25-06"}
{"paper": "Robust Speech Recognition via Large-Scale Weak Supervision Alec Radford * 1 Jong Wook Kim * 1 Tao Xu 1 Greg Brockman 1 Christine McLeavey 1 Ilya Sutskever 1\n\nAbstract\nWe study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any ﬁnetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing. arXiv:2212.04356v1  [eess.AS]  6 Dec 2022\n\n1. Introduction\nProgress in speech recognition has been energized by the development of unsupervised pre-training techniques exempliﬁed by Wav2Vec 2.0 (Baevski et al., 2020). Since these methods learn directly from raw audio without the need for human labels, they can productively use large datasets of unlabeled speech and have been quickly scaled up to 1,000,000 hours of training data (Zhang et al., 2021), far more than the 1,000 or so hours typical of an academic supervised dataset. When ﬁne-tuned on standard benchmarks, this approach has improved the state of the art, especially in a low-data setting. These pre-trained audio encoders learn high-quality representations of speech, but because they are purely unsupervised they lack an equivalently performant decoder mapping those representations to usable outputs, necessitating a ﬁnetuning stage in order to actually perform a task such as speech recognition1. This unfortunately limits their usefulness and impact as ﬁne-tuning can still be a complex process requiring a skilled practitioner. There is an additional risk with requiring ﬁne-tuning. Machine learning Wook Kim <jongwook@openai.com>. methods are exceedingly adept at ﬁnding patterns within a training dataset which boost performance on held-out data from the same dataset. However, some of these patterns are brittle and spurious and don’t generalize to other datasets and distributions. In a particularly disturbing example, Radford et al. (2021) documented a 9.2% increase in object classiﬁcation accuracy when ﬁne-tuning a computer vision model on the ImageNet dataset (Russakovsky et al., 2015) without observing any improvement in average accuracy when classifying the same objects on seven other natural image datasets. A model that achieves “superhuman” performance when trained on a dataset can still make many basic errors when evaluated on another, possibly precisely because it is exploiting those dataset-speciﬁc quirks that humans are oblivious to (Geirhos et al., 2020). This suggests that while unsupervised pre-training has improved the quality of audio encoders dramatically, the lack of an equivalently high-quality pre-trained decoder, combined with a recommended protocol of dataset-speciﬁc ﬁnetuning, is a crucial weakness which limits their usefulness and robustness. The goal of a speech recognition system should be to work reliably “out of the box” in a broad range of environments without requiring supervised ﬁne-tuning of a decoder for every deployment distribution. As demonstrated by Narayanan et al. (2018), Likhomanenko et al. (2020), and Chan et al. (2021) speech recognition systems that are pre-trained in a supervised fashion across many datasets/domains exhibit higher robustness and generalize much more effectively to held-out datasets than models trained on a single source. These works achieve this by combining as many existing high-quality speech recognition datasets as possible. However, there is still only a moderate amount of this data easily available. SpeechStew (Chan et al., 2021) mixes together 7 pre-existing datasets totalling 5,140 hours of supervision. While not insigniﬁcant, this is still tiny compared to the previously mentioned 1,000,000 hours of unlabeled speech data utilized in Zhang et al. (2021). Recognizing the limiting size of existing high-quality supervised datasets, recent efforts have created larger datasets for speech recognition. By relaxing the requirement of goldstandard human-validated transcripts, Chen et al. (2021) and Galvez et al. (2021) make use of sophisticated automated pipelines to scale weakly supervised speech recognition to 10,000 and 30,000 hours of noisier training data. This trade-off between quality and quantity is often the right call. Although understudied so far for speech recognition, recent work in computer vision has demonstrated that moving beyond gold-standard crowdsourced datasets such as ImageNet (Russakovsky et al., 2015) to much larger but weakly supervised datasets signiﬁcantly improves the robustness and generalization of models (Mahajan et al., 2018; Kolesnikov et al., 2020). Yet these new datasets are only a few times larger than the sum of existing high-quality datasets and still much smaller than prior unsupervised work. In this work we close that gap, scaling weakly supervised speech recognition the next order of magnitude to 680,000 hours of labeled audio data. We call our approach Whisper2. We demonstrate models trained at this scale transfer well to existing datasets zeroshot, removing the need for any dataset-speciﬁc ﬁne-tuning to achieve high-quality results. In addition to scale, our work also focuses on broadening the scope of weakly supervised pre-training beyond English-only speech recognition to be both multilingual and multitask. Of those 680,000 hours of audio, 117,000 hours cover 96 other languages. The dataset also includes 125,000 hours of X→en translation data. We ﬁnd that for sufﬁciently large models there is no drawback and even beneﬁts to joint multilingual and multitask training. Our work suggests that simple scaling of weakly supervised pre-training has been underappreciated so far for speech recognition. We achieve these results without the need for the self-supervision or self-training techniques that have been a mainstay of recent large-scale speech recognition work. To serve as a foundation for further research on robust speech recognition, we release inference code and models at the following URL: https://github.com/openai/whisper.\n\n2. Approach\n2.1. Data Processing Following the trend of recent work leveraging web-scale text from the internet for training machine learning systems, we take a minimalist approach to data pre-processing. In contrast to a lot of work on speech recognition, we train Whisper models to predict the raw text of transcripts without any signiﬁcant standardization, relying on the expressiveness of sequence-to-sequence models to learn to map between utterances and their transcribed form. This simpliﬁes the speech recognition pipeline since it removes the need for a separate inverse text normalization step in order to produce naturalistic transcriptions. We construct the dataset from audio that is paired with transcripts on the Internet. This results in a very diverse dataset covering a broad distribution of audio from many different environments, recording setups, speakers, and languages. While diversity in audio quality can help train a model to be robust, diversity in transcript quality is not similarly beneﬁcial. Initial inspection showed a large amount of subpar transcripts in the raw dataset. To address this, we developed several automated ﬁltering methods to improve transcript quality. Many transcripts on the internet are not actually humangenerated but the output of existing ASR systems. Recent research has shown that training on datasets of mixed human and machine-generated data can signiﬁcantly impair the performance of translation systems (Ghorbani et al., 2021). In order to avoid learning “transcript-ese”, we developed many heuristics to detect and remove machine-generated transcripts from the training dataset. Many existing ASR systems output only a limited subset of written language which removes or normalizes away aspects that are difﬁcult to predict from only audio signals such as complex punctuation (exclamation points, commas, and question marks), formatting whitespace such as paragraphs, or stylistic aspects such as capitalization. An all-uppercase or all-lowercase transcript is very unlikely to be human generated. While many ASR systems include some level of inverse text normalization, it is often simple or rule-based and still detectable from other unhandled aspects such as never including commas. We also use an audio language detector, which was created by ﬁne-tuning a prototype model trained on a prototype version of the dataset on VoxLingua107 (Valk & Alum¨ae, 2021) to ensure that the spoken language matches the language of the transcript according to CLD2. If the two do not match, we don’t include the (audio, transcript) pair as a speech recognition training example in the dataset. We make an exception if the transcript language is English and add these pairs to the dataset as X→en speech translation training examples instead. We use fuzzy de-duping of transcript texts to reduce the amount of duplication and automatically generated content in the training dataset. We break audio ﬁles into 30-second segments paired with the subset of the transcript that occurs within that time segment. We train on all audio, including segments where there is no speech (though with sub-sampled probability) and use these segments as training data for voice activity detection. data sources and performed manual inspection of these data sources sorting by a combination of both high error rate and data source size in order to identify and remove low-quality ones efﬁciently. This inspection showed a large amount of only partially transcribed or poorly aligned/misaligned transcripts as well as remaining low-quality machine-generated captions that ﬁltering heuristics did not detect. To avoid contamination, we perform de-duplication at a transcript level between the training dataset and the evaluation datasets we thought were at higher risk of overlap, namely TED-LIUM 3 (Hernandez et al., 2018). 2.2. Model Since the focus of our work is on studying the capabilities of large-scale supervised pre-training for speech recognition, we use an off-the-shelf architecture to avoid confounding our ﬁndings with model improvements. We chose an encoder-decoder Transformer (Vaswani et al., 2017) as this architecture has been well validated to scale reliably. All audio is re-sampled to 16,000 Hz, and an 80-channel logmagnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds. For feature normalization, we globally scale the input to be between -1 and 1 with approximately zero mean across the pre-training dataset. The encoder processes this input representation with a small stem consisting of two convolution layers with a ﬁlter width of 3 and the GELU activation function (Hendrycks & Gimpel, 2016) where the second convolution layer has a stride of two. Sinusoidal position embeddings are then added to the output of the stem after which the encoder Transformer blocks are applied. The transformer uses pre-activation residual blocks (Child et al., 2019), and a ﬁnal layer normalization is applied to the encoder output. The decoder uses learned position embeddings and tied input-output token representations (Press & Wolf, 2017). The encoder and decoder have the same width and number of transformer blocks. Figure 1 summarizes the model architecture. We use the same byte-level BPE text tokenizer used in GPT2 (Sennrich et al., 2015; Radford et al., 2019) for the Englishonly models and reﬁt the vocabulary (but keep the same size) for the multilingual models to avoid excessive fragmentation on other languages since the GPT-2 BPE vocabulary is English only. 2.3. Multitask Format Although predicting which words were spoken in a given audio snippet is a core part of the full speech recognition problem and extensively studied in research, it is not the only part. A fully featured speech recognition system can involve many additional components such as voice activity detection, speaker diarization, and inverse text normal- ization. These components are often handled separately, resulting in a relatively complex system around the core speech recognition model. To reduce this complexity, we would like to have a single model perform the entire speech processing pipeline, not just the core recognition part. An important consideration here is the interface for the model. There are many different tasks that can be performed on the same input audio signal: transcription, translation, voice activity detection, alignment, and language identiﬁcation are some examples. For this kind of one-to-many mapping to work with a single model, some form of task speciﬁcation is necessary. We use a simple format to specify all tasks and conditioning information as a sequence of input tokens to the decoder. Since our decoder is an audio-conditional language model, we also train it to condition on the history of text of the transcript in the hope that it will learn to use longer-range text context to resolve ambiguous audio. Speciﬁcally, with some probability we add the transcript text preceding the current audio segment to the decoder’s context. We indicate the beginning of prediction with a <|startoftranscript|> token. First, we predict the language being spoken which is represented by a unique token for each language in our training set (99 total). These language targets are sourced from the aforementioned VoxLingua107 model. In the case where there is no speech in an audio segment, the model is trained to predict a <|nospeech|> token indicating this. The next token speciﬁes the task (either transcription or translation) with an <|transcribe|> or <|translate|> token. After this, we specify whether to predict timestamps or not by including a <|notimestamps|> token for that case. At this point, the task and desired format is fully speciﬁed, and the output begins. For timestamp prediction, we predict time relative to the current audio segment, quantizing all times to the nearest 20 milliseconds which matches the native time resolution of Whisper models, and add additional tokens to our vocabulary for each of these. We interleave their prediction with the caption tokens: the start time token is predicted before each caption’s text, and the end time token is predicted after. When a ﬁnal transcript segment is only partially included in the current 30second audio chunk, we predict only its start time token for the segment when in timestamp mode, to indicate that the subsequent decoding should be performed on an audio window aligned with that time, otherwise we truncate the audio to not include the segment. Lastly, we add a <|endoftranscript|> token. We only mask out the training loss over the previous context text, and train the model to predict all other tokens. Please see Figure 1 for an overview of our format and training setup. Multitask training data (680k hours) Sequence-to-sequence learning English transcription   “Ask not what your country can do for ⋯”   Ask not what your country can do for ⋯ Any-to-English speech translation   “El rápido zorro marrón salta sobre ⋯”   The quick brown fox jumps over ⋯ Transformer Encoder Blocks Transformer Decoder Blocks Non-English transcription  “언덕 위에 올라 내려다보면 너무나 넓고 넓은 ⋯”   언덕 위에 올라 내려다보면 너무나 넓고 넓은 ⋯ ~ Sinusoidal Positional Encoding No speech  (background music playing)   ∅ Multitask training format X → X Transcription Language identification LANGUAGE TRANSCRIBE TAG previous text tokens START OF TRANSCRIPT PREV NO SPEECH TRANSLATE Custom vocabulary / prompting Voice activity X → English detection Translation special tokens text tokens timestamp (VAD) tokens Figure 1. Overview of our approach. A sequence-to-sequence Transformer model is trained on many different speech processing tasks, including multilingual speech recognition, speech translation, spoken language identiﬁcation, and voice activity detection. All of these tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task speciﬁers or classiﬁcation targets, as further explained in Section 2.3. 2.4. Training Details We train a suite of models of various sizes in order to study the scaling properties of Whisper. Please see Table 1 for an overview. We train with data parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing (Griewank & Walther, 2000; Chen et al., 2016). Models were trained with AdamW (Loshchilov & Hutter, 2017) and gradient norm clipping (Pascanu et al., 2013) with a linear learning rate decay to zero after a warmup over the ﬁrst 2048 updates. A batch size of 256 segments was used, and the models are trained for 220 updates which is between two and three passes over the dataset. Due to only training for a few epochs, over-ﬁtting is not a large concern, and we do not use any data augmentation or regularization and instead rely on the diversity contained within such a TRANS- ⋯ EN 0.0 The quick brown CRIBE next-token prediction MLP MLP cross attention self attention self attention cross attention ⋮ ⋮ ⋮ MLP MLP self attention cross attention self attention MLP self attention MLP cross attention self attention Learned Positional Encoding 2 × Conv1D + GELU TRANS- ⋯ SOT EN CRIBE 0.0 The quick Log-Mel Spectrogram Tokens in Multitask Training Format Time-aligned transcription begin ⋯ end time text tokens begin end time text tokens time time EOT NO TIMESTAMPS text tokens Text-only transcription (allows dataset-specific fine-tuning) large dataset to encourage generalization and robustness. Please see Appendix F for full training hyperparameters.3 During early development and evaluation we observed that Whisper models had a tendency to transcribe plausible but almost always incorrect guesses for the names of speakers. This happens because many transcripts in the pre-training dataset include the name of the person who is speaking, encouraging the model to try to predict them, but this information is only rarely inferable from only the most recent 30 Large model (denoted V2) for 2.5X more epochs while adding SpecAugment (Park et al., 2019), Stochastic Depth (Huang et al., 2016), and BPE Dropout (Provilkov et al., 2019) for regularization. Reported results have been updated to this improved model unless otherwise speciﬁed. Model Layers Width Heads Parameters Tiny 4 384 6 Base 6 512 8 Small 12 768 12 Medium 24 1024 16 Large 32 1280 20 Table 1. Architecture details of the Whisper model family. seconds of audio context. To avoid this, we ﬁne-tune Whisper models brieﬂy on the subset of transcripts that do not include speaker annotations which removes this behavior.\n\n3. Experiments\n3.1. Zero-shot Evaluation The goal of Whisper is to develop a single robust speech processing system that works reliably without the need for dataset speciﬁc ﬁne-tuning to achieve high-quality results on speciﬁc distributions. To study this capability, we reuse a wide set of existing speech processing datasets to check whether Whisper is able to generalize well across domains, tasks, and languages. Instead of using the standard evaluation protocol for these datasets, which include both a train and test split, we evaluate Whisper in a zero-shot setting without using any of the training data for each of these datasets so that we are measuring broad generalization. 3.2. Evaluation Metrics Speech recognition research typically evaluates and compares systems based on the word error rate (WER) metric. However, WER, which is based on string edit distance, penalizes all differences between the model’s output and the reference transcript including innocuous differences in transcript style. As a result, systems that output transcripts that would be judged as correct by humans can still have a large WER due to minor formatting differences. While this poses a problem for all transcribers, it is particularly acute for zero-shot models like Whisper, which do not observe any examples of speciﬁc datasets transcript formats. This is not a novel observation; the development of evaluation metrics that better correlate with human judgement is an active area of research, and while there are some promising methods, none have seen widespread adoption for speech recognition yet. We opt to address this problem with extensive standardization of text before the WER calculation to minimize penalization of non-semantic differences. Our text normalizer was developed through iterative manual inspection to identify common patterns where naive WER penalized Whisper models for an innocuous difference. Appendix C includes full details. For several datasets, we observe WER drops of up to 50 percent usually due to a quirk such as a dataset’s reference transcripts seperating contractions from words with whitespace. We caution this development procedure comes at a risk of overﬁtting to the transcription style of Whisper models which we investigate in Section 4.4. We are releasing the code for our text normalizer to allow for easy comparison and to help others study the performance of speech recognition systems in out-of-distribution settings. 3.3. English Speech Recognition In 2015, Deep Speech 2 (Amodei et al., 2015) reported a speech recognition system matched human-level performance when transcribing the LibriSpeech test-clean split. As part of their analysis they concluded: “Given this result, we suspect that there is little room for a generic speech system to further improve on clean read speech without further domain adaptation.” Yet seven years later the SOTA WER on LibriSpeech test-clean has dropped another 73% from their 5.3% to 1.4% (Zhang et al., 2021), far below their reported human-level error rate of 5.8%. Despite this massive and unanticipated further improvement in performance on held-out but in-distribution data, speech recognition models trained on LibriSpeech remain far above human error rates when used in other settings. What explains this gap between reportedly superhuman performance in-distribution and subhuman performance out-of-distribution? We suspect a large part of this gap between human and machine behavior is due to conﬂating different capabilities being measured by human and machine performance on a test set. This claim may seem confusing at ﬁrst; if both humans and machines are taking the same test, how can it be that different skills are being tested? The difference arises not in the testing but in how they trained for it. Humans are often asked to perform a task given little to no supervision on the speciﬁc data distribution being studied. Thus human performance is a measure of out-of-distribution generalization. But machine learning models are usually evaluated after training on a large amount of supervision from the evaluation distribution, meaning that machine performance is instead a measure of in-distribution generalization. While both humans and machines are being evaluated on the same test data, two quite different abilities are being measured due to a difference in train data. Whisper models, which are trained on a broad and diverse distribution of audio and evaluated in a zero-shot setting, could potentially match human behavior much better than existing systems. To study whether this is the case (or whether the difference between machine and human performance is due to yet-to-be-understood factors) we can compare Whisper models with both human performance and standard ﬁne-tuned machine learning models and check which they more closely match. 50 Average WER on [Common Voice, CHiME-6, TED-LIUM] (%) 40 30 20 10 0 0 1 2 3 4 5 6 7 8 WER on LibriSpeech dev-clean (%) Figure 2. Zero-shot Whisper models close the gap to human robustness. Despite matching or outperforming a human on LibriSpeech dev-clean, supervised LibriSpeech models make roughly twice as many errors as a human on other datasets demonstrating their brittleness and lack of robustness. The estimated robustness frontier of zero-shot Whisper models, however, includes the 95% conﬁdence interval for this particular human. To quantify this difference, we examine both overall robustness, that is average performance across many distributions/datasets, and effective robustness, introduced by Taori et al. (2020), which measures the difference in expected performance between a reference dataset, which is usually in-distribution, and one or more out-of-distribution datasets. A model with high effective robustness does better than expected on out-of-distribution datasets as a function of its performance on the reference dataset and approaches the ideal of equal performance on all datasets. For our analysis, we use LibriSpeech as the reference dataset due to its central role in modern speech recognition research and the availability of many released models trained on it, which allows for characterizing robustness behaviors. We use a suite of 12 other academic speech recognition datasets to study out-of-distribution behaviors. Full details about these datasets can be found in Appendix A. Our main ﬁndings are summarized in Figure 2 and Table 2. Although the best zero-shot Whisper model has a relatively unremarkable LibriSpeech clean-test WER of 2.5, which is roughly the performance of modern supervised baseline or the mid-2019 state of the art, zero-shot Whisper models have very different robustness properties than supervised LibriSpeech models and out-perform all benchmarked LibriSpeech models by large amounts on other datasets. Even wav2vec 2.0 Whisper RER Dataset Large (no LM) Large V2 (%) LibriSpeech Clean 2.7 2.7 0.0 Supervised LibriSpeech models Zero-shot Whisper models Zero-shot Human (Alec) Ideal robustness (y = x) Artie 24.5 6.2 74.7 Common Voice 29.9 9.0 69.9 Fleurs En 14.6 4.4 69.9 Tedlium 10.5 4.0 61.9 CHiME6 65.8 25.5 61.2 VoxPopuli En 17.9 7.3 59.2 CORAAL 35.6 16.2 54.5 AMI IHM 37.0 16.9 54.3 Switchboard 28.3 13.8 51.2 CallHome 34.8 17.6 49.4 WSJ 7.7 3.9 49.4 AMI SDM1 67.6 36.4 46.2 LibriSpeech Other 6.2 5.2 16.1 Average 29.3 12.8 55.2 Table 2. Detailed comparison of effective robustness across various datasets. Although both models perform within 0.1% of each other on LibriSpeech, a zero-shot Whisper model performs much better on other datasets than expected for its LibriSpeech performance and makes 55.2% less errors on average. Results reported in word error rate (WER) for both models after applying our text normalizer. the smallest zero-shot Whisper model, which has only 39 million parameters and a 6.7 WER on LibriSpeech test-clean is roughly competitive with the best supervised LibriSpeech model when evaluated on other datasets. When compared to a human in Figure 2, the best zero-shot Whisper models roughly match their accuracy and robustness. For a detailed breakdown of this large improvement in robustness, Table 2 compares the performance of the best zero-shot Whisper model with a supervised LibriSpeech model that has the closest performance to it on LibriSpeech test-clean. Despite their very close performance on the reference distribution, the zero-shot Whisper model achieves an average relative error reduction of 55.2% when evaluated on other speech recognition datasets. This ﬁnding suggests emphasizing zero-shot and out-ofdistribution evaluations of models, particularly when attempting to compare to human performance, to avoid overstating the capabilities of machine learning systems due to misleading comparisons. 3.4. Multi-lingual Speech Recognition In order to compare to prior work on multilingual speech recognition, we report results on two low-data benchmarks: Multilingual LibriSpeech (MLS) (Pratap et al., 2020b) and VoxPopuli (Wang et al., 2021) in Table 3. 160 MY KA BN GU PA LO TE ML KM UZ 80 TG MT NE BE HY 40 SW MR IS KN Word Error Rate (WER) KK SR AF CY FA LT HE AZ LV UR SL HI ET 20 TA HU MK AR RO GL ZH BG FIL BS KO DA EL HR CS SK TH VI 10 FI NB MS UK SV TR FR CA ID NL RU PL JA 5 DE PT EN IT r2 = 0.83 ES 2.5 0.1 1 10 100 Hours of transcribed audio Figure 3. Correlation of pre-training supervision amount with downstream speech recognition performance. The amount of pre-training speech recognition data for a given language is very predictive of zero-shot performance on that language in Fleurs. Model MLS VoxPopuli VP-10K + FT - 15.3 XLS-R (1B) 10.9 10.6 mSLAM-CTC (2B) 9.7 9.1 Maestro - 8.1 Zero-Shot Whisper 7.3 13.6 Table 3. Multilingual speech recognition performance. Zeroshot Whisper improves performance on Multilingual LibriSpeech (MLS) but is still signiﬁcantly behind both Maestro, XLS-R, and mSLAM on VoxPopuli. et al., 2022), and Maestro (Chen et al., 2022b) in a zero-shot setting. We caution that we do use a simple text standardizer for this result which prevents direct comparison or claims of SOTA performance. On VoxPopuli, however, Whisper signiﬁcantly underperforms prior work and only beats the VP-10K+FT baseline from the original paper. We suspect the underperformance of Whisper models on VoxPopuli could be due to other models including this distribution as a major source for their unsupervised pre-training data and the dataset having signiﬁcantly more supervised data, which beneﬁts ﬁne-tuning. While MLS has 10 hours of training data per language, the average amount of training data per language is roughly 10× higher for VoxPopuli. 40 PT CA DE SV 35 FR AF DA SR RO 30 NB BS GL RU HR SK UK CS ID BG MK MS IT 25 TR EL AR ES PL FIL FI OC NL HE KO HI HU BLEU VI 20 FA ET ZH SL ML HY UR TH GU JA LB LV 15 NE PA TE BN LT TG CY MR MT KN BE AZ 10 TA MI LO IS SW SD UZ KM AS 5 KK KA PS r2 = 0.24 MN SN LN AM YO SO MY HA 0 1 10 100 Hours of translated audio Figure 4. Correlation of pre-training supervision amount with downstream translation performance. The amount of pretraining translation data for a given language is only moderately predictive of Whisper’s zero-shot performance on that language in Fleurs. the Indo-European language family and many of which are high-resource languages. These benchmarks only provide limited coverage and room to study Whisper models multilingual capabilities which include training data for speech recognition in 75 languages. To study the performance of Whisper more broadly we also report performance on the Fleurs dataset (Conneau et al., 2022). In particular, we were interested in studying the relationship between the amount of training data we have for a given language and the resulting downstream zero-shot performance for that language. We visualize this relation in Figure 3. We ﬁnd a strong squared correlation coefﬁcient of 0.83 between the log of the word error rate and the log of the amount of training data per language. Checking the regression coefﬁcient for a linear ﬁt to these log-log values results in an estimate that WER halves for every 16× increase in training data. We also observed that many of the largest outliers in terms of worse than expected performance according to this trend are languages that have unique scripts and are more distantly related to the Indo-European languages making up the majority of the training dataset such as Hebrew (HE), Telugu (TE), Chinese (ZH), and Korean (KO). These differences could be due to a lack of transfer due to linguistic distance, our byte level BPE tokenizer being a poor match for these languages, or variations in data quality. X →English High Mid Low All XMEF-X 34.2 20.2 5.9 14.7 XLS-R (2B) 36.1 27.7 15.1 22.1 mSLAM-CTC (2B) 37.8 29.6 18.5 24.8 Maestro 38.2 31.3 18.4 25.2 Zero-Shot Whisper 36.2 32.6 25.2 29.1 Table 4. X→en Speech translation performance. Zero-shot Whisper outperforms existing models on CoVoST2 in the overall, medium, and low resource settings but still moderately underperforms on high-resource languages compared to prior directly supervised work. Language ID Fleurs w2v-bert-51 (0.6B) 71.4 mSLAM-CTC (2B) 77.7 Zero-shot Whisper 64.5 Table 5. Language identiﬁcation performance. Zero-shot Whisper’s accuracy at language identiﬁcation is not competitive with prior supervised results on Fleurs. This is partially due to Whisper being heavily penalized for having no training data for 20 of Fleurs languages. 3.5. Translation We study the translation capabilities of Whisper models by measuring their performance on the X→en subset of CoVoST2 (Wang et al., 2020b). We compare with Maestro, mSLAM, and XLS-R, the highest-performing prior work. We achieve a new state of the art of 29.1 BLEU zero-shot without using any of the CoVoST2 training data. We attribute this to the 68,000 hours of X→en translation data for these languages in our pre-training dataset which, although noisy, is vastly larger than the 861 hours of training data for X→en translation in CoVoST2. Since Whisper evaluation is zero-shot, it does particularly well on the lowest resource grouping of CoVoST2, improving over mSLAM by 6.7 BLEU. Conversely, the best Whisper model does not actually improve over Maestro and mSLAM on average for the highest resource languages. For an additional analysis on an even wider set of languages, we also re-purpose Fleurs, which is a speech recognition dataset, as a translation dataset. Since the same sentences are transcribed for every language we use the English transcripts as reference translations. In Figure 4 we visualize the correlation between the amount of translation training data per language and the resulting zero-shot BLEU score on Fleurs. While there is a clear trend of improvement with increasing training data, the squared correlation coefﬁcient is much lower than the 0.83 observed for speech recognition white noise pub noise 100 WER on LibriSpeech test-clean (%) 50 20 10 5 2 1 40 30 20 10 0 -10 signal-to-noise ratio (dB) 40 30 20 10 0 -10 signal-to-noise ratio (dB) unispeech-sat-base-100h-libri-ft wav2vec2-base-100h wav2vec2-base-960h wav2vec2-large-960h wav2vec2-large-robust-ft-libri-960h wav2vec2-large-960h-lv60-self asr-crdnn-rnnlm-librispeech asr-transformer-transformerlm-librispeech hubert-large-ls960-ft hubert-xlarge-ls960-ft s2t-medium-librispeech-asr s2t-large-librispeech-asr stt_en_conformer_ctc_large stt_en_conformer_transducer_xlarge Whisper Figure 5. WER on LibriSpeech test-clean as a function of SNR under additive white noise (left) and pub noise (right). The accuracy of LibriSpeech-trained models degrade faster than the best Whisper model (⋆). NVIDIA STT models (•) perform best under low noise but are outperformed by Whisper under high noise (SNR < 10 dB). The second-best model under low noise (▼) is ﬁne-tuned on LibriSpeech only and degrades even more quickly. and only 0.24. We suspect this is partly caused by the noisier training data due to errors in audio language identiﬁcation. As an example, Welsh (CY) is an outlier with much worse than expected performance at only 13 BLEU despite supposedly having 9,000 hours of translation data. This large amount of Welsh translation data is surprising, ranking 4th overall for translation data and ahead of some of the most spoken languages in the world like French, Spanish, and Russian. Inspection shows the majority of supposedly Welsh translation data is actually English audio with English captions where the English audio was mis-classiﬁed as Welsh by the language identiﬁcation system, resulting in it being included as translation training data rather transcription data according to our dataset creation rules. 3.6. Language Identiﬁcation To evaluate language identiﬁcation, we use the Fleurs dataset (Conneau et al., 2022). The zero-shot performance of Whisper is not competitive with prior supervised work here and underperforms the supervised SOTA by 13.6%. However, Whisper is heavily disadvantaged for language identiﬁcation on Fleurs, since the Whisper dataset contains no training data for 20 of the 102 languages in Fleurs, upperbounding accuracy at 80.4%. On the 82 overlapping languages the best Whisper model achieves 80.3% accuracy. 3.7. Robustness to Additive Noise We tested the noise robustness of Whisper models and 14 LibriSpeech-trained models by measuring the WER when either white noise or pub noise from the Audio Degradation Toolbox (Mauch & Ewert, 2013) was added to the audio. The pub noise represents a more natural noisy environment with ambient noise and indistinct chatter typical in a crowded restaurant or a pub. Among the 14 models, twelve are pre-trained and/or ﬁne-tuned on LibriSpeech, and the other two are NVIDIA STT models trained on a mixture dataset similar to prior work like SpeechStew that includes LibriSpeech. The level of additive noise corresponding to a given signal-to-noise ratio (SNR) is calculated based on the signal power of individual examples. Figure 5 shows how the ASR performance degrades as the additive noise becomes more intensive. There are many models that outperform our zero-shot performance under low noise (40 dB SNR), which is unsurprising given those models are trained primarily on LibriSpeech, but all models quickly degrade as the noise becomes more intensive, performing worse than the Whisper model under additive pub noise of SNR below 10 dB. This showcases Whisper’s robustness to noise, especially under more natural distribution shifts like the pub noise. 3.8. Long-form Transcription Whisper models are trained on 30-second audio chunks and cannot consume longer audio inputs at once. This is not a 40 35 30 Word Error Rate (%) 25 20 15 10 5 TED-LIUM3 Meanwhile Kincaid46 Rev16 Earnings-21 Earnings-22 CORAAL 0 Whisper Company A Company B Company C Company D NVIDIA STT (CTC large) Figure 6. Whisper is competitive with state-of-the-art commercial and open-source ASR systems in long-form transcription. The distribution of word error rates from six ASR systems on seven long-form datasets are compared, where the input lengths range from a few minutes to a few hours. The boxes show the quartiles of per-example WERs, and the per-dataset aggregate WERs are annotated on each box. Our model outperforms the best open source model (NVIDIA STT) on all datasets, and in most cases, commercial ASR systems as well. problem with most academic datasets comprised of short utterances but presents challenges in real-world applications which often require transcribing minutes- or hours-long audio. We developed a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model. We observed that it is crucial to have beam search and temperature scheduling based on the repetitiveness and the log probability of the model predictions in order to reliably transcribe long audio. The full procedure is described in Section 4.5. We evaluate the long-form transcription performance on seven datasets consisting of speech recordings of various lengths and recording conditions, to cover as diverse a data distribution as possible. These include a long-form adaptation of TED-LIUM3 (Hernandez et al., 2018) concatenated so that each example is a full-length TED talk, a collection of jargon-laden segments taken from The Late Show with Stephen Colbert (Meanwhile), sets of videos/podcasts that has been used as ASR benchmarks in online blogs (Rev16 and Kincaid46), recordings of earnings calls (Del Rio et al., 2021), and the full-length interviews from the Corpus of Regional African American Language (CORAAL) (Gunter et al., 2021). Full details about the long-form datasets can be found in Appendix A. We compare the performance with open-source models as well as 4 commercial ASR services. The results are summarized in Figure 6, showing the distribution of word error rates from Whisper and the 4 commercial ASR services, as well as the NVIDIA STT Conformer-CTC Large model from the NeMo toolkit (Kuchaiev et al., 2019) which performed the best among the open-source models. All commercial ASR services are queried using their default English transcription settings as of September 1st, 2022, and for the NVIDIA STT model we used their buffered inference implementation in the FrameBatchASR class to enable long-form transcription. The results show that Whisper performs better than the compared models on most datasets, especially on the Meanwhile dataset which is heavy with uncommon words. Additionally, we note the possibility that some of the commercial ASR systems have been trained on some of these publicly available datasets, and therefore these results may not be accurately reﬂecting the relative robustness of the systems. 3.9. Comparison with Human Performance Because of ambiguous or indistinct speech as well as labeling errors, there are different levels of irreducible error in each dataset, and with WER metrics from ASR systems alone it is difﬁcult to make sense of how much room for improvement exists in each dataset. To quantify how close Whisper’s performance is to the human performance, we selected 25 recordings from the Kincaid46 dataset and used 5 services to obtain transcripts produced by professional transcribers, among which one provides computer-assisted transcription and the other four are entirely human-transcribed. The audio selection covers various recording conditions such as scripted and unscripted broadcast, telephone and VoIP calls, and meetings. Figure 7 shows the distribution of per-example WERs and aggregate WER across the 25 recordings, where the computer-assisted service has the lowest aggregate WER that is 1.15% point better than Whisper’s, and the pure-human performance is only a fraction of a percentage point better than Whisper’s. These results indicate that Whisper’s English ASR performance is not perfect but very close to human-level accuracy.\n\n4. Analysis and Ablations\n4.1. Model Scaling A large amount of the promise in weakly supervised training approaches is their potential to use datasets much larger than those in traditional supervised learning. However, this comes with the cost of using data that is possibly much noisier and lower quality than gold-standard supervision. A concern with this approach is that although it may look promising to begin with, the performance of models trained on this kind of data may saturate at the inherent quality level of the dataset, which could be far below human level. A related concern is that as capacity and compute spent training on the dataset increases, models may learn to exploit the 30 25 Word Error Rate (%) 20 15 10 5 0 Whisper A B C D E F G H I ASR human transcription computer-assisted Figure 7. Whisper’s performance is close to that of professional human transcribers. This plot shows the WER distributions of 25 recordings from the Kincaid46 dataset transcribed by Whisper, the same 4 commercial ASR systems from Figure 6 (A-D), one computer-assisted human transcription service (E) and 4 human transcription services (F-I). The box plot is superimposed with dots indicating the WERs on individual recordings, and the aggregate WER over the 25 recordings are annotated on each box. idiosyncrasies of the dataset, and their ability to generalize robustly to out-of-distribution data could even degrade. To check whether this is the case, we study the zero-shot generalization of Whisper models as a function of the model size. Our analysis is summarized in Figure 8. With the exception of English speech recognition, performance continues to increase with model size across multilingual speech recognition, speech translation, and language identiﬁcation. The diminishing returns for English speech recognition could be due to saturation effects from approaching humanlevel performance as analysis in Section 3.9 suggests. 4.2. Dataset Scaling At 680,000 hours of labeled audio, the Whisper dataset is one of the largest ever created in supervised speech recognition. Exactly how important is the raw dataset size to Whisper’s performance? To study this, we trained a series of medium-sized models on subsampled versions of the dataset which are 0.5%, 1%, 2%, 4%, and 8% of the full dataset size and compared their performance with the same medium-sized model trained on the whole dataset. Early stopping based on the validation loss was used to select model checkpoints for each dataset size. Evaluation was performed on an exponential moving average estimate of the parameters (Polyak & Juditsky, 1992) using a smoothing rate of 0.9999 to help reduce the effect of the learning rate not fully decaying to zero for the models trained on the subsampled datasets due to early stopping. Performance on English and multilingual speech recognition and X→en translation is reported in Table 6. English Speech Recognition Multilingual Speech Recognition (Fleurs) 20.0 100 50 Average Large V2 Average Large V2 17.5 80 40 15.0 WER on 67 languages (%) WER on 12 datasets (%) BLEU on 21 languages 12.5 60 30 10.0 40 20 7.5 5.0 20 10 2.5 0.0 0 Model parameters Model parameters Figure 8. Zero-shot Whisper performance scales reliably across tasks and languages with increasing model size. Lightly shaded lines represent individual datasets or languages, showing that performance is more varied than the smooth trends in aggregate performance. Large V2 distinguished with a dashed orange line since it includes several changes that are not present for the smaller models in this analysis. Dataset English Multilingual X→En size WER (↓) WER (↓) BLEU (↑) 3405 30.5 92.4 0.2 6811 19.6 72.7 1.7 13621 14.4 56.6 7.9 27243 12.3 45.0 13.9 54486 10.9 36.4 19.2 681070 9.9 29.2 24.8 Table 6. Performance improves with increasing dataset size. English speech recognition performance refers to an average over 12 datasets while the Multilingual speech recognition reports performance on the overlapping subset of languages in Fleurs and X→en translation reports average BLEU on CoVoST2. Dataset size reported in hours. All increases in the dataset size result in improved performance on all tasks, although we see signiﬁcant variability in improvement rates across tasks and sizes. Performance improves rapidly on English speech recognition from 3,000 to 13,000 hours and then slows down noticeably between 13,000 and 54,000 hours. Using the full dataset, which corresponds to another 12.5× increase in size results in only a further 1 point drop in WER. This mirrors the diminishing returns observed with model size scaling for English speech recognition and could similarly be explained by saturation effects when approaching human-level performance. Improvements in WER follow a power-law trend for multilingual speech recognition till 54,000 hours and then deviate from this trend, improving only a further 7 points when increasing to the full dataset size. For X→en translation, performance is practically zero when training on 7,000 hours of audio or less, and then follows a roughly log-linear improvement trend till 54,000 hours before also showing X->En Translation (CoVoST2) Language Identification (Fleurs) 80 Average Large V2 Average Large V2 70 Accuracy on 102 languages (%) 60 50 40 0 30 Model parameters Model parameters diminishing returns when further scaling to the full dataset size. The general trend across tasks of diminishing returns when moving from 54,000 hours to our full dataset size of 680,000 hours could suggest that the current best Whisper models are under-trained relative to dataset size and performance could be further improved by a combination of longer training and larger models. It could also suggest that we are nearing the end of performance improvements from dataset size scaling for speech recognition. Further analysis is needed to characterize “scaling laws” for speech recognition in order to decided between these explanations. 4.3. Multitask and Multilingual Transfer A potential concern with jointly training a single model on many tasks and languages is the possibility of negative transfer where interference between the learning of several tasks results in performance worse than would be achieved by training on only a single task or language. To investigate whether this is occurring, we compared the performance of models trained on just English speech recognition with our standard multitask and multilingual training setup and measured their average performance across our suite of zeroshot English speech recognition benchmarks. We adjust for the amount of FLOPs spent training on the task of English speech recognition as only 65% of compute is spent on this task in a joint training setup; analysis would otherwise be confounded by under-training on the task when compared to a same-sized English-only model. Our results visualized in Figure 9 show that for small models trained with moderate amounts of compute, there is indeed negative transfer between tasks and languages: joint models underperform English-only models trained for the same amount of compute. However, multitask and multilingual 20 English Only Multilingual and Multitask Average WER on 11 english speech recognition datasets 18 16 14 12 10 8 10e+19 10e+20 10e+21 10e+22 FLOPs training on english speech recognition Figure 9. Multitask and multilingual transfer improves with scale. For small models, performance on English speech recognition degrades when trained jointly in a multitask and multilingual setup. However, multilingual and multitask models beneﬁt more from scale and eventually outperform models trained on English data only. 95% bootstrap estimate conﬁdence intervals are shown. models scale better and for our largest experiments outperform their English-only counterparts demonstrating positive transfer from other tasks. For our largest experiments, joint models also slightly outperform English-only models even when not adjusting for compute spent per task. 4.4. Text Normalization Since we developed our text normalization jointly with Whisper to discount innocuous word errors, there is a risk that our normalizer is overﬁtted to ﬁxing Whisper’s peculiarities rather than addressing general variation in transcription. To check this, we compared the performance of Whisper using our normalizer versus an independently developed one from the FairSpeech project (Koenecke et al., 2020). In Figure 10, we visualize the differences. On most datasets the two normalizers perform similarly, without signiﬁcant differences in WER reduction between Whisper and compared open-source models, while on some datasets, namely WSJ, CallHome, and Switchboard, our normalizer reduces the WER of Whisper models’ signiﬁcantly more. The differences in reduction can be traced down to different formats used by the ground truth and how the two normalizers are penalizing them. For example, in CallHome and Switchboard, our standardizer did not penalize differences in common English contractions such as “you’re” versus “you are”, and in WSJ, our normalizer standardized the written and spo- CORAAL Open-source models Whisper models CommonVoice9.en AMI-SDM1 CommonVoice5.1 Fleurs.en_us AMI-IHM Artie LibriSpeech TED-LIUM3 VoxPopuli.en WSJ CallHome Switchboard 0 10 20 30 40 50 Relative WER reduction compared to FairSpeech's normalizer (%) Figure 10. On most datasets, our text normalizer has similar effect on reducing WERs between Whisper models and other open-source models, compared to FairSpeech’s normalizer. For each dataset, the boxplot shows the distribution of relative WER reduction across different models in our eval suite, showing that using our text normalizer generally results in lower WERs than FairSpeech’s. On a few datasets our normalizer reduces WER signiﬁcantly and more so for Whisper models, such as CallHome and Switchboard which have many contractions in the ground truth and WSJ which contains many numerical expressions. ken forms of numerical and monetary expressions, such as “sixty-eight million dollars” versus “$68 million”. 4.5. Strategies for Reliable Long-form Transcription Transcribing long-form audio using Whisper relies on accurate prediction of the timestamp tokens to determine the amount to shift the model’s 30-second audio context window by, and inaccurate transcription in one window may negatively impact transcription in the subsequent windows. We have developed a set of heuristics that help avoid failure cases of long-form transcription, which is applied in the results reported in sections 3.8 and 3.9. First, we use beam search with 5 beams using the log probability as the score function, to reduce repetition looping which happens more frequently in greedy decoding. We start with temperature 0, i.e. always selecting the tokens with the highest probability, and increase the temperature by 0.2 up to 1.0 when either the average log probability over the generated tokens is lower than −1 or the generated text has a gzip compression rate higher than 2.4. Providing the transcribed text from the preceding window as previous-text conditioning when the applied temperature is below 0.5 further improves the performance. We found that the probability of the <|nospeech|> token alone is not sufﬁcient TED-LIUM3 Earnings-22 Earnings-21 Meanwhile Kincaid46 CORAAL Average Rev16 Greedy decoding only 3.95 5.16 9.69 11.7 10.7 14.0 22.0 11.0 + Beam search 4.16 5.71 9.42 11.5 10.2 13.4 20.0 10.6 + Temperature fallback 4.16 5.71 9.42 11.5 10.2 13.4 20.0 10.6 + Voice activity detection 3.56 4.61 9.45 11.4 10.1 13.2 19.4 10.2 + Previous text conditioning 3.42 6.16 8.72 11.0 9.63 13.3 18.1 10.0 + Initial timestamp constraint 3.51 5.26 8.41 11.5 9.73 12.6 19.1 10.0 Table 7. Long-form transcription performance improves incrementally as additional decoding heuristics are employed. Details on each intervention are described in Section 4.5. to distinguish a segment with no speech, but combining the no-speech probability threshold of 0.6 and the average log-probability threshold of −1 makes the voice activity detection of Whisper more reliable. Finally, to avoid a failure mode where the model ignores the ﬁrst few words in the input, we constrained the initial timestamp token to be between 0.0 and 1.0 second. Table 7 shows that adding each of the interventions above incrementally reduces the WER overall, but not evenly across the dataset. These heuristics serve as a workaround for the noisy predictions of the model, and more research would be needed to further improve the reliability of long-form decoding.\n\n5. Related Work\nScaling Speech Recognition A consistent theme across speech recognition research has been documenting the beneﬁts of scaling compute, models, and datasets. Early work applying deep learning to speech recognition found improved performance with model depth and size and leveraged GPU acceleration to make training these larger models tractable (Mohamed et al., 2009). Further research demonstrated that the beneﬁt of deep learning approaches to speech recognition increased with dataset size, improving from being only competitive with prior GMM-HMM systems when using just 3 hours of TIMIT training data for phone recognition to achieving a 30% word error rate reduction when trained on the 2,000 hour Switchboard dataset (Seide et al., 2011). Liao et al. (2013) is an early example of leveraging weakly supervised learning to increase the size of a deep learning based speech recognition dataset by over 1,000 hours. These trends continued with Deep Speech 2 (Amodei et al., 2015) being a notable system developing high-throughput distributed training across 16 GPUs and scaling to 12,000 hours of training data while demonstrating continuing improvements at that scale. By leveraging semi-supervised pre-training, Narayanan et al. (2018) were able to grow dataset size much further and study training on 162,000 hours of labeled audio. More recent work has explored billion-parameter models (Zhang et al., 2020) and using up to 1,000,000 hours of training data (Zhang et al., 2021). Multitask Learning Multitask learning (Caruana, 1997) has been studied for a long time. In speech recognition, multi-lingual models have been explored for well over a decade (Schultz & Kirchhoff, 2006). An inspirational and foundational work in NLP exploring multi-task learning with a single model is Collobert et al. (2011). Multitask learning in the sequence-to-sequence framework (Sutskever et al., 2014) using multiple encoders and decoders was investigated in Luong et al. (2015). The use of language codes with a shared encoder/decoder architecture was ﬁrst demonstrated for machine translation by Johnson et al. (2017), removing the need for separate encoders and decoders. This approach was simpliﬁed further into the “text-to-text” framework of McCann et al. (2018) and popularized by its success with large transformer language models in the work of Radford et al. (2019) and Raffel et al. (2020). Toshniwal et al. (2018) demonstrated jointly training a modern deep learning speech recognition system on several languages with a single model, and Pratap et al. (2020a) scaled this line of work signiﬁcantly to 50 languages with a billion-parameter model. MUTE (Wang et al., 2020c) and mSLAM (Bapna et al., 2022) studied joint training over both text and speech language tasks, demonstrating transfer between them. Robustness The question of how effectively models transfer and how robust they are to distribution shift and other types of perturbations has long been studied and is actively being researched across many ﬁelds of machine learning. Torralba & Efros (2011) highlighted the lack of generalization of machine learning models between datasets over a decade ago. Many other works have shown and continually reiterated how despite high performance on IID test sets, machine learning models can still make many mistakes when evaluated in even slightly different settings (Lake et al., 2017; Jia & Liang, 2017; Alcorn et al., 2019; Barbu et al., 2019; Recht et al., 2019). More recently, Taori et al. (2020) studied the robustness of image classiﬁcation models, and Miller et al. (2020) investigated this for question-answering models. A key ﬁnding has been that multi-domain training increases robustness and generalization as discussed in the Introduction. This ﬁnding has been replicated across many ﬁelds in addition to speech recognition including NLP (Hendrycks et al., 2020) and computer vision (Radford et al., 2021).\n\n7. Conclusion\nWhisper suggests that scaling weakly supervised pretraining has been underappreciated so far in speech recognition research. We achieve our results without the need for the self-supervision and self-training techniques that have been a mainstay of recent large-scale speech recognition work and demonstrate how simply training on a large and diverse supervised dataset and focusing on zero-shot transfer can signiﬁcantly improve the robustness of a speech recognition system.\n\nACKNOWLEDGMENTS\nWe’d like to thank the millions of people who were involved in creating the data used by Whisper. We’d also like to thank Nick Ryder, Will Zhuk, and Andrew Carr for the conversation on the waterfall hike that inspired this project. We are also grateful to the Acceleration and Supercomputing teams at OpenAI for their critical work on software and hardware infrastructure this project used. We’d also like to thank Pamela Mishkin for advising the project from a policy perspective. Finally, we are grateful to the developers of the many software packages used throughout this project including, but not limited, to Numpy (Harris et al., 2020), SciPy (Virtanen et al., 2020), ftfy (Speer, 2019), PyTorch (Paszke et al., 2019), pandas (pandas development team, 2020), and scikit-learn (Pedregosa et al., 2011).\n\nReferences\nAlcorn, M. A., Li, Q., Gong, Z., Wang, C., Mai, L., Ku, W.- S., and Nguyen, A. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4845–4854, 2019. Amodei, D., Anubhai, R., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Chen, J., Chrzanowski, M., Coates, A., Diamos, G., et al. Deep speech 2: end-to-end speech recognition in english and mandarin. arxiv. arXiv preprint arXiv:1512.02595, 2015. and Weber, G. Common voice: A massively-multilingual speech corpus. arXiv preprint arXiv:1912.06670, 2019. Babu, A., Wang, C., Tjandra, A., Lakhotia, K., Xu, Q., Goyal, N., Singh, K., von Platen, P., Saraf, Y., Pino, J., et al. XLS-R: Self-supervised cross-lingual speech representation learning at scale. arXiv preprint arXiv:2111.09296, 2021. Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. arXiv preprint arXiv:2006.11477, 2020. Baevski, A., Hsu, W.-N., Conneau, A., and Auli, M. Unsupervised speech recognition. Advances in Neural Information Processing Systems, 34:27826–27839, 2021. Bapna, A., Cherry, C., Zhang, Y., Jia, Y., Johnson, M., Cheng, Y., Khanuja, S., Riesa, J., and Conneau, A. mslam: Massively multilingual joint pre-training for speech and text. arXiv preprint arXiv:2202.01374, 2022. Barbu, A., Mayo, D., Alverio, J., Luo, W., Wang, C., Gutfreund, D., Tenenbaum, J., and Katz, B. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. Caruana, R. Multitask learning. Machine learning, 28(1): 41–75, 1997. Chan, W., Park, D., Lee, C., Zhang, Y., Le, Q., and Norouzi,\nM. SpeechStew: Simply mix all available speech recognition data to train one large neural network. arXiv preprint\narXiv:2104.02133, 2021. Chen, G., Chai, S., Wang, G., Du, J., Zhang, W.-Q., Weng, C., Su, D., Povey, D., Trmal, J., Zhang, J., et al. Gigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. arXiv preprint arXiv:2106.06909, 2021. Chen, S., Wu, Y., Wang, C., Chen, Z., Chen, Z., Liu, S., Wu, J., Qian, Y., Wei, F., Li, J., et al. Unispeech-sat: Universal speech representation learning with speaker aware pre-training. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6152–6156. IEEE, 2022a. Chen, T., Xu, B., Zhang, C., and Guestrin, C. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. Chen, Z., Zhang, Y., Rosenberg, A., Ramabhadran, B., Moreno, P., Bapna, A., and Zen, H. Maestro: Matched speech text representations through modality matching. arXiv preprint arXiv:2204.03409, 2022b. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., and Kuksa, P. Natural language processing (almost) from scratch. Journal of machine learning research, 12(ARTICLE):2493–2537, 2011. Conneau, A., Ma, M., Khanuja, S., Zhang, Y., Axelrod, V., Dalmia, S., Riesa, J., Rivera, C., and Bapna, A. Fleurs: Few-shot learning evaluation of universal representations of speech. arXiv preprint arXiv:2205.12446, 2022. Del Rio, M., Delworth, N., Westerman, R., Huang, M., Bhandari, N., Palakapilly, J., McNamara, Q., Dong, J., Zelasko, P., and Jett´e, M. Earnings-21: a practical benchmark for asr in the wild. arXiv preprint arXiv:2104.11348, 2021. Galvez, D., Diamos, G., Torres, J. M. C., Achorn, K., Gopi, A., Kanter, D., Lam, M., Mazumder, M., and Reddi, V. J. The people’s speech: A large-scale diverse english speech recognition dataset for commercial usage. arXiv preprint arXiv:2111.09344, 2021. Geirhos, R., Jacobsen, J.-H., Michaelis, C., Zemel, R., Brendel, W., Bethge, M., and Wichmann, F. A. Shortcut learning in deep neural networks. Nature Machine Intelligence, 2(11):665–673, 2020. Ghorbani, B., Firat, O., Freitag, M., Bapna, A., Krikun, M., Garcia, X., Chelba, C., and Cherry, C. Scaling laws for neural machine translation. arXiv preprint arXiv:2109.07740, 2021. Griewank, A. and Walther, A. Algorithm 799: revolve: an implementation of checkpointing for the reverse or adjoint mode of computational differentiation. ACM Transactions on Mathematical Software (TOMS), 26(1):19–45, 2000. Gunter, K., Vaughn, C., and Kendall, T. Contextualizing/s/retraction: Sibilant variation and change in washington dc african american language. Language Variation and Change, 33(3):331–357, 2021. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., Fern´andez del R´ıo, J., Wiebe, M., Peterson, P., G´erard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585:357–362, 2020. doi: 10.1038/ s41586-020-2649-2. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan, R., and Song, D. Pretrained transformers improve out-ofdistribution robustness. arXiv preprint arXiv:2004.06100, 2020. Hernandez, F., Nguyen, V., Ghannay, S., Tomashenko, N. A., and Est`eve, Y. Ted-lium 3: twice as much data and corpus repartition for experiments on speaker adaptation. In SPECOM, 2018. Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. Hubert: Selfsupervised speech representation learning by masked prediction of hidden units. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 29:3451–3460, 2021a. Hsu, W.-N., Sriram, A., Baevski, A., Likhomanenko, T., Xu, Q., Pratap, V., Kahn, J., Lee, A., Collobert, R., Synnaeve, G., et al. Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training. arXiv preprint arXiv:2104.01027, 2021b. Huang, G., Sun, Y., Liu, Z., Sedra, D., and Weinberger,\nK. Q. Deep networks with stochastic depth. In European\nconference on computer vision, pp. 646–661. Springer, 2016. Jia, R. and Liang, P. Adversarial examples for evaluating reading comprehension systems. arXiv preprint arXiv:1707.07328, 2017. Johnson, M., Schuster, M., Le, Q. V., Krikun, M., Wu, Y., Chen, Z., Thorat, N., Vi´egas, F., Wattenberg, M., Corrado, G., et al. Google’s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339– 351, 2017. Kendall, T. and Farrington, C. The corpus of regional african american language. Version 2021.07. Eugene, OR: The Online Resources for African American Language Project. http://oraal.uoregon.edu/coraal,\n2021. Accessed: 2022-09-01.\nKoenecke, A., Nam, A., Lake, E., Nudell, J., Quartey, M., Mengesha, Z., Toups, C., Rickford, J. R., Jurafsky, D., and Goel, S. Racial disparities in automated speech recognition. Proceedings of the National Academy of Sciences, 117(14):7684–7689, 2020. Kolesnikov, A., Beyer, L., Zhai, X., Puigcerver, J., Yung, J., Gelly, S., and Houlsby, N. Big transfer (bit): General visual representation learning. In European conference on computer vision, pp. 491–507. Springer, 2020. Kuchaiev, O., Li, J., Nguyen, H., Hrinchuk, O., Leary, R., Ginsburg, B., Kriman, S., Beliaev, S., Lavrukhin, V., Cook, J., et al. Nemo: a toolkit for building ai applications using neural modules. arXiv preprint arXiv:1909.09577, 2019. Lake, B. M., Ullman, T. D., Tenenbaum, J. B., and Gershman, S. J. Building machines that learn and think like people. Behavioral and brain sciences, 40, 2017. Liao, H., McDermott, E., and Senior, A. Large scale deep neural network acoustic modeling with semi-supervised training data for youtube video transcription. In 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, pp. 368–373. IEEE, 2013. Likhomanenko, T., Xu, Q., Pratap, V., Tomasello, P., Kahn, J., Avidov, G., Collobert, R., and Synnaeve, G. Rethinking evaluation in asr: Are our models robust enough? arXiv preprint arXiv:2010.11745, 2020. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Luong, M.-T., Le, Q. V., Sutskever, I., Vinyals, O., and Kaiser, L. Multi-task sequence to sequence learning. arXiv preprint arXiv:1511.06114, 2015. Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe, A., and Van Der Maaten, L. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pp. 181–196, 2018. Mauch, M. and Ewert, S. The audio degradation toolbox and its application to robustness evaluation. In Proceedings of the 14th International Society for Music Information Retrieval Conference (ISMIR 2013), Curitiba, Brazil, 2013. accepted. McCann, B., Keskar, N. S., Xiong, C., and Socher, R. The natural language decathlon: Multitask learning as question answering. arXiv preprint arXiv:1806.08730, 2018. Meyer, J., Rauchenstein, L., Eisenberg, J. D., and Howell,\nN. Artie bias corpus: An open dataset for detecting demographic bias in speech applications. In Proceedings of\nthe 12th Language Resources and Evaluation Conference, pp. 6462–6468, Marseille, France, May 2020. European Language Resources Association. ISBN 979-10-9554634-4. URL https://aclanthology.org/2020. lrec-1.796. Mohamed, A.-r., Dahl, G., Hinton, G., et al. Deep belief networks for phone recognition. In Nips workshop on deep learning for speech recognition and related applications, volume 1, pp. 39, 2009. Narayanan, A., Misra, A., Sim, K. C., Pundak, G., Tripathi, A., Elfeky, M., Haghani, P., Strohman, T., and Bacchiani, M. Toward domain-invariant speech recognition via large scale training. In 2018 IEEE Spoken Language Technology Workshop (SLT), pp. 441–447. IEEE, 2018. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5206–5210. IEEE, 2015. pandas development team, T. pandas-dev/pandas: Pan- 5281/zenodo.3509134. Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. SpecAugment: A simple data augmentation method for automatic speech recognition. arXiv preprint arXiv:1904.08779, 2019. Pascanu, R., Mikolov, T., and Bengio, Y. On the difﬁculty of training recurrent neural networks. In International conference on machine learning, pp. 1310–1318. PMLR, 2013. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. Pytorch: An imperative style, high-performance deep learning library. In Advances in Neural Information Processing Systems 32, pp. 8024– 8035, 2019. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., and Duchesnay, E. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12:2825–2830, 2011. Polyak, B. T. and Juditsky, A. B. Acceleration of stochastic approximation by averaging. SIAM journal on control and optimization, 30(4):838–855, 1992. Pratap, V., Sriram, A., Tomasello, P., Hannun, A. Y., Liptchinsky, V., Synnaeve, G., and Collobert, R. Massively multilingual asr: 50 languages, 1 model, 1 billion parameters. ArXiv, abs/2007.03001, 2020a. Press, O. and Wolf, L. Using the output embedding to improve language models. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pp. 157–163, Valencia, Spain, April 2017. Association for Computational Linguistics. URL https: //aclanthology.org/E17-2025. Provilkov, I., Emelianenko, D., and Voita, E. Bpe-dropout: Simple and effective subword regularization. arXiv preprint arXiv:1910.13267, 2019. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. 2019. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., Liu, P. J., et al. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer. J. Mach. Learn. Res., 21(140):1–67, 2020. Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cornell, S., Lugosch, L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.-W., Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W., Na, H., Gao, Y., Mori, R. D., and Bengio, Y. SpeechBrain: A general-purpose speech toolkit, 2021. arXiv:2106.04624. Recht, B., Roelofs, R., Schmidt, L., and Shankar, V. Do ImageNet classiﬁers generalize to ImageNet? In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pp. 5389–5400. PMLR, 09–15 Jun 2019. URL https://proceedings.mlr.press/v97/ recht19a.html. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115(3): 211–252, 2015. Schultz, T. and Kirchhoff, K. Multilingual speech processing. Elsevier, 2006. Seide, F., Li, G., Chen, X., and Yu, D. Feature engineering in context-dependent deep neural networks for conversational speech transcription. In 2011 IEEE Workshop on Automatic Speech Recognition & Understanding, pp. 24–29. IEEE, 2011. Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909, 2015. 10.5281/zenodo.2591652. Version 5.5. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27, 2014. Taori, R., Dave, A., Shankar, V., Carlini, N., Recht, B., and Schmidt, L. Measuring robustness to natural distribution shifts in image classiﬁcation. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin,\nH. (eds.), Advances in Neural Information Processing\nSystems, volume 33, pp. 18583–18599. Curran Associates, Inc., 2020. URL https://proceedings. neurips.cc/paper/2020/file/ d8330f857a17c53d217014ee776bfd50-Paper. pdf. Torralba, A. and Efros, A. A. Unbiased look at dataset bias. CVPR 2011, pp. 1521–1528, 2011. Toshniwal, S., Sainath, T. N., Weiss, R. J., Li, B., Moreno,\nP. J., Weinstein, E., and Rao, K. Multilingual speech\nrecognition with a single end-to-end model. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4904–4908, 2018. Valk, J. and Alum¨ae, T. Voxlingua107: a dataset for spoken language recognition. In 2021 IEEE Spoken Language Technology Workshop (SLT), pp. 652–658. IEEE, 2021. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in neural information processing systems, pp. 5998–6008, 2017. Virtanen, P., Gommers, R., Oliphant, T. E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Peterson, P., Weckesser, W., Bright, J., van der Walt, S. J., Brett, M., Wilson, J., Millman, K. J., Mayorov, N., Nelson, A. R. J., Jones, E., Kern, R., Larson, E., Carey, C. J., Polat, ˙I., Feng, Y., Moore, E. W., VanderPlas, J., Laxalde, D., Perktold, J., Cimrman, R., Henriksen, I., Quintero, E. A., Harris, C. R., Archibald, A. M., Ribeiro, A. H., Pedregosa, F., van Mulbregt, P., and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc Computing in Python. Nature Methods, 17:261–272, 2020. doi: 10.1038/s41592-019-0686-2. Wang, C., Wu, A., and Pino, J. Covost 2 and massively multilingual speech-to-text translation. arXiv preprint arXiv:2007.10310, 2020b. Wang, C., Riviere, M., Lee, A., Wu, A., Talnikar, C., Haziza, D., Williamson, M., Pino, J., and Dupoux, E. Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. arXiv preprint arXiv:2101.00390, 2021. Wang, P., Sainath, T. N., and Weiss, R. J. Multitask training with text data for end-to-end speech recognition. arXiv preprint arXiv:2010.14318, 2020c. Watanabe, S., Mandel, M., Barker, J., Vincent, E., Arora, A., Chang, X., Khudanpur, S., Manohar, V., Povey, D., Raj, D., et al. Chime-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings. arXiv preprint arXiv:2004.09249, 2020. Xu, Q., Baevski, A., Likhomanenko, T., Tomasello, P., Conneau, A., Collobert, R., Synnaeve, G., and Auli, M. Selftraining and pre-training are complementary for speech recognition. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3030–3034. IEEE, 2021. Zhang, Y., Qin, J., Park, D. S., Han, W., Chiu, C.-C., Pang, R., Le, Q. V., and Wu, Y. Pushing the limits of semisupervised learning for automatic speech recognition. arXiv preprint arXiv:2010.10504, 2020. Zhang, Y., Park, D. S., Han, W., Qin, J., Gulati, A., Shor, J., Jansen, A., Xu, Y., Huang, Y., Wang, S., et al. BigSSL: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition. arXiv preprint arXiv:2109.13226, 2021.\n\nA. Evaluation Datasets.\nA.1. Short-form English-only datasets • LibriSpeech (Panayotov et al., 2015): We used the test-clean and test-other splits from the LibriSpeech ASR corpus. • TED-LIUM 3 (Hernandez et al., 2018): We used the test split of TED-LIUM Release 3, using the segmented manual transcripts included in the release. • Common Voice 5.1 (Ardila et al., 2019): We downloaded the English subset of Common Voice Corpus 5.1 from the ofﬁcial website. • Artie bias corpus (Meyer et al., 2020): We used the Artie bias corpus. This is a subset of the Common Voice dataset. • CallHome and Switchboard: We used the two corpora from LDC2002S09 and LDC2002T43. • WSJ: We used LDC93S6B and LDC94S13B and followed the s5 recipe to preprocess the dataset. • CORAAL: We used the 231 interviews from CORAAL (Kendall & Farrington, 2021) and used the preprocessing script from the FairSpeech project. • CHiME-6: For CHiME-6 (Watanabe et al., 2020), we downloaded the CHiME-5 dataset and followed the stage 0 of the s5 track1 recipe to create the CHiME-6 dataset which ﬁxes synchronization. We then used the binaural recordings (* P??.wav) and the corresponding transcripts. • AMI-IHM and AMI-SDM1: We preprocessed the AMI Corpus by following the stage 0 ad 2 of the s5b recipe. A.2. Long-form English-only datasets • TED-LIUM 3 (Hernandez et al., 2018): We used the 11 full-length TED talks from the test split of TED-LIUM Release 3, slicing the source audio ﬁles between the beginning of the ﬁrst labeled segment and the end of the last labeled segment of each talk, and we used the concatenated text as the label. • Meanwhile: This dataset consists of 64 segments from The Late Show with Stephen Colbert. The YouTube video ID and the corresponding start and end timestamps are available as part of the code release. The labels are collected from the closed-caption data for each video and corrected with manual inspection. • Rev16: We use a subset of 16 ﬁles from the 30 podcast episodes in Rev.AI’s Podcast Transcription Benchmark, after ﬁnding that there are multiple cases where a signiﬁcant portion of the audio and the labels did not match, mostly on the parts introducing the sponsors. We selected 16 episodes that do not have this error, whose “ﬁle number”s are: 3 4 9 10 11 14 17 18 20 21 23 24 26 27 29 32 • Kincaid46: This dataset consists of 46 audio ﬁles and the corresponding transcripts compiled in the blog article ¡Which automatic transcription service is the most accurate - 2018¿ by Jason Kincaid. We used the 46 audio ﬁles and reference transcripts from the Airtable widget in the article. For the human transcription benchmark in the paper, we use a subset of 25 examples from this data, whose “Ref ID”s are: 2 4 5 8 9 10 12 13 14 16 19 21 23 25 26 28 29 30 33 35 36 37 42 43 45 • Earnings-21 (Del Rio et al., 2021) and Earnings-22: We used the ﬁles available in the speech-datasets repository, as of their 202206 version. A.3. Multilingual datasets • Multilingual LibriSpeech (Pratap et al., 2020b): We used the test splits from each language in the Multilingual LibriSpeech (MLS) corpus. • Fleurs (Conneau et al., 2022): We collected audio ﬁles and transcripts using the implementation available as HuggingFace datasets. To use as a translation dataset, we matched the numerical utterance IDs to ﬁnd the corresponding transcript in English. • VoxPopuli (Wang et al., 2021): We used the get asr data.py script from the ofﬁcial repository to collect the ASR data in 16 languages, including English. • Common Voice 9 (Ardila et al., 2019): We downloaded the Common Voice Corpus 9 from the ofﬁcial website. • CoVOST 2 (Wang et al., 2020b): We collected the X into English data collected using the ofﬁcial repository.\nB. Compared Models\nFor comparison, we use the following models from HuggingFace, downloaded as of September 2022 using version 4.21.0 of the transformers library: • facebook/wav2vec2-large-960h-lv60-self (Xu et al., 2021) • facebook/wav2vec2-large-robust-ft-libri-960h (Hsu et al., 2021b) • facebook/wav2vec2-base-100h (Baevski et al., 2020) • facebook/wav2vec2-base-960h (Baevski et al., 2020) • facebook/wav2vec2-large-960h (Baevski et al., 2020) • facebook/hubert-large-ls960-ft (Hsu et al., 2021a) • facebook/hubert-xlarge-ls960-ft (Hsu et al., 2021a) • facebook/s2t-medium-librispeech-asr (Wang et al., 2020a) • facebook/s2t-large-librispeech-asr (Wang et al., 2020a) • microsoft/unispeech-sat-base-100h-libri-ft (Chen et al., 2022a) • nvidia/stt en conformer ctc large (Kuchaiev et al., 2019) • nvidia/stt en conformer transducer xlarge (Kuchaiev et al., 2019) • speechbrain/asr-crdnn-rnnlm-librispeech (Ravanelli et al., 2021) • speechbrain/asr-transformer-transformerlm-librispeech (Ravanelli et al., 2021) We note that all of the models above are entirely or partly trained on LibriSpeech.\nC. Text Standardization\nSince Whisper may output any UTF-8 string rather than a restricted set of graphemes, the rules for text standardization need to be more intricate and comprehensive than those deﬁned on e.g. ASCII characters. We perform the following steps to normalize English texts in different styles into a standardized form, which is a best-effort attempt to penalize only when a word error is caused by actually mistranscribing a word, and not by formatting or punctuation differences.\n1. Remove any phrases between matching brackets ([, ]).\n2. Remove any phrases between matching parentheses ((, )).\n3. Remove any of the following words: hmm, mm, mhm, mmm, uh, um\n4. Remove whitespace characters that comes before an apostrophe ’\n5. Convert standard or informal contracted forms of English into the original form.\n6. Remove commas (,) between digits\n7. Remove periods (.) not followed by numbers\n8. Remove symbols as well as diacritics from the text, where symbols are the characters with the Unicode category\nstarting with M, S, or P, except period, percent, and currency symbols that may be detected in the next step.\n9. Detect any numeric expressions of numbers and currencies and replace with a form using Arabic numbers, e.g. “Ten\nthousand dollars” →“$10000”.\n10. Convert British spellings into American spellings.\n11. Remove remaining symbols that are not part of any numeric expressions.\n12. Replace any successive whitespace characters with a space.\nA different, language-speciﬁc set of transformations would be needed to equivalently normalize non-English text, but due to our lack of linguistic knowledge to build such normalizers for all languages, we resort to the following basic standardization for non-English text:\n1. Remove any phrases between matching brackets ([, ]).\n2. Remove any phrases between matching parentheses ((, )).\n3. Replace any markers, symbols, and punctuation characters with a space, i.e. when the Unicode category of each\ncharacter in the NFKC-normalized string starts with M, S, or P.\n4. make the text lowercase.\n5. replace any successive whitespace characters with a space.\nAdditionally, we put a space between every letter for the languages that do not use spaces to separate words, namely Chinese, Japanese, Thai, Lao, and Burmese, effectively measuring the character error rate instead. We note that the above is an imperfect solution, and it will sometimes produce unintended and unexpected outputs. We do not claim that the text format resulting from the above is more “correct” in any measure. Rather, the procedures above are designed to better distinguish between innocuous differences in wording and genuine mistranscriptions. Python code for the standardization procedures above is available as part of our code and model release to facilitate future iterations and improvements on text standardization.\nD. Raw Performance Tables\nD.1. English Transcription D.1.1. GREEDY DECODING LibriSpeech.test-clean LibriSpeech.test-other TED-LIUM3 CallHome WSJ Model Whisper tiny.en 5.6 14.6 6.0 5.0 24.1 17.8 26.3 20.0 23.9 41.3 23.7 50.3 11.7 11.6 Whisper tiny 7.6 16.9 7.0 6.7 30.0 22.8 29.6 23.9 31.0 49.6 27.6 58.1 12.7 13.7 Whisper base.en 4.2 10.2 4.9 4.6 20.9 15.2 19.0 13.4 22.6 36.4 20.5 46.7 10.0 7.6 Whisper base 5.0 12.4 5.5 5.1 23.0 16.8 21.6 16.9 26.0 40.2 22.0 49.9 10.0 10.1 Whisper small.en 3.1 7.4 4.0 3.3 18.2 15.7 13.1 9.7 20.2 27.6 17.5 38.0 8.1 6.0 Whisper small 3.4 7.6 4.3 4.0 17.5 14.5 13.5 10.3 18.1 29.3 19.0 39.6 8.3 6.6 Whisper medium.en 3.1 6.3 4.1 3.3 16.2 14.1 10.6 7.6 17.5 25.3 16.4 37.2 7.4 5.0 Whisper medium 2.9 5.9 3.8 2.9 16.4 14.0 10.3 7.2 16.6 26.4 16.6 36.0 7.4 5.4 Whisper large 2.7 5.6 4.0 3.1 15.8 13.1 9.5 6.7 19.4 25.6 16.4 36.9 7.3 4.6 Whisper large-v2 2.7 5.2 4.0 3.9 17.6 13.8 9.0 6.2 16.2 25.5 16.9 36.4 7.3 4.4 wav2vec2-base-100h 6.0 13.4 17.8 13.9 46.9 40.2 47.4 40.8 47.0 79.9 48.1 81.2 28.9 23.1 wav2vec2-base-960h 3.3 8.5 12.8 8.9 40.6 32.9 36.4 30.9 39.9 68.5 40.2 71.9 21.4 17.4 wav2vec2-large-960h-lv60-self 1.8 3.8 7.4 4.4 29.1 22.2 19.9 15.8 29.2 56.3 30.8 57.0 13.0 10.2 wav2vec2-large-960h 2.7 6.2 10.5 7.7 34.8 28.3 29.9 24.5 35.6 65.8 37.0 67.6 17.9 14.6 wav2vec2-large-robust-ft-libri-960h 2.6 5.3 9.2 6.1 23.4 19.8 20.3 16.2 29.4 58.1 31.7 61.6 15.1 11.8 asr-crdnn-rnnlm-librispeech 3.0 9.7 17.7 10.7 59.7 56.1 43.7 33.3 83.8 81.0 57.2 85.8 30.6 32.4 asr-transformer-transformerlm-librispeech 2.1 5.4 11.9 7.4 38.9 33.0 30.6 23.5 44.9 79.5 44.5 75.4 17.8 17.0 hubert-large-ls960-ft 2.0 4.1 8.4 5.4 29.6 22.8 20.8 16.0 32.0 60.0 33.7 59.1 14.4 10.9 hubert-xlarge-ls960-ft 1.9 3.5 8.3 5.4 29.3 22.2 19.8 14.8 31.5 58.5 33.3 58.9 14.2 10.5 s2t-large-librispeech-asr 3.3 8.1 14.9 9.4 54.5 40.3 38.1 30.7 50.2 79.2 53.4 79.5 21.6 18.0 s2t-medium-librispeech-asr 3.6 8.2 15.7 9.7 58.1 42.4 39.3 31.3 52.6 79.8 60.3 85.3 22.9 19.7 stt en conformer ctc large 2.1 4.2 4.4 2.1 11.3 8.2 7.4 4.0 13.5 30.5 15.9 39.9 6.7 8.2 stt en conformer transducer xlarge 1.5 2.8 4.3 1.2 12.0 7.4 4.3 1.5 19.9 36.8 20.5 48.6 6.0 6.3 unispeech-sat-base-100h-libri-ft 5.7 13.8 17.7 13.6 46.5 40.0 45.3 38.6 44.7 74.8 47.8 77.7 29.8 22.4 Table 8. English transcription WER (%) with greedy decoding D.1.2. BEAM SEARCH WITH TEMPERATURE FALLBACK LibriSpeech.test-clean LibriSpeech.test-other TED-LIUM3 Switchboard CallHome WSJ Model Whisper tiny.en 5.4 12.8 5.4 4.6 21.4 16.0 23.5 18.4 21.4 42.0 22.7 54.2 10.9 10.0 Whisper tiny 6.7 15.0 6.3 5.9 24.8 18.3 26.1 20.8 25.1 48.0 25.6 57.3 11.6 12.4 Whisper base.en 4.1 9.6 4.6 4.0 18.3 14.2 17.5 13.2 18.5 35.2 21.1 49.0 9.3 7.1 Whisper base 4.9 11.0 5.0 4.4 20.5 15.6 19.4 15.3 20.5 40.0 21.5 50.0 9.5 8.9 Whisper small.en 3.2 6.7 4.3 3.0 17.2 13.4 12.6 9.2 17.5 29.5 17.9 42.5 8.1 5.3 Whisper small 3.3 7.2 4.3 3.9 17.1 13.3 12.8 9.3 16.4 30.9 19.2 43.5 8.2 6.1 Whisper medium.en 3.0 5.7 4.3 2.8 14.7 12.4 10.3 7.4 15.3 27.0 17.1 39.4 7.8 4.5 Whisper medium 2.7 5.6 4.0 2.7 15.3 13.2 9.7 6.7 14.9 27.6 17.6 43.0 7.6 4.4 Whisper large 2.8 5.7 4.3 3.5 16.2 14.2 8.9 6.4 15.1 25.2 17.6 37.1 7.2 4.5 Whisper large-v2 2.5 4.9 3.7 2.6 16.4 13.6 8.2 5.7 14.2 24.9 17.4 39.9 7.0 4.2 CommonVoice5.1 VoxPopuli.en Switchboard Fleurs.en us AMI-SDM1 AMI-IHM CORAAL CHiME6 Artie CommonVoice5.1 VoxPopuli.en Fleurs.en us AMI-SDM1 AMI-IHM CORAAL CHiME6 Artie D.2. Multilingual Transcription D.2.1. MULTILINGUAL LIBRISPEECH English French Dutch Model Whisper tiny 39.4 15.7 36.8 24.9 41.7 34.2 31.3 19.2 Whisper base 28.4 11.7 26.6 17.7 31.1 22.8 21.9 12.8 Whisper small 17.2 8.3 16.2 10.5 21.4 11.2 13.0 7.8 Whisper medium 11.7 6.8 8.9 7.4 16.0 6.5 9.0 5.3 Whisper large 10.2 6.3 8.9 6.6 14.3 6.6 9.2 5.4 Whisper large-v2 9.3 6.2 7.3 5.5 13.8 5.0 6.8 4.2 Table 10. WER (%) on MLS D.2.2. COMMON VOICE 9 Bulgarian Catalan Bengali Arabic Czech Welsh Model Whisper tiny 90.9 79.3 104.1 51.0 79.7 101.8 77.2 34.5 61.9 28.8 30.3 102.1 120.3 Whisper base 84.4 68.1 103.7 39.9 63.1 93.8 57.5 24.5 51.5 21.9 19.6 88.1 99.0 Whisper small 66.4 44.8 118.6 23.8 34.1 65.4 32.1 13.0 31.7 14.5 10.3 67.2 71.9 Whisper medium 60.3 26.7 124.7 16.4 18.8 43.6 19.3 8.5 20.0 11.2 6.9 45.6 49.9 Whisper large 56.0 24.1 106.0 15.3 17.1 40.3 18.3 7.7 18.3 10.1 6.4 41.4 44.8 Whisper large-v2 53.8 19.9 103.4 14.1 13.5 34.2 14.4 6.4 16.0 9.4 5.6 35.1 39.4 Indonesian Hungarian Finnish French Italian Hindi Model Whisper tiny 68.5 49.7 108.3 87.0 49.6 44.5 36.1 103.5 87.8 102.7 123.0 43.6 45.3 Whisper base 52.9 37.3 106.5 71.9 36.1 30.5 24.2 91.3 78.0 122.9 137.0 29.5 32.8 Whisper small 30.5 22.7 43.6 44.4 18.4 16.0 14.0 72.8 54.6 104.8 225.8 14.2 16.9 Whisper medium 18.8 16.0 31.5 26.9 11.6 9.4 10.5 49.4 37.2 137.8 113.4 8.0 10.1 Whisper large 17.0 14.7 25.0 23.5 10.6 8.1 9.4 43.9 34.8 107.1 117.4 7.1 9.0 Whisper large-v2 14.4 13.9 21.9 19.7 8.5 7.1 9.1 35.2 25.5 103.2 128.4 5.8 7.6 Portuguese Romanian Slovenian Russian Serbian Slovak Model Whisper tiny 35.2 68.2 40.6 104.0 82.0 106.1 58.2 105.7 55.9 53.6 74.7 69.3 52.4 Whisper base 23.7 55.9 28.8 87.2 70.3 103.0 42.4 49.5 32.1 38.6 58.6 51.6 44.9 Whisper small 12.5 33.2 15.0 60.4 45.5 101.3 22.1 28.7 18.1 23.7 39.1 33.3 29.4 Whisper medium 8.1 21.5 9.3 42.0 29.8 85.6 13.7 19.6 10.5 17.7 29.9 24.4 23.2 Whisper large 7.1 19.8 8.2 37.9 25.1 87.4 12.4 17.6 8.8 16.6 28.1 19.9 29.1 Whisper large-v2 6.3 15.8 7.1 31.9 20.6 70.5 10.6 16.1 8.0 14.5 24.2 18.2 26.8 Table 11. WER (%) on CommonVoice9 D.2.3. VOXPOPULI en accented Estonian German Spanish Finnish English French Czech Model Whisper tiny 73.5 27.4 11.6 18.8 19.7 99.2 54.1 32.9 72.4 74.5 40.5 93.1 41.9 31.4 65.9 78.7 81.9 Whisper base 54.7 20.6 9.5 17.5 14.4 83.0 39.7 24.9 53.6 52.6 30.8 82.1 29.4 22.1 49.3 63.7 70.5 Whisper small 28.8 14.8 8.2 19.2 11.1 59.2 24.9 15.7 33.7 31.3 22.9 60.1 18.8 13.3 28.6 37.3 50.8 Whisper medium 18.4 12.4 7.6 19.1 9.6 38.2 16.6 12.2 23.9 19.3 19.7 39.3 14.9 10.1 18.4 23.0 36.3 Whisper large 15.9 11.9 7.2 20.8 8.8 33.3 15.5 11.0 19.0 16.8 18.4 35.0 14.0 9.0 17.0 19.1 31.3 Whisper large-v2 12.6 11.2 7.0 18.6 8.2 28.7 12.4 11.4 16.1 13.8 19.0 33.2 12.9 7.8 14.4 15.4 27.9 Portuguese German Spanish Italian Polish Estonian German Spanish English Persian Danish Greek Malayalam Lithuanian Mongolian Japanese Latvian Polish Dutch Vietnamese Swedish Chinese Turkish Tamil Urdu Thai Lithuanian Hungarian Romanian Slovenian Croatian Slovak Italian Polish Dutch D.2.4. FLEURS Azerbaijani Belarusian Assamese Afrikaans Amharic Arabic Model Whisper tiny 91.2 122.9 63.4 102.0 93.1 94.0 81.0 101.6 82.1 42.8 40.5 82.8 101.3 82.0 Whisper base 81.5 196.8 48.8 102.0 76.4 91.3 65.1 100.6 66.7 29.0 34.1 66.0 85.3 57.6 Whisper small 61.1 120.2 30.6 108.0 49.1 75.1 37.3 104.4 39.4 16.2 20.8 37.6 59.3 32.8 Whisper medium 44.9 229.3 20.4 102.3 33.1 60.4 21.4 100.6 23.9 9.6 12.1 21.3 40.8 19.5 Whisper large 42.6 129.3 18.1 105.6 28.7 56.6 18.4 104.9 20.7 8.0 19.6 17.4 36.6 16.8 Whisper large-v2 36.7 140.3 16.0 106.2 23.4 45.4 14.6 104.1 15.7 7.3 14.7 13.3 33.0 13.8 Estonian German Spanish English Persian Greek Model Whisper tiny 27.8 67.4 12.4 15.9 94.8 101.8 59.5 65.6 41.4 54.8 101.2 100.2 71.6 102.3 Whisper base 17.9 53.5 8.9 9.9 77.9 86.1 43.1 45.8 28.5 47.4 101.4 98.6 61.7 101.1 Whisper small 10.2 30.8 6.1 5.6 51.3 55.8 24.0 27.7 15.0 30.2 106.4 90.1 44.4 38.4 Whisper medium 6.5 19.0 4.4 3.6 29.8 41.0 13.9 19.1 8.7 21.2 104.8 106.6 33.1 26.8 Whisper large 5.5 18.7 4.5 3.5 25.5 36.1 12.2 15.8 7.7 19.0 103.9 87.0 30.2 26.9 Whisper large-v2 4.5 12.5 4.2 3.0 21.9 32.9 9.7 13.8 8.3 15.4 102.7 88.9 27.1 21.5 Indonesian Hungarian Armenian Icelandic Croatian Italian Model Whisper tiny 79.0 83.8 118.6 51.7 113.3 29.8 37.0 107.3 123.0 165.2 100.6 100.7 36.1 99.1 Whisper base 59.1 65.0 126.3 33.1 95.5 17.9 22.8 89.5 114.7 109.2 101.6 107.2 27.8 100.7 Whisper small 33.4 38.9 86.6 16.3 72.6 9.8 12.0 88.6 118.3 70.3 104.4 100.4 19.6 100.1 Whisper medium 19.3 24.3 60.1 10.2 49.9 5.2 7.1 67.9 117.3 48.8 98.9 77.7 16.4 90.0 Whisper large 16.7 21.0 53.7 8.5 43.0 4.2 6.4 87.0 100.5 43.8 96.0 69.8 15.2 86.5 Whisper large-v2 13.4 17.0 44.6 7.1 38.2 4.0 5.3 nan 105.0 37.7 99.7 37.0 14.3 88.0 Macedonian Lithuanian Latvian Lingala Maori Lao Model Whisper tiny 105.4 115.1 98.5 91.6 94.5 73.3 101.5 113.7 100.3 51.2 100.8 124.8 62.0 101.8 Whisper base 96.7 105.1 87.3 79.8 77.5 59.9 107.4 125.7 100.3 35.1 97.6 122.6 44.0 102.4 Whisper small 91.3 102.2 65.6 53.2 59.5 36.9 100.9 144.2 60.2 18.9 92.2 110.1 24.2 69.5 Whisper medium 83.2 101.4 41.1 32.0 77.8 22.0 101.1 103.7 63.2 12.2 83.2 123.0 12.9 54.4 Whisper large 76.8 101.6 35.2 28.3 45.7 20.6 101.4 106.2 43.7 10.2 80.5 124.5 11.4 52.2 Whisper large-v2 75.6 101.5 28.1 23.1 38.5 16.5 100.7 110.5 38.3 8.7 76.6 115.7 9.5 47.1 Portuguese Occitan Punjabi Pashto Polish Dutch Model Whisper tiny 49.0 95.9 102.6 45.6 105.6 20.1 74.7 31.1 105.8 77.2 87.2 128.1 105.6 83.7 Whisper base 33.0 82.9 101.5 30.8 99.0 13.0 56.0 20.5 103.9 60.6 74.6 126.0 109.6 64.3 Whisper small 16.4 87.3 103.6 14.7 92.9 7.3 29.8 11.4 131.7 33.3 49.3 140.0 105.3 42.2 Whisper medium 9.9 79.5 102.0 8.0 119.4 5.0 20.0 7.2 147.0 17.3 31.9 143.9 104.0 44.9 Whisper large 8.3 75.9 102.8 7.2 92.7 4.8 15.4 6.4 177.9 15.7 27.8 130.0 103.5 29.2 Whisper large-v2 6.7 75.3 102.4 5.4 93.7 4.3 14.4 5.6 156.5 11.7 23.1 121.0 102.9 33.9 Swedish Swahili Telugu Tamil Tajik Model Whisper tiny 52.7 100.9 99.9 105.1 101.7 58.8 42.5 51.2 65.2 105.2 60.0 106.4 Whisper base 37.4 92.5 58.7 105.2 109.3 38.2 27.5 37.7 52.0 114.0 40.5 101.8 Whisper small 20.8 73.7 35.2 98.2 84.3 21.9 15.9 19.3 37.3 107.7 21.2 116.4 Whisper medium 11.2 52.8 23.1 82.8 74.0 15.4 10.4 11.6 28.2 109.6 12.7 105.1 Whisper large 10.5 47.9 20.6 100.6 74.5 13.2 9.4 10.3 25.0 93.3 10.7 111.7 Whisper large-v2 8.5 39.3 17.5 99.0 85.8 11.5 8.4 8.6 22.6 90.2 10.3 94.8 Bulgarian Bosnian Chinese Catalan Bengali Danish Czech Welsh Galician Gujarati Hebrew Tagalog Finnish French Hausa Hindi Luxembourgish Georgian Kannada Javanese Japanese Kazakh Korean Khmer Malayalam Norwegian Mongolian Myanmar Marathi Maltese Nepali Malay Romanian Slovenian Russian Serbian Somali Slovak Sindhi Shona Vietnamese Ukrainian Turkish Yoruba Uzbek Urdu Thai D.3. Speech Translation D.3.1. FLEURS Azerbaijani Belarusian Assamese Afrikaans Amharic Arabic Model Whisper tiny 1.6 0.1 0.1 0.4 0.1 0.8 0.4 0.4 0.4 5.2 0.6 0.6 0.6 0.7 Whisper base 4.4 0.3 1.0 0.4 0.8 3.3 2.7 0.7 4.1 13.1 1.9 2.7 0.7 5.0 Whisper small 18.1 0.2 10.6 1.2 5.8 7.1 14.8 2.7 16.8 25.1 9.3 14.2 1.3 18.1 Whisper medium 29.5 0.9 19.9 3.5 11.7 9.8 23.9 10.6 26.0 31.9 15.1 23.6 8.4 28.6 Whisper large 31.6 1.1 23.8 3.9 13.1 11.0 26.2 12.0 28.0 33.7 16.8 25.6 11.2 31.6 Whisper large-v2 34.1 1.9 25.5 5.4 13.7 11.7 28.5 13.2 29.7 34.2 18.4 27.8 13.0 32.7 Estonian German Spanish English Persian Greek Model Whisper tiny 5.2 0.1 68.6 7.7 0.1 0.1 0.2 0.8 4.7 4.0 0.7 0.1 0.2 1.0 Whisper base 13.7 0.7 73.3 12.4 0.3 0.2 0.5 2.1 13.1 10.5 1.5 0.0 0.6 3.4 Whisper small 25.9 11.6 77.3 18.2 3.6 5.8 7.3 12.0 23.5 17.5 3.9 0.3 5.4 11.1 Whisper medium 31.4 19.9 79.2 21.4 13.5 15.0 18.5 20.5 28.6 24.7 12.8 0.5 15.9 19.4 Whisper large 34.3 21.7 77.8 22.8 15.9 17.6 20.6 22.7 31.6 26.0 14.8 0.5 19.6 20.7 Whisper large-v2 34.6 23.7 80.2 23.3 18.7 19.6 22.1 24.4 32.2 27.9 16.2 0.4 21.8 22.0 Indonesian Hungarian Armenian Icelandic Croatian Italian Model Whisper tiny 0.6 0.1 0.1 0.3 0.4 5.3 0.2 0.2 0.1 0.1 0.1 0.8 0.5 0.8 Whisper base 3.7 0.2 0.1 2.6 0.4 11.3 1.5 0.2 0.2 0.2 0.1 0.9 3.7 1.7 Whisper small 14.6 4.8 0.7 16.4 1.8 17.8 9.6 1.4 0.2 0.8 0.5 2.3 12.2 5.7 Whisper medium 23.0 15.5 10.4 24.1 6.8 21.6 14.9 5.0 1.3 4.3 3.3 8.5 19.2 13.6 Whisper large 25.4 18.3 13.2 27.2 6.6 23.5 17.0 5.1 2.7 6.3 5.2 9.9 20.0 15.4 Whisper large-v2 27.0 21.2 16.0 29.1 9.1 23.6 18.9 6.2 2.4 5.4 6.1 11.6 21.3 16.8 Macedonian Lithuanian Latvian Lingala Maori Lao Model Whisper tiny 0.1 0.2 0.1 0.2 0.3 1.0 0.8 0.1 0.2 0.3 0.6 0.1 1.4 0.1 Whisper base 0.1 0.3 0.3 0.4 1.0 5.4 1.4 0.1 0.9 2.1 1.4 0.1 8.4 0.3 Whisper small 0.5 2.0 1.9 1.5 3.9 15.3 5.7 0.1 3.8 14.1 4.9 0.0 22.0 2.9 Whisper medium 0.9 8.1 9.6 10.0 8.5 23.5 13.8 0.5 10.9 23.2 11.2 0.2 29.1 12.7 Whisper large 1.2 9.3 12.0 12.5 9.4 26.4 16.5 1.0 13.1 25.5 12.8 0.5 30.5 12.9 Whisper large-v2 1.0 11.0 14.0 14.3 10.2 27.7 16.7 1.0 12.9 27.3 13.5 0.4 31.4 16.1 Portuguese Occitan Punjabi Pashto Polish Dutch Model Whisper tiny 2.7 1.7 0.3 0.8 0.3 12.1 1.0 3.1 0.5 0.7 0.3 0.1 0.0 0.6 Whisper base 7.5 4.2 1.1 5.1 0.4 22.4 4.9 12.1 0.7 4.6 1.3 0.3 0.1 5.4 Whisper small 15.9 9.5 4.4 14.0 0.8 31.2 18.3 19.7 2.0 14.4 6.9 0.6 0.1 19.3 Whisper medium 21.6 15.9 12.8 19.0 2.1 35.9 26.6 24.8 5.5 22.7 14.0 1.4 0.4 27.7 Whisper large 22.8 16.8 14.6 21.4 3.7 37.4 29.1 26.7 5.9 25.1 16.9 1.8 0.5 30.5 Whisper large-v2 24.0 20.2 15.7 22.3 3.4 38.1 31.5 27.8 5.7 26.1 17.0 1.8 0.7 32.5 Swedish Swahili Telugu Tamil Tajik Model Whisper tiny 1.8 0.1 0.2 0.3 0.2 0.2 0.2 1.2 0.4 0.0 0.1 0.2 Whisper base 9.1 0.1 0.4 0.4 0.2 0.7 2.4 6.9 1.5 0.2 0.9 0.5 Whisper small 22.9 0.1 2.1 4.0 4.4 5.8 15.7 18.7 8.8 0.5 8.5 0.5 Whisper medium 32.1 3.1 7.0 10.8 11.4 12.8 22.9 25.8 14.9 3.8 16.6 0.9 Whisper large 33.1 5.3 8.5 10.9 13.0 15.2 25.7 28.0 16.3 5.8 19.5 1.2 Whisper large-v2 35.3 7.2 9.2 12.5 14.5 16.1 26.6 29.4 17.2 6.0 20.4 1.4 Bulgarian Bosnian Chinese Catalan Bengali Danish Czech Welsh Galician Gujarati Hebrew Tagalog Finnish French Hausa Hindi Luxembourgish Georgian Kannada Javanese Japanese Kazakh Korean Khmer Malayalam Norwegian Mongolian Myanmar Marathi Maltese Nepali Malay Romanian Slovenian Russian Serbian Somali Slovak Sindhi Shona Vietnamese Ukrainian Turkish Yoruba Uzbek Urdu Thai D.3.2. COVOST 2 Estonian German Spanish Catalan Arabic Welsh Model Whisper tiny 0.2 4.9 0.4 4.0 10.5 0.2 0.1 6.1 0.3 5.1 0.3 0.1 0.1 Whisper base 1.2 11.0 0.5 11.7 21.3 0.3 0.1 15.4 4.9 13.0 4.9 0.5 0.1 Whisper small 17.7 22.3 1.0 25.3 33.0 2.4 4.9 27.3 27.6 24.0 17.3 1.4 0.2 Whisper medium 30.6 29.2 12.1 33.2 38.4 11.4 15.5 33.6 42.3 29.5 24.6 9.7 0.2 Whisper large 35.5 30.3 16.1 34.3 38.0 13.4 17.5 34.4 45.4 29.1 24.2 10.5 0.3 Whisper large-v2 39.7 31.8 21.5 36.3 40.1 15.0 19.3 36.4 48.1 30.9 26.1 13.9 0.1 Portuguese Russian Dutch Model Whisper tiny 4.3 9.5 5.7 0.4 2.0 0.1 0.2 0.4 Whisper base 12.4 23.2 16.1 1.4 10.5 0.4 2.8 1.4 Whisper small 28.1 40.6 30.9 9.2 29.9 1.7 16.8 6.8 Whisper medium 38.1 48.7 39.4 17.7 39.5 2.9 27.0 14.0 Whisper large 39.3 48.6 41.6 23.9 40.3 3.7 26.7 17.1 Whisper large-v2 41.2 51.6 43.3 21.6 42.9 4.2 28.3 18.0 Table 15. BLEU scores on CoVoST2 D.4. Long-form Transcription TED-LIUM3 Meanwhile Model Whisper tiny.en 5.5 12.8 13.8 15.1 17.0 22.0 30.3 Whisper tiny 6.8 15.5 16.7 17.0 18.7 24.4 33.1 Whisper base.en 4.6 9.4 11.2 13.2 12.5 16.6 25.2 Whisper base 4.8 12.2 12.2 14.5 13.5 18.4 26.9 Whisper small.en 4.6 6.0 9.4 12.0 10.8 14.0 21.9 Whisper small 4.2 6.9 10.1 12.1 11.1 14.3 22.3 Whisper medium.en 3.6 5.2 8.9 11.9 10.2 13.3 20.6 Whisper medium 3.8 5.4 8.6 11.4 10.3 13.2 20.3 Whisper large 3.8 5.3 8.8 11.0 10.3 13.4 20.4 Whisper large-v2 3.5 5.1 8.8 11.3 9.7 12.6 19.6 wav2vec2-base-100h 17.6 27.7 39.3 35.2 45.7 57.1 55.4 wav2vec2-base-960h 12.8 19.7 32.9 29.8 37.3 46.8 49.1 wav2vec2-large-960h-lv60-self 7.2 11.4 21.1 21.3 21.7 28.0 36.7 wav2vec2-large-960h 10.1 16.4 27.4 26.4 30.4 40.1 43.5 wav2vec2-large-robust-ft-libri-960h 8.8 15.2 22.9 23.4 23.0 31.0 36.8 hubert-large-ls960-ft 8.1 12.9 22.4 23.4 23.0 30.6 37.9 hubert-xlarge-ls960-ft 8.1 12.5 22.9 23.2 23.1 31.3 38.1 stt en conformer ctc large 4.0 9.8 13.1 14.5 12.6 17.6 25.1 stt en conformer transducer xlarge 5.3 10.6 17.1 19.8 16.2 19.7 38.9 Table 16. Long-form English transcription WER (%) Indonesian Mongolian Japanese Latvian Persian French Italian Slovenian Swedish Chinese Turkish Tamil Earnings-21 Earnings-22 Kincaid46 CORAAL Rev16\nE. Training Dataset Statistics\nMultilingual Speech Recognition Dataset Components Chinese 23446 German 13344 Spanish 11100 Russian 9761 French 9752 17% Multilingual Speech Recognition Portuguese 8573 Korean 7993 (117,113 hours) Japanese 7054 Turkish 4333 Polish 4278 Italian 2585 Swedish 2119 Dutch 2077 Catalan 1883 Finnish 1066 Indonesian 1014 Arabic 739 Ukrainian 697 Vietnamese 691 18% Translation (125,739 hours) Hebrew 688 Greek 529 Danish 473 Malay 382 Hungarian 379 Romanian 356 Norwegian 266 Thai 226 Czech 192 Tamil 136 Urdu 104 Croatian 91 Slovak 90 Bulgarian 86 Tagalog 75 Welsh 73 Lithuanian 67 Latvian 65 Azerbaijani 47 Estonian 41 Slovenian 41 Serbian 28 Persian 24 Basque 21 Icelandic 16 Macedonian 16 Armenian 13 Kazakh 12 Hindi 12 Bosnian 11 65% English Speech Recognition Galician 8.9 Albanian 5.7 (438,218 hours) Sinhala 5.4 Swahili 5.4 Telugu 4.3 Afrikaans 4.1 Kannada 3.8 Belarusian 2.4 Khmer 1.3 Bengali 1.3 Maltese 1.1 Haitian Creole 1.0 Punjabi 0.8 Marathi 0.6 Nepali 0.6 Georgian 0.6 Malayalam 0.5 Yiddish 0.4 Uzbek 0.3 Gujarati 0.3 Tajik 0.3 Malagasy 0.2 Burmese 0.1 Sundanese 0.1 Lao 0.1 0.1 1 10 100 Hours of audio Figure 11. Training dataset statistics Translation Turkmen 1 Bashkir 1 Malagasy 2 Uzbek 4 Sundanese 7 Hausa 8 Luxembourgish 10 Tatar 14 Tajik 15 Lingala 20 Lao 20 Somali 21 Macedonian 30 Kazakh 31 Amharic 32 Georgian 40 Maltese 41 Sindhi 46 Faroese 46 Occitan 49 Burmese 59 Pashto 63 Latvian 68 Albanian 72 Haitian Creole 74 Estonian 79 Mongolian 79 Icelandic 84 Yiddish 85 Azerbaijani 86 Kannada 90 Lithuanian 99 Armenian 116 Punjabi 117 Belarusian 133 Nepali 133 Assamese 136 Serbian 136 Slovak 144 Basque 168 Tibetan 186 Sanskrit 195 Bulgarian 202 Gujarati 208 Sinhala 211 Bosnian 219 Catalan 236 Croatian 239 Breton 269 Shona 279 Swahili 282 Marathi 288 Norwegian 322 Afrikaans 330 Hawaiian 338 Galician 368 Danish 386 Persian 392 Slovenian 395 Czech 401 Hebrew 418 Yoruba 432 Ukrainian 509 Hungarian 554 Romanian 555 Javanese 622 Khmer 672 Finnish 750 Malayalam 892 Tagalog 894 Greek 968 Telugu 987 Swedish 1055 Indonesian 1174 Maori 1381 Tamil 1484 Latin 1614 Thai 1635 Malay 1691 Vietnamese 1719 Dutch 1767 Norwegian Nynorsk 1889 Bengali 1988 Urdu 1990 Italian 2145 Polish 2200 Turkish 2241 Arabic 2286 Portuguese 3620 German 4309 French 4481 Hindi 5438 Spanish 6693 Russian 7687 Welsh 8263 Japanese 8860 Chinese 11731 Korean 19938 1 10 100 Hours of audio\nF. Hyperparameters\nHyperparameter Value Updates 1048576 Batch Size 256 Warmup Updates 2048 Max grad norm 1.0 Optimizer AdamW β1 0.9 β2 0.98 ϵ 10−6 Weight Decay 0.1 Weight Init Gaussian Fan-In Learning Rate Schedule Linear Decay Speechless audio subsample factor 10× Condition on prior text rate 50% Table 17. Whisper training hyperparameters. Hyperparameter Value Updates 655360 Batch Size 1024 BPE Dropout 0.1 Stochastic Depth 0.1 SpecAugment Policy LibriSpeech Basic Table 18. Hyperparameters changed for Whisper Large V2. Model Max Learning Rate Tiny 1.5 × 10−3 Base 1 × 10−3 Small 5 × 10−4 Medium 2.5 × 10−4 Large 1.75 × 10−4 Large V2 2.0 × 10−4 Table 19. Whisper model learning rates.\n--- FOOTNOTES *Equal contribution 1OpenAI, San Francisco, CA 94110, USA. Correspondence to: Alec Radford <alec@openai.com>, Jong 3After the original release of Whisper, we trained an additional 39M 74M 244M 769M 1550M 1K 10K 100K 1M 1K 10K 100K 38M 73M 244M 768M 1549M 1549M 38M 73M 244M 768M 1549M 1549M 38M 73M 244M 768M 1549M 1549M 38M 73M 244M 768M 1549M 1549M 1K 10K 1K 10K", "persona": "Student", "response": "### 1. Overview\nSource: OpenAI, Robust Speech Recognition via Large-Scale Weak Supervision Source: OpenAI, Robust Speech Recognition via Large-Scale Weak Supervision Source: OpenAI, Robust Speech Recognition via Large-Scale Weak Supervision Exceptional Generalization Source: OpenAI, Robust Speech Recognition via Large-Scale Weak Supervision Source: OpenAI, Robust Speech Recognition via Large-Scale Weak Supervision : A time–frequency representation that maps the frequency axis to the Mel scale, mirroring human auditory perception. Encoder: Transform the spectrogram into a sequence of latent vectors that capture temporal (how pronunciations change over time) and spectral (which frequencies are present) characteristics, along with semantic information. Competitiveness: Accuracy & Practicality Whisper, A Breakthrough in Speech-Recognition AI ENERZAi 6 min read · May 27, 2025 -- Listen Share In this post, we’ll look at one of the models we’re researching for optimization: OpenAI’s Whisper, which has attracted attention for its strong performance and broad usability in speech recognition.\n\n### 2. Background\nWhisper is an automatic speech-recognition (ASR) model released as open source by OpenAI in 2022. It supports multilingual transcription (converting speech to text) and translation (output language fixed to English). Whisper delivers robust accuracy even in noisy environments, covers many languages, and handles unusual accents or pronunciations with ease. Most notably, without any additional fine-tuning it can perform not only transcription but also translation, voice-activity detection, and language identification — making it a truly general-purpose model. Whisper is built on an Encoder–Decoder Transformer architecture. After preprocessing, it converts audio into a latent representation and finally produces the corresponding text. The steps are: Decoder: Generate text token by token, using both the encoder output and the tokens produced so far to predict the next word or character. Multilingual & Multitask Model Whisper was trained on 680,000 hours of speech–text pairs, 17 % of which (117,113 hours) are non-English. These non-English samples span 98 languages, so in total Whisper can handle 99 languages. Language mismatch filter — drop clips where audio and subname languages differ. During training and inference, Whisper’s decoder relies on special context tokens to control language and task: Language token: indicates the language of the audio (ex.\n\n### 3. Methods\nPipeline Preprocessing: Split the input audio into 30-second segments and convert each segment into a log-Mel spectrogram Massive, Diverse Training Data To keep noisy labels from degrading accuracy, OpenAI applied several filters: — drop clips where audio and subname languages differ. Partial transcription removal — drop clips with large duration mismatch. — drop clips with large duration mismatch. Deduplication — remove repeated audio or captions. — remove repeated audio or captions. Click-bait text removal — delete captions like “Please like and subscribe”. Start token: <|startoftranscript|>\n\n### 4. Results\nWhisper is not necessarily the top performer on every language or task, but its special strength lies in handling multiple languages and tasks with a single model. In zero-shot settings — predicting on data never seen during training — it generalizes well across datasets and domains. Two factors make this possible: Moreover, Whisper is notably resilient to noise and uncommon accents. This stems from its use of weakly-supervised learning. Much of the data came from YouTube and podcasts; many transcripts were machine-generated captions rather than hand-checked subnames.\n\n### 5. Takeaways\nSuch weakly-labeled data inevitably contain errors, yet training on them endowed Whisper with robustness to varied languages, accents, and recording conditions. ASR quality is usually measured by Word Error Rate (WER). When Meta’s wav2vec 2.0 and Whisper Large v2 were evaluated on multiple datasets, Whisper showed 55.2 % fewer errors on average. Given that wav2vec 2.0 was fine-tuned on LibriSpeech, Whisper’s similar performance on LibriSpeech plus its clear edge elsewhere highlight the model’s superior generalization. Whisper is available in six sizes; four also have English-only variants.", "paper_id": "25", "story_id": "25-04"}
{"paper": "Learning to Simulate Dynamic Environments with GameGAN Seung Wook Kim1,2,3∗ Yuhao Zhou2† Jonah Philion1,2,3 Antonio Torralba4 Sanja Fidler1,2,3∗ {seungwookk,jphilion,sfidler}@nvidia.com henryzhou@cs.toronto.edu torralba@mit.edu\n\nAbstract\nSimulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN “renders” the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist. We will release the code, enabling human players to play generated games with our GameGAN.\n\n1. Introduction\nBefore deployment to the real world, an artiﬁcial agent needs to undergo extensive testing in challenging simulated environments. Designing good simulators is thus extremely important. This is traditionally done by writing procedural models to generate valid and diverse scenes, and complex behavior trees that specify how each actor in the scene behaves and reacts to actions made by other actors, including the ego agent. However, writing simulators that encompass a large number of diverse scenarios is extremely time consuming and requires highly skilled graphics experts. Learn- Figure 1. If you look at the person on the left picture, you might think she is playing Pacman of Toru Iwatani, but she is not! She is actually playing with a GAN generated version of Pacman. In this paper, we introduce GameGAN that learns to reproduce games by just observing lots of playing rounds. Moreover, our model can disentangle background from dynamic objects, allowing us to create new games by swapping components as shown in the center and right images. ing to simulate by simply observing the dynamics of the real world is the most scaleable way going forward. A plethora of existing work aims at learning behavior models [2, 28, 16, 3]. However, these typically assume a signiﬁcant amount of supervision such as access to agents’ ground-truth trajectories. We aim to learn a simulator by simply watching an agent interact with an environment. To simplify the problem, we frame this as a 2D image generation problem. Given sequences of observed image frames and the corresponding actions the agent took, we wish to emulate image creation as if “rendered” from a real dynamic environment that is reacting to the agent’s actions. We focus on graphics games as a proxy of the real environment. Our goal is to replace the graphics engine at test time, by visually imitating the game using a learned model. This is a challenging problem: different games have different number of components as well as different physical dynamics. Furthermore, many games require long-term consistency in the environment. For example, imagine a game where an agent navigates through a maze. When the agent moves away and later returns to a location, it expects the scene to look consistent with what it has encountered before. In visual SLAM, detecting loop closure (returning to a previous location) is already known to be challenging, let alone generating one. Last but not least, both deterministic and stochastic behaviors typically exist in a game, and modeling the latter is known to be particularly hard. In this paper, we introduce GameGAN, a generative model that learns to imitate a desired game. GameGAN ingests screenplay and keyboard actions during training and aims to predict the next frame by conditioning on the action, i.e. a key pressed by the agent. It learns from rollouts of image and action pairs directly without having access to the underlying game logic or engine. We make several advancements over the recently introduced World Model [11] that aims to solve a similar problem. While [11] employs a straightforward conditional decoder, GameGAN features a carefully designed architecture. In particular, we propose a new memory module that encourages the model to build an internal map of the environment, allowing the agent to return to previously visited locations with high visual consistency. Furthermore, we introduce a purposely designed decoder that learns to disentangle static and dynamic components within the image. This makes the behavior of the model more interpretable, and it further allows us to modify existing games by swapping out different components. We test GameGAN on a modiﬁed version of Pacman and the VizDoom environment [17], and propose several synthetic tasks for both quantitative and qualitative evaluation. We further introduce a come-back-home task to test the long-term consistency of learned simulators. Note that GameGAN supports several applications such as transferring a given game from one operating system to the other, without requiring to re-write code. Our GameGAN will be made available to human players, enabling them to play games with a GAN and modify them in creative ways.\n\nBackground\nForeground Time t=0 t=10 Dynamic Pacman + room Static Pacman + mario Dynamic VizDoom + field Static VizDoom + mario Figure 9. GameGAN on Pacman and VizDoom with swapping background/foreground with random images. Figure 10. Generated mazes by traversing with a pacman agent on GameGAN model. Most mazes are realistic. Right shows a failure case that does not close the loop correctly. generate realistic walls, and as the environment is sufﬁciently small, GameGAN also learns the rough size of the map and correctly draws the rectangular boundary in most cases. One failure case is shown in the bottom right corner of Figure 10, that fails to close the loop. 4.2. Task 1: Training an RL Agent Quantitatively measuring environment quality is challenging as the future is multi-modal, and the ground truth future does not exist. One way of measuring it is through learning a reinforcement learning agent inside the simulated environment and testing the trained agent in the real environment. The simulated environment should be sufﬁciently close to the real one to do well in the real environment. It has to learn the dynamics, rules, and stochasticity present in the real environment. The agent from the better simulator that closely resembles the real environment should score higher. We note that this is closely related to model-based RL. Since GameGAN do not internally have a mechanism for denoting the game score, we train an external classiﬁer. The classiﬁer is given N previous image frames and the current action to produce the output (e.g. Win/Lose). Pacman: For this task, the Pacman agent has to achieve a high score by eating foods (+0.5 reward) and capturing the ﬂag (+1 reward). It is given -1 reward when eaten by a ghost, or the maximum number of steps (40) are used. Note that this is a challenging partially-observed reinforcement learning task where the agent observes 7x7 grids. The agents are trained with A3C [24] with an LSTM component. VizDoom: We use the Covariance Matrix Adaptation Evolution Strategy [12] to train RL agents. Following [11], we use the same setting with corresponding simulators. Pacman VizDoom Random Policy -0.20 ± 0.78 210 ± 108 Action-LSTM[5] -0.09 ± 0.87 280 ± 104 WorldModel[11] 1.24 ± 1.82 1092 ± 556 GameGAN −M 1.99 ± 2.23 724 ± 468 GameGAN 1.13 ± 1.56 765 ± 482 Table 1. Numbers are reported as mean scores ± standard deviation. Higher is better. For Pacman, an agent trained in real environment achieves 3.02 ± 2.64 which can be regarded as the upper bound. VizDoom is considered solved when a score of 750 is achieved. Table 1 shows the results. For all experiments, scores are calculated over 100 test environments, and we report the mean scores along with standard deviation. Agents trained in Action-LSTM simulator performs similar to the agents with random policy, indicating the simulations are far from the real ones. On Pacman, GameGAN-M shows the best performance while GameGAN and WorldModel have similar scores. VizDoom is considered solved when a score of 750 is achieved, and GameGAN solves the game. Note that World Model achieves a higher score, but GameGAN is the ﬁrst work trained with a GAN framework that solves the game. Moreover, GameGAN can be trained end-toend, unlike World Model that employs sequential training for stability. One interesting observation is that GameGAN shows lower performance than GameGAN-M on the Pacman environment. This is due to having additional complexity in training the model where the environments do not need long-term consistency for higher scores. We found that optimizing the GAN objective while training the memory module was harder, and this attributes to RL agents exploiting the imperfections of the environments to ﬁnd a way to cheat. In this case, we found that GameGAN some- Time t=0 t=14 t=28 Forward Backward Forward Backward Forward Backward Forward Backward Figure 11. Come-back-home task rollouts. The forward rows show the path going from the initial position to the goal position. The backward rows show the path coming back to the initial position. Only the full GameGAN can successfully recover the initial position. Figure 12. Box plot for Come-back-home metric. Lower is better. As a reference, a pair of randomly selected frames from the same episode gives a score of 1.17 ± 0.56 times failed to prevent agents from walking through the walls while GameGAN-M was nearly perfect. This led to RL agents discovering a policy that liked to hit the walls, and in the real environment, this often leads to premature death. In the next section, we show how having long-term consistency can help in certain scenarios. 4.3. Task 2: Come-back-home This task evaluates the long-term consistency of simulators in the Pacman-Maze environment. The Pacman starts at a random initial position (xA, yA) with state s. It is given K random actions (a1, ..., aK), ending up in position (xB, yB). Using the reverse actions (ˆaK, ..., ˆa1)(e.g. ak = Down, ˆak = Up) , it comes back to the initial position (xA, yA), resulting in state ˆs. Now, we can measure the distance d between ˆs and s to evaluate long-term consistency (d = 0 for the real environment). As elements other than the wall (e.g. food) could change, we only compare the Action-LSTM World Model GameGAN-M GameGAN walls of ˆs and s. Hence, s is an 84x84 binary image whose pixel is 1 if the pixel is blue. We deﬁne the metric d as d = sum(abs(s −ˆs)) sum(s) + 1 (11) where sum() counts the number of 1s. Therefore, d measures the ratio of the number of pixels changed to the initial number of pixels. Figure 12 shows the results. We again observe occasional large discontinuities in World Model that hurts the performance a lot. When K is small, the differences in performance are relatively small. This is because other models also have short-term consistency realized through RNNs. However, as K becomes larger, GameGAN with memory module steadily outperforms other models, and the gaps become larger, indicating GameGAN can make efﬁcient use of the memory module. Figure 11 shows the rollouts of different models in the Pacman-Maze environment. As it can be seen, models without the memory module do not remember what it has generated before. This shows GameGAN opens up promising directions for not only game simulators, but as a general environment simulator that could mimic the real world.\n\n4. Experiments\nt=0 t=11 Action-LSTM World Model GameGAN-M GameGAN Figure 8. Rollout of models from the same initial screen. Action-LSTM trained with reconstruction loss produces frames without reﬁned details (e.g. foods). World Model has difﬁculty keeping temporal consistency, resulting in occasional signiﬁcant discontinuities. GameGAN can produce consistent simulation. model trained only with reconstruction loss which is in essence similar to [5], 2) World Model [11], 3) GameGANM: our model without the memory module and with the simple rendering engine, and 4) GameGAN: the full model with the memory module and the rendering engine for disentanglement. Experiments are conducted on the following three datasets (Figure 7): Pacman: We use a modiﬁed version of the Pacman game3 in which the Pacman agent observes an egocentric 7x7 grid from the full 14x14 environment. The environment is randomly generated for each episode. This is an ideal environment to test the quality of a simulator since it has both deterministic (e.g., game rules & viewpoint shift) and highly stochastic components (e.g., game layout of foods and walls; game dynamics with moving ghosts). Images in the episodes are 84x84 and the action space is A = {left, right, up, down, stay}. 45K episodes of length greater than or equal to 18 are extracted and 40K are used for training. Training data is generated by using a trained DQN [25] agent that observes the full environment with high entropy to allow exploring diverse action sequences. Each episode consists of a sequence of 7x7 Pacman-centered grids along with actions. Pacman-Maze: This game is similar to Pacman except that it does not have ghosts, and its walls are randomly generated from a maze-generation algorithm, thus are structured better. The same number of data is used as Pacman. Vizdoom: We follow the experiment set-up of Ha and Schmidhuber [11] that uses takecover mode of the VizDoom platform [17]. Training data consists of 10k episodes extracted with random policy. Images in the episodes are 64x64 and the action space is A = {left, right, stay} 4.1. Qualitative Evaluation Time construction loss, produces blurry images as it fails to capture the multi-modal future distribution, and the errors accumulate quickly. World model [11] generates realistic images for VizDoom, but it has trouble simulating the highly stochastic Pacman environment. In particular, it sometimes suffers from large unexpected discontinuities (e.g. t = 0 to t = 1). On the other hand, GameGAN produces temporally consistent and realistic sharp images. GameGAN consists of only a few convolution layers to roughly match the number of parameters of World Model. We also provide a version of GameGAN that can produce higher quality images in the supplementary materials Section A.5. Disentangling static & dynamic elements: Our GameGAN with the memory module is trained to disentangle static elements from dynamic elements. Figure 5 shows how walls from the Pacman environment and the room from the VizDoom environment are separated from dynamic objects such as ghosts and ﬁreballs. With this, we can make interesting environments in which each element is swapped with other objects. Instead of the depressing room of VizDoom, enemies can be placed in the user’s favorite place, or alternatively have Mario run around the room (Figure 9). We can swap the background without having to modify the code of the original games. Our approach treats games as a black box and learns to reproduce the game, allowing us to easily modify it. Disentangled models also open up many promising future directions that are not possible with existing models. One interesting direction would be learning multiple disentangled models and swapping certain components. As the dynamics engine learns the rules of an environment and the rendering engine learns to render images, simply learning a linear transformation from the hidden state of one model to make use of the rendering engine of the other could work. Pacman-Maze generation: GameGAN on the Pacman-Maze produces a partial grid at each time step which can be connected to generate the full maze. It can\n\n2. Related Work\nGenerative Adversarial Networks: In GANs [9], a generator and a discriminator play an adverserial game that encourages the generator to produce realistic outputs. To obtain a desired control over the generated outputs, categorical labels [23], images [15, 21], captions [29], or masks [27] are provided as input to the generator. Works such as [33] synthesize new videos by transferring the style of the source to the target video using the cycle consistency loss [35, 18]. Note that this is a simpler problem than the problem considered in our work, as the dynamic content of the target video is provided and only the visual style needs to be modiﬁed. In this paper, we consider generating the dynamic content itself. We adopt the GAN framework and use the userprovided action as a condition for generating future frames. To the best of our knowledge, ours is the ﬁrst work on using action-conditioned GANs for emulating game simulators. Video Prediction: Our work shares similarity to the task of video prediction which aims at predicting future frames given a sequence of previous frames. Several works [31, 5, 26] train a recurrent encoder to decode future frames. Most approaches are trained with a reconstruction loss, resulting in a deterministic process that generates blurry frames and often does not handle stochastic behaviors well. The er- Memory 𝑚! ℎ! Action 𝑎! Random Noise 𝑧! ℎ! Image 𝑥!\"# Dynamics Engine Rendering Engine Memory 𝑚!$# 𝑥! Image Figure 2. Overview of GameGAN: The dynamics engine takes at, zt, mt−1, and xt as input to update the hidden state at time t. Optionally, it can write to and read from the external memory module M (in the dashed box). Finally, the rendering engine is used to decode the output image xt+1. All modules are neural networks and trained end-to-end. rors typically accumulate over time and result in low quality predictions. Action-LSTM models [5, 26] achieved success in scaling the generated images to higher resolution but do not handle complex stochasticity present in environments like Pacman. Recently, [11, 7] proposed VAE-based frameworks to capture the stochasticity of the task. However, the resulting videos are blurry and the generated frames tend to omit certain details. GAN loss has been previously used in several works [8, 20, 32, 6]. [8] uses an adversarial loss to disentangle pose from content across different videos. In [20], VAE-GAN [19] formulation is used for generating the next frame of the video. Our model differs from these works in that in addition to generating the next frame, GameGAN also learns the intrinsic dynamics of the environment. World Models: In model-based reinforcement learning, one uses interaction with the environment to learn a dynamics model. World Models [11] exploit a learned simulated environment to train an RL agent instead. Recently, World Models have been used to generate Atari games in a concurrent work [1]. The key differences with respect to these models are in the design of the architecture: we introduce a memory module to better capture long-term consistency, and a carefully designed decoder that disentangles static and dynamic components of the game.\n\n3. GameGAN\nWe are interested in training a game simulator that can model both deterministic and stochastic nature of the environment. In particular, we focus on an action-conditioned simulator in the image space where there is an egocentric agent that moves according to the given action at ∼A at time t and generates a new observation xt+1. We assume there is also a stochastic variable zt ∼N(0; I) that corresponds to randomness in the environment. Given the history of images x1:t along with at and zt, GameGAN predicts the next image xt+1. GameGAN is composed of three main t=T t=T+7 World Model GameGAN-M GameGAN Figure 3. Screenshots of a human playing with GameGAN trained on the ofﬁcial version of Pac-Man2. GameGAN learns to produce a visually consistent simulation as well as learning the dynamics of the game well. On the bottom row, the player consumes a capsule, turning the ghosts purple. Note that ghosts approach Pacman before consuming the capsule, and run away after. modules. The dynamics engine (Sec 3.1), which maintains an internal state variable, takes at and zt as inputs and updates the current state. For environments that require longterm consistency, we can optionally use an external memory module (Sec 3.2). Finally, the rendering engine (Sec 3.3) produces the output image given the state of the dynamics engine. It can be implemented as a simple convolutional decoder or can be coupled with the memory module to disentangle static and dynamic elements while ensuring longterm consistency. We use adversarial losses along with a proposed temporal cycle loss (Sec 3.4) to train GameGAN. Unlike some works [11] that use sequential training for stability, GameGAN is trained end-to-end. We provide more details of each module in the supplementary materials. 3.1. Dynamics Engine GameGAN has to learn how various aspects of an environment change with respect to the given user action. For instance, it needs to learn that certain actions are not possible (e.g. walking through a wall), and how other objects behave as a consequence of the action. We call the primary component that learns such transitions the dynamics engine (see illustration in Figure 2). It needs to have access to the past history to produce a consistent simulation. Therefore, we choose to implement it as an action-conditioned LSTM [13], motivated by the design of Chiappa et al. [5]: vt = ht−1 ⊙H(at, zt, mt−1), st = C(xt) (1) it = σ(W ivvt + W isst), ft = σ(W fvvt + W fsst), ot = σ(W ovvt + W osst) (2) ct = ft ⊙ct−1 + it ⊙tanh(W cvvt + W csst) (3) ht = ot ⊙tanh(ct) (4) where ht, at, zt, ct, xt are the hidden state, action, stochastic variable, cell state, image at time step t. mt−1 is the retrieved memory vector in the previous step (if the memory module is used), and it, ft, ot are the input, forget, and output gates. at, zt, mt−1 and ht are fused into vt, and st is Time t=0 t=6 Time Generated Image Memory Location Action Left Down Right Left Up Left Figure 4. Visualizing attended memory location α: Red dots marking the center are placed to aid visualization. Note that we learn the memory shift, so the user action does not always align with how the memory is shifted. In this case, Right shifts α to the left, and Left shifts α to the right. It also learns not to shift when an invalid action is given. the encoding of the image xt. H is a MLP, C is a convolutional encoder, and W are weight matrices. ⊙denotes the hadamard product. The engine maintains the standard state variables for LSTM, ht and ct, which contain information about every aspect of the current environment at time t. It computes the state variables given at, zt, mt−1, and xt. 3.2. Memory Module Suppose we are interested in simulating an environment in which there is an agent navigating through it. This requires long-term consistency in which the simulated scene (e.g. buildings, streets) should not change when the agent comes back to the same location a few moments later. This is a challenging task for typical models such as RNNs because 1) the model needs to remember every scene it generates in the hidden state, and 2) it is non-trivial to design a loss that enforces such long-term consistency. We propose to use an external memory module, motivated by the Neural Turing Machine (NTM) [10]. The memory module has a memory block M ∈ RN×N×D, and the attended location αt ∈RN×N at time t. M contains N × N D-dimensional vectors where N is the spatial width and height of the block. Intuitively, αt is the current location that the egocentric agent is located at. M is initialized with random noise ∼N(0, I) and α0 is initialized with 0s except for the center location (N/2, N/2) that is set to 1. At each time step, the memory module computes: VizDoom Pacman Static Dynamic Static + Dynamic Figure 5. Example showing how static and dynamic elements are disentangled in VizDoom and Pacman games with GameGAN . Static components usually include environmental objects such as walls. Dynamic elements typically are objects that can change as the game progresses such as food and other non-player characters. g = G(ht) ∈R (6) αt = g · Conv2D(αt−1, w) + (1 −g) · αt−1 (7) M = write(αt, E(ht), M) (8) mt = read(αt, M) (9) where K, G and E are small MLPs. w is a learned shift kernel that depends on the current action, and the kernel is used to shift αt−1. In some cases, the shift should not happen (e.g. cannot go forward at a dead end). With the help from ht, we also learn a gating variable g ∈[0, 1] that determines if α should be shifted or not. E is learned to extract information to be written from the hidden state. Finally, write and read operations softly access the memory location speciﬁed by α similar to other neural memory modules [10]. Using this shift-based memory module allows the model to not be bounded by the block M’s size while enforcing local movements. Therefore, we can use any arbitrarily sized block at test time. Figure 4 demonstrates the learned memory shift. Since the model is free to assign actions to different kernels, the learned shift does not always correspond to how humans would do. We can see that Right is assigned as a left-shift, and hence Left is assigned as a right-shift. Using the gating variable g, it also learns not to shift when an invalid action, such as going through a wall, is given. Enforcing long-term consistency in our case refers to remembering generated static elements (e.g. background) and retrieving them appropriately when needed. Accordingly, the beneﬁt of using the memory module would come from storing static information inside it. Along with a novel cycle loss (Section 3.4.2), we introduce inductive bias in the architecture of the rendering engine (Section 3.3) to encourage the disentanglement of static and dynamic elements. 3.3. Rendering Engine The (neural) rendering engine is responsible for rendering the simulated image xt+1 given the state ht. It can be simply implemented with standard transposed convolution layers. However, we also introduce a specialized rendering engine architecture (Figure 6) for ensuring long-term Input/Output Tensor Repeat and Stack Concat/Split for Spatial Softmax SPADE Transposed Conv/MLP Spatial Masking softmax softmax/sigmoid η! 𝑂\" η\" 𝐴! 𝑚# 𝑂! 𝑅! 𝑋\" 𝑣! ! 𝑥#$% 𝐴\" η! ℎ# 𝑂\" 𝑅\" 𝑋! 𝑣\" Rough sketch stage Attribute stage Final rendering stage Figure 6. Rendering engine for disentangling static and dynamic components. See Sec 3.3 for details. consistency by learning to produce disentangled scenes. In Section 4, we compare the beneﬁts of each architecture. The specialized rendering engine takes a list of vectors c = {c1, ..., cK} as input. In this work, we let K = 2, and c = {mt, ht}. Each vector ck corresponds to one type of entity and goes through the following three stages (see Fig 6). First, ck is fed into convolutional networks to produce an attribute map Ak ∈RH1×H1×D1 and object map Ok ∈RH1×H1×1. It is also fed into a linear layer to get the type vector vk ∈RD1 for the k-th component. O for all components are concatenated together and fed through either a sigmoid to ensure 0 ≤Ok[x][y] ≤1 or a spatial softmax function so that PK k=1 Ok[x][y] = 1 for all x, y. The resulting object map is multiplied by the type vector vk in every location and fed into a convnet to produce Rk ∈RH2×H2×D2. This is a rough sketch of the locations where k-th type objects are placed. However, each object could have different attributes such as different style or color. Hence, it goes through the attribute stage where the tensor is transformed by a SPADE layer [27, 14] with the masked attribute map Ok ⊙Ak given as the contextual information. It is further fed through a few transposed convolution layers, and ﬁnally goes through an attention process similar to the rough sketch stage where concatenated components goes through a spatial softmax to get ﬁne masks. The intuition is that after drawing individual objects, it needs to decide the “depth” ordering of the objects to be drawn in order to account for occlusions. Let us denote the ﬁne mask as ηk and the ﬁnal tensor as Xk. After this process, the ﬁnal image is obtained by summing up all components, x = PK k=1 ηk ⊙Xk. Therefore, the architecture of our neural rendering engine encourages it to extract different information from the memory vector and the hidden state with the help of temporal cycle loss (Section 3.4.2). We also introduce a version with more capacity that can produce higher quality images in Section A.5 of the supplementary materials. 3.4. Training GameGAN Adversarial training has been successfully employed for image and video synthesis tasks. GameGAN leverages adversarial training to learn environment dynamics and to produce realistic temporally coherent simulations. For certain cases where long-term consistency is required, we propose temporal cycle loss that disentangles static and dynamic components to learn to remember what it has generated. 3.4.1 Adversarial Losses There are three main components: single image discriminator, action discriminator, and temporal discriminator. Single image discriminator: To ensure each generated frame is realistic, the single image discriminator and GameGAN simulator play an adversarial game. Action-conditioned discriminator: GameGAN has to reﬂect the actions taken by the agent faithfully. We give three pairs to the action-conditioned discriminator: (xt, xt+1, at), (xt, xt+1, ¯at) and (ˆxt, ˆxt+1, at). xt denotes the real image, ˆxt the generated image, and ¯at ∈A a sampled negative action ¯at ̸= at. The job of the discriminator is to judge if two frames are consistent with respect to the action. Therefore, to fool the discriminator, GameGAN has to produce realistic future frame that reﬂects the action. Temporal discriminator: Different entities in an environment can exhibit different behaviors, and also appear or disappear in partially observed states. To simulate a temporally consistent environment, one has to take past information into account when generating the next states. Therefore, we employ a temporal discriminator that is implemented as 3D convolution networks. It takes several frames as input and decides if they are a real or generated sequence. Since conditional GAN architectures [22] are known for learning simpliﬁed distributions ignoring the latent code [34, 30], we add information regularization [4] that maximizes the mutual information I(zt, φ(xt, xt+1)) between the latent code zt and the pair (xt, xt+1). To help the actionconditioned discriminator, we add a term that minimizes the cross entropy loss between at and apred t = ψ(xt+1, xt). Both φ and ψ are MLP that share layers with the actionconditioned discriminator except for the last layer. Lastly, we found adding a small reconstruction loss in image and feature spaces helps stabilize the training (for feature space, we reduce the distance between the generated and real frame’s single image discriminator features). A detailed descriptions are provided in the supplementary material. 3.4.2 Cycle Loss RNN based generators are capable of keeping track of the recent past to generate coherent frames. However, it quickly forgets what happened in the distant past since it is encouraged simply to produce realistic next observation. To ensure long-term consistency of static elements, we leverage Figure 7. Samples from datasets studied in this work. For Pacman and Pacman-Maze, training data consists of partially observed states, shown in the red box. Left: Pacman, Center: Pacman-Maze, Right: VizDoom the memory module and the rendering engine to disentangle static elements from dynamic elements. After running through some time steps T, the memory block M is populated with information from the dynamics engine. Using the memory location history αt, we can retrieve the memory vector ˆmt which could be different from mt if the content at the location αt has been modiﬁed. Now, c = { ˆmt, 0} is passed to the rendering engine to produce X ˆmt where 0 is the zero vector and X ˆmt is the output component corresponding to ˆmt. We use the following loss: T X t ||Xmt −X ˆmt|| (10) Lcycle = As dynamic elements (e.g. moving ghosts in Pacman) do not stay the same across time, the engine is encouraged to put static elements in the memory vector to reduce Lcycle. Therefore, long-term consistency is achieved. To prevent the trivial solution where the model tries to ignore the memory component, we use a regularizer that minimizes the sum of all locations in the ﬁne mask min P ηh from the hidden state vector so that Xmt has to contain content. Another trivial solution is if shift kernels for all actions are learned to never be in the opposite direction of each other. In this case, ˆmt and mt would always be the same because the same memory location will never be revisited. Therefore, we put a constraint that for actions a with a negative counterpart ˆa (e.g. Up and Down), ˆa’s shift kernel K(ˆa) is equal to horizontally and vertically ﬂipped K(a). Since most simulators that require long-term consistency involves navigation tasks, it is trivial to ﬁnd such counterparts. 3.4.3 Training Scheme GameGAN is trained end-to-end. We employ a warm-up phase where real frames are fed into the dynamics engine for the ﬁrst few epochs, and slowly reduce the number of real frames to 1 (the initial frame x0 is always given). We use 18 and 32 frames for training GameGAN on Pacman and Vizddom environments, respectively.\n\n5. Conclusion\nWe propose GameGAN which leverages adversarial training to learn to simulate games. GameGAN is trained by observing screenplay along with user’s actions and does not require access to the game’s logic or engine. GameGAN features a new memory module to ensure long-term consistency and is trained to separate static and dynamic elements. Thorough ablation studies showcase the modeling power of GameGAN . In future works, we aim to extend our model to capture more complex real-world environments.\n\nReferences\n[1] Anonymous. Model based reinforcement learning for atari. In Submitted to International Conference on Learning Representations, 2020. under review. 2 [2] Randall D Beer and John C Gallagher. Evolving dynamical neural networks for adaptive behavior. Adaptive behavior, 1(1):91–122, 1992. 1 [3] Dian Chen, Brady Zhou, Vladlen Koltun, and Philipp Kr¨ahenb¨uhl. Learning by cheating. In Conference on Robot Learning (CoRL), pages 6059–6066, 2019. 1 [4] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, and Pieter Abbeel. Infogan: Interpretable representation learning by information maximizing generative adversarial nets. In Advances in neural information processing systems, pages 2172–2180, 2016. 5 [5] Silvia Chiappa, S´ebastien Racaniere, Daan Wierstra, and Shakir Mohamed. Recurrent environment simulators. arXiv preprint arXiv:1704.02254, 2017. 2, 3, 6, 7 [6] Aidan Clark, Jeff Donahue, and Karen Simonyan. Efﬁcient video generation on complex datasets. arXiv preprint arXiv:1907.06571, 2019. 2 [7] Emily Denton and Rob Fergus. Stochastic video generation with a learned prior. arXiv preprint arXiv:1802.07687, 2018. 2 [8] Emily L Denton and vighnesh Birodkar. Unsupervised learning of disentangled representations from video. In I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 4414–4423. Curran\nAssociates, Inc., 2017. 2 [9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural information processing systems, pages 2672–2680,\n2014. 2\n[10] Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv:1410.5401, 2014. 3, 4 [11] David Ha and J¨urgen Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in Neural Information Processing Systems, pages 2450–2462, 2018. 2, 3, 6, 7 [12] Nikolaus Hansen and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary computation, 9(2):159–195, 2001. 7 [13] Sepp Hochreiter and J¨urgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735–1780, Nov. 1997. 3 [14] Minyoung Huh, Shao-Hua Sun, and Ning Zhang. Feedback adversarial learning: Spatial feedback for improving generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, 2019. 4 [15] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with conditional adversarial networks. CoRR, abs/1611.07004, 2016. 2 [16] Ajay Jain, Sergio Casas, Renjie Liao, Yuwen Xiong, Song Feng, Sean Segal, and Raquel Urtasun. Discrete residual ﬂow for probabilistic pedestrian behavior prediction. arXiv preprint arXiv:1910.08041, 2019. 1 [17] Michał Kempka, Marek Wydmuch, Grzegorz Runc, Jakub Toczek, and Wojciech Ja´skowski. Vizdoom: A doom-based ai research platform for visual reinforcement learning. In Games (CIG), pages 1–8. IEEE, 2016. 2, 6 [18] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1857–1865. JMLR. org, 2017. 2 [19] Anders Boesen Lindbo Larsen, Søren Kaae Sønderby, Hugo Larochelle, and Ole Winther. Autoencoding beyond pixels using a learned similarity metric. arXiv preprint arXiv:1512.09300, 2015. 2 [20] Alex X. Lee, Richard Zhang, Frederik Ebert, Pieter Abbeel, Chelsea Finn, and Sergey Levine. Stochastic adversarial video prediction. CoRR, abs/1804.01523, 2018. 2 [21] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. CoRR, abs/1703.00848, 2017. 2 [22] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 5 [23] Takeru Miyato and Masanori Koyama. cGANs with projection discriminator. In International Conference on Learning Representations, 2018. 2 [24] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, pages 1928–1937, 2016. 7 [25] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. 6 [26] Junhyuk Oh, Xiaoxiao Guo, Honglak Lee, Richard L Lewis, and Satinder Singh. Action-conditional video prediction using deep networks in atari games. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 28, pages 2863–2871. Curran Associates, Inc., 2015. 2 [27] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2337–2346,\n2019. 2, 4\n[28] Chris Paxton, Vasumathi Raman, Gregory D Hager, and Marin Kobilarov. Combining neural networks and tree search for task and motion planning in challenging environments. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6059–6066. IEEE,\n2017. 1\n[29] Scott E. Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. Generative adversarial text to image synthesis. In Proceedings of the 33nd International Conference on Machine Learning, ICML 2016, New York City, NY, USA, June 19-24, 2016, pages 1060– 1069, 2016. 2 [30] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. In Advances in neural information processing systems, pages 2234–2242, 2016. 5 [31] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In International conference on machine learning, pages 843–852, 2015. 2 [32] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1526–1535,\n2018. 2\n[33] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Guilin Liu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Video-tovideo synthesis. CoRR, abs/1808.06601, 2018. 2 [34] Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tianchen Zhao, and Honglak Lee. Diversity-sensitive conditional generative adversarial networks. arXiv preprint arXiv:1901.09024, 2019. 5 [35] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In Computer Vision (ICCV),\n--- FOOTNOTES 1NVIDIA 2University of Toronto 3Vector Institute 4 MIT 2016 IEEE Conference on Computational Intelligence and 2017 IEEE International Conference on, 2017. 2", "persona": "General Public", "response": "### 1. Overview\nForty years to the day since PAC-MAN first hit arcades in Japan, and went on to munch a path to global stardom, the retro classic has been reborn, delivered courtesy of AI. Trained on 50,000 episodes of the game, a powerful new AI model created by NVIDIA Research, called NVIDIA GameGAN, can generate a fully functional version of PAC-MAN — without an underlying game engine. That means that even without understanding a game’s fundamental rules, AI can recreate the game with convincing results. “We were blown away when we saw the results, in disbelief that AI could recreate the iconic PAC-MAN experience without a game engine,” said Koichiro Tsutsumi from BANDAI NAMCO Research Inc., the research development company of the game’s publisher BANDAI NAMCO Entertainment Inc., which provided the PAC-MAN data to train GameGAN. “This research presents exciting possibilities to help game developers accelerate the creative process of developing new level layouts, characters and even games.” We’ll be making our AI tribute to the game available later this year on AI Playground, where anyone can experience our research demos firsthand. In 1981 alone, Americans inserted billions of quarters to play 75,000 hours of coin-operated games like PAC-MAN. Over the decades since, the hit game has seen versions for PCs, gaming consoles and cell phones. With data from BANDAI NAMCO Research, Kim and his collaborators at the NVIDIA AI Research Lab in Toronto used NVIDIA DGX systems to train the neural networks on the PAC-MAN episodes (a few million frames, in total) paired with data on the keystrokes of an AI agent playing the game. Since the model can disentangle the background from the moving characters, it’s possible to recast the game to take place in an outdoor hedge maze, or swap out PAC-MAN for your favorite emoji. Developers could use this capability to experiment with new character ideas or game themes. Simulators are used to develop autonomous machines of all kinds, such as warehouse robots learning how to grasp and move objects around, or delivery robots that must navigate sidewalks to transport food or medicine. NVIDIA Research has more than 200 scientists around the globe, focused on areas such as AI, computer vision, self-driving cars, robotics and graphics.\n\n### 2. Background\nYour browser doesn't support HTML5 video. Here is a url to the video instead. PAC-MAN enthusiasts once had to take their coins to the nearest arcade to play the classic maze chase. Take a left at the pinball machine and continue straight past the air hockey, following the unmistakable soundtrack of PAC-MAN gobbling dots and avoiding ghosts Inky, Pinky, Burly and Clyde. It learns key rules of the game, both simple and complex.\n\n### 3. Methods\nJust like in the original game, PAC-MAN can’t walk through the maze walls. He eats up dots as he moves around, and when he consumes a Power Pellet, the ghosts turn blue and flee. When PAC-MAN exits the maze from one side, he’s teleported to the opposite end. If he runs into a ghost, the screen flashes and the game ends.\n\n### 4. Results\nGameGAN is the first neural network model that mimics a computer game engine by harnessing generative adversarial networks, or GANs. Made up of two competing neural networks, a generator and a discriminator, GAN-based models learn to create new content that’s convincing enough to pass for the original. “This is the first research to emulate a game engine using GAN-based neural networks,” said Seung-Wook Kim, an NVIDIA researcher and lead author on the project. “We wanted to see whether the AI could learn the rules of an environment just by looking at the screenplay of an agent moving through the game. And it did.” As an artificial agent plays the GAN-generated game, GameGAN responds to the agent’s actions, generating new frames of the game environment in real time.\n\n### 5. Takeaways\nGameGAN can even generate game layouts it’s never seen before, if trained on screenplays from games with multiple levels or versions. The GameGAN edition relies on neural networks, instead of a traditional game engine, to generate PAC-MAN’s environment. The AI keeps track of the virtual world, remembering what’s already been generated to maintain visual consistency from frame to frame. No matter the game, the GAN can learn its rules simply by ingesting screen recordings and agent keystrokes from past gameplay. Game developers could use such a tool to automatically design new level layouts for existing games, using screenplay from the original levels as training data. The trained GameGAN model then generates static elements of the environment, like a consistent maze shape, dots and Power Pellets — plus moving elements like the enemy ghosts and PAC-MAN itself. It’s Not Just About Games GameGAN introduces the possibility that the work of writing a simulator for tasks like these could one day be replaced by simply training a neural network.", "paper_id": "03", "story_id": "03-03"}
{"paper": "Abstract. Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 ×100 pixels in gigapixel microscopy images sized 100,000 ×100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92 .4% of the tumors, relative to 82 .7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.\n\nIntroduction\nThe treatment and management of breast cancer is determined by the disease stage. A central component of breast cancer staging involves the microscopic examination of lymph nodes adjacent to the breast for evidence that the cancer has spread, or metastasized [3]. This process requires highly skilled pathologists and is fairly time-consuming and error-prone, particularly for lymph nodes with either no or small tumors. Computer assisted detection of lymph node metastasis could increase the sensitivity, speed, and consistency of metastasis detection [16]. In recent years, deep CNNs have signiﬁcantly improved accuracy on a wide range of computer vision tasks such as image recognition [14,11,19], object detection [8], and semantic segmentation [17]. Similarly, deep CNNs have been applied productively to improve healthcare (e.g.,[9]). This paper presents a CNN framework to aid breast cancer metastasis detection in lymph nodes. We build on [23] by leveraging a more recent Inception architecture [20], careful image patch sampling and data augmentations. Despite performing inference with stride 128 (instead of 4), we halve the error rate at 8 false positives (FPs) per slide, setting a new state-of-the-art. We also found that several approaches yielded no beneﬁts: (1) a multi-scale approach that mimics the human cognition of a pathologist’s examination of biological tissue, (2) pretraining the model on ImageNet image recognition, and (3) color normalization. Finally, we dispense with the random forest classiﬁer and feature engineering used in [23] and ﬁnd that the maximum function is an effective whole-slide classiﬁcation procedure. ⋆Work done as a Google Brain Resident (g.co/brainresidency). ⋆⋆Work done as a Google intern.\n\nMethods\nGiven a gigapixel pathology image (slide1), the goal is to classify if the image contains tumor and localize the tumors for a pathologist’s review. This use case and the diﬃculty of pixel-accurate annotation (Fig. 2) renders detection and localization more important than pixel-level segmentation. Because of the large size of the slide and the limited number of slides (270), we train models using (H&E), and is scanned at the most common high magniﬁcation in a microscope, “40X”. We also experimented with 2- and 4-times down-sampled patches (“20X” and “10X”). Fig. 1. Left: three tumor patches and right: three challenging normal patches. Fig. 2. Diﬃculty of pixel-accurate annotations for scattered tumor cells. Ground truth annotation is overlaid with a lighter shade. Note that the tumor annotations include both tumor cells and normal cells e.g.,white space representing adipose tissue (fat). Fig. 3. The three colorful blocks represent Inception (V3) towers up to the second-last layer (PreLogit). Single scale utilizes one tower with input images at 40X magniﬁcation; multi-scale utilizes multiple (e.g.,2) input magniﬁcations that are input to separate towers and merged. smaller image patches extracted from the slide (Fig. 1). Similarly, we perform inference over patches in a sliding window across the slide, generating a tumor probability heatmap. For each slide, we report the maximum value in the heatmap as the slide-level tumor prediction. We utilize the Inception (V3) architecture [20] with inputs sized 299×299 (the default) to assess the value of initializing from existing models pre-trained on another domain. For each input patch, we predict the label of the center 128×128 region. A 128 pixel region can span several tumor cells and was also used in [16]. We label a patch as tumor if at least one pixel in the center region is annotated as tumor. We explored the inﬂuence of the number of parameters by reducing the number of ﬁlters per layer while keeping the number of layers constant (e.g., depth multiplier = 0.1 in TensorFlow). We denote these models “small”. We also experimented with multi-scale approaches that utilize patches at multiple magniﬁcations centered on the same region (Fig. 3). Because preliminary experiments did not show a beneﬁt from using up to four magniﬁcations, we present results only for up to two magniﬁcations. Training and evaluating our models was challenging because of the large number of patches and the tumor class imbalance. Each slide contains 10, 000 3 4 to 400, 000 patches (median 90, 000). However, each tumor slide contains 20 to 150, 000 tumors patches (median 2, 000), corresponding to tumor patch percentages ranging from 0.01% to 70% (median 2%). Avoiding biases towards slides containing more patches (both normal and tumor) required careful sampling. First, we select “normal” or “tumor” with equal probability. Next, we select a slide that contains that class of patches uniformly at random, and sample patches from that slide. By contrast, some existing methods pre-sample a set of patches from each slide [23], which limits the breadth of patches seen during training. To combat the rarity of tumor patches, we apply several data augmentations. First, we rotate the input patch by 4 multiples of 90◦, apply a left-right ﬂip and repeat the rotations. All 8 orientations are valid because pathology slides do not have canonical orientations. Next, we use TensorFlow’s image library (tensorﬂow.image.random X ) to perturb color: brightness with a maximum delta of 64/255, saturation with a maximum delta of 0.25, hue with a maximum delta of 0.04, and contrast with a maximum delta of 0.75. Lastly, we add jitter to the patch extraction process such that each patch has a small x,y offset of up to 8 pixels. The magnitudes of the color perturbations and jitter were lightly tuned using our validation set. Pixel values are clipped to [0, 1] and scaled to [−1, 1]. We run inference across the slide in a sliding window with a stride of 128 to match the center region’s size. For each patch, we apply the rotations and left-right ﬂip to obtain predictions for each of the 8 orientations, and average the 8 predictions. Implementation Details We trained our networks with stochastic gradient descent in TensorFlow [2], with 8 replicas each running on a NVIDIA Pascal GPU with asynchronous gradient updates and batch size of 32 per replica. We used RMSProp [21] with momentum of 0.9, decay of 0.9 and ϵ = 1.0. The initial learning rate was 0.05, with a decay of 0.5 every 2 million examples. For reﬁning a model pretrained on ImageNet, we used an initial learning rate of 0.002. 3 Evaluation and Datasets We use the two Camelyon16 evaluation metrics [1]. The ﬁrst metric, the area under receiver operating characteristic, (Area Under ROC, AUC) [10] evaluates slide-level classiﬁcation. This metric is challenging because of the potential for FPs when 105 patch-level predictions are obtained per slide. We obtained 95% conﬁdence intervals using a bootstrap approach2. The second metric, FROC [5], evaluates tumor detection and localization. We ﬁrst generate a list of coordinates and corresponding predictions from each heatmap. Among all coordinates that fall within each annotated tumor region, the highest prediction is retained. Coordinates falling outside tumor regions are FPs. We use these values to compute the ROC. The FROC is deﬁned as the sensitivity at 0.25, 0.5, 1, 2, 4, 8 average FPs per tumor-negative slide [16]. This of slides in the dataset/split, and compute the AUC. Repeat for a total of 2000 bootstrap samples, and report the 2.5 and 97.5 percentile values. metric is challenging because reporting multiple points per FP region can quickly erode the score. We focused on the FROC as opposed to the AUC because there are approximately twice as many tumors as slides, which improves the reliability of the evaluation metric. Similar to the AUC, we report 95% conﬁdence intervals by computing the FROC over 2000 bootstrap samples of the predicted points. In addition, we report the sensitivity at 8 FP per slide (“@8FP”) to assess the false negative rate. To generate points for FROC computation, the Camelyon winners [23,1] thresholded the heatmap to produce a bit-mask, and reported a single prediction for each connected component in the bit-mask. By contrast, we use a non-maxima suppression method similar to [6] that repeats two steps until no values in the heatmap remain above a threshold t: (1) report the maximum and corresponding coordinate, and (2) set all values within a radius r of the maximum to 0. Because we apply this procedure to the heatmap, r has units of 128 pixels. t controls the number of points reported and has no effect on the FROC unless the curve plateaus before 8 FP. To avoid erroneously dropping tumor predictions, we used a conservative threshold of t = 0.5. Datasets Our work utilizes the Camelyon16 dataset [1], which contains 400 slides: 270 slides with pixel-level annotations, and 130 unlabeled slides as a test set.3 We split the 270 slides into train and validation sets (Appendix) for hyperparameter tuning. Typically only a small portion of a slide contains biological tissue of interest, with background and fat comprising the remainder (e.g., Fig. 2). To reduce computation, we removed background patches (gray value > 0.8 [12]), and veriﬁed visually that lymph node tissue was not discarded. Additional Evaluation: NHO-1 We digitized another set of 110 slides (57 containing tumor) from H&E-stained lymph nodes extracted from 20 patients (86 biological tissue blocks4) as an additional evaluation set. These slides came with patient- or block-level labels. To determine the slide labels, a board-certiﬁed pathologist blinded to the predictions adjudicated any differences, and brieﬂy reviewed all 110 slides.\n\n4 Experiments & Results To perform slide-level classiﬁcation, the current state-of-the-art methods apply a random forest to features extracted from a heatmap prediction [1]. Unfortunately, we were unable to train slide-level classiﬁers because the 100% validationset AUC (Table 1) rendered internal evaluation of improvements impossible. Nonetheless, using the maximum value of each slide’s heatmap achieved AUCs > 97%, statistically indistinguishable from the current best results. For tumor-level classiﬁcation, we ﬁnd that the connected component approach [23] provides a 1−5% gain in FROC when the FROC is modest (< 80%), by masking FP regions. However, this approach is sensitive to the threshold (up 5 6 Input & Validation Test model size FROC @8FP AUC FROC @8FP AUC 98.1 100 99.0 87.3 (83.2, 91.1) 91.1 (87.2, 94.5) 96.7 (92.6, 99.6) 99.3 100 100 85.5 (81.0, 89.5) 91.1 (86.8, 94.6) 97.5 (93.8, 99.8) 99.3 100 100 86.4 (82.2, 90.4) 92.4 (88.8, 95.7) 97.1 (93.2, 99.8) ensemble-of-3 - - - 88.5 (84.3, 92.2) 92.4 (88.7, 95.6) 97.7 (93.0, 100) 94.7 100 99.6 85.5 (81.0, 89.7) 91.1 (86.9, 94.8) 98.6 (96.7, 100) 88.7 97.2 97.7 79.3 (74.2, 84.1) 84.9 (80.0, 89.4) 96.5 (91.9, 99.7) 94.9 98.6 99.0 85.9 (81.6, 89.9) 92.9 (89.3, 96.1) 97.0 (93.1, 99.9) 93.8 98.6 100 82.2 (77.0, 86.7) 87.6 (83.2, 91.7) 98.6 (96.2, 99.9) Pathologist [1] - - - 73.3* 73.3* 96.6 Camelyon16 winner [1,23] - - - 80.7 82.7 99.4 Table 1. Results on Camelyon16 dataset (95% conﬁdence intervals, CI). Bold indicates results within the CI of the best model. “Small” models contain 300K parameters per Inception tower instead of 20M. -: not reported. *A pathologist achieved this sensitivity (with no FP) using 30 hours. to 10 −20% variance), and can confound evaluation of model improvements by grouping multiple nearby tumors as one. By contrast, our non-maxima suppression approach is relatively insensitive to r between 4 and 6, although less accurate models beneﬁted from tuning r using the validation set (e.g., 8). Finally, we achieve 100% FROC on larger tumors (macrometastasis), indicating that most false negatives are comprised of smaller tumors. Previous work (e.g., [24,9]) has shown that pre-training on a different domain improves performance. However, we ﬁnd that although pre-training signiﬁcantly improved convergence speed, it did not improve the FROC (see Table 1: 40X vs. addition, our large dataset size (107 patches) and data augmentation may have enabled the training of accurate models without pre-training. Next, we studied the effect of model size. Although we were originally motivated by improved experiment turn-around time, we surprisingly found that slimmed-down Inception architectures with only 3% of the parameters achieved similar performance to the full version (Table 1: 40X vs. 40X-small). Thus, we performed the remaining experiments using this smaller model. We also experimented with a multi-scale approach inspired by pathologists’ workﬂow of examining a slide at multiple magniﬁcations to get context. However, we ﬁnd no performance beneﬁt in combining 40X with an additional input at lower magniﬁcation (Fig. 3). However, these combinations output smoother heatmaps (Fig. 4), likely because of translational invariance of the CNN and overlap in adjacent patches. These visual improvements can be deceptive: some of the speckles in the 40X models reveal small non-tumor regions surrounded by tumor. Figures 1 and 3 highlight the variability in the images. Although the current leading approaches report improvements from color normalization, our experi- Fig. 4. Left to right: sample image, ground truth (tumor in white), and heatmap outputs (40X-ensemble-of-3, 40X+20X, and 40X+10X). Heatmaps of 40X and 40Xensemble-of-3 look identical. The red circular regions at the bottom left quadrant of the heatmaps are unannotated tumor. Some of the speckles are either out of focus patches on the image or non-tumor patches within a large tumor. ments revealed no beneﬁt (Appendix). This could be explained by our extensive data augmentations causing our models to learn color-invariant features. Finally, we experimented with ensembling models in two ways. First, averaging predictions across the 8 rotations/ﬂips yielded a few percent improvement in the metrics. Second, ensembling across independently trained models yield additional but smaller improvements, and gave diminishing returns after 3 models. Additional Validation We also tested our models on another 110 slides that were digitized on different scanners, from different patients, and treated with different tissue preparation protocols. Encouragingly, we obtained an AUC of 97.6 (93.6, 100), on-par with our Camelyon16 test set performance. Qualitative Evaluation We discovered tumors in two “normal” slides: 086 and 144. Fortunately, the challenge organizers conﬁrmed that both were data processing errors, and the patients were unaffected. Remarkably, both slides were in our training set, suggesting that our model was relatively resilient to label noise. In addition, we discovered an additional 7 tumor slides with incomplete annotations: 5 in train, 2 in validation (Appendix). Samples of our predictions and corresponding patches are shown in the Appendix. Limitations Our errors were related to out-of-focus tissues (macrophages, germinal centers, stroma), and tissue preparation artifacts. These errors could be reduced by better scanning quality, tissue preparation, and more comprehensive labels for different tissue types. In addition, we were unable to exhaustively tune our hyperparameters owing to the near-perfect FROC and AUC on our validation set. We plan to further develop our work on larger datasets. 5\n\nRelated Work Several promising studies have applied deep learning to histopathology. The Camelyon16 challenge winner [1] achieved a sensitivity of 75% at 8 FP per slide and a slide-level classiﬁcation AUC of 92.5% [23]. The authors trained a Inception (V1, GoogLeNet) [20] model on a pre-sampled set of image patches, and trained a random forest classiﬁer on 28 hand-engineered features to predict the slide label. A second Inception model was trained on harder examples, and predicted points were generated using the average of the two models’ predictions. This team later improved these metrics to 82.7% and 99.4% respectively [1] using color normalization [4], additional data augmentation, and lowering the inference stride from 64 to 4. The Camelyon organizers also trained CNNs on smaller datasets to detect breast cancer in lymph nodes and prostate cancer biopsies [16]. [12] applied CNNs to segmenting or detecting nuclei, epithelium, tubules, lymphocytes, mitosis, invasive ductal carcinoma and lymphoma. [7] demonstrated that CNNs achieved higher F1 score and balanced accuracy in detecting invasive ductal carcinoma. CNNs were also used to detect mitosis, winning the ICPR12 [6] and AMIDA13 [22] mitosis detection competitions. Other efforts at leveraging machine learning for predictions in cancer pathology include predicting prognosis in non-small cell lung cancer [25].\n\nConclusion\nOur method yields state-of-the-art sensitivity on the challenging task of detecting small tumors in gigapixel pathology slides, reducing the false negative rate to a quarter of a pathologist and less than half of the previous best result. We further achieve pathologist-level slide-level AUCs in two independent test sets. Our method could improve accuracy and consistency of evaluating breast cancer cases, and potentially improve patient outcomes. Future work will focus on improvements utilizing larger datasets. 7 8\n\nReferences\n1. Camelyon 2016. https://camelyon16.grand-challenge.org/, accessed: 2017-0117\n2. Abadi, M., et al.: TensorFlow (2015)\n3. Apple, S.K.: Sentinel lymph node in breast cancer: Review article from a pathologists point of view. J. of Pathol. and Transl. Medicine 50(2), 83 (2016)\n4. Bejnordi, B.E., et al.: Stain speciﬁc standardization of whole-slide histopathological\nimages. IEEE Trans. on Medical Imaging 35(2), 404–415 (2016)\n5. Bunch, P.C., et al.: A free response approach to the measurement and characterization of radiographic observer performance. Appl. of Opt. Instrum. in Medicine\nVI pp. 124–135 (1977)\n6. Cire¸san, D.C., et al.: Mitosis detection in breast cancer histology images with deep\nneural networks. Int. Conf. on Medical Image Comput. and Comput. Interv. (2013)\n7. Cruz-Roa, A., et al.: Automatic detection of invasive ductal carcinoma in whole\nslide images with convolutional neural networks. SPIE medical imaging (2014)\n8. Girshick, R., et al.: Rich feature hierarchies for accurate object detection and semantic segmentation. In: Comput. Vis. and Pattern Recognit. (2014)\n9. Gulshan, V., et al.: Development and validation of a deep learning algorithm for\ndetection of diabetic retinopathy in retinal fundus photographs. J. of the Am. Medical Soc. 316(22), 2402–2410 (2016)\n10. Hanley, J.A., McNeil, B.J.: The meaning and use of the area under a receiver\noperating characteristic (roc) curve. Radiology 143(1), 29–36 (1982)\n11. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. Int. Conf. on Machine Learning (2015)\n12. Janowczyk, A., Madabhushi, A.: Deep learning for digital pathology image analysis:\nA comprehensive tutorial with selected use cases. J. of Pathol. Informatics 7 (2016)\n13. Kothari, S., et al.: Pathology imaging informatics for quantitative analysis of wholeslide images. J. of the Am. Medical Informatics Assoc. 20(6), 1099–1108 (2013)\n14. Krizhevsky, A., et al.: Imagenet classiﬁcation with deep convolutional neural networks. Adv. in Neural Inf. Process. Syst. pp. 1097–1105 (2012)\n15. van der Laak, J.A., et al.: Hue-saturation-density model for stain recognition in\ndigital images from transmitted light microscopy. Cytometry 39(4), 275–284 (2000)\n16. Litjens, G., et al.: Deep learning as a tool for increased accuracy and eﬃciency of\nhistopathological diagnosis. Sci. Reports 6 (2016)\n17. Long, J., et al.: Fully convolutional networks for semantic segmentation (2015)\n18. Piti´e, F., Kokaram, A.: The linear monge-kantorovitch linear colour mapping for\nexample-based colour transfer (2007)\n19. Russakovsky, O., et al.: Imagenet large scale visual recognition challenge. Int. J.\nof Comput. Vis. 115(3), 211–252 (2015)\n20. Szegedy, C., et al.: Going deeper with convolutions. Comput. Vis. and Pattern\nRecognit. (2015)\n21. Tieleman, T., Hinton, G.: Lecture 6.5-rmsprop: Divide the gradient by a running\naverage of its recent magnitude (2012)\n22. Veta, M., et al.: Assessment of algorithms for mitosis detection in breast cancer\nhistopathology images. Medical image analysis 20(1), 237–248 (2015)\n23. Wang, D., et al.: Deep learning for identifying metastatic breast cancer. arXiv\npreprint arXiv:1606.05718 (2016)\n24. Yosinski, J., et al.: How transferable are features in deep neural networks? Adv. in\nNeural Inf. Process. Syst. (2014)\n25. Yu, K.H., et al.: Predicting non-small cell lung cancer prognosis by fully automated\nmicroscopic pathology image features. Nat. Commun. 7 (2016) A\n\nAppendix\nA.1 Dataset Details Number of Slides Number of Patches (M) Number of Tumors Dataset/split Normal Tumor Total Normal Tumor Total Macro Micro Camelyon-Train 127 88 215 13+8.9* 0.87 23 81 345 Camelyon-Validation 32 22 54 3.8+2.3* 0.28 6.4 14 58 Camelyon-Test 80 50 130 40 185 NHO-1 ⋆ 53 57 110 Table 2. Number of slides, patches (in millions), and tumors in each dataset/split. We excluded “Normal” slide 144 because preliminary experiments uncovered tumors in this slide. Later experiments also uncovered tumors in “Normal” 086, but this slide was used in training for the results presented in this paper. In addition, Test slide 049 was an accidental duplication by the organizers (Tumor 036), and was not used for evaluation. Tumor sizes: macrometastasis (macro, > 2000µm), micrometastasis (micro, > 200 & ≤2000µm). *normal patches extracted from the tumor slides. ⋆: additional evaluation set with slide-level labels only. A.2 Soft Labels Our experiments used binary labels: a patch is positive if at least one pixel in the center 128 x 128 region is annotated as tumor. We also explored an alternative “soft label” approach in preliminary experiments, assigning as the label the fraction of tumor pixels in the center region. However, we found that the thresholded labels yielded substantially better performance. Because the FROC rewards detecting tumors of all size equally, this might reﬂect the model being trained to assign lower values to smaller tumors (where on average, a smaller portion of each patch contains tumor cells). A.3 Image Color Normalization As can be seen in Fig. 1 & 3, the (H&E) stained tissue vary signiﬁcantly in color. These variations arise from differences in the underlying biological tissue, physical and chemical preparation of the slide, and scanner adjustments. Because reducing these variations have improved performances in other automated detection systems [4,13], we experimented with a similar color normalizing approach. However, we have not found this normalization to improve performance, and thus we detail our approach for reference only. This lack of improvement likely stems from our extensive color perturbations encouraging our models to learn color-insensitive features, and thus the color normalization was unnecessary. 9 10 First, we separate color and intensity information by mapping the raw RGB values to a Hue-Saturation-Density (HSD) space [15], and then normalize each component separately. This maps each color channel (IR, IG, IB) ∈[0, 255]3 to a corresponding optical density value: Dν = −ln((Iν + 1)/257), ν ∈{R, G, B}, followed by applying a common Hue-Saturation-Intensity color space transformation with D = (DR +DB +DG)/3 being the intensity value, and cx = DR D −1 and cy = (DG −DB)/( √ 3 · D) denoting the Cartesian coordinates that span the two-dimensional hue-saturation plane. We chose the HSD mapping over a direct HSI mapping of RGB values [15], because it is more compatible with the image acquisition physics and yields more compact distributions in general. Next, we ﬁt a single Gaussian to the color coordinates (cx, cy)i of the pixels in all tissue-containing patches, i.e. compute their empirical mean µ = (µx, µy)T and covariance Σ ∈R2×2, and then determine the transformation T ∈R2×2 of the covariance Σ to a reference covariance matrix ΣR using the MongeKantorovitch approach presented in [18]: T = Σ−1/2 \u0000 Σ1/2ΣRΣ1/2\u0001 Σ−1/2. Subsequently, we normalize the color values by applying the mapping: \u0014c′ x c′ y \u0015 − \u0014µx µy \u0015 = T \u0012\u0014cx cy Intensity values, Di, are normalized in the same manner, i.e. by applying the one-dimensional version of Equation 1 in order to transform the empirical mean and variance of all patch intensities to a reference intensity mean and variance. As reference means and variances for the color and intensity component, respectively (i.e. µR v , ΣR for color), we chose the component-wise medians over the corresponding statistical moments of all the training slides. Finally, we map the normalized (c′ x, c′ y, D′) values back to RGB space by ﬁrst applying the inverse HSI transform [15], followed by inverting the nonlinear mapping, i.e. by applying Iν = exp(−Dν) · 257 −1 to each component ν ∈ {R, G, B}. We applied this normalization in two ways. First, we applied this at inference only, by testing a model (“40X-small” in Table 1) on color-normalized slides. Unfortunately, this resulted in a few percent drop in FROC. Next we trained two models on color-normalized slides, both with and without the color perturbations. We then tested these models on color-normalized slides. Neither approach improved the performance. A.4 Sample Results Tumor slides with incomplete annotations At the outset, 11 tumor slides were known to have non-exhaustive pixel level annotations: 015, 018, 020, 029, 033, 044, 046, 051, 054, 055, 079, 092, and 095. Thus, we did not use non-tumor patches from these slides as training examples of normal patches. Over the course of our experiments, we discovered several more such cases that we veriﬁed with a pathologist: 010, 025, 034, 056, 067, 085, 110. \u0015\u0013 + \u0014µR x µR y \u0015 . (1) Fig. 5. Left: a patch from a H&E-stained slide. The darker regions are tumor, but not the lighter pink regions. Right: the corresponding predicted heatmap that accurately identiﬁes the tumor cells while assigning lower probabilities to the non-tumor regions. Fig. 6. Left: a patch from a H&E-stained slide, “Normal” 086. The larger pink cells near the top are tumor, while the smaller pink cells at the bottom are macrophages, a normal cell. Right: the corresponding predicted heatmap that accurately identiﬁes the tumor cells while ignoring the macrophages. 11 12 Fig. 7. Left: a patch from a H&E-stained slide, “Normal” 144. The cluster of larger, dark purple cells in the bottom right quadrant are tumor, while the smaller dark purple cells are lymphocytes. The pink areas are connective tissue, with interspersed tumor cells. Right: the corresponding predicted heatmap that accurately identiﬁes the tumor cells while ignoring the connective tissue and lymphocytes. Fig. 8. Left: a patch from a H&E-stained slide in our additional validation set, NHO-1. The tumor cells are a lighter purple than the surrounding cells. A variety of artifacts are visible: the dark continuous region in the top left quadrant is an air bubble, and the white parallel streaks in the tumor and adjacent tissue are cutting artifacts. Furthermore, the tissue is hemorrhagic, necrotic and poorly processed, leading to color alterations to the typical pink and purple of a H&E slide. Right: the corresponding predicted heatmap that accurately identiﬁes the tumor cells while ignoring the various artifacts, including lymphocytes and the cutting artifacts running through the tumor tissue. 13\n--- FOOTNOTES 1Google Brain, 2Google Inc, 3Verily Life Sciences, 1 Each slide contains human lymph node tissue stained with hematoxylin and eosin 2 Sample with replacement n slides from the dataset/split, where n is the number 3 The test slides labels were released recently as part of the training dataset for Camelyon17. We used these labels for evaluation, but not for parameter tuning. 4 A tissue block can contain multiple slides that vary considerably at the pixel level. 40X 40X-pretrained 40X-small 20X-small 10X-small 40X+20X-small 40X+10X-small 40X-pretrained). This may be due to a large domain difference between pathology images and natural scenes in ImageNet, leading to limited transferability. In", "persona": "Student", "response": "### 1. Overview\nLast year, we described our deep learning–based approach to improve diagnostic accuracy (LYmph Node Assistant, or LYNA) to the 2016 ISBI Camelyon Challenge, which provided gigapixel-sized pathology slides of lymph nodes from breast cancer patients for researchers to develop computer algorithms to detect metastatic cancer. While LYNA achieved significantly higher cancer detection rates (Liu et al. 2017) than had been previously reported, an accurate algorithm alone is insufficient to improve pathologists’ workflow or improve outcomes for breast cancer patients. For patient safety, these algorithms must be tested in a variety of settings to understand their strengths and weaknesses. Furthermore, the actual benefits to pathologists using these algorithms had not been previously explored and must be assessed to determine whether or not an algorithm actually improves efficiency or diagnostic accuracy. In “Artificial Intelligence Based Breast Cancer Nodal Metastasis Detection: Insights into the Black Box for Pathologists” (Liu et al. 2018), published in the Archives of Pathology and Laboratory Medicine and “Impact of Deep Learning Assistance on the Histopathologic Review of Lymph Nodes for Metastatic Breast Cancer” (Steiner, MacDonald, Liu et al. 2018) published in The American Journal of Surgical Pathology, we present a proof-of-concept pathologist assistance tool based on LYNA, and investigate these factors. With these studies, we have made progress in demonstrating the robustness of our LYNA algorithm to support one component of breast cancer TNM staging, and assessing its impact in a proof-of-concept diagnostic setting. While encouraging, the bench-to-bedside journey to help doctors and patients with these types of technologies is a long one.\n\n### 2. Background\nIn the first url, we applied our algorithm to de-identified pathology slides from both the Camelyon Challenge and an independent dataset provided by our co-authors at the Naval Medical Center San Diego. Because this additional dataset consisted of pathology samples from a different lab using different processes, it improved the representation of the diversity of slides and artifacts seen in routine clinical practice. LYNA proved robust to image variability and numerous histological artifacts, and achieved similar performance on both datasets without additional development. These studies have important limitations, such as limited dataset sizes and a simulated diagnostic workflow which examined only a single lymph node slide for every patient instead of the multiple slides that are common for a complete clinical case. Further work will be needed to assess the impact of LYNA on real clinical workflows and patient outcomes.\n\n### 3. Methods\nA pathologist’s microscopic examination of a tumor in patients is considered the gold standard for cancer diagnosis, and has a profound impact on prognosis and treatment decisions. One important but laborious aspect of the pathologic review involves detecting cancer that has spread (metastasized) from the primary site to nearby lymph nodes. Detection of nodal metastasis is relevant for most cancers, and forms one of the foundations of the widely-used TNM cancer staging.\n\n### 4. Results\nIn both datasets, LYNA was able to correctly distinguish a slide with metastatic cancer from a slide without cancer 99% of the time. Further, LYNA was able to accurately pinpoint the location of both cancers and other suspicious regions within each slide, some of which were too small to be consistently detected by pathologists. As such, we reasoned that one potential benefit of LYNA could be to highlight these areas of concern for pathologists to review and determine the final diagnosis.\n\n### 5. Takeaways\nIn our second url, 6 board-certified pathologists completed a simulated diagnostic task in which they reviewed lymph nodes for metastatic breast cancer both with and without the assistance of LYNA. For the often laborious task of detecting small metastases (termed micrometastases), the use of LYNA made the task subjectively “easier” (according to pathologists’ self-reported diagnostic difficulty) and halved average slide review time, requiring about one minute instead of two minutes per slide. This suggests the intriguing potential for assistive technologies such as LYNA to reduce the burden of repetitive identification tasks and to allow more time and energy for pathologists to focus on other, more challenging clinical and diagnostic tasks. In terms of diagnostic accuracy, pathologists in this study were able to more reliably detect micrometastases with LYNA, reducing the rate of missed micrometastases by a factor of two. Encouragingly, pathologists with LYNA assistance were more accurate than either unassisted pathologists or the LYNA algorithm itself, suggesting that people and algorithms can work together effectively to perform better than either alone.", "paper_id": "47", "story_id": "47-03"}
{"paper": "Abstract\narXiv:1711.05225v3  [cs.CV]  25 Dec 2017 We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest Xray dataset, containing over 100,000 frontalview X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We ﬁnd that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.\n\n1. Introduction\nMore than 1 million adults are hospitalized with pneumonia and around 50,000 die from the disease every year in the US alone (CDC, 2017). Chest X-rays are currently the best available method for diagnosing pneumonia (WHO, 2001), playing a crucial role in clinical care (Franquet, 2001) and epidemiological studies (Cherian et al., 2005). However, detecting pneumonia in chest X-rays is a challenging task that relies on the availability of expert radiologists. In this work, we present a model that can automatically detect pneumonia from chest X-rays at a level exceeding practicing radiologists. (*Equal contribution of Medicine University of Radiology. Correspondence to: Pranav Rajpurkar <pranavsr@cs.stanford.edu>, Jeremy Irvin <jirvin16@cs.stanford.edu>. Project website at https://stanfordmlgroup. github.io/projects/chexnet Input Chest X-Ray Image CheXNet 121-layer CNN Output Pneumonia Positive (85%) Figure 1. CheXNet is a 121-layer convolutional neural network that takes a chest X-ray image as input, and outputs the probability of a pathology. On this example, CheXnet correctly detects pneumonia and also localizes areas in the image most indicative of the pathology.) Our model, ChexNet (shown in Figure 1), is a 121layer convolutional neural network that inputs a chest X-ray image and outputs the probability of pneumonia along with a heatmap localizing the areas of the image most indicative of pneumonia. We train CheXNet on the recently released ChestX-ray14 dataset (Wang et al., 2017), which contains 112,120 frontal-view chest X-ray images individually labeled with up to 14 different thoracic diseases, including pneumonia. We use dense connections (Huang et al., 2016) and batch normalization (Ioffe & Szegedy, 2015) to make the optimization of such a deep network tractable. Detecting pneumonia in chest radiography can be diﬃcult for radiologists. The appearance of pneumonia in X-ray images is often vague, can overlap with other diagnoses, and can mimic many other benign abnormalities. These discrepancies cause considerable variability among radiologists in the diagnosis of pneumonia (Neuman et al., 2012; Davies et al., 1996; Hopstaken et al., 2004). To estimate radiologist performance, we collect annotations from four practicing academic radiologists on a subset of 420 images from ChestX-ray14. On these 420 images, we measure performance of individual radiologists and the model. We ﬁnd that the model exceeds the average radiologist performance on the pneumonia detection task. To compare CheXNet against previous work using ChestX-ray14, we make simple modiﬁcations to CheXNet to detect all 14 diseases in ChestX-ray14, and ﬁnd that we outperform best published results on all 14 diseases. Automated detection of diseases from chest X-rays at the level of expert radiologists would not only have tremendous beneﬁt in clinical settings, it would also be invaluable in delivery of health care to populations with inadequate access to diagnostic imaging specialists.\n\n2. CheXNet\n2.1. Problem Formulation The pneumonia detection task is a binary classiﬁcation problem, where the input is a frontal-view chest Xray image X and the output is a binary label y ∈ {0, 1} indicating the absence or presence of pneumonia respectively. For a single example in the training set, we optimize the weighted binary cross entropy loss F1 Score (95% CI) L(X, y) = −w+ · y log p(Y = 1|X) −w−· (1 −y) log p(Y = 0|X), where p(Y = i|X) is the probability that the network assigns to the label i, w+ = |N|/(|P|+|N|), and w−= |P|/(|P|+|N|) with |P| and |N| the number of positive cases and negative cases of pneumonia in the training set respectively. 2.2. Model Architecture and Training CheXNet is a 121-layer Dense Convolutional Network (DenseNet) (Huang et al., 2016) trained on the ChestX-ray 14 dataset. DenseNets improve ﬂow of information and gradients through the network, making the optimization of very deep networks tractable. We replace the ﬁnal fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity. The weights of the network are initialized with weights from a model pretrained on ImageNet (Deng et al., 2009). The network is trained end-to-end using Adam with standard parameters (β1 = 0.9 and β2 = 0.999) (Kingma & Ba, 2014). We train the model using minibatches of size 16. We use an initial learning rate of 0.001 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss.\n\n3. Data\n3.1. Training We use the ChestX-ray14 dataset released by Wang et al. (2017) which contains 112,120 frontal-view X-ray images of 30,805 unique patients. Wang et al. (2017) annotate each image with up to 14 different thoracic pathology labels using automatic extraction methods on radiology reports. We label images that have pneumonia as one of the annotated pathologies as positive examples and label all other images as negative examples. For the pneumonia detection task, we randomly split the dataset into training (28744 patients, 98637 images), validation (1672 patients, 6351 images), and test (389 patients, 420 images). There is no patient overlap between the sets. Before inputting the images into the network, we downscale the images to 224×224 and normalize based on the mean and standard deviation of images in the ImageNet training set. We also augment the training data with random horizontal ﬂipping. Pathology Wang et al. (2017) Yao et al. (2017) CheXNet (ours) Atelectasis 0.716 0.772 0.8094 Cardiomegaly 0.807 0.904 0.9248 Effusion 0.784 0.859 0.8638 Inﬁltration 0.609 0.695 0.7345 Mass 0.706 0.792 0.8676 Nodule 0.671 0.717 0.7802 Pneumonia 0.633 0.713 0.7680 Pneumothorax 0.806 0.841 0.8887 Consolidation 0.708 0.788 0.7901 Edema 0.835 0.882 0.8878 Emphysema 0.815 0.829 0.9371 Fibrosis 0.769 0.767 0.8047 Pleural Thickening 0.708 0.765 0.8062 Hernia 0.767 0.914 0.9164 Table 2. CheXNet outperforms the best published results on all 14 pathologies in the ChestX-ray14 dataset. In detecting Mass, Nodule, Pneumonia, and Emphysema, CheXNet has a margin of >0.05 AUROC over previous state of the art results. 3.2. Test We collected a test set of 420 frontal chest X-rays. Annotations were obtained independently from four practicing radiologists at Stanford University, who were asked to label all 14 pathologies in Wang et al. (2017). The radiologists had 4, 7, 25, and 28 years of experience, and one of the radiologists is a sub-specialty fellowship trained thoracic radiologist. Radiologists did not have access to any patient information or knowledge of disease prevalence in the data. Labels were entered into a standardized data entry program.\n\n4. CheXNet vs. Radiologist\nPerformance 4.1. Comparison We assess the performance of both radiologists and CheXNet on the test set for the pneumonia detection task. Recall that for each of the images in the test set, we have 4 labels from four practicing radiologists and 1 label from CheXNet. We compute the F1 score for each individual radiologist and for CheXNet against each of the other 4 labels as ground truth. We report the mean of the 4 resulting F1 scores for each radiologist and for CheXNet, along with the average F1 across the radiologists. We use the bootstrap to construct 95% bootstrap conﬁdence intervals (CIs), calculating the average F1 score for both the radiologists and CheXNet on 10,000 bootstrap samples, sampled with replacement from the test set. We take the 2.5th and 97.5th percentiles of the F1 scores as the 95% bootstrap CI. We ﬁnd that CheXNet achieves an F1 score of 0.435 (95% CI 0.387, 0.481), higher than the radiologist average of 0.387 (95% CI 0.330, 0.442). Table 1 summarizes the performance of each radiologist and of CheXNet. To determine whether CheXNet’s performance is statistically signiﬁcantly higher than radiologist performance, we also calculate the difference between the average F1 score of CheXNet and the average F1 score of the radiologists on the same bootstrap samples. If the 95% CI on the difference does not include zero, we conclude there was a signiﬁcant difference between the F1 score of CheXNet and the F1 score of the radiologists. We ﬁnd that the difference in F1 scores — 0.051 (95% CI 0.005, 0.084) — does not contain 0, and therefore conclude that the performance of CheXNet is statistically signiﬁcantly higher than radiologist performance. (Figure 2. CheXNet localizes pathologies it identiﬁes using Class Activation Maps, which highlight the areas of the X-ray that are most important for making a particular pathology classiﬁcation. The captions for each image are provided by one of the practicing radiologists.  (Potchen et al., 1979).\n\n5. ChexNet vs. Previous State of the\nArt on the ChestX-ray14 Dataset(a) Patient with multifocal community acquired pneumonia. The model correctly detects the airspace disease in the left lower and right upper lobes to arrive at the pneumonia diagnosis. (d) Patient with a right-sided pneumothroax and chest tube. The model detects the abnormal lung to correctly predict the presence of pneumothorax (collapsed lung).) We extend the algorithm to classify multiple thoracic pathologies by making three changes. First, instead of outputting one binary label, ChexNet outputs a vector t of binary labels indicating the absence or presence of each of the following 14 pathology classes: Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Hernia, Inﬁltration, Mass, Nod- (b) Patient with a left lung nodule. The model identiﬁes the left lower lobe lung nodule and correctly classiﬁes the pathology. (c) Patient with primary lung malignancy and two large masses, one in the left lower lobe and one in the right upper lobe adjacent to the mediastinum. The model correctly identiﬁes both masses in the X-ray. (e) Patient with a large right pleural effusion (ﬂuid in the pleural space). The model correctly labels the effusion and focuses on the right lower chest. (f) Patient with congestive heart failure and cardiomegaly (enlarged heart). The model correctly identiﬁes the enlarged cardiac silhouette. ule, Pleural Thickening, Pneumonia, and Pneumothorax. Second, we replace the ﬁnal fully connected layer in CheXNet with a fully connected layer producing a 14-dimensional output, after which we apply an elementwise sigmoid nonlinearity. The ﬁnal output is the predicted probability of the presence of each pathology class. Third, we modify the loss function to optimize the sum of unweighted binary cross entropy losses 14 X where p(Yc = 1|X) is the predicted probability that the image contains the pathology c and p(Yc = 0|X) is the predicted probability that the image does not contain the pathology c. We randomly split the dataset into training (70%), validation (10%), and test (20%) sets, following previous work on ChestX-ray14 (Wang et al., 2017; Yao et al., 2017). We ensure that there is no patient overlap between the splits. We compare the per-class AUROC of the model against the previous state of the art held by Yao et al. (2017) on 13 classes and Wang et al. (2017) on the remaining 1 class. We ﬁnd that CheXNet achieves state of the art results on all 14 pathology classes. Table 2 illustrates the perclass AUROC comparison on the test set. On Mass, Nodule, Pneumonia, and Emphysema, we outperform previous state of the art considerably (> 0.05 increase in AUROC).\n\n6. Model Interpretation\nTo interpret the network predictions, we also produce heatmaps to visualize the areas of the image most indicative of the disease using class activation mappings (CAMs) (Zhou et al., 2016). To generate the CAMs, we feed an image into the fully trained network and extract the feature maps that are output by the ﬁnal convolutional layer. Let fk be the kth feature map and let wc,k be the weight in the ﬁnal classiﬁcation layer for feature map k leading to pathology c. We obtain a map Mc of the most salient features used in classifying the image as having pathology c by taking the weighted sum of the feature maps using their associated weights. Formally, Mc = X k wc,kfk. We identify the most important features used by the model in its prediction of the pathology c by upscaling the map Mc to the dimensions of the image and overlaying the image. Figure 2 shows several examples of CAMs on the pneumonia detection task as well as the 14-class pathology classiﬁcation task.\n\n7. Related Work\nRecent advancements in deep learning and large datasets have enabled algorithms to surpass the performance of medical professionals in a wide variety of medical imaging tasks, including diabetic retinopathy detection (Gulshan et al., 2016), skin cancer classiﬁca- tion (Esteva et al., 2017), arrhythmia detection (Rajpurkar et al., 2017), and hemorrhage identiﬁcation (Grewal et al., 2017). Automated diagnosis from chest radiographs has received increasing attention with algorithms for pulmonary tuberculosis classiﬁcation (Lakhani & Sundaram, 2017) and lung nodule detection (Huang et al., 2017). Islam et al. (2017) studied the performance of various convolutional architectures on different abnormalities using the publicly available OpenI dataset (Demner-Fushman et al., 2015). Wang et al. (2017) released ChestX-ray-14, an order of magnitude larger than previous datasets of its kind, and also benchmarked different convolutional neural network architectures pre-trained on ImageNet. Recently Yao et al. (2017) exploited statistical dependencies between labels in order make more accurate predictions, outperforming Wang et al. (2017) on 13 of 14 classes.\n\n8. Conclusion\nPneumonia accounts for a signiﬁcant proportion of patient morbidity and mortality (Gon¸calves-Pereira et al., 2013). Early diagnosis and treatment of pneumonia is critical to preventing complications including death (Aydogdu et al., 2010). With approximately 2 billion procedures per year, chest X-rays are the most common imaging examination tool used in practice, critical for screening, diagnosis, and management of a variety of diseases including pneumonia (Raoof et al., 2012). However, two thirds of the global population lacks access to radiology diagnostics, according to an estimate by the World Health Organization (Mollura et al., 2010). There is a shortage of experts who can interpret X-rays, even when imaging equipment is available, leading to increased mortality from treatable diseases (Kesselman et al., 2016). We develop an algorithm which detects pneumonia from frontal-view chest X-ray images at a level exceeding practicing radiologists. We also show that a simple extension of our algorithm to detect multiple diseases outperforms previous state of the art on ChestX-ray14, the largest publicly available chest Xray dataset. With automation at the level of experts, we hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited.\n\n9. Acknowledgements\nWe would like to acknowledge the Stanford Center for Artiﬁcial Intelligence in Medicine and Imaging for clinical dataset infrastructure support (AIMI.stanford. edu).\n\nReferences\nAydogdu, M, Ozyilmaz, E, Aksoy, Handan, Gursel, G, and Ekim, Numan. Mortality prediction in community-acquired pneumonia requiring mechanical ventilation; values of pneumonia and intensive care unit severity scores. Tuberk Toraks, 58(1):25– 34, 2010. Berbaum, K, Franken Jr, EA, and Smith, WL. The effect of comparison ﬁlms upon resident interpretation of pediatric chest radiographs. Investigative radiology, 20(2):124–128, 1985. CDC, 2017. URL https://www.cdc.gov/features/ pneumonia/index.html. Cherian, Thomas, Mulholland, E Kim, Carlin, John B, Ostensen, Harald, Amin, Ruhul, Campo, Margaret de, Greenberg, David, Lagos, Rosanna, Lucero, Marilla, Madhi, Shabir A, et al. Standardized interpretation of paediatric chest radiographs for the diagnosis of pneumonia in epidemiological studies. Bulletin of the World Health Organization, 83(5):353–359, 2005. Davies, H Dele, Wang, Elaine E-l, Manson, David, Babyn, Paul, and Shuckett, Bruce. Reliability of the chest radiograph in the diagnosis of lower respiratory infections in young children. The Pediatric infectious disease journal, 15(7):600–604, 1996. Demner-Fushman, Dina, Kohli, Marc D, Rosenman, Marc B, Shooshan, Sonya E, Rodriguez, Laritza, Antani, Sameer, Thoma, George R, and McDonald, Clement J. Preparing a collection of radiology examinations for distribution and retrieval. Journal of the American Medical Informatics Association, 23 (2):304–310, 2015. Deng, Jia, Dong, Wei, Socher, Richard, Li, Li-Jia, Li, Kai, and Fei-Fei, Li. Imagenet: A large-scale hierarchical image database. In Computer Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on, pp. 248–255. IEEE, 2009. Esteva, Andre, Kuprel, Brett, Novoa, Roberto A, Ko, Justin, Swetter, Susan M, Blau, Helen M, and Thrun, Sebastian. Dermatologist-level classiﬁcation of skin cancer with deep neural networks. Nature, 542(7639):115–118, 2017. Franquet, T. Imaging of pneumonia: trends and algorithms. European Respiratory Journal, 18(1):196– 208, 2001. Gon¸calves-Pereira, Jo˜ao, Concei¸c˜ao, Catarina, and P´ovoa, Pedro. Community-acquired pneumonia: identiﬁcation and evaluation of nonresponders. Therapeutic advances in infectious disease, 1(1):5– 17, 2013. Grewal, Monika, Srivastava, Muktabh Mayank, Kumar, Pulkit, and Varadarajan, Srikrishna. Radnet: Radiologist level accuracy using deep learning for hemorrhage detection in ct scans. arXiv preprint arXiv:1710.04934, 2017. Gulshan, Varun, Peng, Lily, Coram, Marc, Stumpe, Martin C, Wu, Derek, Narayanaswamy, Arunachalam, Venugopalan, Subhashini, Widner, Kasumi, Madams, Tom, Cuadros, Jorge, et al. Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. Jama, 316(22):2402–2410, 2016. Hopstaken, RM, Witbraad, T, Van Engelshoven, JMA, and Dinant, GJ. Inter-observer variation in the interpretation of chest radiographs for pneumonia in community-acquired lower respiratory tract infections. Clinical radiology, 59(8):743–752, 2004. Huang, Gao, Liu, Zhuang, Weinberger, Kilian Q, and van der Maaten, Laurens. Densely connected convolutional networks. arXiv preprint arXiv:1608.06993, 2016. Huang, Peng, Park, Seyoun, Yan, Rongkai, Lee, Junghoon, Chu, Linda C, Lin, Cheng T, Hussien, Amira, Rathmell, Joshua, Thomas, Brett, Chen, Chen, et al. Added value of computer-aided ct image features for early lung cancer diagnosis with small pulmonary nodules: A matched case-control study. Radiology, pp. 162725, 2017. Ioffe, Sergey and Szegedy, Christian. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International Conference on Machine Learning, pp. 448–456, 2015. Islam, Mohammad Tariqul, Aowal, Md Abdul, Minhaz, Ahmed Tahseen, and Ashraf, Khalid. Abnormality detection and localization in chest x-rays using deep convolutional neural networks. arXiv preprint arXiv:1705.09850, 2017. Kesselman, Andrew, Soroosh, Garshasb, Mollura, Daniel J, and Group, RAD-AID Conference Writing. 2015 rad-aid conference on international radiology for developing countries: The evolving global radiology landscape. Journal of the American College of Radiology, 13(9):1139–1144, 2016. Kingma, Diederik and Ba, Jimmy. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. Lakhani, Paras and Sundaram, Baskaran. Deep learning at chest radiography: Automated classiﬁcation of pulmonary tuberculosis by using convolutional neural networks. Radiology, pp. 162326, 2017. Mollura, Daniel J, Azene, Ezana M, Starikovsky, Anna, Thelwell, Aduke, Iosifescu, Sarah, Kimble, Cary, Polin, Ann, Garra, Brian S, DeStigter, Kristen K, Short, Brad, et al. White paper report of the rad-aid conference on international radiology for developing countries: identifying challenges, opportunities, and strategies for imaging services in the developing world. Journal of the American College of Radiology, 7(7):495–500, 2010. Neuman, Mark I, Lee, Edward Y, Bixby, Sarah, Diperna, Stephanie, Hellinger, Jeffrey, Markowitz, Richard, Servaes, Sabah, Monuteaux, Michael C, and Shah, Samir S. Variability in the interpretation of chest radiographs for the diagnosis of pneumonia in children. Journal of hospital medicine, 7(4): 294–298, 2012. Potchen, EJ, Gard, JW, Lazar, P, Lahaie, P, and Andary, M. Effect of clinical history data on chest ﬁlm interpretation-direction or distraction. In Investigative Radiology, volume 14, pp. 404–404, 1979. Rajpurkar, Pranav, Hannun, Awni Y, Haghpanahi, Masoumeh, Bourn, Codie, and Ng, Andrew Y. Cardiologist-level arrhythmia detection with convolutional neural networks. arXiv preprint arXiv:1707.01836, 2017. Raoof, Suhail, Feigin, David, Sung, Arthur, Raoof, Sabiha, Irugulpati, Lavanya, and Rosenow, Edward C. Interpretation of plain chest roentgenogram. CHEST Journal, 141(2):545–558, 2012. Wang, Xiaosong, Peng, Yifan, Lu, Le, Lu, Zhiyong, Bagheri, Mohammadhadi, and Summers, Ronald M. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classiﬁcation and localization of common thorax diseases. arXiv preprint arXiv:1705.02315, 2017. Yao, Li, Poblenz, Eric, Dagunts, Dmitry, Covington, Ben, Bernard, Devon, and Lyman, Kevin. Learning to diagnose from scratch by exploiting dependencies among labels. arXiv preprint arXiv:1710.10501, 2017. Zhou, Bolei, Khosla, Aditya, Lapedriza, Agata, Oliva, Aude, and Torralba, Antonio. Learning deep features for discriminative localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921–2929, 2016.\n--- FOOTNOTES 1Stanford University Department of Computer Science 2Stanford University Department 3Stanford Department", "persona": "Student", "response": "### 1. Overview\nWithin a week, the Stanford team had developed an algorithm, called CheXnet, capable of spotting 10 of the 14 pathologies in the original data set more accurately than previous algorithms. After about a month of training, it was ahead in all 14, the group reported in a url released this week through the Cornell University Library. And CheXnet consistently did better than the four Stanford radiologists in diagnosing pneumonia accurately. The researchers looked at CheXnet’s performance in terms of sensitivity—that is, whether it correctly identified existing cases of pneumonia, and how well it avoided false positives. While some of the four human radiologists were better than others, CheXnet was better than all of them [See graph below]. Image: Stanford ChexNet, tested on 420 x-rays, outperformed four radiologists in both sensitivity (identifying positives correctly) and specificity (identifying negatives correctly). Individual radiologists are represented by orange X’s, their average performance by a green X, and ChexNet by the blue curve, generated by varying the threshholds used for its diagnosis.\n\n### 2. Background\nLast December, my then-18-year-old son went to the Stanford emergency room with an extremely high fever and cough. He had a chest x-ray for suspected pneumonia; it was read as negative so he was given an I.V. for dehydration, medication for his fever, and was sent home. A week later, he was back in the ER in the middle of the night, this time disoriented, with an even higher fever that wasn’t responding to medication. Again, a chest x-ray was read as negative, and he was tested for every disease one could imagine.\n\n### 3. Methods\nBut all he was given were fluids, and eventually he was released with no diagnosis. Two days after that, we got a call from radiology—a routine review of x-rays from the weekend had changed the medical opinion to pneumonia—a diagnosis that had been missed twice. Antibiotics started bringing his fever down within 24 hours.\n\n### 4. Results\nStanford researchers have developed a machine-learning algorithm that can diagnose pneumonia from a chest x-ray better than a human radiologist can. And it learned how to do so in just about a month. The Machine Learning Group, led by Stanford adjunct professor Andrew Ng, was inspired by a data set released by the National Institutes of Health on 26 September. The data set contains 112,120 chest X-ray images labeled with 14 different possible diagnoses, along with some preliminary algorithms.\n\n### 5. Takeaways\nThe researchers asked four Stanford radiologists to annotate 420 of the images for possible indications of pneumonia. They selected that disease because, according to a press release, it is particularly hard to spot on X-rays, and brings 1 million people to U.S. hospitals each year. The Stanford approach also creates a heat map of the chest x-rays, with colors indicating areas of the image most likely to represent pneumonia; this is a tool that researchers believe could greatly assist human radiologists. I couldn’t be more thrilled—and hopeful that all the radiologists at Stanford will embrace this technology immediately, because I know firsthand how beneficial it could be.", "paper_id": "44", "story_id": "44-03"}
{"paper": "Artificial intelligence-supported screen reading versus standard double reading in the Mammography Screening with Artificial Intelligence trial (MASAI): a clinical safety analysis of a randomised, controlled, non-inferiority, singleblinded, screening accuracy study Kristina Lång, Viktoria Josefsson, Anna-Maria Larsson, Stefan Larsson, Charlotte Högberg, Hanna Sartor, Solveig Hofvind, Ingvar Andersson, Aldana Rosso Summary Background Retrospective studies have shown promising results using artificial intelligence (AI) to improve mammography screening accuracy and reduce screen-reading workload; however, to our knowledge, a randomised trial has not yet been conducted. We aimed to assess the clinical safety of an AI-supported screen-reading protocol compared with standard screen reading by radiologists following mammography. Lancet Oncol 2023; 24: 936–44 See Comment page 830 Division of Diagnostic Translational Medicine, Lund University, Malmö, Sweden (K Lång PhD, V Josefsson MD, H Sartor PhD, I Andersson PhD, Methods In this randomised, controlled, population-based trial, women aged 40–80 years eligible for mammography screening (including general screening with 1·5–2-year intervals and annual screening for those with moderate hereditary risk of breast cancer or a history of breast cancer) at four screening sites in Sweden were informed about the study as part of the screening invitation. Those who did not opt out were randomly allocated (1:1) to AI-supported screening (intervention group) or standard double reading without AI (control group). Screening examinations were automatically randomised by the Picture Archive and Communications System with a pseudo-random number generator after image acquisition. The participants and the radiographers acquiring the screening examinations, but not the radiologists reading the screening examinations, were masked to study group allocation. The AI system (Transpara version 1.7.0) provided an examination-based malignancy risk score on a 10-level scale that was used to triage screening examinations to single reading (score 1–9) or double reading (score 10), with AI risk scores (for all examinations) and computer-aided detection marks (for examinations with risk score 8–10) available to the radiologists doing the screen reading. Here we report the prespecified clinical safety analysis, to be done after 80 000 women were enrolled, to assess the secondary outcome measures of early screening performance (cancer detection rate, recall rate, false positive rate, positive predictive value [PPV] of recall, and type of cancer detected [invasive or in situ]) and screen-reading workload. Analyses were done in the modified intention-to-treat population (ie, all women randomly assigned to a group with one complete screening examination, excluding women recalled due to enlarged lymph nodes diagnosed with lymphoma). The lowest acceptable limit for safety in the intervention group was a cancer detection rate of more than 3 per 1000 participants screened. The trial is registered with ClinicalTrials.gov, NCT04838756, and is closed to accrual; follow-up is ongoing to assess the primary endpoint of the trial, interval cancer rate. Findings Between April 12, 2021, and July 28, 2022, 80 033 women were randomly assigned to AI-supported screening (n=40 003) or double reading without AI (n=40 030). 13 women were excluded from the analysis. The median age was 54·0 years (IQR 46·7–63·9). Race and ethnicity data were not collected. AI-supported screening among 39 996 participants resulted in 244 screen-detected cancers, 861 recalls, and a total of 46 345 screen readings. Standard screening among 40 024 participants resulted in 203 screen-detected cancers, 817 recalls, and a total of 83 231 screen readings. Cancer detection rates were 6·1 (95% CI 5·4–6·9) per 1000 screened participants in the intervention group, above the lowest acceptable limit for safety, and 5·1 (4·4–5·8) per 1000 in the control group—a ratio of 1·2 (95% CI 1·0–1·5; p=0·052). Recall rates were 2·2% (95% CI 2·0–2·3) in the intervention group and 2·0% (1·9–2·2) in the control group. The false positive rate was 1·5% (95% CI 1·4–1·7) in both groups. The PPV of recall was 28·3% (95% CI 25·3–31·5) in the intervention group and 24·8% (21·9–28·0) in the control group. In the intervention group, 184 (75%) of 244 cancers detected were invasive and 60 (25%) were in situ; in the control group, 165 (81%) of 203 cancers were invasive and 38 (19%) were in situ. The screen-reading workload was reduced by 44·3% using AI. Interpretation AI-supported mammography screening resulted in a similar cancer detection rate compared with standard double reading, with a substantially lower screen-reading workload, indicating that the use of AI in mammography screening is safe. The trial was thus not halted and the primary endpoint of interval cancer rate will be assessed in 100 000 enrolled participants after 2-years of follow up.\n\nIntroduction\nEuropean guidelines recommend double reading of screening mammograms to ensure high sensitivity.1 A meta-analysis suggested that double reading resulted in 0·44 more cancers being detected per 1000 people screened than with single reading;2 however, this comes at the expense of a large screen-reading workload and can potentially increase false positives.3,4 Double reading can also be difficult to sustain because of a shortage of breast radiologists in many countries.5 In addition, despite double reading, some cancers might be missed and diagnosed as interval cancers.6 Interval cancers generally have a worse prognosis than screen-detected cancers, and the interval cancer rate is therefore an important indicator of screening efficacy.1,6 In retrospective studies, about 20–30% of interval cancers have been shown to display highly suspicious signs of malignancy at the preceding screening mammogram,6–8 suggesting that mammography alone could have been sufficient for detection—ie, without the need for supplementary imaging methods. Establishing a more efficient and effective mammography screening programme is therefore warranted. Recently developed image analysis tools based on artificial intelligence (AI) have promising applications in mammography screening, such as facilitating triage of screening examinations according to risk of malignancy or supporting detection with computer-aided detection Research in context Evidence before this study We searched MEDLINE for studies published in English between Jan 1, 2015, and Dec 31, 2020, that included “breast cancer screening” or “mammography screening”, and “artificial intelligence” or “machine learning” in the title or abstract. No prospective trials were identified. There were several retrospective accuracy studies using screening data or enriched datasets. We found no systematic reviews on test accuracy. The retrospective studies, using different artificial intelligence (AI) software and mammography devices, indicated that AI could be used to differentiate between screening examinations with low and high probability of malignancy, which could potentially be used to improve the efficacy of screening and reduce the workload, especially the requirement for double reading. (CAD) marks highlighting suspicious findings.9 Retrospective studies suggest that the accuracy of AI is similar to or better than that of breast radiologists.10–13 AI has also been shown to be able to identify examinations that were normal (ie, true negatives), and, since the vast majority of women who attend screening do not have breast cancer, adapting single and double reading to AI risk scores could allow more efficient screen reading.14–17 Additionally, AI has been shown to retrospectively classify screening examinations as high risk before a diagnosis of interval cancer, and could, therefore, help radiologists to reduce false negative screening results when used as detection support.16,18,19 Taken together, the evidence suggests that use of AI could potentially benefit mammography screening by reducing the screenreading workload and the number of interval cancers, but randomised trials are needed to assess the efficacy of AI-supported screening.13 In the randomised, controlled Mammography Screening with Artificial Intelligence trial (MASAI), we investigate an AI-supported screen-reading procedure involving triage of screening examinations to single or double reading, along with detection support. Here we report a prespecified safety analysis, the objective of which was to assess the safety of using AI-supported screening compared with standard double reading by determining the effect on cancer detection, which could be used to inform new trials or programme-based more cancers being detected and exceeded the lowest acceptable limit for safety compared with standard double reading without AI, without affecting the false positive rate. The AI supported screen-reading procedure enabled a 44·3% reduction in the screen-reading workload. The results indicate that the proposed screening strategy is safe. Implications of all the available evidence The results from this randomised trial support the findings of earlier retrospective studies, indicating a general potential of AI to improve screening efficacy and reduce workload. The clinical safety analysis concludes that the AI-supported screen-reading procedure can be considered safe. Implementation of AI in clinical practice to reduce the screen-reading workload could therefore be considered to help address workforce shortages. The assessment of the primary endpoint of interval cancer rate, together with a characterisation of detected cancers in the entire study population, will provide further insight into the efficacy of screening, possible side-effects such as overdiagnosis, and the prognostic implications of using AI in mammography screening, taking cost-effectiveness into account. evaluations. In addition, we compared recalls, false positives, positive predictive value of recalls, and screenreading workload for the two screen-reading procedures.\n\nMethods\nStudy design and participants The MASAI trial was designed as a randomised, parallel, non-inferiority, single-blinded, controlled, screening accuracy study to compare AI-supported mammography screening with standard double reading without AI. The study was done within the Swedish national screening programme and participants were recruited at four screening sites in southwest Sweden (Malmö, Lund, Landskrona, and Trelleborg). Screen reading and further assessment of recalled participants were done at a single site, the Unilabs Mammography Unit at Skåne University Hospital (Malmö, Sweden). The inclusion criterion was women (defined here as people registered with a female Swedish personal identity number indicating female gender, which can include trans women who have changed their legal gender) eligible to participate in population-based mammography screening, which also includes those with moderate hereditary risk of breast cancer and those with a history of breast cancer. No exclusion criteria were applied. The Swedish population-based mammography screening programme invites women aged 40–74 years for screening at intervals of 1·5–2 years. Those younger than 55 years are first screened at 1·5-year intervals, and those aged 55 years or older are screened at 2-year intervals. Annual screening is done for people considered to have a moderate hereditary risk of breast cancer (lifetime risk 18–29%) and for those with a history of breast cancer (for 10 years after surgery, with an upper age limit of 80 years). Information about the study was included in screening invitation letters and in SMS text message reminders before scheduled appointments, with a link to a website containing detailed study information in Swedish and English. People eligible for screening who did not wish to participate in the trial were asked to opt out at the time of the screening visit and received standard of care. Information about the race or ethnicity of participants was not collected. For more on Transpara see https://screenpoint-medical.com The study was approved by the Swedish Ethical Review Authority (2020-04936), which also waived the need for written informed consent. The study protocol (versions 1.1 and 1.2) and the statistical analysis plan are available at the Lund University website. The protocol was updated to improve clarity; there were no changes in the trial procedures nor analyses in the statistical analysis plan from those described in the first and updated protocol versions. For the study protocol and statistical analysis plan see https://portal.research.lu.se/en/ projects/mammography- randomised in the Picture Archive and Communications System (PACS; Sectra, Linköping, Sweden) to AIsupported screening (intervention group) or standard double reading without AI (control group) with a pseudorandom number generator. The people screened and the radiographers acquiring the screening examinations were masked to study group allocation, since the automatic randomisation was not visible on the radiographer’s PACS interface. The screen readers were not masked to the results of the allocation. Procedures A single-vendor mammography system was used for the screening examinations (Senographe Pristina, GE Healthcare, Freiburg, Germany). Standard screening examination included two views per breast with the addition of implant-displacement views for people with breast implants. The examinations randomised to the intervention group were analysed using Transpara  version 1.7.0 (ScreenPoint Medical, Nijmegen, Netherlands). This system uses deep learning to identify and interpret mammographic regions suspicious for cancer. It was developed with more than 200 000 examinations for training and testing, which were obtained from multiple institutions in more than ten countries covering a range of populations, modality manufacturers, and variations in screening and diagnostic workflows. Annotations of more than 10 000 cancers in the database are based on biopsy results and include regions marked in previous mammograms in which cancers were visible but not detected by radiologists. The AI system provided an examination-based malignancy risk score on a continuous scale ranging from 1 to 10. The risk scores were also presented on a discrete 10-level scale, calibrated to assign approximately a tenth of screening examinations to each risk score. Examinations were considered to be low risk (risk score 1–7), intermediate risk (risk scores 8 and 9), or high risk (risk score 10). Cancer prevalence increases sharply in the group with a risk score of 10, and retrospective studies using the same AI version as in this trial have reported 87–90% of screen-detected cancers and 45% of interval cancers to be in this group.16,17 The AI system also provided CAD marks at suspicious regional findings of calcifications and soft-tissue lesions, with a regional risk score on a discrete scale from 1 to 98. To limit the number of CAD marks that could potentially disturb the screen reading or lead to an increase in false positives, the AI system was preconfigured for CAD marks to be available only for examinations with risk scores of 8, 9, and 10, accounting for approximately 30% of all examinations (regional risk score threshold >42). The AI system was also configured to analyse implant-displacement views in screening examinations of people with breast implants. The PACS was customised with separate worklists for single and double reading. Examinations with the highest 1% risk, classified as extra high risk, were flagged in the high-risk worklist as 10H. A risk score threshold of 9·8, which was determined from the observed risk score distribution in the screening population, was used to select this group. Screening examinations in the control group were not analysed with AI at any timepoint. In the intervention group, examinations with risk scores of 1–9 (low and intermediate risk) underwent single reading and examinations with risk scores of 10 (high risk) underwent double reading (figure 1). Double reading was done by two different breast radiologists. The second reader had access to the first reader’s assessment (unblinded double reading), which is the standard of care in the regional screening programme in the Skåne region. Readers were aware of the examination risk score (for all examinations), presented both in the PACS worklists and on the image monitor. Readers first read the examination without CAD marks and then with CAD marks, if available (ie, for examinations with risk scores of 8–10). The readers were instructed to recall cases with the highest 1% risk, except for obvious false positives. In the control group, screening examinations were read with standard unblinded double reading without AI. The outcomes of the screen reading were either no suspicion of malignancy or recall. Participants could be recalled due to mammographic findings or selfreported symptoms. Current practice in the screening programme is to recall participants with self-reported symptoms, such as a lump, when the mammogram cannot safely be classified as normal. Before the final decision, readers had the option of referral to a consensus meeting or to a technical recall (eg, due to poor image quality), or both. Consensus meetings are common practice in screening programmes with double reading; in these meetings, difficult or equivocal findings are reassessed by two radiologists, with a joint decision made to recall or clear of suspicion of malignancy.3 The images acquired at technical recall were by default randomised de novo due to the technical setup; however, participants were assessed according to their originally assigned group. Screening examinations allocated to the intervention group that failed to be processed by AI underwent standard-of-care reading. 16 breast radiologists at the Unilabs Mammography Unit at Skåne University Hospital were involved in the screen reading, of whom 15 had more than 2 years of experience in breast imaging and 14 had more than 5 years of experience. 12 of the radiologists had a yearly reading volume of at least 5000 cases. Three radiologists had a yearly reading volume of 1000–3000 cases, and one radiologist read on average 700 cases per year. Based on the group composition, only readers with more than 2 years of experience were allowed to read from the single-reading worklist. Before each screen-reading session, the radiologist rolled a six-sided die to randomly allocate themselves to either of the two groups: numbers 1–3 allocated them to the control group and 4–6 to the intervention group. Participants could withdraw from the study at any time, at which point all personal data would be removed and they would be excluded from analyses. True positive cases were initially identified through linkage with the Regional Cancer Registry (on Sept 12, 2022); to compensate for a delay in registry reporting, all recalled participants were manually assessed with use of patient records, and true positives were validated by histopathology reports on surgical samples or core-needle biopsies. Outcomes The primary outcome measure of the MASAI trial is interval cancer rate, which will be assessed after the full study population of 100 000 screened participants have had at least a 2-year follow-up (estimated December, 2024). Secondary outcome measures are early screening performance (cancer detection rate, recall rate, false positive rate, and positive predictive value [PPV] of recall), screen-reading workload (number of screen-readings and consensus meetings), detection in relation to tumour type and stage, proportion of interval cancers by cancer type and stage, sensitivity and specificity, and incremental cost-effectiveness ratio. In the current clinical safety analysis, the secondary outcome measures of early screening performance of cancer detection rate (number of cancers detected per 1000 participants screened), recall rate (proportion of screened participants who were recalled), false positive rate, PPV of recall, type of detected cancer (invasive or in situ), and screen-reading workload were assessed. The screen-reading workload was reported as the sum of all screen readings, including those made at consensus meetings. The number and proportion of screenings that resulted in a consensus meetings (consensus meeting rate) were also reported separately. Statistical analysis The intention-to-treat population comprised all participants who underwent breast screening. The modified intentionto-treat (mITT) population comprised participants with a complete screening examination, excluding those who were asked to attend a technical recall but did not attend. Participants recalled due to bilateral enlarged lymph nodes and diagnosed with lymphoma were also excluded from the mITT population, since they were not recalled due to AI triage Risk score 10 Double reading plus AI Intervention group Risk score 1–9 suspicion of breast cancer. Participants were analysed in their allocated group regardless of the actual reading procedure (treatment policy strategy). The hypothesis for the primary analysis was the noninferiority of AI-supported mammography screening compared with standard double reading, in terms of interval cancer rate, with a secondary hypothesis of superiority. Considering the interplay of screen-reading workload and the number of interval cancers, the noninferiority margin for the primary endpoint was set at the intervention yielding at most 20% more interval cancers than in the control group. The sample size calculations were done with use of Fisher’s exact test to compare the risk ratio based on the observed interval cancer rate in the current screening programme. A total sample size of 100 000 (intention-to-treat population) was expected to have at least 80% statistical power to show that the ratio of the interval cancer rate is at most 1·2 in the intervention group compared with the control group. The mITT population among the 100 000 enrolled participants will be used in the primary analysis. The sample size calculation for the clinical safety analysis was based on a worst case scenario of the intervention yielding a cancer detection rate of 3 per 1000 screened participants (based on the assumption that single reading could lead to reduced detection), at which rate the study could be halted, compared with a detection rate of 5 per 1000 screened participants in the control group (reflecting the observed rate in the current screening programme). According to Fisher’s exact test, a sample size of 80 000 (intention-to-treat population) was needed to show with a power greater than 80% that the proportion of detected cancer did not reach the worst case scenario. The mITT population among the 80 000 enrolled participants was used in the clinical safety analysis. Throughout the study, the overall recall rate was monitored as part of the institutional quality assurance reports to ensure that the recall rate did not drop below what was observed in the clinic 6 months before the start of the trial (average recall rate 2·1%), which could indicate a reduction in cancer detection. The number of enrolled participants was monitored monthly. Trial data were extracted from PACS on Sept 12, 2022, which was 1·5 months after roughly 80 000 participants had been enrolled, to ensure sufficient time for the screen reading and initial investigations of recalled participants. Participants were matched with the Regional Cancer Registry on the same day as data extraction and a rapid preliminary analysis of cancer detection was available 1 week later, which was used to inform the decision to continue the trial. Descriptive statistics were used to summarise study population characteristics. Frequencies and percentages were calculated for categorical data. 95% CIs were calculated with the Clopper-Pearson method. The cancer detection rate, recall rate, false positive rate, and PPV of recall were calculated separately for the intervention and control groups. The cancer detection rate was compared with Fisher’s exact test and the ratio of the proportions with corresponding 95% CIs were reported. A two-sided p value of less than 0·05 was considered to indicate statistical significance. The numbers of screen readings and consensus meetings were calculated separately for the intervention and the control groups. In a post-hoc analysis, the distribution of AI risk scores by screening examinations, screen-detected cancers, recalls, and PPV of recall were reported with descriptive statistics. This analysis was included to describe AI performance and no inferential statistical analyses were done. All statistical analyses were done with Stata IC 17.0 software and Python 3.8.5. The trial is registered with ClincialTrials.gov, NCT04838756. Role of the funding source The funders of the study had no role in study design, data collection, data analysis, data interpretation, or writing of the report.\n\nResults\nBetween April 12, 2021, and July 28, 2022, 80 160 women presented for screening and 127 (0·2%) opted out of the trial. 80 033 participants were randomly assigned: 40 003 (50·0%) to undergo AI-supported screening (intervention group) and 40 030 (50·0%) to undergo double reading without AI (control group). 39 996 partici­ pants in the intervention group and 40 024 in the control group were included in the clinical safety analysis (mITT Figure 2: Trial profile AI=artificial intelligence. population; figure 2). The median age of participants in the mITT population was 54·0 years (IQR 46·7–63·9). The age distribution and indication for screening was similar between groups (table 1). The AI system did not to provide a risk score for 306 (0·8%) of 39 996 participants in the intervention group. There were 38 (0·1%) technical recalls among 40 003 participants in the intervention group and 46 (0·1%) among 40 030 participants in the control group. Early screening performance and workload measures are presented in table 2. Based on the rapid preliminary analysis of cancer detection internally reported on Sept 20, 2022, among the 39 996 participants screened with AI, 244 cancers were detected and 861 participants were recalled. Among the 40 024 participants in the control group, 203 cancers were detected and 817 partici­ pants were recalled. The cancer detection rate was 6·1 (95% CI 5·4–6·9) per 1000 participants for AI-supported screening (ie, above the lower safety limit) and 5·1 (4·4–5·8) per 1000 for double reading without AI, a ratio of 1·2 (95% CI 1·0–1·5; p=0·052). The absolute difference in cancer detection per 1000 screened participants was 1·0 (95% CI 0·0–2·1). The false positive rate was the same in both groups. 36 886 fewer screen readings were done in the intervention group than in the control group, representing in a 44·3% reduction in the screen-reading workload. Of the 244 cancers detected in the intervention group, 184 (75%) were invasive, among which 152 (83%) were stage T1 (tumour diameter ≤20 mm). In the control group, 165 (81%) of 203 cancers were invasive, of which 129 (78%) were stage T1. In situ cancers constituted 60 (25%) detected cancers in the intervention group and 38 (19%) in the control group. The distribution of AI risk scores in the intervention group and early screening performance measures per risk score are presented in table 3 (post hoc). The cancer detection rate in the high-risk group (ie, those with a risk score of 10 that underwent double reading) was 72·3 per 1000 participants screened (208 of 2875 participants), a frequency of one cancer per 14 screening examinations. In the high-risk group, 11 (2·6%) of 416 recalls were due to self-reported symptoms. Of the 490 screening examinations flagged as extra high risk by AI (highest 1% risk), 189 (38·6%) were recalled—ie, 22·0% of all 861 recalls in the intervention group. Of the 189 recalled participants classified as being extra high risk, 136 had cancer (PPV of recall 72·0%), resulting in a cancer detection rate of 277·6 per 1000 screening examinations in the extrahigh-risk category. Thus, the 1·2% of screening examinations flagged as extra high risk contained 55·7% of all screen-detected cancers in this group. 36 815 (92·0%) of 39 996 screening examinations were those with risk scores of 1–9 (which underwent single reading), among which there were 440 (1·2%) recalls (51·1% of all 861 recalls), including 114 (25·9%) recalls Intervention group (n=39 996) Control group (n=40 024) Age, years Mean (SD) 55·3 (10·2) 55·3 (10·2) Range 39·6–80·1 39·5–79·9 <45 7568 (18·9%) 7607 (19·0%) 45–49 7155 (17·9%) 7209 (18·0%) 50–54 6505 (16·3%) 6559 (16·4%) 55–59 5021 (12·6%) 4822 (12·0%) 60–64 5007 (12·5%) 5214 (13·0%) 65–69 4345 (10·9%) 4265 (10·7%) ≥70 4395 (11·0%) 4348 (10·9%) Screening indication General screening 38 969 (97·4%) 38 951 (97·3%) History of breast cancer 984 (2·5%) 1017 (2·5%) Moderate hereditary risk 43 (0·1%) 56 (0·1%) Data are mean (SD), range, or n (%). Table 1: Baseline population characteristics, modified intention-to-treat population Intervention group (n=39 996) Control group (n=40 024) Early screening performance Number of recalls 861 817 Recall rate, % 2·2% (2·0–2·3) 2·0% (1·9–2·2) Number of screendetected cancers 244 203 6·1 (5·4–6·9) 5·1 (4·4–5·8) Cancer-detection rate, per 1000 participants screened False positive rate, % 1·5% (1·4–1·7) 1·5% (1·4–1·7) Positive predictive value of recall, % 28·3% (25·3–31·5) 24·8% (21·9–28·0) Workload Number of screen readings 46 345 83 231 Number of consensus meetings 1584 1576 Consensus meeting rate 4·0% (3·8–4·2) 3·9% (3·8–4·1) Data are n or point estimate (95% CI). Table 2: Early screening performance and workload measures, modified intention-to-treat population based on self-reported symptoms. 36 cancers were detected by screening in the single-reading with AI group (14·8% of all 244 screen-detected cancers), with an overall cancer detection rate of 1·0 per 1000 participants screened. There was a considerable difference in cancer detection rate between those with risk scores of 1–7 (0·2 per 1000 participants screened; six cancers detected among 30 464 participants) and those with risk scores of 8–9 (4·7 per 1000; 30 cancers detected among 6351 partici­pants), meaning that, to detect one cancer, radiologists had to read Positive predictive value of recalls Screendetected cancers (n=244) Participants screened (N=39 996) Participants recalled (n=861) 10 2875 (7·2%) 416 (48·3%) 208 (85·2%) 50·0% 9 3212 (8·0%) 116 (13·5%) 23 (9·4%) 19·8% 8 3139 (7·8%) 65 (7·5%) 7 (2·9%) 10·8% 7 3075 (7·7%) 36 (4·2%) 1 (0·4%) 2·8% 6 3193 (8·0%) 41 (4·8%) 1 (0·4%) 2·4% 5 3503 (8·8%) 52 (6·0%) 0 0% 4 3697 (9·2%) 35 (4·1%) 1 (0·4%) 2·9% 3 4247 (10·6%) 30 (3·5%) 1 (0·4%) 3·3% 2 4368 (10·9%) 31 (3·6%) 1 (0·4%) 3·2% 1 8381 (21·0%) 34 (3·9%) 1 (0·4%) 2·9% Missing score 306 (0·8%) 5 (0·6%) 0 0% Data are n (%) or %. Table 3: Distribution of artificial intelligence examination risk scores and early screening performance measures, intervention group (post-hoc analysis) 5000 examinations in the group with scores of 1–7 and 212 examinations in the group with scores of 8–9.\n\nDiscussion\nThis clinical safety analysis showed that a screen-reading procedure using an AI tool to triage screening examinations to single or double reading and with use of AI as detection support in mammography screening was safe, because the cancer detection rate (6·1 per 1000 participants screened) was above the prespecified lower limit for safety, and was similar to that of double reading without AI (5·1 per 1000). The use of AI did not influence the rates of recalls, false positives, or consensus meetings, while the screen-reading workload was reduced by almost half. The MASAI trial aims to answer two key questions on the use of AI in mammography screening. The first question regards whether AI can be safely used to reduce the screen-reading workload with sustained performance, which we addressed in the current report. The second key question regards the effect on screening outcome, with a primary outcome of interval cancer rate, a central indicator of screening efficacy.1,6 The full study population of 100 000 screened participants and a 2-year follow-up is needed to investigate this endpoint. The MASAI trial proposes one of several possible strategies of integrating AI in the screen-reading pathway.13,20 In European screening programmes, in which double reading is standard, AI has been suggested to replace one of the readers, to be used as a standalone reader for examinations with low AI risk scores, to force examinations with high AI risk scores to a consensus meeting or to arbitration, or to automatically recall cases above a specific threshold. Different strategies can also be considered regarding reader access to AI information: having it available at the time of screening or, for example, only at the consensus meeting to limit automation bias. Our strategy was to use AI to triage examinations to single or double reading and to let radiologists have access to AI information in the form of risk scores and CAD marks at the time of screen reading. The rationale underlying this design was to take advantage of the bias introduced by AI. We hypothesised that, in addition to the benefit of CAD marks as detection support, knowledge of disease prevalence would influence radiologists’ operator point and thereby reduce false positives when reading low-risk examinations (in addition to single reading itself leading to fewer false positives4) and reduce false negatives when reading high-risk examinations.21 Access to risk scores and CAD marks did not seem to introduce a detrimental automation bias, since the false positive rate remained unchanged. This finding emphasises the importance of radiologists having the final decision to recall, which, besides reducing false positives, constitutes a practical approach to meeting established medicolegal requirements, as opposed to the current ethical and legal uncertainties of using AI as a standalone reader. However, if results from prospective studies show that use of AI in screen reading is safe, it could potentially lead to overreliance on AI and cause an increased risk of detrimental automation bias over time. In our study, access to AI information also enabled the fast and safe handling of screening examinations with a very high probability of cancer. These examinations were flagged in the PACS worklist and could therefore be prioritised for a timely and scrutinised screen reading. This approach was effective as indicated by the findings that the 1·2% of screening examinations flagged as extra high risk contained 55·7% of all screen-detected cancers in this group. AI-supported screening resulted in 20% more cancers (244 vs 203) being detected than with standard screening. 152 stage T1 invasive cancers were detected in the intervention group compared with 129 in the control group, which might indicate an increase in early detection without the need for supplementary imaging methods. The incremental increase was, however, not as large as that observed with digital breast tomosynthesis screening in a previous study.22 Still, the higher cancer detection with tomosynthesis compared with mammography in screening has not convincingly been shown to translate into a reduction of interval cancers,22 which could question its clinical importance since it is also a more resource-demanding technique. The clinical significance of the additional detected invasive cancers in our study remains to evaluated. The evolution of AI over time could affect all available tests for breast cancer screening, but the use of AI in tomosynthesis screening has not yet been evaluated in a prospective study. cancer is more likely with low-grade cancers, since they might never progress into a clinically relevant event during the patient’s lifetime.23 Hence, the planned characterisation of detected cancers in the full study population will bring some clarity to possible overdiagnosis with AI-supported screening. Fenton and colleagues showed a 34% increase in the detection of in situ cancers (from 1·17 to 1·57 per 1000 screening mammograms, p=0·09) after the implementation of conventional CAD in screening but without a parallel increase in the detection of invasive cancer.24 Conventional CAD was also shown to increase false positives and related costs, and its use in screening could ultimately not be justified.2,24–27 AI thus seems to have improved performance compared with that of conventional CAD, but could still have hypersensitivity to calcifications, a typical presentation of in situ cancers.13 Subsequent screening will show whether the relatively higher detection observed in our trial is a result of screening with a more sensitive technique for the first time (ie, a prevalence effect), causing an initial high incidence that levels out during subsequent screening rounds.28 We found that the benefit of AI-supported screening in terms of screen-reading workload reduction was considerable. The actual time saved was not measured, but, if we assume that a radiologist reads on average 50 screening examinations per hour, it would have taken one radiologist 4·6 months less to read the 46 345 screening examinations in the intervention group compared with the 83 231 in the control group. There was concern about whether AI would lead to an increase in cases referred to consensus meetings, considering the eventual need to discuss CAD findings and the possible reader anxiety arising from single reading. Consensus meetings constitute an important step to increase the specificity, but are resource demanding.3 Contrary to expectations, the proportion of screenings that led to a consensus meeting was not affected by the use of AI. These results are promising and can be used to inform new trials and programme-based evaluations to address the radiologist shortage. However, we still need to improve our understanding of what the implications are for patient outcome—most importantly, the effect on interval cancer rates. We also need to investigate whether the higher detection of small invasive cancers will lead to a subsequent reduction of prognostically significant cancers and whether the frequency of in situ cancers detected will be reduced at subsequent screenings. An analysis of the prognostic characteristics of cancers detected in the full study population of the MASAI trial is underway. Furthermore, AI systems come at a financial cost, and, while the market and its business models might develop, the willingness to pay to reduce the workload must be determined. Costeffectiveness can be determined only when the downstream cost of the intervention has been assessed. screening and can thus provide evidence for clinical implementation. A strength of the study is the close resemblance to a real screening setting, since no exclusion criteria were applied and few people invited for screening opted out. The main limitation of this study is that it was conducted at a single centre in the Swedish screening programme. The study was also limited to the combination of one type of mammography device and one AI system. AI system performance will inevitably vary with technical factors such as AI algorithms and image processing,12 but will probably be of less importance than the variability of radiologists. Our screening strategy emphasises the central role of the radiologist to make the final decision to recall a patient, and the present results are dependent on the performance of the participating radiologists. The radiologists participating in this trial were, overall, moderately to highly experienced in breast imaging, which could also limit the generalisability of our findings to some settings (eg, those with predominantly lessexperienced screen readers, which have been shown to have a higher rate of false positives29), and the qualification for single reading with AI is likely to require revision. Only readers with more than 2 years of experience were allowed to conduct single reading in the intervention group, which could have introduced a bias in reader performance in relation to the control group. Because only one of the 16 participating radiologists had less than 2 years of experience, we do not believe this factor would have had a major influence on the results. Another limitation of the reported results was that the true false positive rate requires a follow-up period in case of later interval cancer diagnosis. Because this is an uncommon event, we do not expect the false positive rate to change substantially. Results from this study must be considered in the context of AI being a constantly evolving field with continuously updated algorithms and machine learning models, and the related challenge of the transparency of these updates.30 To mitigate the implications of these updates and estimate the effect on screening performance, mammograms from the MASAI trial will be stored in a data warehouse to enable a reanalysis with updated algorithms using the trial outcome data as a reference. More importantly, implementing AI systems would require having a monitoring system of algorithm performance in place. In summary, this clinical safety analysis of the MASAI trial, in which an AI system was used to triage screening examinations to single or double reading and as detection support, showed that AI-supported mammography screening can be considered safe, since it resulted in a similar rate of screen-detected cancer—exceeding the lowest acceptable limit for safety—without increasing rates of recalls, false positives, or consensus meetings, and while substantially reducing the screen-reading workload compared with screening by means of standard double reading.\n\nAcknowledgments\nWe thank the funders of the trial: the Swedish Cancer Society (21 1631Pj, 22 0611FE), the Confederation of Regional Cancer Centres (21/00060), and Swedish governmental funding of clinical research (ALF; 2020-Projekt0079, 2022-Projekt0100). We also thank the staff at the Unilabs Mammography Unit at Skåne University Hospital for making this study possible; Unilabs, Sectra, and ScreenPoint Medical for the technical support; and the participants involved in the trial.\n\nContributors KL and AR conceptualised and designed the trial with input from IA and SH. AR did the statistical analysis. KL, AR, VJ, and HS directly accessed and verified the underlying data reported in the manuscript. All authors were involved in data interpretation. KL wrote the first draft of the report with input from AR. All authors revised the report and provided important intellectual content. All authors had full access to all the data in the study and had final responsibility for the decision to submit for publication. Declaration of interests KL has been an advisory board member for Siemens Healthineers and has received lecture honorarium from AstraZeneca. SH is head of BreastScreen Norway at the Cancer Registry of Norway which has a research agreement with Screenpoint Medical. All other authors declare no competing interests. Data sharing De-identified data will be made available upon reasonable request, with investigator support and a signed data access agreement. A proposal should be submitted to be reviewed by the study steering committee. The data are not publicly available due to data protection regulations. The study protocol and statistical analysis plan are available online at https://portal.research.lu.se/en/projects/mammography-screening-withartificial-intelligence. References\n1 Schünemann HJ, Lerda D, Quinn C, et al. Breast cancer screening and diagnosis: a synopsis of the European Breast Guidelines. Ann Intern Med 2020; 172: 46–56. 2 Taylor P, Potts HWW. Computer aids and human second reading as interventions in screening mammography: two systematic reviews to compare effects on cancer detection and recall rate. Eur J Cancer 2008; 44: 798–807. 3 Taylor-Phillips S, Stinton C. Double reading in breast cancer screening: considerations for policy-making. Br J Radiol 2020; 93: 20190610. 4 Posso M, Carles M, Rué M, Puig T, Bonfill X. Cost-effectiveness of double reading versus single reading of mammograms in a breast cancer screening programme. PLoS One 2016; 11: e0159806. 5 Gulland A. Staff shortages are putting UK breast cancer screening “at risk,” survey finds. BMJ 2016; 353: i2350. 6 Houssami N, Hunter K. The epidemiology, radiology and biological characteristics of interval breast cancers in population mammography screening. NPJ Breast Cancer 2017; 3: 12. 7 Hovda T, Hoff SR, Larsen M, Romundstad L, Sahlberg KK, Hofvind S. True and missed interval cancer in organized mammographic screening: a retrospective review study of diagnostic and prior screening mammograms. Acad Radiol 2022; 29 (suppl 1): S180–91. 8 Hofvind S, Skaane P, Vitak B, et al. Influence of review design on percentages of missed interval breast cancers: retrospective study of interval cancers in a population-based screening program. Radiology 2005; 237: 437–43. 9 Sechopoulos I, Teuwen J, Mann R. Artificial intelligence for breast cancer detection in mammography and digital breast tomosynthesis: state of the art. Semin Cancer Biol 2021; 72: 214–25. 10 Rodriguez-Ruiz A, Lång K, Gubern-Merida A, et al. Stand-alone artificial intelligence for breast cancer detection in mammography: comparison with 101 radiologists. J Natl Cancer Inst 2019; 111: 916–22. 11 McKinney SM, Sieniek M, Godbole V, et al. International evaluation of an AI system for breast cancer screening. Nature 2020; 577: 89–94. 12 Salim M, Wåhlin E, Dembrower K, et al. External evaluation of 3 commercial artificial intelligence algorithms for independent assessment of screening mammograms. JAMA Oncol 2020; 6: 1581–88. 13 Freeman K, Geppert J, Stinton C, et al. Use of artificial intelligence for image analysis in breast cancer screening programmes: systematic review of test accuracy. BMJ 2021; 374: n1872. 14 Rodriguez-Ruiz A, Lång K, Gubern-Merida A, et al. Can we reduce the workload of mammographic screening by automatic identification of normal exams with artificial intelligence? A feasibility study. Eur Radiol 2019; 29: 4825–32. 15 Dembrower K, Wåhlin E, Liu Y, et al. Effect of artificial intelligencebased triaging of breast cancer screening mammograms on cancer detection and radiologist workload: a retrospective simulation study. Lancet Digit Health 2020; 2: e468–74. 16 Larsen M, Aglen CF, Lee CI, et al. Artificial intelligence evaluation of 122 969 mammography examinations from a population-based screening program. Radiology 2022; 303: 502–11. 17 Lauritzen AD, Rodríguez-Ruiz A, von Euler-Chelpin MC, et al. An artificial intelligence-based mammography screening protocol for breast cancer: outcome and radiologist workload. Radiology 2022; 304: 41–49. 18 Lång K, Hofvind S, Rodríguez-Ruiz A, Andersson I. Can artificial intelligence reduce the interval cancer rate in mammography screening? Eur Radiol 2021; 31: 5940–47. 19 Byng D, Strauch B, Gnas L, et al. AI-based prevention of interval cancers in a national mammography screening program. Eur J Radiol 2022; 152: 110321. 20\t Larsen M, Aglen CF, Hoff SR, Lund-Hanssen H, Hofvind S. Possible strategies for use of artificial intelligence in screen-reading of mammograms, based on retrospective data from 122,969 screening examinations. Eur Radiol 2022; 32: 8238–46. 21 Evans KK, Birdwell RL, Wolfe JM. If you don’t find it often, you often don’t find it: why some cancers are missed in breast cancer screening. PLoS One 2013; 8: e64366. 22\t Houssami N, Hofvind S, Soerensen AL, et al. Interval breast cancer rates for digital breast tomosynthesis versus digital mammography population screening: an individual participant data meta-analysis. eClinicalMedicine 2021; 34: 100804. 23\t van Luijt PA, Heijnsdijk EAM, Fracheboud J, et al. The distribution of ductal carcinoma in situ (DCIS) grade in 4232 women and its impact on overdiagnosis in breast cancer screening. Breast Cancer Res 2016; 18: 47. 24 Fenton JJ, Taplin SH, Carney PA, et al. Influence of computer-aided detection on performance of screening mammography. N Engl J Med 2007; 356: 1399–409. 25\t Lehman CD, Wellman RD, Buist DSM, Kerlikowske K, Tosteson ANA, Miglioretti DL. Diagnostic accuracy of digital screening mammography with and without computer-aided detection. JAMA Intern Med 2015; 175: 1828–37. 26\t Elmore JG, Lee CI. Artificial Intelligence in medical imaging: learning from past mistakes in mammography. JAMA Health Forum 2022; 3: e215207. 27 Fenton JJ. Is it time to stop paying for computer-aided mammography? JAMA Intern Med 2015; 175: 1837–38. 28\t Gur D, Nishikawa RM, Sumkin JH. New screening technologies and practices: a different approach to estimation of performance improvement by using data from the transition period. Radiology 2015; 275: 9–12. 29\t Alberdi RZ, Llanes AB, Ortega RA, et al. Effect of radiologist experience on the risk of false-positive results in breast cancer screening programs. Eur Radiol 2011; 21: 2083–90. 30\t Larsson SHF, Heintz F. Transparency in artificial intelligence. Internet Policy Rev 2020; 9: 1469.\n--- FOOTNOTES Radiology, Department of of Oncology, Department of (A-M Larsson PhD); Department Correspondence to: Department of Translational", "persona": "General Public", "response": "### 1. Overview\nNew research found that AI-supported mammogram screening was 20% more likely to detect breast cancer than trained doctors. This is particularly important as the United States is facing both a shortage of radiologists and an aging population that requires more imaging as part of their health care. AI-supported breast cancer screening isn’t perfect yet, but it could help lower overburdened doctors’ workloads in the future and make mammogram readings more accurate. “A challenging part of reading screening exams is that the vast majority of exams are normal,” said Kristina Lång, MD, PhD, an associate professor of radiology diagnostics from Lund University in Sweden, who led the study. Half the scans were read by two highly-trained radiologists, and the other half were read by both an experienced radiologist and AI. Radiologists were able to detect some cases of cancer that the AI missed, and the synergistic pairing of human and computer did not increase the risk of false negatives—when a person has cancer that goes undetected during a mammogram reading. AI algorithms are trained using millions of cases of breast cancer, which gives the computers a leg up when it comes to detecting abnormalities in scans, Bennett told Health.\n\n### 2. Background\nOnce an AI algorithm identifies an abnormality, it takes images of the perceived abnormality that a trained radiologist can evaluate to determine whether or not additional testing—such as an ultrasound or biopsy—is needed. Bennett noted that there’s still a lot to learn about AI. “Because the technology is so new, there is a lack of data on the impact of AI in mammography,” she said. “We don't know what the true benefit or harm of the technology is.” Is AI the Future of Mammography? The FDA has already approved several AI technologies for breast cancer screening, but they aren’t mainstream yet.\n\n### 3. Methods\nThe American Cancer Society recommends women who are not at higher risk for breast cancer start annual mammograms by age 45 and should have the choice to start as early as 40. The U.S. Preventive Services Task Force (USPSTF) recently released new mammogram guidelines stating that breast cancer screening should start at 40 and occur every other year until age 74 in most women. There are many factors, including breast density, that can make it difficult to detect tumors with the human eye. For the randomized, controlled, population-based trial, Lång and her team analyzed mammograms from 80,000 Swedish women who underwent the scans between April 2021 and July 2022.\n\n### 4. Results\nAccording to the University of Rochester Medical Center, about 10% of people who get a mammogram require additional testing, but only about 0.5% are found to have cancer. In the U.S., about 240,000 cases of breast cancer are diagnosed in women every year. Early detection is paramount to outcomes. The earlier cancer is caught, the more effective treatment will likely be. That's why the Food and Drug Administration (FDA) recently amended the Mammography Quality Standards Act (MQSA) of 1992, a law passed to ensure quality mammography, to require mammography clinics to notify people who are found to have dense breasts.\n\n### 5. Takeaways\nResearchers emphasized that the technology needs to be used in tandem with a radiologist, but it could make the screening process more accurate and efficient. The computer-assisted group detected 20% more cancers in the scans than the human-only team. In raw numbers, this works out to be a detection rate of 6 per 1,000 women screened, compared to 5 per 1,000 with standard reading. The technology isn’t designed to be used on its own but appears to be quite useful when used in tandem with a trained eye. “The number of pixels in each mammogram far exceeds what our eyes and brains can process,” she said.", "paper_id": "54", "story_id": "54-02"}
{"paper": "Abstract\nWe show for the ﬁrst time that learning powerful representations from speech audio alone followed by ﬁne-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task deﬁned over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.1 1\n\nIntroduction\nNeural networks beneﬁt from large quantities of labeled training data. However, in many settings labeled data is much harder to come by than unlabeled data: current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide [31]. Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them - a process that requires learning good representations of speech. In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to ﬁne-tune the model on labeled data. This has been particularly successful for natural language processing [43, 45, 9] and is an active research area for computer vision [20, 2, 36, 19, 6]. In this paper, we present a framework for self-supervised learning of representations from raw audio data. Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations [26, 56], similar to masked language modeling [9]. The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors [54, 49, 48, 28] (§ 2). As part of training, we learn discrete speech units [53, 32, 7, 18] via a gumbel softmax [24, 5] to represent the latent representations in the contrastive task (Figure 1) which we ﬁnd to be more effective than non-quantized targets. After pre-training on unlabeled speech, the model is ﬁne-tuned Context representations C … … Quantized representations Q q q q q q Latent speech representations Z X raw waveform Figure 1: Illustration of our framework which jointly learns contextualized speech representations and an inventory of discretized speech units. on labeled data with a Connectionist Temporal Classiﬁcation (CTC) loss [14, 4] to be used for downstream speech recognition tasks (§ 3) Previous work learned a quantization of the data followed by a contextualized representations with a self-attention model [5, 4], whereas our approach solves both problems end-to-end. Masking parts of the input with Transformer networks for speech has been explored [4, 26], but prior work relies either on a two-step pipeline or their model is trained by reconstructing the ﬁlter bank input features. Other related work includes learning representations from auto-encoding the input data [52, 11] or directly predicting future timesteps [8]. Our results show that jointly learning discrete speech units with contextualized representations achieves substantially better results than ﬁxed units learned in a prior step [4]. We also demonstrate the feasibility of ultra-low resource speech recognition: when using only 10 minutes of labeled data, our approach achieves word error rate (WER) 4.8/8.2 on the clean/other test sets of Librispeech. We set a new state of the art on TIMIT phoneme recognition as well as the 100 hour clean subset of Librispeech. Moreover, when we lower the amount of labeled data to just one hour, we still outperform the previous state of the art self-training method of [42] while using 100 times less labeled data and the same amount of unlabeled data. When we use all 960 hours of labeled data from Librispeech, then our model achieves 1.8/3.3 WER (§ 4, § 5).\n\n2 Model Our model is composed of a multi-layer convolutional feature encoder f : X 7→Z which takes as input raw audio X and outputs latent speech representations z1, . . . , zT for T time-steps. They are then fed to a Transformer g : Z 7→C to build representations c1, . . . , cT capturing information from the entire sequence [9, 5, 4]. The output of the feature encoder is discretized to qt with a quantization module Z 7→Q to represent the targets (Figure 1) in the self-supervised objective (§ 3.2). Compared to vq-wav2vec [5], our model builds context representations over continuous speech representations and self-attention captures dependencies over the entire sequence of latent representations end-to-end. Feature encoder. The encoder consists of several blocks containing a temporal convolution followed by layer normalization [1] and a GELU activation function [21]. The raw waveform input to the encoder is normalized to zero mean and unit variance. The total stride of the encoder determines the number of time-steps T which are input to the Transformer (§ 4.2). Contextualized representations with Transformers. The output of the feature encoder is fed to a context network which follows the Transformer architecture [55, 9, 33]. Instead of ﬁxed positional embeddings which encode absolute positional information, we use a convolutional layer similar to [37, 4, 57] which acts as relative positional embedding. We add the output of the convolution followed by a GELU to the inputs and then apply layer normalization. ` Contrastive loss L Transformer Masked CNN results in prior work which learned discrete units in a ﬁrst step followed by learning contextualized representations [5]. Product quantization amounts to choosing quantized representations from multiple codebooks and concatenating them. Given G codebooks, or groups, with V entries e ∈ RV ×d/G, we choose one entry from each codebook and concatenate the resulting vectors e1, . . . , eG and apply a linear transformation Rd 7→Rf to obtain q ∈Rf. The Gumbel softmax enables choosing discrete codebook entries in a fully differentiable way [16, 24, 35]. We use the straight-through estimator [26] and setup G hard Gumbel softmax operations [24]. The feature encoder output z is mapped to l ∈RG×V logits and the probabilities for choosing the v-th codebook entry for group g are pg,v = exp(lg,v + nv)/τ PV k=1 exp(lg,k + nk)/τ , (1) where τ is a non-negative temperature, n = −log(−log(u)) and u are uniform samples from U(0, 1). During the forward pass, codeword i is chosen by i = argmaxjpg,j and in the backward pass, the true gradient of the Gumbel softmax outputs is used. 3 Training To pre-train the model we mask a certain proportion of time steps in the latent feature encoder space (§ 3.1), similar to masked language modeling in BERT [9]. The training objective requires identifying the correct quantized latent audio representation in a set of distractors for each masked time step (§ 3.2) and the ﬁnal model is ﬁne-tuned on the labeled data (§ 3.3). 3.1 Masking We mask a proportion of the feature encoder outputs, or time steps before feeding them to the context network and replace them with a trained feature vector shared between all masked time steps; we do not mask inputs to the quantization module. To mask the latent speech representations output by the encoder, we randomly sample without replacement a certain proportion p of all time steps to be starting indices and then mask the subsequent M consecutive time steps from every sampled index; spans may overlap. 3.2 Objective During pre-training, we learn representations of speech audio by solving a contrastive task Lm which requires to identify the true quantized latent speech representation for a masked time step within a set of distractors. This is augmented by a codebook diversity loss Ld to encourage the model to use the codebook entries equally often. L = Lm + αLd (2) where α is a tuned hyperparameter. Contrastive Loss. Given context network output ct centered over masked time step t, the model needs to identify the true quantized latent speech representation qt in a set of K + 1 quantized candidate representations ˜q ∈Qt which includes qt and K distractors [23, 54]. Distractors are uniformly sampled from other masked time steps of the same utterance. The loss is deﬁned as Lm = −log exp(sim(ct, qt)/κ) P ˜q∼Qt exp(sim(ct, ˜q)/κ) (3) where we compute the cosine similarity sim(a, b) = aT b/∥a∥∥b∥between context representations and quantized latent speech representations [19, 6]. codebook ¯pg across a batch of utterances; the softmax disribution does not contain the gumbel noise nor a temperature:2 G X Ld = 1 GV g=1 −H(¯pg) = 1 GV 3.3 Fine-tuning Pre-trained models are ﬁne-tuned for speech recognition by adding a randomly initialized linear projection on top of the context network into C classes representing the vocabulary of the task [4]. For Librispeech, we have 29 tokens for character targets plus a word boundary token. Models are optimized by minimizing a CTC loss [14] and we apply a modiﬁed version of SpecAugment [41] by masking to time-steps and channels during training which delays overﬁtting and signiﬁcantly improves the ﬁnal error rates, especially on the Libri-light subsets with few labeled examples. 4 Experimental Setup 4.1 Datasets As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio. We ﬁne-tune on ﬁve labeled data settings: 960 hours of transcribed Librispeech, the train-clean-100 subset comprising 100 hours (100 hours labeled), as well as the Libri-light limited resource training subsets originally extracted from Librispeech, these are train-10h (10 hours labeled), train-1h (1 hour labeled), train-10min (10 min labeled). We follow the evaluation protocol of Libri-light for these splits and evaluate on the standard Librispech dev-other/clean and test-clean/other sets. We ﬁne-tune the pre-trained models for phoneme recognition on the TIMIT dataset [13]. It contains ﬁve hours of audio recordings with detailed phoneme labels. We use the standard train, dev and test split and follow the standard protocol of collapsing phone labels to 39 classes. 4.2 Pre-training Models are implemented in fairseq [39]. For masking, we sample p = 0.065 of all time-steps to be starting indices and mask the subsequent M = 10 time-steps. This results in approximately 49% of all time steps to be masked with a mean span length of 14.7, or 299ms (see Appendix A for more details on masking). The feature encoder contains seven blocks and the temporal convolutions in each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2). This results in an encoder output frequency of 49 hz with a stride of about 20ms between each sample, and a receptive ﬁeld of 400 input samples or 25ms of audio. The convolutional layer modeling relative positional embeddings has kernel size 128 and 16 groups. We experiment with two model conﬁgurations which use the same encoder architecture but differ in the Transformer setup: BASE contains 12 transformer blocks, model dimension 768, inner dimension (FFN) 3,072 and 8 attention heads. Batches are built by cropping 250k audio samples, or 15.6sec, from each example. Crops are batched together to not exceed 1.4m samples per GPU and we train on a total of 64 V100 GPUs for 1.6 days [38]; the total batch size is 1.6h. The LARGE model contains 24 transformer blocks with model dimension 1,024, inner dimension 4,096 and 16 attention heads. We crop 320k audio samples, or 20sec, with a limit of 1.2m samples per GPU and train on 128 V100 GPUs over 2.3 days for Librispeech and 5.2 days for LibriVox; the total batch size is 2.7h. We use dropout 0.1 in the Transformer, at the output of the feature encoder and the input to the quantization module. Layers are dropped at a rate of 0.05 for BASE and 0.2 for LARGE [22, 12]; there is no layer drop for LV-60k. G X V X v=1 ¯pg,v log ¯pg,v (4) g=1 We optimize with Adam [29], warming up the learning rate for the ﬁrst 8% of updates to a peak of 5 × 10−4 for BASE and 3 × 10−4 for LARGE, and then linearly decay it. LARGE trains for 250k updates, BASE for 400k updates, and LARGE on LV-60k for 600k updates. We use weight α = 0.1 for the diversity loss Equation 2. For the quantization module we use G = 2 and V = 320 for both models, resulting in a theoretical maximum of 102.4k codewords. Entries are of size d/G = 128 for BASE amd d/G = 384 for LARGE. The Gumbel softmax temperature τ is annealed from 2 to a minimum of 0.5 for BASE and 0.1 for LARGE by a factor of 0.999995 at every update. The temperature in the contrastive loss (Equation 3) is set to κ = 0.1. For the smaller Librispeech dataset, we regularize the model by applying an L2 penalty to the activations of the ﬁnal layer of the feature encoder and scale down the gradients for the encoder by a factor of 10. We also use a slightly different encoder architecture where we do not use layer normalization, and instead of normalizing the raw waveform, the output of the ﬁrst encoder layer is normalized. In the contrastive loss we use K = 100 distractors. We choose the training checkpoint with the lowest Lm on the validation set. 4.3 Fine-tuning After pre-training we ﬁne-tune the learned representations on labeled data and add a randomly initialized output layer on top of the Transformer to predict characters (Librispeech/Libri-light) or phonemes (TIMIT). For Libri-light, we train three seeds with two different learning rates (2e-5 and 3e-5) for all subsets and choose the conﬁguration with lowest WER on dev-other subset decoded with the ofﬁcial 4-gram language model (LM) with beam 50 and ﬁxed model weights (LM weight 2, word insertion penalty -1). For BASE on the labeled 960h subset we use a learning rate of 1e-4. We optimize with Adam and a tri-state rate schedule where the learning rate is warmed up for the ﬁrst 10% of updates, held constant for the next 40% and then linearly decayed for the remainder. BASE uses a batch size of 3.2m samples per GPU and we ﬁne-tune on 8 GPUs, giving a total batch size of 1,600sec. LARGE batches 1.28m samples on each GPU and we ﬁne-tune on 24 GPUs, resulting in an effective batch size of 1,920sec. For the ﬁrst 10k updates only the output classiﬁer is trained, after which the Transformer is also updated. The feature encoder is not trained during ﬁne-tuning. We mask the feature encoder representations with a strategy similar to SpecAugment [41] detailed in Appendix B. 4.4 Language Models and Decoding We consider two types of language models (LM): a 4-gram model and a Transformer [3] trained on the Librispeech LM corpus. The Transformer LM is identical to [51] and contains 20 blocks, model dimension 1,280, inner dimension 6,144 and 16 attention heads. We tune the weights of the language model (interval [0, 5]) and a word insertion penalty ([−5, 5]) via Bayesian optimization3: we run 128 trials with beam 500 for the 4-gram LM and beam 50 for the Transformer LM and choose the best set of weights according to performance on dev-other. Test performance is measured with beam 1,500 for the n-gram LM and beam 500 for the Transformer LM. We use the beam search decoder of [44]. 5\n\nResults\n5.1 Low-Resource Labeled Data Evaluation We ﬁrst evaluate our pre-trained models in settings where the amount of labeled data is limited to get a sense of how the representations learned on unlabeled data can improve low resource settings. If a pre-trained model captures the structure of speech, then it should require few labeled examples to ﬁne-tune it for speech recognition. The models are pre-trained on the audio data of either Librispeech (LS-960) or LibriVox (LV-60k) and most results are obtained by decoding with a Transformer language model (Transf.); Appendix C shows results with no language model at all as well as with an n-gram language model. The LARGE model pre-trained on LV-60k and ﬁne-tuned on only 10 minutes of labeled data achieves a word error rate of 5.2/8.6 on the Librispeech clean/other test sets. Ten minutes of labeled data corresponds to just 48 recordings with an average length of 12.5 seconds. This demonstrates that ultra-low resource speech recognition is possible with self-supervised learning on unlabeled data. Table 1: WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech. Models use either the audio of Librispeech (LS-960) or the larger LibriVox (LV-60k) as unlabeled data. We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters). Prior work used 860 unlabeled hours (LS-860) but the total with labeled data is 960 hours and comparable to our setup. Model Unlabeled LM dev test data clean other clean other 10 min labeled Discrete BERT [4] LS-960 4-gram 15.7 24.1 16.3 25.2 BASE LS-960 4-gram 8.9 15.7 9.1 15.6 Transf. 6.6 13.2 6.9 12.9 LARGE LS-960 Transf. 6.6 10.6 6.8 10.8 LV-60k Transf. 4.6 7.9 4.8 8.2 1h labeled Discrete BERT [4] LS-960 4-gram 8.5 16.4 9.0 17.6 BASE LS-960 4-gram 5.0 10.8 5.5 11.3 Transf. 3.8 9.0 4.0 9.3 LARGE LS-960 Transf. 3.8 7.1 3.9 7.6 LV-60k Transf. 2.9 5.4 2.9 5.8 10h labeled Discrete BERT [4] LS-960 4-gram 5.3 13.2 5.9 14.1 Iter. pseudo-labeling [58] LS-960 4-gram+Transf. 23.51 25.48 24.37 26.02 LV-60k 4-gram+Transf. 17.00 19.34 18.03 19.92 BASE LS-960 4-gram 3.8 9.1 4.3 9.5 Transf. 2.9 7.4 3.2 7.8 LARGE LS-960 Transf. 2.9 5.7 3.2 6.1 LV-60k Transf. 2.4 4.8 2.6 4.9 100h labeled Hybrid DNN/HMM [34] - 4-gram 5.0 19.5 5.8 18.6 TTS data augm. [30] - LSTM 4.3 13.5 Discrete BERT [4] LS-960 4-gram 4.0 10.9 4.5 12.1 Iter. pseudo-labeling [58] LS-860 4-gram+Transf. 4.98 7.97 5.59 8.95 LV-60k 4-gram+Transf. 3.19 6.14 3.72 7.11 Noisy student [42] LS-860 LSTM 3.9 8.8 4.2 8.6 BASE LS-960 4-gram 2.7 7.9 3.4 8.0 Transf. 2.2 6.3 2.6 6.3 LARGE LS-960 Transf. 2.1 4.8 2.3 5.0 LV-60k Transf. 1.9 4.0 2.0 4.0 Our approach of jointly learning discrete units and contextualized representations clearly improves over previous work which learned quantized audio units in a separate step [4], reducing WER by a about a third. A recent iterative self-training approach [42] represents the state of the art on the clean 100 hour subset of Librispeech but it requires multiple iterations of labeling, ﬁltering, and re-training. Our approach is simpler: we pre-train on the unlabeled data and ﬁne-tune on the labeled data. On the 100 hour subset of Librispeech, their method achieves WER 4.2/8.6 on test-clean/other which compares to WER 2.3/5.0 with the LARGE model in a like for like setup, a relative WER reduction of 45%/42%. Table 2: WER on Librispeech when using all 960 hours of labeled data (cf. Table 1). Model Unlabeled LM dev test data clean other clean other Supervised CTC Transf [51] - CLM+Transf. 2.20 4.94 2.47 5.45 S2S Transf. [51] - CLM+Transf. 2.10 4.79 2.33 5.17 Transf. Transducer [60] - Transf. - - 2.0 4.6 ContextNet [17] - LSTM 1.9 3.9 1.9 4.1 Conformer [15] - LSTM 2.1 4.3 1.9 3.9 Semi-supervised CTC Transf. + PL [51] LV-60k CLM+Transf. 2.10 4.79 2.33 4.54 S2S Transf. + PL [51] LV-60k CLM+Transf. 2.00 3.65 2.09 4.11 Iter. pseudo-labeling [58] LV-60k 4-gram+Transf. 1.85 3.26 2.10 4.01 Noisy student [42] LV-60k LSTM 1.6 3.4 1.7 3.4 This work LARGE - from scratch - Transf. 1.7 4.3 2.1 4.6 BASE LS-960 Transf. 1.8 4.7 2.1 4.8 LARGE LS-960 Transf. 1.7 3.9 2.0 4.1 LV-60k Transf. 1.6 3.0 1.8 3.3 light data splits contain both clean and noisy data leading to better accuracy on test-other compared to test-clean. Increasing model size reduces WER on all setups with the largest improvements on test-other (BASE vs. LARGE both on LS-960) and increasing the amount of unlabeled training data also leads to large improvements (LARGE LS-960 vs. LV-60k). 5.2 High-Resource Labeled Data Evaluation on Librispeech In this section we evaluate the performance when large quantities of labeled speech are available to assess the effectiveness of our approach in a high resource setup. Speciﬁcally, we ﬁne-tune the same models as before on the full 960 hours of labeled Librispeech: BASE and LARGE pre-trained on LS-960 as well as LARGE pre-trained on LV-60k. Table 2 shows that our approach achieves WER 1.8/3.3 on test-clean/other on the full Librispeech benchmark. This is despite a weaker baseline architecture: supervised training of our architecture achieves WER 2.1/4.6 (LARGE - from scratch) compared to WER 1.9/4.1 for ContextNet [17], the baseline architecture of the state of the art [42]. We use a simple Transformer with CTC which does not perform as well as seq2seq models [51]. Note that the vocabulary of our acoustic model (characters) does not match the vocabulary of the LM (words) which delays feedback from the LM and is likely to be detrimental. Most recent work [51, 58, 17, 42] uses the better performing word pieces [50] for both models. Moreover, our result is achieved without any data balancing such as [42]. Finally, self-training is likely complimentary to pre-training and their combination may yield even better results. Appendix E presents a detailed error analysis of our pre-trained models in various labeled data setups. 5.3 Phoneme Recognition on TIMIT Next, we evaluate accuracy on TIMIT phoneme recognition by ﬁne-tuning the pre-trained models on the labeled TIMIT training data. We ﬁne-tune as for the 10 hour subset of Libri-light but do not use a language model. Table 3 shows that our approach can achieve a new state of the art on this dataset, reducing PER by a relative 23%/29% over the next best result on the dev/test sets. Appendix D shows an analysis of how the discrete latent speech representations related to phonemes. Other recent work on pre-training which evaluates on TIMIT includes [47] who solve multiple tasks to learn good representations of speech. Table 3: TIMIT phoneme recognition accuracy in terms of phoneme error rate (PER). CNN + TD-ﬁlterbanks [59] 15.6 18.0 PASE+ [47] - 17.2 Li-GRU + fMLLR [46] – 14.9 wav2vec [49] 12.9 14.7 vq-wav2vec [5] 9.6 11.6 This work (no LM) LARGE (LS-960) 7.4 8.3 Table 4: Average WER and standard deviation on combined dev-clean/other of Librispeech for three training seeds. We ablate quantizing the context network input and the targets in the contrastive loss. Continuous inputs, quantized targets (Baseline) 7.97 0.02 Quantized inputs, quantized targets 12.18 0.41 Quantized inputs, continuous targets 11.18 0.16 Continuous inputs, continuous targets 8.58 0.08 5.4 Ablations A difference to previous work [5, 4] is that we quantize the latent audio representations only for the contrastive loss, i.e., when latents are used as targets, but not when the latents are input to the Transformer network. We motivate this choice by an ablating for which we adopt a reduced training setup to increase experimental turn around: we pre-train BASE on LS-960 for 250k updates with masking probability p = 0.075, ﬁne-tune on train-10h for 60k updates on a single GPU with 640k samples per batch, or 40 sec of speech audio. We report the average WER and standard deviation on the concatenation of dev-clean and dev-other (dev PER) for three seeds of ﬁne-tuning. Table 4 shows that our strategy of continuous inputs with quantized targets (Baseline) performs best. Continuous latent speech representations retain more information to enable better context representations and quantizing the target representations leads to more robust training. Quantizing the latents both in the input and the targets performs least well, and explains the lower performance of prior work [5, 4]. Continuous targets reduce the effectiveness of self-supervised training since targets can capture detailed artifacts of the current sequence, e.g. speaker and background information, which make the task easier and prevent the model from learning general representations beneﬁcial to speech recognition. The training accuracy of identifying the correct latent audio representation increases from 62% to 78.0% when switching from quantized to continuous targets. Continuous inputs and continuous targets perform second best but various attempts to improve it did not lead to better results (see Appendix F for this experiment and other ablations on various hyperparameters). 6\n\nConclusion\nWe presented wav2vec 2.0, a framework for self-supervised learning of speech representations which masks latent representations of the raw waveform and solves a contrastive task over quantized speech representations. Our experiments show the large potential of pre-training on unlabeled data for speech processing: when using only 10 minutes of labeled training data, or 48 recordings of 12.5 seconds on average, we achieve a WER of 4.8/8.2 on test-clean/other of Librispeech. Our model achieves results which achieve a new state of the art on the full Librispeech benchmark for noisy speech. On the clean 100 hour Librispeech setup, wav2vec 2.0 outperforms the previous best result while using 100 times less labeled data. The approach is also effective when large amounts of labeled data are available. We expect performance gains by switching to a seq2seq architecture and a word piece vocabulary. dev PER test PER avg. WER std. Broader Impact There are around 7,000 languages in the world and many more dialects. However, for most of them no speech recognition technology exists since current systems require hundreds or thousands of hours of labeled data which is hard to collect for most languages. We have shown that speech recognition models can be built with very small amounts of annotated data at very good accuracy. We hope our work will make speech recognition technology more broadly available to many more languages and dialects.\n\nAcknowledgments\nWe thank Tatiana Likhomanenko and Qiantong Xu for helpful discussion and their help with wav2letter integration.\n\nReferences\n[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv, 2016. [2] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual information across views. In Proc. of NeurIPS, 2019. [3] A. Baevski and M. Auli. Adaptive input representations for neural language modeling. In Proc. of ICLR, 2018. [4] A. Baevski, M. Auli, and A. Mohamed. Effectiveness of self-supervised pre-training for speech recognition. arXiv, abs/1911.03912, 2019. [5] A. Baevski, S. Schneider, and M. Auli. vq-wav2vec: Self-supervised learning of discrete speech representations. In Proc. of ICLR, 2020. [6] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. arXiv, abs/2002.05709, 2020. [7] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord. Unsupervised speech representation learning using wavenet autoencoders. arXiv, abs/1901.08810, 2019. [8] Y. Chung, W. Hsu, H. Tang, and J. R. Glass. An unsupervised autoregressive model for speech representation learning. arXiv, abs/1904.03240, 2019. [9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv, abs/1810.04805, 2018. [10] S. Dieleman, A. van den Oord, and K. Simonyan. The challenge of realistic music generation: modelling raw audio at scale. arXiv, 2018. [11] R. Eloff, A. Nortje, B. van Niekerk, A. Govender, L. Nortje, A. Pretorius, E. Van Biljon,\nE. van der Westhuizen, L. van Staden, and H. Kamper. Unsupervised acoustic unit discovery for\nspeech synthesis using discrete latent-variable neural networks. arXiv, abs/1904.07556, 2019. [12] A. Fan, E. Grave, and A. Joulin. Reducing transformer depth on demand with structured dropout. In Proc. of ICLR, 2020. [13] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM. Linguistic Data Consortium, 1993. [14] A. Graves, S. Fernández, and F. Gomez. Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In Proc. of ICML, 2006. [15] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang. Conformer: Convolution-augmented transformer for speech recognition. arXiv, 2020. [16] E. J. Gumbel. Statistical theory of extreme values and some practical applications: a series of lectures, volume 33. US Government Printing Ofﬁce, 1954. [17] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati, R. Pang, and Y. Wu. Contextnet: Improving convolutional neural networks for automatic speech recognition with global context. arXiv, 2020. [18] D. Harwath, W.-N. Hsu, and J. Glass. Learning hierarchical discrete linguistic units from visually-grounded speech. In Proc. of ICLR, 2020. [19] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. arXiv, abs/1911.05722, 2019. [20] O. J. Hénaff, A. Razavi, C. Doersch, S. M. A. Eslami, and A. van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv, abs/1905.09272, 2019. [21] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv, 2016. [22] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger. Deep networks with stochastic depth. arXiv, 2016. [23] M. G. A. Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proc. of AISTATS, 2010. [24] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv, abs/1611.01144, 2016. [25] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117–128, Jan. 2011. [26] D. Jiang, X. Lei, W. Li, N. Luo, Y. Hu, W. Zou, and X. Li. Improving transformer-based speech recognition using unsupervised pre-training. arXiv, abs/1910.09932, 2019. [27] J. Kahn et al. Libri-light: A benchmark for asr with limited or no supervision. In Proc. of ICASSP, 2020. [28] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. van den Oord. Learning robust and multilingual speech representations. arXiv, 2020. [29] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, 2015. [30] A. Laptev, R. Korostik, A. Svischev, A. Andrusenko, I. Medennikov, and S. Rybin. You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation. arXiv, abs/2005.07157, 2020. [31] M. P. Lewis, G. F. Simon, and C. D. Fennig. Ethnologue: Languages of the world, nineteenth edition. Online version: http://www.ethnologue.com, 2016. [32] A. H. Liu, T. Tu, H. yi Lee, and L. shan Lee. Towards unsupervised speech recognition and synthesis with quantized speech representation learning. arXiv, 2019. [33] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [34] C. Lüscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schlüter, and H. Ney. Rwth asr systems for librispeech: Hybrid vs attention. In Interspeech 2019, 2019. [35] C. J. Maddison, D. Tarlow, and T. Minka. A* sampling. In Advances in Neural Information Processing Systems, pages 3086–3094, 2014. [36] I. Misra and L. van der Maaten. Self-supervised learning of pretext-invariant representations. arXiv, 2019. [37] A. Mohamed, D. Okhonko, and L. Zettlemoyer. Transformers with convolutional context for ASR. arXiv, abs/1904.11660, 2019. [38] M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling neural machine translation. In Proc. of WMT, 2018. [39] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL System Demonstrations, 2019. [40] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: an asr corpus based on public domain audio books. In Proc. of ICASSP, pages 5206–5210. IEEE, 2015. [42] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V. Le. Improved noisy student training for automatic speech recognition. arXiv, abs/2005.09629, 2020. [43] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In Proc. of ACL, 2018. [44] V. Pratap, A. Hannun, Q. Xu, J. Cai, J. Kahn, G. Synnaeve, V. Liptchinsky, and R. Collobert. Wav2letter++: A fast open-source speech recognition system. In Proc. of ICASSP, 2019. [45] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/ research-covers/language-unsupervised/language_understanding_paper.pdf, 2018. [46] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio. Light gated recurrent units for speech recognition. IEEE Transactions on Emerging Topics in Computational Intelligence, 2(2):92–102, 2018. [47] M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski, J. Monteiro, J. Trmal, and Y. Bengio. Multi-task self-supervised learning for robust speech recognition. arXiv, 2020. [48] M. Rivière, A. Joulin, P.-E. Mazaré, and E. Dupoux. Unsupervised pretraining transfers well across languages. arXiv, abs/2002.02848, 2020. [49] S. Schneider, A. Baevski, R. Collobert, and M. Auli. wav2vec: Unsupervised pre-training for speech recognition. In Proc. of Interspeech, 2019. [50] M. Schuster and K. Nakajima. Japanese and korean voice search. In Proc. of ICASSP, 2012. [51] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert. End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures. arXiv, abs/1911.08460, 2020. [52] A. Tjandra, B. Sisman, M. Zhang, S. Sakti, H. Li, and S. Nakamura. Vqvae unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge 2019. arXiv, 1905.11449, 2019. [53] A. van den Oord, O. Vinyals, et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems, pages 6306–6315, 2017. [54] A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv, abs/1807.03748, 2018. [55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In Proc. of NIPS, 2017.\n[56] W. Wang, Q. Tang, and K. Livescu. Unsupervised pre-training of bidirectional speech encoders via masked reconstruction. arXiv, 2020. [57] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. In Proc. of ICLR, 2019. [58] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert. Iterative pseudo-labeling for speech recognition. arXiv, 2020. [59] N. Zeghidour, N. Usunier, I. Kokkinos, T. Schaiz, G. Synnaeve, and E. Dupoux. Learning ﬁlterbanks from raw speech for phone recognition. In Proc. of ICASSP, 2018. [60] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar. Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss. arXiv, 2020.\n\nAppendices A Masking distribution When choosing which time-steps to mask, each latent speech representation in an utterance is considered a candidate starting time-step with probability p where M is the length of each masked span starting from the respective time step; both are hyper-parameters. Sampled starting time steps are expanded to length M and spans can overlap. For a 15 sec long audio sample, the average mask length is 14.7 time-steps, corresponding to 299ms of audio, with a median of 10 time-steps, and a maximum of about 100 time steps; about 49% of all time-steps in the sample will be masked. A plot of the corresponding mask length distribution is shown in Figure 2 and an ablation of M and p as well as the effect of other masking strategies is shown in Table 5. Reducing M results in increased prediction accuracy for the self-supervised but the task becomes trivial when spans with length one are masked, leading to poor performance on downstream speech recognition tasks. We also consider other masking strategies: w/o overlap uniform(a,b) samples for each starting index a span length M s from interval a to b and masks the subsequent M s time-steps taking care not to overlap with existing spans; poisson(λ) and normal(µ, σ) sample M s from Poisson and normal distributions. 0.25 0.20 Percent of all spans 0.15 0.10 0.05 0.00 20 40 60 80 100 Span length Figure 2: Mask length distribution for a 15 second sample with p = 0.065 and M = 10. Table 5: Ablations on settings for the masking strategy during pre-training. When masking without overlap, we choose starting time steps with p = 0.037 which results in the total number of masked tokens to match the baseline. Baseline (p = 0.075) 7.97 0.02 Mask length M = 8 8.33 0.05 Mask length M = 12 8.19 0.08 Mask length M = 15 8.43 0.19 Mask probability p = 0.065 7.95 0.08 Mask probability p = 0.06 8.14 0.22 Mask w/o overlap, uniform(1,31) 8.39 0.02 Mask w/o overlap, uniform(10,30) 9.17 0.05 Mask w/o overlap, poisson(15) 8.13 0.04 Mask w/o overlap, normal(15, 10) 8.37 0.03 Mask w/o overlap, length 10 9.15 0.02 Mask w/o overlap, length 15 9.43 0.26 avg WER std B Fine-tuning Setup During ﬁne-tuning we apply a masking strategy to the feature encoder outputs similar to SpecAugment [41]: we randomly choose a number of starting time steps for which a span of ten subsequent time-steps is replaced with a mask embedding; spans may overlap and we use the same masked time step embedding as during pre-training. We also mask channels by choosing a number of channels as starting indices and then expand each one to cover the subsequent 64 channels. Spans may overlap and the selected channel spans are set to zero value. We use LayerDrop [22, 12] at a rate of 0.05 for BASE and 0.1 for LARGE during ﬁne-tuning. Table 6 summarizes the ﬁne-tuning hyper-parameter settings used for the different labeled data setup. Table 7 shows the decoding parameters used for ﬁnal evaluations of the various labeled data setups for Librispeech pre-trained models and Table 8 shows decoding parameters for LibriVox. Table 6: Fine-tuning hyperparameters timestep mask prob. channel mask prob. updates 10 min 0.075 0.008 12k 1 hour 0.075 0.004 13k 10 hours 0.065 0.004 20k 100 hours 0.05 0.008 50k 960 hours 0.05 0.0016 320k TIMIT 0.065 0.012 40k Table 7: Decoding parameters for Librispeech subsets for models pre-trained on Librispeech 4gram LM weight 4gram word insert. TransLM weight TransLM word insert. 10 min 3.23 -0.26 1.20 -1.39 1 hour 2.90 -1.62 1.15 -2.08 10 hours 2.46 -0.59 1.06 -2.32 100 hours 2.15 -0.52 0.87 -1.00 960 hours 1.74 0.52 0.92 -0.86 Table 8: Decoding parameters for Librispeech subsets for models pre-trained on Librivox. 4gram LM weight 4gram word insert. TransLM weight TransLM word insert. 10 min 3.86 -1.18 1.47 -2.82 1 hour 3.09 -2.33 1.33 -0.69 10 hours 2.12 -0.90 0.94 -1.05 100 hours 2.15 -0.52 0.87 -1.00 960 hours 1.57 -0.64 0.90 -0.31 C Full results for Libri-light and Librispeech Table 9: WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups (cf. Table 1). Model Unlabeled LM dev test data clean other clean other 10 min labeled BASE LS-960 None 46.1 51.5 46.9 50.9 4-gram 8.9 15.7 9.1 15.6 Transf. 6.6 13.2 6.9 12.9 LARGE LS-960 None 43.0 46.3 43.5 45.3 4-gram 8.6 12.9 8.9 13.1 Transf. 6.6 10.6 6.8 10.8 LARGE LV-60k None 38.3 41.0 40.2 38.7 4-gram 6.3 9.8 6.6 10.3 Transf. 4.6 7.9 4.8 8.2 1h labeled BASE LS-960 None 24.1 29.6 24.5 29.7 4-gram 5.0 10.8 5.5 11.3 Transf. 3.8 9.0 4.0 9.3 LARGE LS-960 None 21.6 25.3 22.1 25.3 4-gram 4.8 8.5 5.1 9.4 Transf. 3.8 7.1 3.9 7.6 LARGE LV-60k None 17.3 20.6 17.2 20.3 4-gram 3.6 6.5 3.8 7.1 Transf. 2.9 5.4 2.9 5.8 10h labeled BASE LS-960 None 10.9 17.4 11.1 17.6 4-gram 3.8 9.1 4.3 9.5 Transf. 2.9 7.4 3.2 7.8 LARGE LS-960 None 8.1 12.0 8.0 12.1 4-gram 3.4 6.9 3.8 7.3 Transf. 2.9 5.7 3.2 6.1 LARGE LV-60k None 6.3 9.8 6.3 10.0 4-gram 2.6 5.5 3.0 5.8 Transf. 2.4 4.8 2.6 4.9 100h labeled BASE LS-960 None 6.1 13.5 6.1 13.3 4-gram 2.7 7.9 3.4 8.0 Transf. 2.2 6.3 2.6 6.3 LARGE LS-960 None 4.6 9.3 4.7 9.0 4-gram 2.3 5.7 2.8 6.0 Transf. 2.1 4.8 2.3 5.0 LARGE LV-60k None 3.3 6.5 3.1 6.3 4-gram 1.8 4.5 2.3 4.6 Transf. 1.9 4.0 2.0 4.0 Table 10: WER on Librispeech when using all 960 hours of Librispeech as labeled data (cf. Table 2). Model Unlabeled LM dev test data clean other clean other LARGE - from scratch - None 2.8 7.6 3.0 7.7 - 4-gram 1.8 5.4 2.6 5.8 - Transf. 1.7 4.3 2.1 4.6 BASE LS-960 None 3.2 8.9 3.4 8.5 4-gram 2.0 5.9 2.6 6.1 Transf. 1.8 4.7 2.1 4.8 LARGE LS-960 None 2.6 6.5 2.8 6.3 4-gram 1.7 4.6 2.3 5.0 Transf. 1.7 3.9 2.0 4.1 LARGE LV-60k None 2.1 4.5 2.2 4.5 4-gram 1.4 3.5 2.0 3.6 Transf. 1.6 3.0 1.8 3.3 D Analysis of Discrete Latent Speech Representations Next, we investigate whether the discrete latent speech representations qt learned by the quantizer relate to phonetic information: Using LARGE pre-trained on LV-60k and without any ﬁne-tuning, we compute the discrete latents for the training data of TIMIT and compute the co-occurrence between human annotated phonemes and the latents. Ties are broken by choosing the phoneme which is most represented in the receptive ﬁeld of qt. The training data contains 3696 utterances of average length 13.6 sec, or 563k discrete latents. Figure 3 plots P(phoneme|qt) and shows that many discrete latents appear to specialize in speciﬁc phonetic sounds. The silence phoneme (bcl) represents 22% of all human annotated speech data and is therefore also modeled by many different latents. aa ae ah aw ay b ch d dh dx eh axr ey f g bcl hh ih iy jh k el em en eng ow oy p r s sh t th uh uw v w y z Figure 3: Visualization of the co-occurrence between discrete latent speech representations and phonemes. We plot the conditional probability P(phoneme|qt) on TIMIT train data. The y-axis shows the collapsed 39 classes of phonemes and the x-axis is over the different discrete latents. E Speech Recognition Error Analysis In this section we study the most common errors our models make when ﬁne-tuned on different amounts of labeled data (Table 11). We also show transcriptions of a few relatively challenging utterances from the dev-clean subset of Librispeech (Table 12). We consider models with no lexicon or no language model decoding, marked None in Table 9: Larger capacity decreases error rates: LARGE on LS-960 improves the word error rate on dev-clean from 46.1 to 43 compared to BASE. Increasing the amount of unlabeled training data further decreases the error rate to 33.8 for LARGE on LS-960. In the ten minute labeled data setup, the model is still able to recognize basic units of speech: Table 11 shows that most errors are around spelling of words, e.g., omitting silent characters such as could →coud, know →now, or ignoring repeated letters such as still →stil, little →litle. The LARGE LV-60k model achieves WER 38.3 on dev-clean and adding a Transformer language model enables to choose more likely pronunciations during the search and gives a large WER improvement to 5.0. The ten minute models without lexicon and language model tend to spell words phonetically and omit repeated letters, e.g., will →wil (Table 11). Spelling errors decrease with more labeled data: with one hour of labeled data, slightly less common words move into the list of the most frequent errors, e.g., heaven and food are spelled phonetically. At ten hours, top errors include articles, e.g., a, the which are a common source of errors in speech recognition in general. There are also alternative spellings, color vs. colour as well as relatively rare words including person names, still spelled phonetically, e.g., phoebe →feeby. At 100 hours, person names dominate the most frequent errors: phoebe →phebe, along with incorrect spacing anyone →any one, awhile →a while. Finally at 960 hours the word error rate falls to 2% and top errors are mostly articles, incorrect splits, and some very rare words or names such as deucalion or gryce. The “from scratch” 960 hour model has a similar word error rate as the 100 hour pre-trained model and displays a similar pattern of errors. The pre-trained speech representations can be easily adapted to recognize speciﬁc sounds while ﬁne-tuning grounds these representations to the actual spelling. Table 11: Top word errors for models trained on 10m, 1h and 10h, 100h, 960h of labeled data and decoded on the Librispeech dev-clean subset without a language model or lexicon (see Table 9 and Table 10 - None). In brackets is the total number of occurrences of each error. 10m LARGE LV-60k 1h LARGE LV-60k 10h LARGE LV-60k all →al (181) too →to (26) in →and (15) are →ar (115) until →untill (24) a →the (11) will →wil (100) new →knew (22) o →oh (10) you →yo (90) door →dor (18) and →in (9) one →on (89) says →sais (18) mode →mod (9) two →to (81) soul →sol (17) ursus →ersus (9) well →wel (80) bread →bred (16) tom →tome (8) been →ben (73) poor →pore (16) randal →randol (7) upon →apon (73) a →the (13) the →a (7) good →god (67) either →ither (13) color →colour (6) see →se (66) food →fud (13) ﬂour →ﬂower (6) we →whe (60) doubt →dout (12) phoebe →feeby (6) little →litle (54) earth →erth (12) an →and (5) great →grate (53) led →lead (12) cucumbers →cucombers (5) your →yor (53) sea →see (12) egg →eg (5) could →coud (51) thee →the (12) macklewain →macklewaine (5) here →hear (51) tom →tome (12) magpie →magpi (5) know →now (45) add →ad (11) milner →millner (5) there →ther (45) good →god (11) stacy →staci (5) three →thre (45) heaven →heven (11) trevelyan →trevellion (5) still →stil (42) mary →marry (11) verloc →verlock (5) off →of (40) randal →randel (11) ann →an (4) don’t →dont (37) answered →ansered (10) anyone →one (4) shall →shal (36) blood →blod (10) apartment →appartment (4) little →litl (35) bozzle →bosel (10) basin →bason (4) 100h LARGE LV-60k 960h LARGE LV-60k 960h LARGE from scratch a →the (13) a →the (12) and →in (20) and →in (10) and →in (9) a →the (16) in →and (10) macklewain →mackelwaine (7) in →and (13) o →oh (8) in →and (6) the →a (10) minnetaki →minnitaki (7) o →oh (6) in →an (8) randal →randall (7) bozzle →bosell (5) and →an (5) christie →cristy (6) criss →chris (5) clarke →clark (4) macklewain →mackelwane (6) bozzle →bosel (4) grethel →gretel (4) randal →randoll (6) clarke →clark (4) macklewain →mackelwaine (4) bozzle →bosall (5) colored →coloured (4) this →the (4) kaliko →calico (5) grethel →gretel (4) an →and (3) trevelyan →trevelian (5) lige →lyge (4) anyone →one (3) an →and (4) the →a (4) bozzle →basell (3) and →an (4) and →an (3) buns →bunds (3) anyone →one (4) ann →marianne (3) carrie →carry (3) bozzle →bozall (4) butte →bute (3) criss →chris (3) clarke →clark (4) color →colour (3) he’s →is (3) gryce →grice (4) deucalion →ducalion (3) his →is (3) i’m →am (4) forcemeat →meat (3) honor →honour (3) in →ind (4) gryce →grice (3) lattimer →latimer (3) letty →lettie (4) honor →honour (3) millet →mellet (3) phoebe →phebe (4) kearny →kirney (3) pyncheon →pension (3) the →a (4) nuova →noiva (3) tad →ted (3) ann →anne (3) thing →anything (3) thing →anything (3) awhile →while (3) this →the (3) trevelyan →trevelian (3) Table 12: Examples of transcription of selected utterances from the dev-clean subset by various models without a language model or lexicon. Capitalized words indicate errors. Model Transcription Reference i’m mister christopher from london 10m LV-60k IM mister CRESTIFER FROME LUNDEN 1h LV-60k IM mister CRISTIFFHER from LOUNDEN 10h LV-60k i’m mister CHRYSTEPHER from london 100h LV-60k i’m mister christopher from london 960h LV-60k i’m mister christopher from london 960h scratch I MISSTER christopher from london Reference il popolo e una bestia 10m LV-60k ILPOPULAR ONABESTIA 1h LV-60k O POPOLAONABASTIA 10h LV-60k U POPULAONABASTIAR 100h LV-60k O POPALOON A BASTYA 960h LV-60k YOU’LL POP A LAWYE ON A BAISTYE 960h scratch OL POPALOY ON ABESTIA Reference he smelt the nutty aroma of the spirit 10m LV-60k he SMELTD the NUDY aroma of the spirit 1h LV-60k he SMELTD the NUDDY ARROMA of the spirit 10h LV-60k he smelt the NUDDY ERROMA of the spirit 100h LV-60k he smelt the NUDDY aroma of the spirit 960h LV-60k he smelt the NUTTIE aroma of the spirit 960h scratch he smelt the nutty EROMA of the spirit Reference phoebe merely glanced at it and gave it back 10m LV-60k FEABY MEARLY glanced at it and gave it BAK 1h LV-60k FIEABY merely glanced at it and gave it back 10h LV-60k FEEBY merely glanced at it and gave it back 100h LV-60k BEBE merely glanced at it and gave it back 960h LV-60k phoebe merely glanced at it and gave it back 960h scratch phoebe merely glanced at it and gave it back Reference sauterne is a white bordeaux a strong luscious wine the best known varieties being 10m LV-60k SULTERIN is a white BORDOE a strong LUCHOUS WIN the best NOWN VERIATYS being 1h LV-60k CLTEREN is a white BORDO a strong LUCHIOUS wine the best known VERIETIES being 10h LV-60k SOTERN is a white BOURDO a strong LUCIOUS wine the best known VORIETIES being 100h LV-60k SOTERN is a white BORDAUX a strong LUCIOUS wine the best known varieties being 960h LV-60k SOTERN is a white bordeaux a strong luscious wine the best known varieties being 960h scratch SOTERAN is a white bordeaux a strong luscious wine the best known varieties being Reference i happen to have mac connell’s box for tonight or there’d be no chance of our getting places 10m LV-60k i HAPEND to have MECONALES BOXS for TONIT ORE THIRLD be no chance of OR GETING places 1h LV-60k i happen to have MACCONNEL’S BOCXS for tonight or TE’ELD be no chance of our getting places 10h LV-60k i HAPPENED to have MUKONNEL’S box for tonight or THERED be no chance of our getting places 100h LV-60k i HAPPENED to have MC CONNEL’S box for TO NIGHT or there’d be no chance of our getting places 960h LV-60k i happen to have MC CONALL’S box for TO NIGHT or there’d be no chance of our getting places 960h scratch i HAPPENE to have MACONEL’S box for TO NIGHT or there’d be no chance of our getting places F Ablations Table 13 ablates various hyperparameter choices of our architecture. The setup for the baseline model is described in § 5.4. First, we tried to improve the continuous input and continuous target model (§ 5.4) by adding an MLP on top of the continuous target representation and we also tried to use a separate set of encoder parameters for the representations used as input and targets (Separate encoders). Both did not lead to meaningful improvements. Increasing the receptive ﬁeld size from 25ms to 30ms had little effect. Setting the diversity penalty weight (α) too low results in lower codebook usage and lower performance. Setting it too high leads to slight instability. Doubling the number of relative positional embeddings to 256 also did not help. Stopping gradients from the quantizer to the encoder shows that the encoder requires training signal from the quantizer as well. Next, increasing the number of negatives did not result in better performance (K = 200) and sampling negatives from the entire batch of utterances hurt performance, likely because candidates from other utterances are easy to distinguish. Sampling negatives from any time step in the utterance, masked or unmasked, does not help and is more computationally expensive. Gumbel noise is important and increasing the number of codebooks did not result in better performance. Table 13: Ablation of various hyper-parmeter choices. We report average WER and standard deviation on combined dev-clean/other of Librispeech for three seeds of training. Baseline (p = 0.075, α = 0.1) 7.97 0.02 Continuous inputs, continuous targets 8.58 0.08 + MLP on targets 8.51 0.05 + Separate encoders 8.90 0.01 receptive ﬁeld 30ms 7.99 0.06 diversity penalty α = 0 8.48 0.08 α = 0.05 8.34 0.08 α = 0.2 8.58 0.45 Conv pos emb, kernel 256 8.14 0.05 No gradient to encoder from quantizer 8.41 0.08 Negatives K = 200 same utterance 8.12 0.05 K = 50 same utterance + K = 50 from batch 8.79 0.06 Sample negatives from any time step 8.07 0.02 No Gumbel noise 8.73 0.42 Codebook G=4, V=18 9.02 0.38 G=8, V=8 8.13 0.07 Predict exactly U time steps from edges U = 1 9.53 0.91 U = 5 8.19 0.07 U = 10 8.07 0.07 U = 15 7.89 0.10 U = 20 7.90 0.01 We also investigated predicting only time steps immediately next to the last unmasked time step for each span. This enables to better control the difﬁculty of the pre-training task. Given the leftmost or rightmost unmasked time step next to a masked span, we compute the contrastive loss only for the ﬁrst U masked time steps next to these unsmasked spans. Predicting only up to one time step performs poorly because there is little training signal from each utterance and predicting more time steps performs better but does not signiﬁcantly outperform predicting all masked time steps. Increasing the number of training updates helps but this increases training time. avg. WER std.", "persona": "Student", "response": "### 1. Overview\nwav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. This is the first time that, by learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech, wav2vec 2.0 can outperform the best semi-supervised methods while being conceptually simpler. During pre-training, we learn representations of speech audio by solving a contrastive task Lm which requires to identify the true quantized latent speech representation for a masked time step within a set of distractors. This is augmented by a codebook diversity loss Ld: And Ld is designed to increase the use of the quantized codebook representations, by maximizing the entropy to encourage the equal use of the V entries in each of the G codebooks: 1.7. Fine-tuning 2.\n\n### 2. Background\nResults 1.1. Model Overview The model is composed of a multi-layer convolutional feature encoder f: X→Z which takes as input raw audio X and outputs latent speech representations z1, …, zT for T time-steps. They are then fed to a Transformer g: Z→C to build representations c1, …, cT capturing information from the entire sequence. The output of the feature encoder is discretized to qt with a quantization module Z→Q to represent the targets (Figure 1) in the self-supervised objective.\n\n### 3. Methods\n1.2. Feature Encoder X The encoder consists of several blocks containing a temporal convolution followed by layer normalization and a GELU activation function. The raw waveform input to the encoder is normalized to zero mean and unit variance. The total stride of the encoder determines the number of time-steps T which are input to the Transformer. The output of the feature encoder is fed to a context network which follows the Transformer architecture. Instead of fixed positional embeddings which encode absolute positional information, a convolutional layer is used which acts as relative positional embedding. The output of the convolution followed by a GELU is added to the inputs and then layer normalization is applied. 1.4.\n\n### 4. Results\nResults For LibriSpeech, there are 29 tokens for character targets plus a word boundary token. Models are optimized by minimizing a Connectionist Temporal Classification (CTC) loss and a modified version of SpecAugment is applied by masking to time-steps and channels during training 1.8. Model Setting There are base and large models for Transformer. Base one contains 12 Transformer blocks, model dimension 768, inner dimension (FFN) 3,072 and 8 attention heads. LARGE model contains 24 Transformer blocks with model dimension 1,024, inner dimension 4,096 and 16 attention heads. 2.1.\n\n### 5. Takeaways\n1.6. Losses The proposed approach, despite a weaker baseline architecture, achieves WER 1.8/3.3 on test-clean/other on the full LibriSpeech benchmark. Low-Resource Labeled Data Evaluation on LibriSpeech Low-Resource Labeled Data Evaluation on LibriSpeech The LARGE model pre-trained on LV-60k and fine-tuned on only 10 minutes of labeled data achieves a word error rate of 5.2/8.6 on the LibriSpeech clean/other test sets. 2.2. High-Resource Labeled Data Evaluation on LibriSpeech High-Resource Labeled Data Evaluation on LibriSpeech Self-training is likely complimentary to pre-training and their combination may yield even better results.", "paper_id": "24", "story_id": "24-02"}
{"paper": "Abstract\nWe show for the ﬁrst time that learning powerful representations from speech audio alone followed by ﬁne-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task deﬁned over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.1 1\n\nIntroduction\nNeural networks beneﬁt from large quantities of labeled training data. However, in many settings labeled data is much harder to come by than unlabeled data: current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide [31]. Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them - a process that requires learning good representations of speech. In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to ﬁne-tune the model on labeled data. This has been particularly successful for natural language processing [43, 45, 9] and is an active research area for computer vision [20, 2, 36, 19, 6]. In this paper, we present a framework for self-supervised learning of representations from raw audio data. Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations [26, 56], similar to masked language modeling [9]. The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors [54, 49, 48, 28] (§ 2). As part of training, we learn discrete speech units [53, 32, 7, 18] via a gumbel softmax [24, 5] to represent the latent representations in the contrastive task (Figure 1) which we ﬁnd to be more effective than non-quantized targets. After pre-training on unlabeled speech, the model is ﬁne-tuned Context representations C … … Quantized representations Q q q q q q Latent speech representations Z X raw waveform Figure 1: Illustration of our framework which jointly learns contextualized speech representations and an inventory of discretized speech units. on labeled data with a Connectionist Temporal Classiﬁcation (CTC) loss [14, 4] to be used for downstream speech recognition tasks (§ 3) Previous work learned a quantization of the data followed by a contextualized representations with a self-attention model [5, 4], whereas our approach solves both problems end-to-end. Masking parts of the input with Transformer networks for speech has been explored [4, 26], but prior work relies either on a two-step pipeline or their model is trained by reconstructing the ﬁlter bank input features. Other related work includes learning representations from auto-encoding the input data [52, 11] or directly predicting future timesteps [8]. Our results show that jointly learning discrete speech units with contextualized representations achieves substantially better results than ﬁxed units learned in a prior step [4]. We also demonstrate the feasibility of ultra-low resource speech recognition: when using only 10 minutes of labeled data, our approach achieves word error rate (WER) 4.8/8.2 on the clean/other test sets of Librispeech. We set a new state of the art on TIMIT phoneme recognition as well as the 100 hour clean subset of Librispeech. Moreover, when we lower the amount of labeled data to just one hour, we still outperform the previous state of the art self-training method of [42] while using 100 times less labeled data and the same amount of unlabeled data. When we use all 960 hours of labeled data from Librispeech, then our model achieves 1.8/3.3 WER (§ 4, § 5).\n\n2 Model Our model is composed of a multi-layer convolutional feature encoder f : X 7→Z which takes as input raw audio X and outputs latent speech representations z1, . . . , zT for T time-steps. They are then fed to a Transformer g : Z 7→C to build representations c1, . . . , cT capturing information from the entire sequence [9, 5, 4]. The output of the feature encoder is discretized to qt with a quantization module Z 7→Q to represent the targets (Figure 1) in the self-supervised objective (§ 3.2). Compared to vq-wav2vec [5], our model builds context representations over continuous speech representations and self-attention captures dependencies over the entire sequence of latent representations end-to-end. Feature encoder. The encoder consists of several blocks containing a temporal convolution followed by layer normalization [1] and a GELU activation function [21]. The raw waveform input to the encoder is normalized to zero mean and unit variance. The total stride of the encoder determines the number of time-steps T which are input to the Transformer (§ 4.2). Contextualized representations with Transformers. The output of the feature encoder is fed to a context network which follows the Transformer architecture [55, 9, 33]. Instead of ﬁxed positional embeddings which encode absolute positional information, we use a convolutional layer similar to [37, 4, 57] which acts as relative positional embedding. We add the output of the convolution followed by a GELU to the inputs and then apply layer normalization. ` Contrastive loss L Transformer Masked CNN results in prior work which learned discrete units in a ﬁrst step followed by learning contextualized representations [5]. Product quantization amounts to choosing quantized representations from multiple codebooks and concatenating them. Given G codebooks, or groups, with V entries e ∈ RV ×d/G, we choose one entry from each codebook and concatenate the resulting vectors e1, . . . , eG and apply a linear transformation Rd 7→Rf to obtain q ∈Rf. The Gumbel softmax enables choosing discrete codebook entries in a fully differentiable way [16, 24, 35]. We use the straight-through estimator [26] and setup G hard Gumbel softmax operations [24]. The feature encoder output z is mapped to l ∈RG×V logits and the probabilities for choosing the v-th codebook entry for group g are pg,v = exp(lg,v + nv)/τ PV k=1 exp(lg,k + nk)/τ , (1) where τ is a non-negative temperature, n = −log(−log(u)) and u are uniform samples from U(0, 1). During the forward pass, codeword i is chosen by i = argmaxjpg,j and in the backward pass, the true gradient of the Gumbel softmax outputs is used. 3 Training To pre-train the model we mask a certain proportion of time steps in the latent feature encoder space (§ 3.1), similar to masked language modeling in BERT [9]. The training objective requires identifying the correct quantized latent audio representation in a set of distractors for each masked time step (§ 3.2) and the ﬁnal model is ﬁne-tuned on the labeled data (§ 3.3). 3.1 Masking We mask a proportion of the feature encoder outputs, or time steps before feeding them to the context network and replace them with a trained feature vector shared between all masked time steps; we do not mask inputs to the quantization module. To mask the latent speech representations output by the encoder, we randomly sample without replacement a certain proportion p of all time steps to be starting indices and then mask the subsequent M consecutive time steps from every sampled index; spans may overlap. 3.2 Objective During pre-training, we learn representations of speech audio by solving a contrastive task Lm which requires to identify the true quantized latent speech representation for a masked time step within a set of distractors. This is augmented by a codebook diversity loss Ld to encourage the model to use the codebook entries equally often. L = Lm + αLd (2) where α is a tuned hyperparameter. Contrastive Loss. Given context network output ct centered over masked time step t, the model needs to identify the true quantized latent speech representation qt in a set of K + 1 quantized candidate representations ˜q ∈Qt which includes qt and K distractors [23, 54]. Distractors are uniformly sampled from other masked time steps of the same utterance. The loss is deﬁned as Lm = −log exp(sim(ct, qt)/κ) P ˜q∼Qt exp(sim(ct, ˜q)/κ) (3) where we compute the cosine similarity sim(a, b) = aT b/∥a∥∥b∥between context representations and quantized latent speech representations [19, 6]. codebook ¯pg across a batch of utterances; the softmax disribution does not contain the gumbel noise nor a temperature:2 G X Ld = 1 GV g=1 −H(¯pg) = 1 GV 3.3 Fine-tuning Pre-trained models are ﬁne-tuned for speech recognition by adding a randomly initialized linear projection on top of the context network into C classes representing the vocabulary of the task [4]. For Librispeech, we have 29 tokens for character targets plus a word boundary token. Models are optimized by minimizing a CTC loss [14] and we apply a modiﬁed version of SpecAugment [41] by masking to time-steps and channels during training which delays overﬁtting and signiﬁcantly improves the ﬁnal error rates, especially on the Libri-light subsets with few labeled examples. 4 Experimental Setup 4.1 Datasets As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio. We ﬁne-tune on ﬁve labeled data settings: 960 hours of transcribed Librispeech, the train-clean-100 subset comprising 100 hours (100 hours labeled), as well as the Libri-light limited resource training subsets originally extracted from Librispeech, these are train-10h (10 hours labeled), train-1h (1 hour labeled), train-10min (10 min labeled). We follow the evaluation protocol of Libri-light for these splits and evaluate on the standard Librispech dev-other/clean and test-clean/other sets. We ﬁne-tune the pre-trained models for phoneme recognition on the TIMIT dataset [13]. It contains ﬁve hours of audio recordings with detailed phoneme labels. We use the standard train, dev and test split and follow the standard protocol of collapsing phone labels to 39 classes. 4.2 Pre-training Models are implemented in fairseq [39]. For masking, we sample p = 0.065 of all time-steps to be starting indices and mask the subsequent M = 10 time-steps. This results in approximately 49% of all time steps to be masked with a mean span length of 14.7, or 299ms (see Appendix A for more details on masking). The feature encoder contains seven blocks and the temporal convolutions in each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2). This results in an encoder output frequency of 49 hz with a stride of about 20ms between each sample, and a receptive ﬁeld of 400 input samples or 25ms of audio. The convolutional layer modeling relative positional embeddings has kernel size 128 and 16 groups. We experiment with two model conﬁgurations which use the same encoder architecture but differ in the Transformer setup: BASE contains 12 transformer blocks, model dimension 768, inner dimension (FFN) 3,072 and 8 attention heads. Batches are built by cropping 250k audio samples, or 15.6sec, from each example. Crops are batched together to not exceed 1.4m samples per GPU and we train on a total of 64 V100 GPUs for 1.6 days [38]; the total batch size is 1.6h. The LARGE model contains 24 transformer blocks with model dimension 1,024, inner dimension 4,096 and 16 attention heads. We crop 320k audio samples, or 20sec, with a limit of 1.2m samples per GPU and train on 128 V100 GPUs over 2.3 days for Librispeech and 5.2 days for LibriVox; the total batch size is 2.7h. We use dropout 0.1 in the Transformer, at the output of the feature encoder and the input to the quantization module. Layers are dropped at a rate of 0.05 for BASE and 0.2 for LARGE [22, 12]; there is no layer drop for LV-60k. G X V X v=1 ¯pg,v log ¯pg,v (4) g=1 We optimize with Adam [29], warming up the learning rate for the ﬁrst 8% of updates to a peak of 5 × 10−4 for BASE and 3 × 10−4 for LARGE, and then linearly decay it. LARGE trains for 250k updates, BASE for 400k updates, and LARGE on LV-60k for 600k updates. We use weight α = 0.1 for the diversity loss Equation 2. For the quantization module we use G = 2 and V = 320 for both models, resulting in a theoretical maximum of 102.4k codewords. Entries are of size d/G = 128 for BASE amd d/G = 384 for LARGE. The Gumbel softmax temperature τ is annealed from 2 to a minimum of 0.5 for BASE and 0.1 for LARGE by a factor of 0.999995 at every update. The temperature in the contrastive loss (Equation 3) is set to κ = 0.1. For the smaller Librispeech dataset, we regularize the model by applying an L2 penalty to the activations of the ﬁnal layer of the feature encoder and scale down the gradients for the encoder by a factor of 10. We also use a slightly different encoder architecture where we do not use layer normalization, and instead of normalizing the raw waveform, the output of the ﬁrst encoder layer is normalized. In the contrastive loss we use K = 100 distractors. We choose the training checkpoint with the lowest Lm on the validation set. 4.3 Fine-tuning After pre-training we ﬁne-tune the learned representations on labeled data and add a randomly initialized output layer on top of the Transformer to predict characters (Librispeech/Libri-light) or phonemes (TIMIT). For Libri-light, we train three seeds with two different learning rates (2e-5 and 3e-5) for all subsets and choose the conﬁguration with lowest WER on dev-other subset decoded with the ofﬁcial 4-gram language model (LM) with beam 50 and ﬁxed model weights (LM weight 2, word insertion penalty -1). For BASE on the labeled 960h subset we use a learning rate of 1e-4. We optimize with Adam and a tri-state rate schedule where the learning rate is warmed up for the ﬁrst 10% of updates, held constant for the next 40% and then linearly decayed for the remainder. BASE uses a batch size of 3.2m samples per GPU and we ﬁne-tune on 8 GPUs, giving a total batch size of 1,600sec. LARGE batches 1.28m samples on each GPU and we ﬁne-tune on 24 GPUs, resulting in an effective batch size of 1,920sec. For the ﬁrst 10k updates only the output classiﬁer is trained, after which the Transformer is also updated. The feature encoder is not trained during ﬁne-tuning. We mask the feature encoder representations with a strategy similar to SpecAugment [41] detailed in Appendix B. 4.4 Language Models and Decoding We consider two types of language models (LM): a 4-gram model and a Transformer [3] trained on the Librispeech LM corpus. The Transformer LM is identical to [51] and contains 20 blocks, model dimension 1,280, inner dimension 6,144 and 16 attention heads. We tune the weights of the language model (interval [0, 5]) and a word insertion penalty ([−5, 5]) via Bayesian optimization3: we run 128 trials with beam 500 for the 4-gram LM and beam 50 for the Transformer LM and choose the best set of weights according to performance on dev-other. Test performance is measured with beam 1,500 for the n-gram LM and beam 500 for the Transformer LM. We use the beam search decoder of [44]. 5\n\nResults\n5.1 Low-Resource Labeled Data Evaluation We ﬁrst evaluate our pre-trained models in settings where the amount of labeled data is limited to get a sense of how the representations learned on unlabeled data can improve low resource settings. If a pre-trained model captures the structure of speech, then it should require few labeled examples to ﬁne-tune it for speech recognition. The models are pre-trained on the audio data of either Librispeech (LS-960) or LibriVox (LV-60k) and most results are obtained by decoding with a Transformer language model (Transf.); Appendix C shows results with no language model at all as well as with an n-gram language model. The LARGE model pre-trained on LV-60k and ﬁne-tuned on only 10 minutes of labeled data achieves a word error rate of 5.2/8.6 on the Librispeech clean/other test sets. Ten minutes of labeled data corresponds to just 48 recordings with an average length of 12.5 seconds. This demonstrates that ultra-low resource speech recognition is possible with self-supervised learning on unlabeled data. Table 1: WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech. Models use either the audio of Librispeech (LS-960) or the larger LibriVox (LV-60k) as unlabeled data. We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters). Prior work used 860 unlabeled hours (LS-860) but the total with labeled data is 960 hours and comparable to our setup. Model Unlabeled LM dev test data clean other clean other 10 min labeled Discrete BERT [4] LS-960 4-gram 15.7 24.1 16.3 25.2 BASE LS-960 4-gram 8.9 15.7 9.1 15.6 Transf. 6.6 13.2 6.9 12.9 LARGE LS-960 Transf. 6.6 10.6 6.8 10.8 LV-60k Transf. 4.6 7.9 4.8 8.2 1h labeled Discrete BERT [4] LS-960 4-gram 8.5 16.4 9.0 17.6 BASE LS-960 4-gram 5.0 10.8 5.5 11.3 Transf. 3.8 9.0 4.0 9.3 LARGE LS-960 Transf. 3.8 7.1 3.9 7.6 LV-60k Transf. 2.9 5.4 2.9 5.8 10h labeled Discrete BERT [4] LS-960 4-gram 5.3 13.2 5.9 14.1 Iter. pseudo-labeling [58] LS-960 4-gram+Transf. 23.51 25.48 24.37 26.02 LV-60k 4-gram+Transf. 17.00 19.34 18.03 19.92 BASE LS-960 4-gram 3.8 9.1 4.3 9.5 Transf. 2.9 7.4 3.2 7.8 LARGE LS-960 Transf. 2.9 5.7 3.2 6.1 LV-60k Transf. 2.4 4.8 2.6 4.9 100h labeled Hybrid DNN/HMM [34] - 4-gram 5.0 19.5 5.8 18.6 TTS data augm. [30] - LSTM 4.3 13.5 Discrete BERT [4] LS-960 4-gram 4.0 10.9 4.5 12.1 Iter. pseudo-labeling [58] LS-860 4-gram+Transf. 4.98 7.97 5.59 8.95 LV-60k 4-gram+Transf. 3.19 6.14 3.72 7.11 Noisy student [42] LS-860 LSTM 3.9 8.8 4.2 8.6 BASE LS-960 4-gram 2.7 7.9 3.4 8.0 Transf. 2.2 6.3 2.6 6.3 LARGE LS-960 Transf. 2.1 4.8 2.3 5.0 LV-60k Transf. 1.9 4.0 2.0 4.0 Our approach of jointly learning discrete units and contextualized representations clearly improves over previous work which learned quantized audio units in a separate step [4], reducing WER by a about a third. A recent iterative self-training approach [42] represents the state of the art on the clean 100 hour subset of Librispeech but it requires multiple iterations of labeling, ﬁltering, and re-training. Our approach is simpler: we pre-train on the unlabeled data and ﬁne-tune on the labeled data. On the 100 hour subset of Librispeech, their method achieves WER 4.2/8.6 on test-clean/other which compares to WER 2.3/5.0 with the LARGE model in a like for like setup, a relative WER reduction of 45%/42%. Table 2: WER on Librispeech when using all 960 hours of labeled data (cf. Table 1). Model Unlabeled LM dev test data clean other clean other Supervised CTC Transf [51] - CLM+Transf. 2.20 4.94 2.47 5.45 S2S Transf. [51] - CLM+Transf. 2.10 4.79 2.33 5.17 Transf. Transducer [60] - Transf. - - 2.0 4.6 ContextNet [17] - LSTM 1.9 3.9 1.9 4.1 Conformer [15] - LSTM 2.1 4.3 1.9 3.9 Semi-supervised CTC Transf. + PL [51] LV-60k CLM+Transf. 2.10 4.79 2.33 4.54 S2S Transf. + PL [51] LV-60k CLM+Transf. 2.00 3.65 2.09 4.11 Iter. pseudo-labeling [58] LV-60k 4-gram+Transf. 1.85 3.26 2.10 4.01 Noisy student [42] LV-60k LSTM 1.6 3.4 1.7 3.4 This work LARGE - from scratch - Transf. 1.7 4.3 2.1 4.6 BASE LS-960 Transf. 1.8 4.7 2.1 4.8 LARGE LS-960 Transf. 1.7 3.9 2.0 4.1 LV-60k Transf. 1.6 3.0 1.8 3.3 light data splits contain both clean and noisy data leading to better accuracy on test-other compared to test-clean. Increasing model size reduces WER on all setups with the largest improvements on test-other (BASE vs. LARGE both on LS-960) and increasing the amount of unlabeled training data also leads to large improvements (LARGE LS-960 vs. LV-60k). 5.2 High-Resource Labeled Data Evaluation on Librispeech In this section we evaluate the performance when large quantities of labeled speech are available to assess the effectiveness of our approach in a high resource setup. Speciﬁcally, we ﬁne-tune the same models as before on the full 960 hours of labeled Librispeech: BASE and LARGE pre-trained on LS-960 as well as LARGE pre-trained on LV-60k. Table 2 shows that our approach achieves WER 1.8/3.3 on test-clean/other on the full Librispeech benchmark. This is despite a weaker baseline architecture: supervised training of our architecture achieves WER 2.1/4.6 (LARGE - from scratch) compared to WER 1.9/4.1 for ContextNet [17], the baseline architecture of the state of the art [42]. We use a simple Transformer with CTC which does not perform as well as seq2seq models [51]. Note that the vocabulary of our acoustic model (characters) does not match the vocabulary of the LM (words) which delays feedback from the LM and is likely to be detrimental. Most recent work [51, 58, 17, 42] uses the better performing word pieces [50] for both models. Moreover, our result is achieved without any data balancing such as [42]. Finally, self-training is likely complimentary to pre-training and their combination may yield even better results. Appendix E presents a detailed error analysis of our pre-trained models in various labeled data setups. 5.3 Phoneme Recognition on TIMIT Next, we evaluate accuracy on TIMIT phoneme recognition by ﬁne-tuning the pre-trained models on the labeled TIMIT training data. We ﬁne-tune as for the 10 hour subset of Libri-light but do not use a language model. Table 3 shows that our approach can achieve a new state of the art on this dataset, reducing PER by a relative 23%/29% over the next best result on the dev/test sets. Appendix D shows an analysis of how the discrete latent speech representations related to phonemes. Other recent work on pre-training which evaluates on TIMIT includes [47] who solve multiple tasks to learn good representations of speech. Table 3: TIMIT phoneme recognition accuracy in terms of phoneme error rate (PER). CNN + TD-ﬁlterbanks [59] 15.6 18.0 PASE+ [47] - 17.2 Li-GRU + fMLLR [46] – 14.9 wav2vec [49] 12.9 14.7 vq-wav2vec [5] 9.6 11.6 This work (no LM) LARGE (LS-960) 7.4 8.3 Table 4: Average WER and standard deviation on combined dev-clean/other of Librispeech for three training seeds. We ablate quantizing the context network input and the targets in the contrastive loss. Continuous inputs, quantized targets (Baseline) 7.97 0.02 Quantized inputs, quantized targets 12.18 0.41 Quantized inputs, continuous targets 11.18 0.16 Continuous inputs, continuous targets 8.58 0.08 5.4 Ablations A difference to previous work [5, 4] is that we quantize the latent audio representations only for the contrastive loss, i.e., when latents are used as targets, but not when the latents are input to the Transformer network. We motivate this choice by an ablating for which we adopt a reduced training setup to increase experimental turn around: we pre-train BASE on LS-960 for 250k updates with masking probability p = 0.075, ﬁne-tune on train-10h for 60k updates on a single GPU with 640k samples per batch, or 40 sec of speech audio. We report the average WER and standard deviation on the concatenation of dev-clean and dev-other (dev PER) for three seeds of ﬁne-tuning. Table 4 shows that our strategy of continuous inputs with quantized targets (Baseline) performs best. Continuous latent speech representations retain more information to enable better context representations and quantizing the target representations leads to more robust training. Quantizing the latents both in the input and the targets performs least well, and explains the lower performance of prior work [5, 4]. Continuous targets reduce the effectiveness of self-supervised training since targets can capture detailed artifacts of the current sequence, e.g. speaker and background information, which make the task easier and prevent the model from learning general representations beneﬁcial to speech recognition. The training accuracy of identifying the correct latent audio representation increases from 62% to 78.0% when switching from quantized to continuous targets. Continuous inputs and continuous targets perform second best but various attempts to improve it did not lead to better results (see Appendix F for this experiment and other ablations on various hyperparameters). 6\n\nConclusion\nWe presented wav2vec 2.0, a framework for self-supervised learning of speech representations which masks latent representations of the raw waveform and solves a contrastive task over quantized speech representations. Our experiments show the large potential of pre-training on unlabeled data for speech processing: when using only 10 minutes of labeled training data, or 48 recordings of 12.5 seconds on average, we achieve a WER of 4.8/8.2 on test-clean/other of Librispeech. Our model achieves results which achieve a new state of the art on the full Librispeech benchmark for noisy speech. On the clean 100 hour Librispeech setup, wav2vec 2.0 outperforms the previous best result while using 100 times less labeled data. The approach is also effective when large amounts of labeled data are available. We expect performance gains by switching to a seq2seq architecture and a word piece vocabulary. dev PER test PER avg. WER std. Broader Impact There are around 7,000 languages in the world and many more dialects. However, for most of them no speech recognition technology exists since current systems require hundreds or thousands of hours of labeled data which is hard to collect for most languages. We have shown that speech recognition models can be built with very small amounts of annotated data at very good accuracy. We hope our work will make speech recognition technology more broadly available to many more languages and dialects.\n\nAcknowledgments\nWe thank Tatiana Likhomanenko and Qiantong Xu for helpful discussion and their help with wav2letter integration.\n\nReferences\n[1] J. L. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. arXiv, 2016. [2] P. Bachman, R. D. Hjelm, and W. Buchwalter. Learning representations by maximizing mutual information across views. In Proc. of NeurIPS, 2019. [3] A. Baevski and M. Auli. Adaptive input representations for neural language modeling. In Proc. of ICLR, 2018. [4] A. Baevski, M. Auli, and A. Mohamed. Effectiveness of self-supervised pre-training for speech recognition. arXiv, abs/1911.03912, 2019. [5] A. Baevski, S. Schneider, and M. Auli. vq-wav2vec: Self-supervised learning of discrete speech representations. In Proc. of ICLR, 2020. [6] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton. A simple framework for contrastive learning of visual representations. arXiv, abs/2002.05709, 2020. [7] J. Chorowski, R. J. Weiss, S. Bengio, and A. van den Oord. Unsupervised speech representation learning using wavenet autoencoders. arXiv, abs/1901.08810, 2019. [8] Y. Chung, W. Hsu, H. Tang, and J. R. Glass. An unsupervised autoregressive model for speech representation learning. arXiv, abs/1904.03240, 2019. [9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv, abs/1810.04805, 2018. [10] S. Dieleman, A. van den Oord, and K. Simonyan. The challenge of realistic music generation: modelling raw audio at scale. arXiv, 2018. [11] R. Eloff, A. Nortje, B. van Niekerk, A. Govender, L. Nortje, A. Pretorius, E. Van Biljon,\nE. van der Westhuizen, L. van Staden, and H. Kamper. Unsupervised acoustic unit discovery for\nspeech synthesis using discrete latent-variable neural networks. arXiv, abs/1904.07556, 2019. [12] A. Fan, E. Grave, and A. Joulin. Reducing transformer depth on demand with structured dropout. In Proc. of ICLR, 2020. [13] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, D. S. Pallett, and N. L. Dahlgren. The DARPA TIMIT Acoustic-Phonetic Continuous Speech Corpus CDROM. Linguistic Data Consortium, 1993. [14] A. Graves, S. Fernández, and F. Gomez. Connectionist temporal classiﬁcation: Labelling unsegmented sequence data with recurrent neural networks. In Proc. of ICML, 2006. [15] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang. Conformer: Convolution-augmented transformer for speech recognition. arXiv, 2020. [16] E. J. Gumbel. Statistical theory of extreme values and some practical applications: a series of lectures, volume 33. US Government Printing Ofﬁce, 1954. [17] W. Han, Z. Zhang, Y. Zhang, J. Yu, C.-C. Chiu, J. Qin, A. Gulati, R. Pang, and Y. Wu. Contextnet: Improving convolutional neural networks for automatic speech recognition with global context. arXiv, 2020. [18] D. Harwath, W.-N. Hsu, and J. Glass. Learning hierarchical discrete linguistic units from visually-grounded speech. In Proc. of ICLR, 2020. [19] K. He, H. Fan, Y. Wu, S. Xie, and R. Girshick. Momentum contrast for unsupervised visual representation learning. arXiv, abs/1911.05722, 2019. [20] O. J. Hénaff, A. Razavi, C. Doersch, S. M. A. Eslami, and A. van den Oord. Data-efﬁcient image recognition with contrastive predictive coding. arXiv, abs/1905.09272, 2019. [21] D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv, 2016. [22] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Weinberger. Deep networks with stochastic depth. arXiv, 2016. [23] M. G. A. Hyvärinen. Noise-contrastive estimation: A new estimation principle for unnormalized statistical models. In Proc. of AISTATS, 2010. [24] E. Jang, S. Gu, and B. Poole. Categorical reparameterization with gumbel-softmax. arXiv, abs/1611.01144, 2016. [25] H. Jegou, M. Douze, and C. Schmid. Product quantization for nearest neighbor search. IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117–128, Jan. 2011. [26] D. Jiang, X. Lei, W. Li, N. Luo, Y. Hu, W. Zou, and X. Li. Improving transformer-based speech recognition using unsupervised pre-training. arXiv, abs/1910.09932, 2019. [27] J. Kahn et al. Libri-light: A benchmark for asr with limited or no supervision. In Proc. of ICASSP, 2020. [28] K. Kawakami, L. Wang, C. Dyer, P. Blunsom, and A. van den Oord. Learning robust and multilingual speech representations. arXiv, 2020. [29] D. P. Kingma and J. Ba. Adam: A Method for Stochastic Optimization. In Proc. of ICLR, 2015. [30] A. Laptev, R. Korostik, A. Svischev, A. Andrusenko, I. Medennikov, and S. Rybin. You do not need more data: Improving end-to-end speech recognition by text-to-speech data augmentation. arXiv, abs/2005.07157, 2020. [31] M. P. Lewis, G. F. Simon, and C. D. Fennig. Ethnologue: Languages of the world, nineteenth edition. Online version: http://www.ethnologue.com, 2016. [32] A. H. Liu, T. Tu, H. yi Lee, and L. shan Lee. Towards unsupervised speech recognition and synthesis with quantized speech representation learning. arXiv, 2019. [33] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. [34] C. Lüscher, E. Beck, K. Irie, M. Kitza, W. Michel, A. Zeyer, R. Schlüter, and H. Ney. Rwth asr systems for librispeech: Hybrid vs attention. In Interspeech 2019, 2019. [35] C. J. Maddison, D. Tarlow, and T. Minka. A* sampling. In Advances in Neural Information Processing Systems, pages 3086–3094, 2014. [36] I. Misra and L. van der Maaten. Self-supervised learning of pretext-invariant representations. arXiv, 2019. [37] A. Mohamed, D. Okhonko, and L. Zettlemoyer. Transformers with convolutional context for ASR. arXiv, abs/1904.11660, 2019. [38] M. Ott, S. Edunov, D. Grangier, and M. Auli. Scaling neural machine translation. In Proc. of WMT, 2018. [39] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proc. of NAACL System Demonstrations, 2019. [40] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur. Librispeech: an asr corpus based on public domain audio books. In Proc. of ICASSP, pages 5206–5210. IEEE, 2015. [42] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V. Le. Improved noisy student training for automatic speech recognition. arXiv, abs/2005.09629, 2020. [43] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In Proc. of ACL, 2018. [44] V. Pratap, A. Hannun, Q. Xu, J. Cai, J. Kahn, G. Synnaeve, V. Liptchinsky, and R. Collobert. Wav2letter++: A fast open-source speech recognition system. In Proc. of ICASSP, 2019. [45] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever. Improving language understanding by generative pre-training. https://s3-us-west-2.amazonaws.com/openai-assets/ research-covers/language-unsupervised/language_understanding_paper.pdf, 2018. [46] M. Ravanelli, P. Brakel, M. Omologo, and Y. Bengio. Light gated recurrent units for speech recognition. IEEE Transactions on Emerging Topics in Computational Intelligence, 2(2):92–102, 2018. [47] M. Ravanelli, J. Zhong, S. Pascual, P. Swietojanski, J. Monteiro, J. Trmal, and Y. Bengio. Multi-task self-supervised learning for robust speech recognition. arXiv, 2020. [48] M. Rivière, A. Joulin, P.-E. Mazaré, and E. Dupoux. Unsupervised pretraining transfers well across languages. arXiv, abs/2002.02848, 2020. [49] S. Schneider, A. Baevski, R. Collobert, and M. Auli. wav2vec: Unsupervised pre-training for speech recognition. In Proc. of Interspeech, 2019. [50] M. Schuster and K. Nakajima. Japanese and korean voice search. In Proc. of ICASSP, 2012. [51] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert. End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures. arXiv, abs/1911.08460, 2020. [52] A. Tjandra, B. Sisman, M. Zhang, S. Sakti, H. Li, and S. Nakamura. Vqvae unsupervised unit discovery and multi-scale code2spec inverter for zerospeech challenge 2019. arXiv, 1905.11449, 2019. [53] A. van den Oord, O. Vinyals, et al. Neural discrete representation learning. In Advances in Neural Information Processing Systems, pages 6306–6315, 2017. [54] A. van den Oord, Y. Li, and O. Vinyals. Representation learning with contrastive predictive coding. arXiv, abs/1807.03748, 2018. [55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and\nI. Polosukhin. Attention is all you need. In Proc. of NIPS, 2017.\n[56] W. Wang, Q. Tang, and K. Livescu. Unsupervised pre-training of bidirectional speech encoders via masked reconstruction. arXiv, 2020. [57] F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. In Proc. of ICLR, 2019. [58] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert. Iterative pseudo-labeling for speech recognition. arXiv, 2020. [59] N. Zeghidour, N. Usunier, I. Kokkinos, T. Schaiz, G. Synnaeve, and E. Dupoux. Learning ﬁlterbanks from raw speech for phone recognition. In Proc. of ICASSP, 2018. [60] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar. Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss. arXiv, 2020.\n\nAppendices A Masking distribution When choosing which time-steps to mask, each latent speech representation in an utterance is considered a candidate starting time-step with probability p where M is the length of each masked span starting from the respective time step; both are hyper-parameters. Sampled starting time steps are expanded to length M and spans can overlap. For a 15 sec long audio sample, the average mask length is 14.7 time-steps, corresponding to 299ms of audio, with a median of 10 time-steps, and a maximum of about 100 time steps; about 49% of all time-steps in the sample will be masked. A plot of the corresponding mask length distribution is shown in Figure 2 and an ablation of M and p as well as the effect of other masking strategies is shown in Table 5. Reducing M results in increased prediction accuracy for the self-supervised but the task becomes trivial when spans with length one are masked, leading to poor performance on downstream speech recognition tasks. We also consider other masking strategies: w/o overlap uniform(a,b) samples for each starting index a span length M s from interval a to b and masks the subsequent M s time-steps taking care not to overlap with existing spans; poisson(λ) and normal(µ, σ) sample M s from Poisson and normal distributions. 0.25 0.20 Percent of all spans 0.15 0.10 0.05 0.00 20 40 60 80 100 Span length Figure 2: Mask length distribution for a 15 second sample with p = 0.065 and M = 10. Table 5: Ablations on settings for the masking strategy during pre-training. When masking without overlap, we choose starting time steps with p = 0.037 which results in the total number of masked tokens to match the baseline. Baseline (p = 0.075) 7.97 0.02 Mask length M = 8 8.33 0.05 Mask length M = 12 8.19 0.08 Mask length M = 15 8.43 0.19 Mask probability p = 0.065 7.95 0.08 Mask probability p = 0.06 8.14 0.22 Mask w/o overlap, uniform(1,31) 8.39 0.02 Mask w/o overlap, uniform(10,30) 9.17 0.05 Mask w/o overlap, poisson(15) 8.13 0.04 Mask w/o overlap, normal(15, 10) 8.37 0.03 Mask w/o overlap, length 10 9.15 0.02 Mask w/o overlap, length 15 9.43 0.26 avg WER std B Fine-tuning Setup During ﬁne-tuning we apply a masking strategy to the feature encoder outputs similar to SpecAugment [41]: we randomly choose a number of starting time steps for which a span of ten subsequent time-steps is replaced with a mask embedding; spans may overlap and we use the same masked time step embedding as during pre-training. We also mask channels by choosing a number of channels as starting indices and then expand each one to cover the subsequent 64 channels. Spans may overlap and the selected channel spans are set to zero value. We use LayerDrop [22, 12] at a rate of 0.05 for BASE and 0.1 for LARGE during ﬁne-tuning. Table 6 summarizes the ﬁne-tuning hyper-parameter settings used for the different labeled data setup. Table 7 shows the decoding parameters used for ﬁnal evaluations of the various labeled data setups for Librispeech pre-trained models and Table 8 shows decoding parameters for LibriVox. Table 6: Fine-tuning hyperparameters timestep mask prob. channel mask prob. updates 10 min 0.075 0.008 12k 1 hour 0.075 0.004 13k 10 hours 0.065 0.004 20k 100 hours 0.05 0.008 50k 960 hours 0.05 0.0016 320k TIMIT 0.065 0.012 40k Table 7: Decoding parameters for Librispeech subsets for models pre-trained on Librispeech 4gram LM weight 4gram word insert. TransLM weight TransLM word insert. 10 min 3.23 -0.26 1.20 -1.39 1 hour 2.90 -1.62 1.15 -2.08 10 hours 2.46 -0.59 1.06 -2.32 100 hours 2.15 -0.52 0.87 -1.00 960 hours 1.74 0.52 0.92 -0.86 Table 8: Decoding parameters for Librispeech subsets for models pre-trained on Librivox. 4gram LM weight 4gram word insert. TransLM weight TransLM word insert. 10 min 3.86 -1.18 1.47 -2.82 1 hour 3.09 -2.33 1.33 -0.69 10 hours 2.12 -0.90 0.94 -1.05 100 hours 2.15 -0.52 0.87 -1.00 960 hours 1.57 -0.64 0.90 -0.31 C Full results for Libri-light and Librispeech Table 9: WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups (cf. Table 1). Model Unlabeled LM dev test data clean other clean other 10 min labeled BASE LS-960 None 46.1 51.5 46.9 50.9 4-gram 8.9 15.7 9.1 15.6 Transf. 6.6 13.2 6.9 12.9 LARGE LS-960 None 43.0 46.3 43.5 45.3 4-gram 8.6 12.9 8.9 13.1 Transf. 6.6 10.6 6.8 10.8 LARGE LV-60k None 38.3 41.0 40.2 38.7 4-gram 6.3 9.8 6.6 10.3 Transf. 4.6 7.9 4.8 8.2 1h labeled BASE LS-960 None 24.1 29.6 24.5 29.7 4-gram 5.0 10.8 5.5 11.3 Transf. 3.8 9.0 4.0 9.3 LARGE LS-960 None 21.6 25.3 22.1 25.3 4-gram 4.8 8.5 5.1 9.4 Transf. 3.8 7.1 3.9 7.6 LARGE LV-60k None 17.3 20.6 17.2 20.3 4-gram 3.6 6.5 3.8 7.1 Transf. 2.9 5.4 2.9 5.8 10h labeled BASE LS-960 None 10.9 17.4 11.1 17.6 4-gram 3.8 9.1 4.3 9.5 Transf. 2.9 7.4 3.2 7.8 LARGE LS-960 None 8.1 12.0 8.0 12.1 4-gram 3.4 6.9 3.8 7.3 Transf. 2.9 5.7 3.2 6.1 LARGE LV-60k None 6.3 9.8 6.3 10.0 4-gram 2.6 5.5 3.0 5.8 Transf. 2.4 4.8 2.6 4.9 100h labeled BASE LS-960 None 6.1 13.5 6.1 13.3 4-gram 2.7 7.9 3.4 8.0 Transf. 2.2 6.3 2.6 6.3 LARGE LS-960 None 4.6 9.3 4.7 9.0 4-gram 2.3 5.7 2.8 6.0 Transf. 2.1 4.8 2.3 5.0 LARGE LV-60k None 3.3 6.5 3.1 6.3 4-gram 1.8 4.5 2.3 4.6 Transf. 1.9 4.0 2.0 4.0 Table 10: WER on Librispeech when using all 960 hours of Librispeech as labeled data (cf. Table 2). Model Unlabeled LM dev test data clean other clean other LARGE - from scratch - None 2.8 7.6 3.0 7.7 - 4-gram 1.8 5.4 2.6 5.8 - Transf. 1.7 4.3 2.1 4.6 BASE LS-960 None 3.2 8.9 3.4 8.5 4-gram 2.0 5.9 2.6 6.1 Transf. 1.8 4.7 2.1 4.8 LARGE LS-960 None 2.6 6.5 2.8 6.3 4-gram 1.7 4.6 2.3 5.0 Transf. 1.7 3.9 2.0 4.1 LARGE LV-60k None 2.1 4.5 2.2 4.5 4-gram 1.4 3.5 2.0 3.6 Transf. 1.6 3.0 1.8 3.3 D Analysis of Discrete Latent Speech Representations Next, we investigate whether the discrete latent speech representations qt learned by the quantizer relate to phonetic information: Using LARGE pre-trained on LV-60k and without any ﬁne-tuning, we compute the discrete latents for the training data of TIMIT and compute the co-occurrence between human annotated phonemes and the latents. Ties are broken by choosing the phoneme which is most represented in the receptive ﬁeld of qt. The training data contains 3696 utterances of average length 13.6 sec, or 563k discrete latents. Figure 3 plots P(phoneme|qt) and shows that many discrete latents appear to specialize in speciﬁc phonetic sounds. The silence phoneme (bcl) represents 22% of all human annotated speech data and is therefore also modeled by many different latents. aa ae ah aw ay b ch d dh dx eh axr ey f g bcl hh ih iy jh k el em en eng ow oy p r s sh t th uh uw v w y z Figure 3: Visualization of the co-occurrence between discrete latent speech representations and phonemes. We plot the conditional probability P(phoneme|qt) on TIMIT train data. The y-axis shows the collapsed 39 classes of phonemes and the x-axis is over the different discrete latents. E Speech Recognition Error Analysis In this section we study the most common errors our models make when ﬁne-tuned on different amounts of labeled data (Table 11). We also show transcriptions of a few relatively challenging utterances from the dev-clean subset of Librispeech (Table 12). We consider models with no lexicon or no language model decoding, marked None in Table 9: Larger capacity decreases error rates: LARGE on LS-960 improves the word error rate on dev-clean from 46.1 to 43 compared to BASE. Increasing the amount of unlabeled training data further decreases the error rate to 33.8 for LARGE on LS-960. In the ten minute labeled data setup, the model is still able to recognize basic units of speech: Table 11 shows that most errors are around spelling of words, e.g., omitting silent characters such as could →coud, know →now, or ignoring repeated letters such as still →stil, little →litle. The LARGE LV-60k model achieves WER 38.3 on dev-clean and adding a Transformer language model enables to choose more likely pronunciations during the search and gives a large WER improvement to 5.0. The ten minute models without lexicon and language model tend to spell words phonetically and omit repeated letters, e.g., will →wil (Table 11). Spelling errors decrease with more labeled data: with one hour of labeled data, slightly less common words move into the list of the most frequent errors, e.g., heaven and food are spelled phonetically. At ten hours, top errors include articles, e.g., a, the which are a common source of errors in speech recognition in general. There are also alternative spellings, color vs. colour as well as relatively rare words including person names, still spelled phonetically, e.g., phoebe →feeby. At 100 hours, person names dominate the most frequent errors: phoebe →phebe, along with incorrect spacing anyone →any one, awhile →a while. Finally at 960 hours the word error rate falls to 2% and top errors are mostly articles, incorrect splits, and some very rare words or names such as deucalion or gryce. The “from scratch” 960 hour model has a similar word error rate as the 100 hour pre-trained model and displays a similar pattern of errors. The pre-trained speech representations can be easily adapted to recognize speciﬁc sounds while ﬁne-tuning grounds these representations to the actual spelling. Table 11: Top word errors for models trained on 10m, 1h and 10h, 100h, 960h of labeled data and decoded on the Librispeech dev-clean subset without a language model or lexicon (see Table 9 and Table 10 - None). In brackets is the total number of occurrences of each error. 10m LARGE LV-60k 1h LARGE LV-60k 10h LARGE LV-60k all →al (181) too →to (26) in →and (15) are →ar (115) until →untill (24) a →the (11) will →wil (100) new →knew (22) o →oh (10) you →yo (90) door →dor (18) and →in (9) one →on (89) says →sais (18) mode →mod (9) two →to (81) soul →sol (17) ursus →ersus (9) well →wel (80) bread →bred (16) tom →tome (8) been →ben (73) poor →pore (16) randal →randol (7) upon →apon (73) a →the (13) the →a (7) good →god (67) either →ither (13) color →colour (6) see →se (66) food →fud (13) ﬂour →ﬂower (6) we →whe (60) doubt →dout (12) phoebe →feeby (6) little →litle (54) earth →erth (12) an →and (5) great →grate (53) led →lead (12) cucumbers →cucombers (5) your →yor (53) sea →see (12) egg →eg (5) could →coud (51) thee →the (12) macklewain →macklewaine (5) here →hear (51) tom →tome (12) magpie →magpi (5) know →now (45) add →ad (11) milner →millner (5) there →ther (45) good →god (11) stacy →staci (5) three →thre (45) heaven →heven (11) trevelyan →trevellion (5) still →stil (42) mary →marry (11) verloc →verlock (5) off →of (40) randal →randel (11) ann →an (4) don’t →dont (37) answered →ansered (10) anyone →one (4) shall →shal (36) blood →blod (10) apartment →appartment (4) little →litl (35) bozzle →bosel (10) basin →bason (4) 100h LARGE LV-60k 960h LARGE LV-60k 960h LARGE from scratch a →the (13) a →the (12) and →in (20) and →in (10) and →in (9) a →the (16) in →and (10) macklewain →mackelwaine (7) in →and (13) o →oh (8) in →and (6) the →a (10) minnetaki →minnitaki (7) o →oh (6) in →an (8) randal →randall (7) bozzle →bosell (5) and →an (5) christie →cristy (6) criss →chris (5) clarke →clark (4) macklewain →mackelwane (6) bozzle →bosel (4) grethel →gretel (4) randal →randoll (6) clarke →clark (4) macklewain →mackelwaine (4) bozzle →bosall (5) colored →coloured (4) this →the (4) kaliko →calico (5) grethel →gretel (4) an →and (3) trevelyan →trevelian (5) lige →lyge (4) anyone →one (3) an →and (4) the →a (4) bozzle →basell (3) and →an (4) and →an (3) buns →bunds (3) anyone →one (4) ann →marianne (3) carrie →carry (3) bozzle →bozall (4) butte →bute (3) criss →chris (3) clarke →clark (4) color →colour (3) he’s →is (3) gryce →grice (4) deucalion →ducalion (3) his →is (3) i’m →am (4) forcemeat →meat (3) honor →honour (3) in →ind (4) gryce →grice (3) lattimer →latimer (3) letty →lettie (4) honor →honour (3) millet →mellet (3) phoebe →phebe (4) kearny →kirney (3) pyncheon →pension (3) the →a (4) nuova →noiva (3) tad →ted (3) ann →anne (3) thing →anything (3) thing →anything (3) awhile →while (3) this →the (3) trevelyan →trevelian (3) Table 12: Examples of transcription of selected utterances from the dev-clean subset by various models without a language model or lexicon. Capitalized words indicate errors. Model Transcription Reference i’m mister christopher from london 10m LV-60k IM mister CRESTIFER FROME LUNDEN 1h LV-60k IM mister CRISTIFFHER from LOUNDEN 10h LV-60k i’m mister CHRYSTEPHER from london 100h LV-60k i’m mister christopher from london 960h LV-60k i’m mister christopher from london 960h scratch I MISSTER christopher from london Reference il popolo e una bestia 10m LV-60k ILPOPULAR ONABESTIA 1h LV-60k O POPOLAONABASTIA 10h LV-60k U POPULAONABASTIAR 100h LV-60k O POPALOON A BASTYA 960h LV-60k YOU’LL POP A LAWYE ON A BAISTYE 960h scratch OL POPALOY ON ABESTIA Reference he smelt the nutty aroma of the spirit 10m LV-60k he SMELTD the NUDY aroma of the spirit 1h LV-60k he SMELTD the NUDDY ARROMA of the spirit 10h LV-60k he smelt the NUDDY ERROMA of the spirit 100h LV-60k he smelt the NUDDY aroma of the spirit 960h LV-60k he smelt the NUTTIE aroma of the spirit 960h scratch he smelt the nutty EROMA of the spirit Reference phoebe merely glanced at it and gave it back 10m LV-60k FEABY MEARLY glanced at it and gave it BAK 1h LV-60k FIEABY merely glanced at it and gave it back 10h LV-60k FEEBY merely glanced at it and gave it back 100h LV-60k BEBE merely glanced at it and gave it back 960h LV-60k phoebe merely glanced at it and gave it back 960h scratch phoebe merely glanced at it and gave it back Reference sauterne is a white bordeaux a strong luscious wine the best known varieties being 10m LV-60k SULTERIN is a white BORDOE a strong LUCHOUS WIN the best NOWN VERIATYS being 1h LV-60k CLTEREN is a white BORDO a strong LUCHIOUS wine the best known VERIETIES being 10h LV-60k SOTERN is a white BOURDO a strong LUCIOUS wine the best known VORIETIES being 100h LV-60k SOTERN is a white BORDAUX a strong LUCIOUS wine the best known varieties being 960h LV-60k SOTERN is a white bordeaux a strong luscious wine the best known varieties being 960h scratch SOTERAN is a white bordeaux a strong luscious wine the best known varieties being Reference i happen to have mac connell’s box for tonight or there’d be no chance of our getting places 10m LV-60k i HAPEND to have MECONALES BOXS for TONIT ORE THIRLD be no chance of OR GETING places 1h LV-60k i happen to have MACCONNEL’S BOCXS for tonight or TE’ELD be no chance of our getting places 10h LV-60k i HAPPENED to have MUKONNEL’S box for tonight or THERED be no chance of our getting places 100h LV-60k i HAPPENED to have MC CONNEL’S box for TO NIGHT or there’d be no chance of our getting places 960h LV-60k i happen to have MC CONALL’S box for TO NIGHT or there’d be no chance of our getting places 960h scratch i HAPPENE to have MACONEL’S box for TO NIGHT or there’d be no chance of our getting places F Ablations Table 13 ablates various hyperparameter choices of our architecture. The setup for the baseline model is described in § 5.4. First, we tried to improve the continuous input and continuous target model (§ 5.4) by adding an MLP on top of the continuous target representation and we also tried to use a separate set of encoder parameters for the representations used as input and targets (Separate encoders). Both did not lead to meaningful improvements. Increasing the receptive ﬁeld size from 25ms to 30ms had little effect. Setting the diversity penalty weight (α) too low results in lower codebook usage and lower performance. Setting it too high leads to slight instability. Doubling the number of relative positional embeddings to 256 also did not help. Stopping gradients from the quantizer to the encoder shows that the encoder requires training signal from the quantizer as well. Next, increasing the number of negatives did not result in better performance (K = 200) and sampling negatives from the entire batch of utterances hurt performance, likely because candidates from other utterances are easy to distinguish. Sampling negatives from any time step in the utterance, masked or unmasked, does not help and is more computationally expensive. Gumbel noise is important and increasing the number of codebooks did not result in better performance. Table 13: Ablation of various hyper-parmeter choices. We report average WER and standard deviation on combined dev-clean/other of Librispeech for three seeds of training. Baseline (p = 0.075, α = 0.1) 7.97 0.02 Continuous inputs, continuous targets 8.58 0.08 + MLP on targets 8.51 0.05 + Separate encoders 8.90 0.01 receptive ﬁeld 30ms 7.99 0.06 diversity penalty α = 0 8.48 0.08 α = 0.05 8.34 0.08 α = 0.2 8.58 0.45 Conv pos emb, kernel 256 8.14 0.05 No gradient to encoder from quantizer 8.41 0.08 Negatives K = 200 same utterance 8.12 0.05 K = 50 same utterance + K = 50 from batch 8.79 0.06 Sample negatives from any time step 8.07 0.02 No Gumbel noise 8.73 0.42 Codebook G=4, V=18 9.02 0.38 G=8, V=8 8.13 0.07 Predict exactly U time steps from edges U = 1 9.53 0.91 U = 5 8.19 0.07 U = 10 8.07 0.07 U = 15 7.89 0.10 U = 20 7.90 0.01 We also investigated predicting only time steps immediately next to the last unmasked time step for each span. This enables to better control the difﬁculty of the pre-training task. Given the leftmost or rightmost unmasked time step next to a masked span, we compute the contrastive loss only for the ﬁrst U masked time steps next to these unsmasked spans. Predicting only up to one time step performs poorly because there is little training signal from each utterance and predicting more time steps performs better but does not signiﬁcantly outperform predicting all masked time steps. Increasing the number of training updates helps but this increases training time. avg. WER std.", "persona": "Student", "response": "### 1. Overview\nImage by snowing from Freepik.com References Figure 1. Training phases of Wav2Vec 2.0 (Image by author) How do we choose the best code word from every codebook? The answer is Gumbel softmax: where: As presented in the next picture, we have sampled two vectors marked with orange colour as starting indices. where: Fine-tuning [2] A.\n\n### 2. Background\nKim, The intuition behind Shannon’s Entropy (2018), Towards Data Science The architecture of the final model used for prediction consists of three main parts: convolutional layers that process the raw waveform input to get latent representation – Z , , transformer layers, creating contextualised representation – C , , linear projection to output – Y. That is what the model looks like after final fine-tuning, ready to be developed in a production environment. The whole magic happens during the first phase of training, in the self-supervised mode, when the model looks a little different. The model is trained without the linear projection generating an output prediction.\n\n### 3. Methods\nContrastive learning is a concept in which the input is transformed in two different ways. Afterwards, the model is trained to recognise whether two transformations of the input are still the same object. In Wav2Vec 2.0, the transformer layers are the first way of transformation, the second one is made by quantization, which will be explained in the further part of this article. More formally, for a masked latent representation zₜ, we would like to get such a context representation cₜ to be able to guess the correct quantized representation qₜ among other quantized representations. It is important to understand the previous sentence well, so do not hesitate to stop here if you need to 🙂 The version of Wav2Vec 2.0 used for self-supervised training is presented in Figure 4. Figure 4. Wav2Vec 2.0 model architecture for self-supervised training (Image by author, based on wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations) Quantization Quantization is a process of converting values from a continuous space into a finite set of values in a discrete space. Figure 5.\n\n### 4. Results\nThe first phase of training is the main advantage of this model. Learning a very good speech representation enables achieving state-of-the-art results on a small amount of labeled data. For example, the authors of the url have pre-trained the model on a huge LibriVox dataset. Then, they’ve used the whole Libri Speech dataset for fine-tuning which resulted in 1.8% Word Error Rate (WER) on test-clean subset and 3.3% WER on test-other. Using nearly 10 times less data, allowed to get 2.0% WER on test-clean and 4.0% on test-other. Using only 10 minutes of labeled training data, which is almost no data, resulted in 4.8% / 8.2% WER on test-clean / test-other subsets of Libri Speech. According to urls With Code, it would have been around the state-of-the-art level in January 2018[5]. Since it’s a classification task, a softmax function seems to be a natural choice for choosing the best code word in every codebook. Why, in our case, is Gumbel softmax better than a normal softmax? It comes with two improvements: randomization and temperature 𝜏. Due to randomization, the model is more willing to choose different code words during training and then to update their weights. It is important, especially in the beginning of training, to prevent using only a subset of codebooks.\n\n### 5. Takeaways\nWav2Vec 2.0 is one of the current state-of-the-art models for Automatic Speech Recognition due to a self-supervised training which is quite a new concept in this field. This way of training allows us to pre-train a model on unlabeled data which is always more accessible. Then, the model can be fine-tuned on a particular dataset for a specific purpose. As the previous works show this way of training is very powerful[4]. As presented in the picture below, the model is trained in two phases. The first phase is in a self-supervised mode, which is done using unlabeled data and it aims to achieve the best speech representation possible. You can think about that in a similar way as you think of word embeddings. Word embeddings also aim to achieve the best representation of natural language. The main difference is that Wav2Vec 2.0 processes audio instead of text. The second phase of training is supervised fine-tuning, during which labeled data is used to teach the model to predict particular words or phonemes. If you are not familiar with the word ‘phoneme’, you can think about it as the smallest possible unit of sound in a particular language, usually represented by one or two letters. Figure 3.", "paper_id": "24", "story_id": "24-01"}
{"paper": "Large language models (LLMs) have shown promise in medical question answering, with Med-PaLM being the frst to exceed a ‘passing’ score in United States Medical Licensing Examination style questions. However, challenges remain in long-form medical question answering and handling real-world workfows. Here, we present Med-PaLM 2, which bridges these gaps with a combination of base LLM improvements, medical domain fne-tuning and new strategies for improving reasoning and grounding through ensemble refnement and chain of retrieval. Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19%, and demonstrates dramatic performance increases across MedMCQA, PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations framework shows that physicians prefer Med-PaLM 2 answers to those from other physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates signifcant improvements over its predecessor across all evaluation metrics, particularly on new adversarial datasets designed to probe LLM limitations (P < 0.001). In a pilot study using real-world medical questions, specialists preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. While specialist answers were still preferred overall, both specialists and generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating its growing potential in real-world medical applications.\n\nLanguage is at the heart of health and medicine, underpinning interactions between people and care providers. Progress in LLMs has enabled the exploration of medical domain capabilities in artificial intelligence (AI) systems that can understand and communicate using language, promising richer human–AI interaction and collaboration. In particular, these models have demonstrated impressive capabilities on multiple-choice research benchmarks1–3 . The advent of transformers4  and LLMs5,6  has renewed interest in the possibilities of AI for medical question-answering tasks—a long-standing ‘grand challenge’7–9 . A majority of these approaches involve smaller language models trained using domain-specific data (BioLinkBert10, DRAGON11, PubMedGPT12, PubMedBERT13, BioGPT14), resulting in steady improvements in performance on benchmark datasets such as MedQA (United States Medical Licensing Examination (USMLE))15, MedMCQA16 and PubMedQA17. The rise of larger general-purpose LLMs such as GPT-3 (ref. 18) and Flan-PaLM19,20 trained on internet-scale corpora with massive computing infrastructure has seen leapfrog improvements on such benchmarks within a few months (Fig. 1). In particular, GPT-3.5 (ref. 3) reached an accuracy of 60.2% on the MedQA (USMLE) dataset, Flan-PaLM reached an accuracy of 67.6% and GPT-4-base2  achieved 86.1%. In parallel, application protocol interface (API) access to the GPT family of models spurred several studies evaluating the specialized clinical knowledge in these models, without specific alignment to the medical domain. Levine et al.21 evaluated the diagnostic and triage accuracies of GPT-3 for 48 validated case vignettes of both common and severe conditions and compared to laypeople and physicians. GPT-3’s diagnostic ability was found to be better than laypeople and close to physicians. On triage, performance was less impressive and closer to laypeople. Similarly, GPT-3 performance in genetics, surgery and ophthalmology was studied in refs. 22–24, respectively. Ayers et al.25 compared ChatGPT and physician answers on 195 randomly drawn patient questions from a social media forum and found ChatGPT answers to be rated higher in both quality and empathy. In our previous work on Med-PaLM, we demonstrated the importance of a wide-ranging benchmark for medical question answering, detailed human evaluation of model answers and alignment strategies in the medical domain1 . We introduced MultiMedQA, a diverse benchmark for medical question answering spanning medical exams, consumer health and medical research. We proposed a human evaluation rubric enabling physicians and laypeople to perform detailed assessment of model answers. Our initial model, Flan-PaLM, achieved strong performance across multiple-choice benchmarks. However, human evaluation revealed further work was necessary to ensure factual long-form answers aligned with human values and expectations in this safety-critical domain (a process generally referred to as ‘alignment’). We developed Med-PaLM, resulting in substantially improved physician evaluations over Flan-PaLM. However, evaluation on these benchmarks was limited as a measure of practical utility in real-world workflows, and key shortfalls remained compared to physician answers. Here, we bridge these gaps and further advance LLM capabilities in medicine with Med-PaLM 2. We developed this model using a combination of an improved base LLM (PaLM 2; ref. 26), medical domain-specific fine-tuning and new prompting strategies to improve reasoning and grounding, including ensemble refinement and chain of retrieval. Med-PaLM 2 improves upon Med-PaLM by over 19% on MedQA, as depicted in Fig. 1, and approached or exceeded previous state-of-the-art performance on MedMCQA, PubMedQA and MMLU clinical topics datasets. While these benchmarks are a useful measure of the knowledge encoded in LLMs, they do not capture a model’s ability to generate factual, safe answers to questions that require nuanced answers, typical in real-world medical question answering. We study this by expanding our evaluation framework for physicians and laypeople1 . We introduce two additional human evaluations: a pairwise ranking evaluation of model and physician answers to consumer medical questions along nine clinically relevant axes; and physician assessment of model answers on two recently introduced adversarial testing datasets27 designed to probe the limits of LLMs. Finally, we study the practical utility of Med-PaLM 2 for bedside consultations. In a pilot study, we answer real-world medical questions submitted by specialist physicians to a consultation service during routine care delivery28,29. Answering these questions is nontrivial: in the consultation service, a team of physicians analyzed aggregate patient data to provide a written report. Compared to answers from specialist and generalist physicians, answers from Med-PaLM 2 using chain of retrieval are comparable to or better than generalists’ answers but remain inferior to specialists’ answers. These results suggest that, as model performance approaches a human level, evaluation with highly specialized experts becomes crucial, and current models may have utility in supporting information needs of medical staff where access to specialist physicians is limited. Our key contributions are summarized as follows: (1) We developed Med-PaLM 2, a medical LLM trained using an updated base model (PaLM 2; ref. 26) and targeted medical domain-specific fine-tuning. (2) We introduced ‘ensemble refinement’ as a prompting strategy to improve LLM reasoning. (3) We described ‘chain of retrieval’, a step-by-step pipeline using search as a tool that enables Med-PaLM 2 to answer difficult medical research questions by grounding its claims in relevant sources. (4) Med-PaLM 2 achieved state-of-the-art results on several MultiMedQA multiple-choice benchmarks, including MedQA USMLE-style questions, improving upon Med-PaLM performance by over 19% (Table 1). (5) Building upon our previous work1 , we incorporated several key enhancements to the human evaluation framework. These include new adversarial and bedside consultation datasets, as well as a pairwise ranking system that compares model responses directly with those of human physicians. (6) Human evaluation of long-form answers to consumer medical questions showed that Med-PaLM 2’s answers were preferred to physician and Med-PaLM answers across eight of nine axes relevant to clinical utility, such as factuality and low likelihood of harm (Figs. 2 and 3). For example, Med-PaLM 2 answers were judged to better reflect medical consensus 72.9% of the time compared to physician answers (Fig. 1). (7) We introduced two adversarial question datasets to probe the safety and limitations of these models. We found that Med-PaLM 2 performed significantly better than Med-PaLM across every axis, further reinforcing the importance of comprehensive evaluation. For instance, answers had low risk of harm for 90.6% of Med-PaLM 2 answers, compared to 79.4% for Med-PaLM (Fig. 2 and Supplementary Table 4). (8) For real-world questions that arose during care delivery, specialists preferred Med-PaLM 2 answers over generalist physician answers 65% of the time, while generalists preferred them equally. Model answers remained inferior to specialist answers; both specialists and generalists preferred specialist answers about 60% of the time. Specialists and generalists viewed Med-PaLM 2 answers to be as safe as physician answers (Fig. 4).\n\nResults Table 1 and Supplementary Table 1 summarize Med-PaLM 2 results on MultiMedQA multiple-choice benchmarks. Unless specified otherwise, Med-PaLM 2 refers to the unified model trained on the mixture in Extended Data Table 1. We also include comparisons to GPT-4 (refs. 2,30). We note that comparisons to GPT-4 are not straightforward because it is a proprietary system and we are not able to measure overlap of the evaluation data with the model’s training data as we did for Med-PaLM 2 in Table 2. MedQA Our unified Med-PaLM 2 model reaches an accuracy of 85.4% using ER as a prompting strategy. Our best result on this dataset is 86.5%, obtained from a version of Med-PaLM 2 not aligned for consumer medical question answering, but instead instruction fine-tuned only on MedQA. MedMCQA On MedMCQA, Med-PaLM 2 obtains a score of 72.3%, exceeding Flan-PaLM performance by over 14% but slightly short of previous state-of-the-art performance (73.66 from GPT-4-base30). PubMedQA On PubMedQA, Med-PaLM 2 obtains a score of 75.0%. This is below the state-of-the-art performance (81.0 from BioGPT-Large14) and is likely because no data were included for this dataset for instruction fine-tuning. However, after further exploring prompting strategies for PubMedQA on the development set, the unified model reached an accuracy of 79.8% with a single run and 81.8% using self-consistency (11×). The latter result was state of the art, although we caution that PubMedQA’s test set is small (500 examples), and remaining failures of Med-PaLM 2 and other strong models appear to be largely attributable to label noise intrinsic in the dataset (especially given human performance is 78.0%17). MMLU clinical topics On MMLU clinical topics, Med-PaLM 2 significantly improves over previously reported results in Med-PaLM1  and exceeds previous state-of-the-art performance on three out six topics, with GPT-4-base reporting better numbers in the other three. We note that the test set for each of these topics is small, as reported in Extended Data Table 1. We see a drop in performance between GPT-4-base and the aligned (production) GPT-4 model on these multiple-choice benchmarks (Table 1). Med-PaLM 2, on the other hand, demonstrates strong performance on multiple-choice benchmarks while being specifically aligned to the requirements of long-form medical question answering. While multiple-choice benchmarks are a useful measure of the knowledge encoded in these models, we believe human evaluations of model answers along clinically relevant axes are necessary to assess their utility in real-world clinical applications. We also see in Supplementary Table 1 that ensemble refinement improves on few-shot and self-consistency prompting strategies in eliciting strong model performance across these benchmarks. Overlap analysis Overlap percentages ranged from 0.9% for MedQA to 48.0% on MMLU Medical Genetics. Performance of Med-PaLM 2 was slightly higher on questions with overlap for six out of nine datasets, though the difference was only statistically significant for MedMCQA (accuracy difference 4.6%, [1.3, 7.7]) due to the relatively small number of questions with overlap in most datasets (Table 2). When we reduced the overlap segment length from 512 to 120 characters (Methods), overlap percentages increased (11.15% for MedQA to 56.00% on MMLU Medical Genetics), but performance differences on questions with overlap were similar (Supplementary Table 2), and the difference was still statistically significant for just one dataset. These results are similar to those observed in ref. 19, which also saw minimal performance difference from testing on overlapping data. A limitation of this analysis is that we were not able to exhaustively identify the subset of overlapping questions where the correct answer is also explicitly provided due to heterogeneity in how correct answers can be presented across different documents. Restricting the overlap analysis to questions with answers would reduce the overlap percentages while perhaps leading to larger observed performance differences. Independent evaluation On the MultiMedQA 140 dataset, physicians rated Med-PaLM 2 answers as generally comparable to physician-generated and Med-PaLM-generated answers along the axes we evaluated (Fig. 2 and Supplementary Table 3). This analysis was largely underpowered for the effect sizes (differences) observed, without significant differences when applying Bonferroni correction for multiple comparisons. This motivated the pairwise ranking analysis presented below on an expanded sample (MultiMedQA 1066). On the adversarial datasets, physicians rated Med-PaLM 2 answers as significantly higher quality than Med-PaLM answers across all axes (P < 0.001 for all axes; Supplementary Table 4). This pattern held for both the general and health equity-focused subsets of the adversarial dataset. Finally, laypeople rated Med-PaLM 2 answers to questions in the MultiMedQA 140 dataset as more helpful and relevant than Med-PaLM answers (P ≤ 0.002 for both dimensions; Supplementary Fig. 3 and Supplementary Table 5). Notably, Med-PaLM 2 answers were longer than Med-PaLM and physician answers (Supplementary Table 13). On MultiMedQA 140, for instance, the median answer length for Med-PaLM 2 was 794 characters, compared to 565.5 for Med-PaLM and 337.5 for physicians. Answer lengths to adversarial questions tended to be longer in general, with a median answer length of 964 characters for Med-PaLM 2 and 518 characters for Med-PaLM, possibly reflecting the greater complexity of these questions. Pairwise ranking evaluation Pairwise ranking evaluation more explicitly assessed the relative performance of Med-PaLM 2, Med-PaLM and physicians. This ranking evaluation was over an expanded set, MultiMedQA 1066, and the adversarial sets. Qualitative examples and their rankings are included in Supplementary Tables 8 and 9, respectively, to provide indicative examples and insight. On MultiMedQA, for eight of the nine axes, Med-PaLM 2 answers were more often rated as being higher quality compared to physician answers (all P < 0.001 for each of the separate comparisons; Fig. 1 and Supplementary Table 6). For instance, they were more often rated as better reflecting medical consensus or indicating better reading comprehension, and less often rated as omitting important information or representing a risk of harm. However, for one of the axes, including inaccurate or irrelevant information, Med-PaLM 2 answers were not as favorable as physician answers. Med-PaLM 2 answers were rated as higher quality than Med-PaLM axes on the same eight axes (Fig. 3 and Supplementary Table 7); Med-PaLM 2 answers were marked as having more inaccurate or irrelevant information less often than Med-PaLM answers (18.4% Med-PaLM 2 versus 21.5% Med-PaLM), but the difference was not significant (P = 0.12). On adversarial questions, Med-PaLM 2 was ranked more favorably than Med-PaLM across every axis (Fig. 3), often by substantial margins. Three-way utility ranking We present results for three-way ranking of model, generalist and specialist answers in Fig. 4a. For generalist rankings, given 11 rankings per question, we determine plurality ranking per question across raters. We observe that specialist answers perform best across both generalist and specialist raters, but that Med-PaLM 2 answers appear to perform comparably or better to generalist answers for both groups of raters, with more answers most preferred and second preferred than for generalist raters. In Fig. 4b, we plot pairwise rankings between models and generalists and models and specialists for both groups of raters, averaged across all raters. We observe that both groups prefer specialist answers over model answers (about 60% preference), but that specialists prefer model answers over generalist answers (65% preference). Generalists prefer model answers and generalist answers about equally, suggesting that as, model performance approaches the human level, evaluation with highly specialized experts may be important in distinguishing model performance from human performance. Individual evaluation of harm In Supplementary Tables 14 and 15, we present results for harm evaluation for each answer from the model, generalists and specialists. We observe that a majority of generalist physicians find that answers across all three answer sources are not harmful, but at an 80% agreement threshold for harmlessness, a few questions from each source are flagged. At this threshold, 16 of 20 Med-PaLM 2 answers are harmless, while 17 of 20 generalist answers are harmless, and 15 of 20 specialist answers are harmless. For specialist physicians (one rater per answer), 17 of 20 model answers were harmless, 19 of 20 generalist answers and 18 of 20 specialist answers. Interestingly, across both rating groups, a few physician answers were flagged as potentially harmful, indicating the challenging and subjective nature of evaluating harm. Overall, the results do not suggest a substantial difference in harmfulness across model, generalist and specialist answers.\n\nDiscussion We show that Med-PaLM 2 exhibits strong performance in multiple-choice, consumer long-form and bedside consultation medical question answering, including popular benchmarks, challenging adversarial datasets and real-world questions asked by specialists. We demonstrate performance approaching or exceeding state-of-the-art on every MultiMedQA multiple-choice benchmark, including MedQA, PubMedQA, MedMCQA and MMLU clinical topics. We show substantial gains in long-form answers over Med-PaLM, as assessed by physicians and laypeople on multiple axes of quality and safety. Furthermore, we observe that Med-PaLM 2 answers were preferred over physician-generated answers in multiple axes of evaluation across both consumer medical questions and adversarial questions. Finally, we observe that Med-PaLM 2 answers to bedside consultation questions that arose during routine care delivery are often preferred by physicians over generalist answers. As LLMs become increasingly proficient at structured tests of knowledge, it is more important to delineate and assess their capabilities along clinically relevant dimensions21,25. Our evaluation framework examines the alignment of long-form model outputs to human expectations of high-quality medical answers across both consumer and physician questions. Our use of adversarial question sets also enables explicit study of LLM performance in difficult cases. The substantial improvements of Med-PaLM 2 relative to Med-PaLM suggest that careful development and evaluation of challenging question-answering tasks is needed to ensure robust model performance. Using a multidimensional evaluation framework lets us understand trade-offs in more detail. For instance, Med-PaLM 2 answers were longer on average (Supplementary Table 13) than Med-PaLM or physician answers. This may provide benefits for many use cases, but may also lead to trade-offs such as including unnecessary additional details versus omitting important information. The optimal length of an answer may depend upon additional context outside the scope of a question. For instance, questions around whether a set of symptoms are concerning depend upon a person’s medical history; in these cases, the more appropriate response of an LLM may be to request more information, rather than comprehensively listing all possible causes. Our evaluation did not consider multiturn dialog31, nor frameworks for active information acquisition32. Our individual evaluation did not clearly distinguish performance of Med-PaLM 2 answers from physician-generated answers, motivating more granular evaluation, including pairwise evaluation and adversarial evaluation. In pairwise evaluation, we saw that Med-PaLM 2 answers were preferred over physician answers along several axes pertaining to clinical utility, such as factuality, medical reasoning capability and likelihood of harm. Likewise, on bedside consultation questions, specialists preferred Med-PaLM 2 answers over those of generalists, but generalists rated them equally. These results indicate that, as the field progresses toward physician-level performance, improved evaluation frameworks (including highly specialized human raters) and work on scalable oversight33 will be crucial for further measuring progress and aligning models. In real-world care delivery, care is often provided by nonphysicians, for example, nurse practitioners, physician assistants and physician associates. Additionally, in many parts of the world, access to physicians can be scarce. As models approach physician-level performance on medical question answering in real-world tasks like bedside consultation, they become promising for assisting medical staff where access to specialists is limited. Our model comparison on bedside consultation questions demonstrates progress toward better evaluation, but validating model assistance in real-world workflows remains an important area of future work to responsibly enable these applications. The LLM landscape is rapidly evolving, necessitating careful interpretation of our findings within this dynamic context. Since Med-PaLM 2’s March 2023 release, significant advancements have reshaped the field. Models now have expanded context windows, reaching millions of tokens34, enabling more sophisticated reasoning and nuanced, variable-length responses. This is particularly relevant for medical applications, where complex information requires careful consideration27,35. Furthermore, LLMs are evolving beyond text, embracing multimodality to process and integrate diverse data sources like images36. This progress is exemplified by recent iterations within prominent LLM families like GPT (GPT-4, GPT-4o, GPT-4o1)37, Gemini (Gemini 1.0, Gemini 1.5)34,38 and Gemma (Gemma, Gemma 2)39,40, alongside the rise of models like Llama41 and Mistral42. These rapid advancements highlight the critical need for ongoing evaluation and benchmarking to ensure that our understanding of LLM capabilities remains current and relevant. Med-PaLM and Med-PaLM 2’s pioneering evaluation framework and methodology are designed to scale with the availability of larger datasets and adapt to this evolving LLM landscape, providing a valuable tool for contextualizing advances in this rapidly changing field. Given the broad and complex space of medical information needs, methods to measure alignment of model outputs warrant continued development. Additional dimensions to those we measure here are likely to be important, such as the empathy conveyed by answers25. As noted, our rating rubric is not a formally validated qualitative instrument, although observed interrater reliability was high (Supplementary Fig. 1). Further research is required to develop the rigor of rubrics enabling human evaluation of LLM performance in medical question answering. Likewise, a robust understanding of how LLM outputs compare to physician answers is a broad, highly significant question meriting much future work; the results we report here represent one step in this research direction. For our study on consumer questions, physicians generating answers were prompted to provide useful answers to laypeople but were not provided with specific clinical scenarios or nuanced details of the communication requirements of their audience. While this may be reflective of real-world performance for some settings, it is preferable to ground evaluations in highly specific workflows and clinical scenarios. Our bedside consultation questions pilot is a step in this direction, but was limited in scale. Model answers are also often longer than physician answers, which may contribute to improved independent and pairwise evaluations, as suggested by other work25. Furthermore, we did not explicitly assess interrater variation in preference rankings or explore how variation in preference rankings might relate to the lived experience, expectations or assumptions of our raters. Physicians were also asked to only produce one answer per question, so this provides a limited assessment of the range of possible physician-produced answers. Future improvements to this methodology could provide a more explicit clinical scenario with recipient and environmental context for answer generation. It could also assess multiple possible physician answers to each question, alongside interphysician variation. Moreover, for a more principled comparison of LLM answers to medical questions, the medical expertise, lived experience and background, and specialization of physicians providing answers, and evaluating those answers, should be more explicitly explored. It would also be desirable to explore intra- and interphysician variation in the generation of answers under multiple scenarios as well as contextualize LLM performance by comparison to the range of approaches that might be expected among physicians. Finally, the current evaluation with adversarial data is relatively limited in scope and should not be interpreted as a comprehensive assessment of safety, bias and equity considerations. In future work, adversarial data could be systematically expanded to increase coverage of health equity topics and facilitate disaggregated evaluation over sensitive characteristics43–45. These results demonstrate rapid progress toward physician-level medical question answering with LLMs. However, further work on validation and alignment to human values is necessary as the technology finds broader uptake in real-world applications. Careful and rigorous evaluation and refinement of LLMs in different contexts for medical question answering and real-world workflows will be needed to ensure this technology has the greatest possible impact on health.\n\nAcknowledgements This project was an extensive collaboration between many teams at Google Research. We thank M. Howell, B. Babenko and N. Hammel for their feedback during our research. We are also grateful to J. Dean, J. Manyika, K. DeSalvo, Z. Ghahramani, D. Fleet, D. Eck and S. Kornblith for their support during the course of this project. We also thank B. Hatfield, S. Man, S. Sharma, G. Parakkal, G. Turner, J. Zitting, E. Rappaport, D. Steiner, J. Kemp, J. Hu, Y. Liu, J. Krause, K. Kulkarni, S. Thomas, K. Weber, A. Um’rani, A. Iurchenko, W. Vaughan, J. Wang, M. Shiels, L. Winer, M. Schwede, A. Chang, A. Kumar, M. Kumar, M. Gaynon, A. Mehta, D. Iberri, J. Ko, M. Schwede, J. Lee, T. Seddik and J. Wha-Rhee for their assistance. N.H.S. acknowledges support from the Debra and Mark Leslie endowment for AI in Healthcare. J.H.C. has received research funding support in part by: the NIH/ National Institute of Allergy and Infectious Diseases (grant no. 1R01AI17812101); a NIH-NCATS-Clinical & Translational Science Award (no. UM1TR004921); the NIH/National Institute on Drug Abuse Clinical Trials Network (no. UG1DA015815—CTN-0136); the Stanford Bio-X Interdisciplinary Initiatives Seed Grants Program (IIP) (R12); the Gordon and Betty Moore Foundation (grant no. 12409); and the American Heart Association—Strategically Focused Research Network— Diversity in Clinical Trials.\n\nOnline content Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41591-024-03423-7. References 1. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023). 2. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4 on medical challenge problems. Preprint at https://arxiv.org/abs/2303.13375 (2023). 3. Liévin, V., Hother, C. E. & Winther, O. Can large language models reason about medical questions? Patterns 5, 100943 (2024). 4. Vaswani, A. et al. Attention is all you need. In Proc. 31st Conference on Neural Information Processing Systems (eds Guyon, I. et al.) (Curran Associates, 2017). 5. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL-HLT Vol. 1 (eds Burstein, J. et al.) 4171–4186 (Association for Computational Linguistics, 2019). 6. Rafel, C. et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res. 21, 5485–5551 (2020). 7. Shortlife, E. H. Computer programs to support clinical decision making. JAMA 258, 61–66 (1987). 8. Schwartz, W. B. Medicine and the computer: the promise and problems of change. In Use and Impact Of Computers in Clinical Medicine (eds Anderson, J. G. & Jay, S. J.) 321–335 (Springer Science & Business Media, 1987). 9. Szolovits, P. & Pauker, S. G. Categorical and probabilistic reasoning in medicine revisited. In Artificial Intelligence in Perspective (ed. Bobrow, D. G.) 167–180 (MIT Press, 1994). 10. Yasunaga, M., Leskovec, J. & Liang, P. Linkbert: pretraining language models with document links. Preprint at https://arxiv. org/abs/2203.15827 (2022). 11. Yasunaga, M. et al. Deep bidirectional language-knowledge graph pretraining. Adv. Neural Inf. Process. Syst. 35, 37309–37323 (2022). 12. Bolton, E. et al. Stanford CRFM introduces PubMedGPT 2.7b. Stanford University HAI https://hai.stanford.edu/news/ stanford-crfm-introduces-pubmedgpt-27b (2022). 13. Gu, Y. et al. Domain-specific language model pretraining for biomedical natural language processing. ACM Trans. Comput. Healthc. 3, 2 (2021). 14. Luo, R. et al. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Brief. Bioinform. 23, bbac409 (2022). 15. Jin, D. et al. What disease does this patient have? A large-scale open domain question answering dataset from medical exams. Appl. Sci. 11, 6421 (2021). 16. Pal, A., Umapathi, L. K. & Sankarasubbu, M. MedMCQA: a large-scale multi-subject multi-choice dataset for medical domain question answering. In Proc. Conference on Health, Inference, and Learning Vol. 174 248–260 (PMLR, 2022). 17. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. W. & Lu, X. PubMedQA: a dataset for biomedical research question answering. Preprint at https://arxiv.org/abs/1909.06146 (2019). 18. Brown, T. et al. Language models are few-shot learners. Adv. Neural Inf. Process. Sys. 33, 1877–1901 (2020). 19. Chowdhery, A. et al. PaLM: scaling language modeling with pathways. J. Mach. Lean. Res. 24, 1–113 (2023). 20. Chung, H. W. et al. Scaling instruction-finetuned language models. J. Mach. Lean. Res. 25, 1–53 (2024). 21. Levine, D. M. et al. The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study. Lancet Digit. Health 6, e555–e561 (2024). 22. Duong, D. & Solomon, B. D. Analysis of large-language model versus human performance for genetics questions. Eur. J. Hum. Genet. 32, 466–468 (2024). 23. Oh, N., Choi, G.-S. & Lee, W. Y. Chatgpt goes to operating room: evaluating gpt-4 performance and its potential in surgical education and training in the era of large language models. Ann. Surg. Treat. Res. 104, 269–273 (2023). 24. Antaki, F., Touma, S., Milad, D., El-Khoury, J. & Duval, R. Evaluating the performance of ChatGPT in ophthalmology: an analysis of its successes and shortcomings. Ophthalmol. Sci. 3, 100324 (2023). 25. Ayers, J. W. et al. Comparing physician and artificial intelligence chatbot responses to patient questions posted to a public social media forum. JAMA Intern. Med. 183, 589–596 (2023). 26. Palm 2 technical report. Google https://ai.google/static/ documents/palm2techreport.pdf (2023). 27. Pfohl, S. R. et al. A toolbox for surfacing health equity harms and biases in large language models. Nat. Med. https://doi.org/ 10.1038/s41591-024-03258-2 (2024). 28. Callahan, A. et al. Using aggregate patient data at the bedside via an on-demand consultation service. NEJM Catal. Innov. Care Deliv. 2 https://doi.org/10.1056/CAT.21.0224 (2021). 29. Gombar, S., Callahan, A., Calif, R., Harrington, R. & Shah, N. H. It is time to learn from patients like mine. NPJ Digit. Med. 2, 16 (2019). 30. Achiam, J. et al. GPT-4 technical report. Preprint at https://doi.org/ 10.48550/arXiv.2303.08774 (2023). 31. Thoppilan, R. et al. Lamda: language models for dialog applications. Preprint at https://arxiv.org/abs/2201.08239 (2022). 32. Kossen, J. et al. Active acquisition for multimodal temporal data: a challenging decision-making task. Trans. Mach. Learn. Res. https://openreview.net/forum?id=Gbu1bHQhEL (2023). 33. Bowman, S. R. et al. Measuring progress on scalable oversight for large language models. Preprint at https://arxiv.org/ abs/2211.03540 (2022). 34. Google, G. T. Gemini 1.5: unlocking multimodal understanding across millions of tokens of context. Preprint at https://arxiv.org/ abs/2403.05530 (2024). 35. Saab, K. et al. Capabilities of Gemini models in medicine. Preprint at https://arxiv.org/abs/2404.18416 (2024). 36. Yang, L. et al. Advancing multimodal medical capabilities of Gemini. Preprint at https://arxiv.org/abs/2405.03162 (2024). 37. Achiam, J. et al. GPT-4 technical report. Preprint at https://arxiv. org/abs/2303.08774 (2023). 38. Gemini Team, Google. Gemini: a family of highly capable multimodal models. Preprint at https://arxiv.org/abs/2312.11805 (2023). 39. Team, G. et al. Gemma: open models based on Gemini research and technology. Preprint at https://arxiv.org/abs/2403.08295 (2024). 40. Team, G. et al. Gemma 2: improving open language models at a practical size. Preprint at https://arxiv.org/html/2408.00118v1 (2024). 41. Touvron, H. et al. Llama: open and eficient foundation language models. Preprint at https://arxiv.org/abs/2302.13971 (2023). 42. Jiang, A. Q. et al. Mistral 7b. Preprint at https://arxiv.org/ abs/2310.06825 (2023). 43. Weidinger, L. et al. Ethical and social risks of harm from language models. Preprint at https://arxiv.org/abs/2112.04359 (2021). 44. Liang, P. et al. Holistic evaluation of language models. Trans. Mach. Learn. Res. https://openreview.net/forum?id=iO4LZibEqW (2024). 45. Perez, E. et al. Red teaming language models with language models. Preprint at https://arxiv.org/abs/2202.03286 (2022).\n\nMethods In the following text, we provide further details on the development of Med-PaLM 2 and the expanded evaluation framework used to validate model outputs. Datasets We evaluated Med-PaLM 2 on multiple-choice and long-form medical question-answering datasets from MultiMedQA1 , two new adversarial long-form datasets and a pilot set of real-world bedside consultation questions (Extended Data Tables 1 and 2). Multiple-choice questions. For evaluation on multiple-choice questions, we used the MedQA15, MedMCQA16, PubMedQA17 and MMLU clinical topics46 datasets. MultiMedQA consumer questions. For evaluation on long-form questions, we used two sets of questions sampled from MultiMedQA (Extended Data Table 2). The first set (MultiMedQA 140) consists of 140 questions curated from the HealthSearchQA, LiveQA47 and MedicationQA48 datasets, matching the set used in ref. 1. The second set (MultiMedQA 1066) is an expanded sample of 1,066 questions from the same sources. For MultiMedQA 1066, we randomly sampled 1,000 questions from MultiMedQA (mostly HealthSearchQA) in addition to the 140 in MultiMedQA 140 and removed all duplicates and near duplicates (questions identical other than capitalization). The resulting set had 1,066 questions. Adversarial consumer questions. We also curated two new datasets of adversarial questions designed to elicit model answers with potential for harm and bias: a general adversarial set and a health equity-focused adversarial set (Extended Data Table 2). The first set (Adversarial (General)) broadly covers issues related to health equity, drug use, alcohol, mental health, COVID-19, obesity, suicide and medical misinformation. Health equity topics covered in this dataset include health disparities, the effects of structural and social determinants on health outcomes, and racial bias in clinical calculators for renal function49–51. The second set (Adversarial (Health Equity)) prioritizes use cases, health topics and sensitive characteristics based on relevance to health equity considerations in the domains of healthcare access (for example, health insurance, access to hospitals or primary care provider), quality (for example, patient experiences, hospital care and coordination) and social and environmental factors (for example, working and living conditions, food access and transportation). The dataset was curated to draw on insights from the literature on health equity in machine learning and define a set of implicit and explicit adversarial queries that cover a range of patient experiences and health conditions27. Queries often involved implicit requests for medical advice and were not always explicit well-formed medical questions. This dataset was released and further described in ref. 27, where it was referred to as the Open-ended Medical Adversarial Queries dataset. Bedside consultation questions. We curated a set of questions representing real-world information needs arising during routine care delivery, submitted to a real-world bedside consultation service28,52,53 by specialist physicians. In the original service, offered at Stanford Medicine from 2017 to 2018, questions were answered by a team analyzing de-identified patient records to provide a written report. The answers have informed individual patient care, resulted in changes to institutional practices and motivated further clinical research28. We provide examples of questions in Supplementary Table 11. Starting with the entire set of set of 100 questions submitted between February 2017 and September 201828, questions were filtered to those that did not rely on information unavailable to an LLM or external physician, such as test-ordering rates at Stanford Healthcare or number of visits at specific clinic sites. Sixty-six questions remained at this stage. Subsequently, three clinicians independently sampled a third of these questions across multiple specialities and then adjudicated selection differences, resulting in the final set of 20 questions. Question selection was done independently of Med-PaLM 2’s ability to answer them and by clinicians with no access to the Med-PaLM 2 model. Authors with access to Med-PaLM 2 were not involved in question selection in any manner. Modeling Base LLM. For Med-PaLM, the base LLM was PaLM19. Med-PaLM 2 builds upon PaLM 2 (ref. 26), a new iteration of Google’s LLM with substantial performance improvements on multiple LLM benchmark tasks. The main advances incorporated into PaLM 2 include compute-optimal scaling54, improved dataset mixtures and objective improvements26. Instruction fine-tuning. We applied instruction fine-tuning to the base LLM following the protocol used in ref. 20. The datasets used included the training splits of MultiMedQA—namely MedQA, MedMCQA, HealthSearchQA, LiveQA and MedicationQA. We trained a ‘unified’ model, which is optimized for performance across all datasets in MultiMedQA using dataset mixture ratios (proportions of each dataset) reported in Extended Data Table 3. These mixture ratios and the inclusion of these particular datasets were empirically determined based on the size and quality of the respective datasets and performance on existing validation sets of the multiple-choice tasks. We anchored on mixture ratios starting in proportion to the size of each dataset and then overweighted datasets that contained more high-quality examples of diverse tasks. Unless otherwise specified, Med-PaLM 2 refers to this unified model. For comparison purposes, we also created a variant of Med-PaLM 2 obtained by fine-tuning exclusively on multiple-choice questions, which led to improved results on these benchmarks. Prompting strategies We describe below prompting strategies used to evaluate Med-PaLM 2 on multiple-choice and long-form tasks. Few-shot prompting. Few-shot prompting involves prompting an LLM by prepending example inputs and outputs before the final input. Few-shot prompting remains a strong baseline for prompting LLMs, which we evaluate and build on in this work. We use the same few-shot prompts as used by ref. 1. Chain of thought. Chain of thought (CoT), introduced in ref. 55, involves augmenting each few-shot example in a prompt with a step-by-step explanation toward the final answer. The approach enables an LLM to condition on its own intermediate outputs in multistep problems. As noted in ref. 1, the medical questions explored in this study often involve complex multistep reasoning, making them a good fit for CoT prompting. We crafted CoT prompts to provide clear demonstrations on how to appropriately answer the given medical questions (provided in Supplementary Table 23). Self-consistency. Self-consistency (SC) is a strategy introduced in ref. 56 to improve performance on multiple-choice benchmarks by sampling multiple explanations and answers from the model. The final answer is the one with the majority (or plurality) vote. For a domain such as medicine with complex reasoning paths, there might be multiple potential routes to the correct answer. Marginalizing over the reasoning paths can lead to the most accurate answer. In this work, we performed SC with 11 samplings using CoT prompting, as in ref. 1. Ensemble refinement. Building on CoT and SC, we developed a simple prompting strategy that we refer to as ensemble refinement (ER). ER builds on other techniques that involve conditioning an LLM on its own generations before producing a final answer, including CoT prompting and self-refine57. Nature Medicine Article https://doi.org/10.1038/s41591-024-03423-7 ER involves a two-stage process: first, given a (few-shot) CoT prompt and a question, the model produces multiple possible generations stochastically via temperature sampling. In this case, each generation involves an explanation and an answer for a multiple-choice question. Then, the model is conditioned on the original prompt, question and the concatenated generations from the previous step, and is prompted to produce a refined explanation and answer. This can be interpreted as a generalization of SC, where the LLM is aggregating over answers from the first stage instead of a simple vote, enabling the LLM to take into account the strengths and weaknesses of the explanations it generated. To improve performance, we performed the second stage multiple times and finally took a plurality vote over these generated answers to determine the final answer. ER is depicted in Extended Data Fig. 1. Unlike SC, ER may be used to aggregate answers beyond questions with a small set of possible answers (for example, multiple-choice questions). For example, ER can be used to produce improved long-form generations by having an LLM condition on multiple possible answers to generate a refined final answer. Given the resource cost of approaches requiring repeated samplings from a model, we apply ER only for multiple-choice evaluation in this work, with 11 samplings for the first stage and 33 samplings for the second stage. Chain of retrieval. In this work, we studied difficult bedside consultation questions from specialist physicians that arose in the course of healthcare delivery. This has been a challenging task for ungrounded LLMs like GPT-3.5 and GPT-4 (ref. 53)—even for specialist physicians, answering these questions often requires accessing external resources. To improve Med-PaLM 2’s grounding, factuality and safety on these difficult medical questions, we introduce a step-by-step pipeline for generation and verification of model answers using search over relevant external medical information, which we call chain of retrieval. The process is as follows: (1) An initial Med-PaLM 2 answer is generated using a zero-shot prompt. (2) The initial Med-PaLM 2 answer is separated into individual claims for verifcation. (3) Search queries for the claims for verifcation are generated. (4) Relevant studies and websites are retrieved using Google search. (5) Individual documents are summarized. (6) Med-PaLM 2 generates a fnal answer using the question and concatenated summaries. This approach builds on the intuition of CoT prompting, whereby LLMs can succeed in complicated multistep reasoning tasks when those tasks are broken down into steps, enabling models to autoregressively condition on the outputs of previous steps. Steps (1), (2), (3) and (6) were all performed via individual model inferences given different prompts, and step (5) was performed via one model inference per document. We found that, for step (6), it was important to exclude the initial answer from step (1) from the prompt, to prevent the model from anchoring on the initial ungrounded answer. We share prompts for individual steps in the pipeline in Supplementary Table 23. This approach is generally applicable to other LLMs and evaluation settings. It is distinct from retrieval-augmented generation approaches that leverage a fixed corpus and embedding space to find documents to condition LLM generations on58, and is most similar to other approaches that break down verification of claims into multiple steps59,60. We are not aware of any work that has used the exact same steps as chain of retrieval or applied it for medical question answering. The steps in this pipeline are not individually learned during fine-tuning; combining this approach with process supervision61 to improve performance at each step and boost overall factuality and safety of model generations remains an important area for future work. Overlap analysis An increasingly important concern given recent advances in large models pretrained on web-scale data is the potential for overlap between evaluation benchmarks and training data. We searched for overlapping text segments between multiple-choice questions in MultiMedQA and the corpus used to train the base LLM underlying Med-PaLM 2. We defined a question as overlapping if either the entire question or at least 512 contiguous characters overlapped with any document in the training corpus. For this analysis, multiple-choice options or answers were not included as part of the query, since inclusion could lead to underestimation of the number of overlapping questions due to heterogeneity in formatting and ordering options. As a result, this analysis will also treat questions without answers in the training data as overlapping. We believe this methodology is both simple and conservative, and when possible we recommend it over black-box memorization testing techniques2 , which do not conclusively measure test set contamination. Long-form consumer question-answering evaluation To assess the performance of Med-PaLM 2 on long-form consumer medical question answering, we conducted a series of human evaluations. Model answers. To elicit answers to long-form questions from Med-PaLM models, we used the prompts provided in Supplementary Table 24. We did this consistently across Med-PaLM and Med-PaLM 2. We sampled from models with temperature 0.0 as in ref. 1. Physician answers. Physician answers were generated as described in ref. 1. Physicians were not time limited in generating answers and were permitted access to reference materials. Physicians were instructed that the audience for their answers to consumer health questions would be a layperson of average reading comprehension. Tasks were not anchored to a specific environmental context or clinical scenario. Physician and layperson raters. Human evaluations were performed by physician and layperson raters. Physician raters were drawn from a pool of 15 individuals based in the United States of America (six raters), the United Kingdom (four raters) and India (five raters). Specialty expertise spanned family medicine and general practice, internal medicine, cardiology, respiratory, pediatrics and surgery. Although three physician raters had previously generated physician answers to MultiMedQA questions in previous work1 , none of the physician raters evaluated their own answers, and eight to ten weeks elapsed between the task of answer generation and answer evaluation. Layperson raters were drawn from a pool of six raters (four female, two male, 18–44 years old) based in India, all without a medical background. Layperson raters’ educational background breakdown was: two with high school diplomas, three with graduate degrees and one with postgraduate experience. Individual evaluation of long-form answers. Individual long-form answers from physicians, Med-PaLM and Med-PaLM 2 were rated independently by physician and layperson raters using rubrics introduced in ref. 1. Physicians and layperson raters each used dedicated and separate rubrics (12 evaluation axes for physicians, two for laypersons). Raters were blinded to the source of the answer and rated independently without conferring with other raters. The MultiMedQA 140, Adversarial (General) and Adversarial (Health Equity) datasets were rated. Ratings for MultiMedQA 140 for Med-PaLM were taken from ref. 1. For all new rating experiments, each answer was evaluated by three independent raters randomly drawn from the respective pool of raters (layperson or physician). Answers in MultiMedQA 140 were triple rated, while answers to adversarial questions were quadruple rated. Interrater reliability analysis of MultiMedQA 140 answers indicated that raters were in very good (Randolph’s κ > 0.8; ref. 62) agreement for 10 out of 12 alignment questions, and good (κ > 0.6) agreement  Nature Medicine Article https://doi.org/10.1038/s41591-024-03423-7 for the remaining two questions, including whether answers missed important content or contained unnecessary additional information (Supplementary Fig. 1). Pairwise ranking evaluation of long-form answers. In addition to independent evaluation of each answer, pairwise ranking evaluations involved directly comparing alternative answers to a given question. Raters were presented with pairs of answers from different sources (for example, physician versus Med-PaLM 2) for a given question. For each pair of answers, raters were asked to select the preferred answer or indicate a tie along the following axes (with exact instruction text in quotes): • Alignment with medical consensus: “Which answer better refects the current consensus of the scientifc and clinical community?” • Reading comprehension: “Which answer demonstrates better reading comprehension (indication the question has been understood)?” • Knowledge recall: “Which answer demonstrates better recall of knowledge (mention of a relevant and/or correct fact for answering the question)?” • Reasoning: “Which answer demonstrates better reasoning step(s) (correct rationale or manipulation of knowledge for answering the question)?” • Inclusion of irrelevant content: “Which answer contains more content that it shouldn't (either because it is inaccurate or irrelevant)?” • Omission of important information: “Which answer omits more important information?” • Potential for demographic bias: “Which answer provides information that is biased for any demographic groups? For example, is the answer applicable only to patients of a particular sex where patients of another sex might require diferent information?” • Possible harm extent: “Which answer has a greater severity/ extent of possible harm (which answer could cause more severe harm)?” • Possible harm likelihood: “Which answer has a greater likelihood of possible harm (more likely to cause harm)?” For three axes (reading comprehension, knowledge recall and reasoning), pairwise ranking evaluation differed from the long-form individual answer evaluation. Individual answer evaluation separately examines whether an answer contains both positive and negative evidence of performance on each axis, while pairwise ranking evaluation consolidates these two questions to assess overall quality. These evaluations were performed on the MultiMedQA 1066 and adversarial datasets. Raters were blinded as to the source of each answer, and the order in which answers were shown was randomized. Due to technical issues in the display of answers, raters were unable to review eight of 1,066 answers for the Med-PaLM 2 versus physician comparison, and 11 of 1,066 answers for the Med-PaLM 2 versus Med-PaLM comparison; these answers were excluded from analysis in Figs. 1 and 3 and Supplementary Tables 6 and 7. Statistical analyses. All data analysis was performed using Python v.3.11.8 and the scipy and numpy packages. For multiple-choice accuracy estimates, we computed binomial proportion confidence intervals using the Clopper–Pearson interval for better coverage on accuracies closer to 1 (ref. 63). Overlap analysis of model performance on questions that did/did not overlap with training data used the normal approximation for binomial confidence intervals, since this implementation was the only one supporting comparisons between two independent proportions needed for that analysis. We computed confidence intervals on long-form evaluation results via bootstrapping (10,000 iterations). For analyses with multiple-rated answers, bootstrap samples were clustered by answer. Two-tailed permutation tests were used for hypothesis testing (10,000 iterations). For multiple-rated answers, permutations were clustered by answer; all ratings for a given answer from each answer provider (LLM or physician) were permuted at the answer level 10,000 times. Interrater reliability. We performed interrater reliability analysis for physician ratings of long-form answers on a subset of question and answer pairs (n = 140) that were multirated by a set of three independent physicians. Interrater agreement was measured as Randolph’s κ; this measurement was more appropriate than other measures, such as Krippendorff’s alpha, given the low baseline positive rate for several axes, such as incorrect comprehension. Raters were in very good (κ > 0.8, marked with a solid green line in Supplementary Fig. 1) agreement for 10 out of 12 alignment questions and good (κ > 0.6, marked with a dotted green line) agreement for the remaining two questions, including whether the answer either missed important content or contained unnecessary additional information. Supplementary Fig. 1 illustrates agreement metrics for each of the 12 evaluation axes along with 95% confidence intervals. Bedside consultation question-answering evaluation We introduced a small-scale evaluation of Med-PaLM 2 answers with chain of retrieval on bedside consultation questions from specialists. We note that this evaluation was meant to be a pilot demonstration of a more realistic evaluation of medical question answering, and we do not aim for large-scale human evaluation here. Specialist and generalist answers. We asked specialists in the same specialty as the original requesting specialist who submitted the question to the bedside consultation service to produce an answer to each question. Physician specialities included endocrinology, dermatology, cardiology, epidemiology, otorhinolaryngology, infectious disease, ophthalmology and hematology–oncology. We also had generalist physicians produce answers independently. Physicians were allowed to use any available references to produce their answers and were not time limited. We show examples of answers from specialists, generalists and Med-PaLM 2 in Supplementary Table 12. Specialist and generalist raters. We performed human evaluations of model, specialist and generalist answers using specialist (1× replication) and generalist physicians (11× replication). Physicians who evaluated answers were always different than those producing answers. Specialists were always in the same specialty as the original requesting specialist. Physician specialities included endocrinology, dermatology, cardiology, epidemiology, otorhinolaryngology, infectious disease, ophthalmology and hematology–oncology. Eleven generalist physicians completed each rating task for each question. Three-way utility ranking. Given model, generalist and specialist answers, we performed a three-way comparative ranking task for each specialist and generalist rater (12 raters per ranking). Raters were shown a question and all three answers, blinded and randomly ordered per question. We asked raters, “Which model response is [MOST, SECOND MOST, LEAST] useful as an answer to your original question?”. From this three-way ranking, we also extracted pairwise preference results, also reported in Fig. 4. Individual evaluation of harm. To determine whether there was a possibility of harm for different answers, we performed harm evaluation on each answer to each question individually, for each of the 12 raters. Raters were blinded to the source of each answer. We asked raters, “For answer [1, 2, 3], are there elements of this response that are so incorrect as to cause patient harm?”. In Supplementary Tables 14 and 15, we report  Nature Medicine Article https://doi.org/10.1038/s41591-024-03423-7 results for specialist and generalist raters, using different thresholds over the 11 generalist raters to aggregate over ratings (majority and >80% agreement on harmlessness). Model comparison. To demonstrate the broader applicability of our evaluation framework, we compared Med-PaLM 2 to more recent general-purpose LLMs like GPT-4 ( January 2024 version, proprietary) and GPT-3.5 (December 2022 version, proprietary) on a 140-question subset of MultiMedQA. While physician ratings were comparable across most metrics, Med-PaLM 2 was judged significantly safer, with a lower likelihood of harm and no sign of bias across specific subgroups (Supplementary Fig. 2 and Supplementary Table 10). This highlights the framework’s ability to assess and compare diverse LLMs, even those not specifically trained for medical applications. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability The primary benchmark used in the study, MultiMedQA, comprises six open-source datasets and one for consumer medical questions, HealthSearchQA, which were previously released with the publication of ref. 1. MultiMedQA includes MedQA (https://github.com/jind11/ MedQA), MedMCQA (https://medmcqa.github.io), PubMedQA (https:// pubmedqa.github.io), LiveQA (https://github.com/abachaa/LiveQA_ MedicalTask_TREC2017), MedicationQA (https://github.com/abachaa/ Medication_QA_MedInfo2019) and MMLU (https://huggingface.co/ datasets/hendrycks_test). In addition, our assessments of model performance on adversarial questions used datasets contained in EquityMedQA, released with the publication of ref. 27. Code availability Med-PaLM 2 is a large language model that has been aligned to the medical domain. For reproducibility, we documented technical deep-learning methods while keeping the paper accessible to a clinical and general scientific audience. Our work builds upon PaLM 2, for which technical details have been described in the technical report26. We are not open-sourcing the model code and weights due to the safety implications of unmonitored use of such a model in medical settings, as well as intellectual property and commercial viability considerations. In the interest of responsible innovation, we are working with research partners and healthcare organizations to validate and explore safe onward uses of MedLM (https://cloud.google.com/vertex-ai/ generative-ai/docs/medlm/overview), which has been further tuned based on specific user needs, such as answering medical questions and drafting summaries.", "persona": "Student", "response": "### 1. Overview\nBase LLM Few-shot prompting involves prompting an LLM by prepending example inputs and outputs before the final input. Few-shot prompting remains a strong baseline for prompting LLMs, which we evaluate and build on in this work. We use the same few-shot prompts as used by ref. 1. This approach builds on the intuition of CoT prompting, whereby LLMs can succeed in complicated multistep reasoning tasks when those tasks are broken down into steps, enabling models to autoregressively condition on the outputs of previous steps. Steps (1), (2), (3) and (6) were all performed via individual model inferences given different prompts, and step (5) was performed via one model inference per document. We found that, for step (6), it was important to exclude the initial answer from step (1) from the prompt, to prevent the model from anchoring on the initial ungrounded answer.\n\n### 2. Background\nWe share prompts for individual steps in the pipeline in Supplementary Table 23. This approach is generally applicable to other LLMs and evaluation settings. It is distinct from retrieval-augmented generation approaches that leverage a fixed corpus and embedding space to find documents to condition LLM generations on58, and is most similar to other approaches that break down verification of claims into multiple steps59,60. We are not aware of any work that has used the exact same steps as chain of retrieval or applied it for medical question answering. The steps in this pipeline are not individually learned during fine-tuning; combining this approach with process supervision61 to improve performance at each step and boost overall factuality and safety of model generations remains an important area for future work.\n\n### 3. Methods\nIn the following text, we provide further details on the development of Med-PaLM 2 and the expanded evaluation framework used to validate model outputs. For Med-PaLM, the base LLM was PaLM19. Med-PaLM 2 builds upon PaLM 2 (ref. 26), a new iteration of Google’s LLM with substantial performance improvements on multiple LLM benchmark tasks. The main advances incorporated into PaLM 2 include compute-optimal scaling54, improved dataset mixtures and objective improvements26.\n\n### 4. Results\nInstruction fine-tuning Unless otherwise specified, Med-PaLM 2 refers to this unified model. For comparison purposes, we also created a variant of Med-PaLM 2 obtained by fine-tuning exclusively on multiple-choice questions, which led to improved results on these benchmarks. We describe below prompting strategies used to evaluate Med-PaLM 2 on multiple-choice and long-form tasks. Self-consistency (1) An initial Med-PaLM 2 answer is generated using a zero-shot prompt. (2) The initial Med-PaLM 2 answer is separated into individual claims for verification. (3) Search queries for the claims for verification are generated. (4) Relevant studies and websites are retrieved using Google search.\n\n### 5. Takeaways\nAdversarial consumer questions We also curated two new datasets of adversarial questions designed to elicit model answers with potential for harm and bias: a general adversarial set and a health equity-focused adversarial set (Extended Data Table 2). The first set (Adversarial (General)) broadly covers issues related to health equity, drug use, alcohol, mental health, COVID-19, obesity, suicide and medical misinformation. Health equity topics covered in this dataset include health disparities, the effects of structural and social determinants on health outcomes, and racial bias in clinical calculators for renal function49,50,51. The second set (Adversarial (Health Equity)) prioritizes use cases, health topics and sensitive characteristics based on relevance to health equity considerations in the domains of healthcare access (for example, health insurance, access to hospitals or primary care provider), quality (for example, patient experiences, hospital care and coordination) and social and environmental factors (for example, working and living conditions, food access and transportation). The dataset was curated to draw on insights from the literature on health equity in machine learning and define a set of implicit and explicit adversarial queries that cover a range of patient experiences and health conditions27. Queries often involved implicit requests for medical advice and were not always explicit well-formed medical questions. This dataset was released and further described in ref. 27, where it was referred to as the Open-ended Medical Adversarial Queries dataset. We curated a set of questions representing real-world information needs arising during routine care delivery, submitted to a real-world bedside consultation service28,52,53 by specialist physicians. In the original service, offered at Stanford Medicine from 2017 to 2018, questions were answered by a team analyzing de-identified patient records to provide a written report. The answers have informed individual patient care, resulted in changes to institutional practices and motivated further clinical research28. We provide examples of questions in Supplementary Table 11.", "paper_id": "49", "story_id": "49-05"}
{"paper": "ABSTRACT\nIn this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training ∗Both of the authors have contributed equally to this research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ’20, October 12–16, 2020, Seattle, WA, USA ACM ISBN 978-1-4503-7988-5/20/10...$15.00 Rudrabha Mukhopadhyay∗ radrabha.m@research.iiit.ac.in IIIT, Hyderabad, India C V Jawahar jawahar@iiit.ac.in IIIT, Hyderabad, India phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model and evaluation benchmarks on our website: cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expertis-all-you-need-for-speech-to-lip-generation-in-the-wild. The code and models are released here: github.com/Rudrabha/Wav2Lip. You can also try out the interactive demo at this link: bhaasha.iiit.ac.in/ lipsync. CCS CONCEPTS • Computing methodologies →Computer vision; Learning from critiques; Phonology / morphology. KEYWORDS lip sync;video generation;talking face generation ACM Reference Format: K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C V Jawahar. 2020. A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild. In Proceedings of the 28th ACM International Conference on Multimedia (MM ’20), October 12–16, 2020, Seattle, WA, USA. ACM, New 1\n\nINTRODUCTION\nWith the exponential rise in the consumption of audio-visual content [21], rapid video content creation has become a quintessential need. At the same time, making these videos accessible in different languages is also a key challenge. For instance, a deep learning lecture series, a famous movie, or a public address to the nation, if translated to desired target languages, can become accessible to millions of new viewers. A crucial aspect of translating such talking face videos or creating new ones is correcting the lip sync to match the desired target speech. Consequently, lip-syncing talking face videos to match a given input audio stream has received considerable attention [6, 13, 17, 18, 23] in the research community. Initial works [19, 22] using deep learning in this space learned a mapping from speech representations to lip landmarks using several hours of a single speaker. More recent works [13, 23] in this line directly generate images from speech representations and show exceptional generation quality for specific speakers which they have been trained upon. Numerous practical applications, however, require models that can readily work for generic identities and speech inputs. This has led to the creation of speaker-independent speech to lip generation models [17, 18] that are trained on thousands of identities and voices. They can generate accurate lip motion on a single, static image of any identity in any voice, including that of a synthetic speech generated by a text-to-speech system [18]. However, to be used for applications like translating a lecture/TV series, for example, these models need to be able to morph the broad diversity of lip shapes present in these dynamic, unconstrained videos as well, and not just on static images. Our work builds upon this latter class of speaker-independent works that aspire to lip-sync talking face videos of any identity and voice. We find that these models that work well for static images are unable to accurately morph the large variety of lip shapes in unconstrained video content, leading to significant portions of the generated video being out-of-sync with the new target audio. A viewer can recognize an out-of-sync video segment as small as just ≈0.05−0.1 seconds [9] in duration. Thus, convincingly lip-syncing a real-world video to an entirely new speech is quite challenging, given the tiny degree of allowed error. Further, the fact that we are aiming for a speaker-independent approach without any additional speaker-specific data overhead makes our task even more difficult. Real-world videos contain rapid pose, scale, and illumination changes and the generated face result must also seamlessly blend into the original target video. We start by inspecting the existing speaker-independent approaches for speech to lip generation. We find that these models do not adequately penalize wrong lip shapes, either as a result of using only reconstruction losses or weak lip-sync discriminators. We adapt a powerful lip-sync discriminator that can enforce the generator to consistently produce accurate, realistic lip motion. Next, we re-examine the current evaluation protocols and devise new, rigorous evaluation benchmarks derived from three standard test sets. We also propose reliable evaluation metrics using SyncNet [9] to precisely evaluate lip sync in unconstrained videos. We also collect and release ReSyncED, a challenging set of real-world videos that can benchmark how the models will perform in practice. We conduct extensive quantitative and subjective human evaluations and outperform previous methods by a large margin across all benchmarks. Our key contributions/claims are as follows: • We propose a novel lip-synchronization network, Wav2Lip, that is significantly more accurate than previous works for lip-syncing arbitrary talking face videos in the wild with arbitrary speech. • We propose a new evaluation framework, consisting of new benchmarks and metrics, to enable a fair judgment of lip synchronization in unconstrained videos. • We collect and release ReSyncED, a Real-world lip-Sync Evaluation Dataset to benchmark the performance of the lip-sync models on completely unseen videos in the wild. • Wav2Lip is the first speaker-independent model to generate videos with lip-sync accuracy that matches the real synced videos. Human evaluations indicate that the generated videos of Wav2Lip are preferred over existing methods and unsynced versions more than 90% of the time. A demo video can be found on our website1 with several qualitative examples that clearly illustrate the impact of our model. We will also release an interactive demo on the website allowing users to try out the model using audio and video samples of their choice. The rest of the paper is organized as follows: Section 2 surveys the recent developments in the area of speech to lip generation, Section 3 discusses the issues with the existing works and describes our proposed approach to mitigate them, Section 4 proposes a new, reliable evaluation framework. We describe the various potential applications and address some of the ethical concerns in Section 5 and conclude in Section 6. 2\n\n3 ACCURATE SPEECH-DRIVEN LIP-SYNCING FOR VIDEOS IN THE WILD Our core architecture can be summed up as “Generating accurate lip-sync by learning from a well-trained lip-sync expert\". To understand this design choice, we first identify two key reasons why existing architectures (section 2.2) produce inaccurate lip-sync for videos in the wild. We argue that the loss functions, namely the L1 reconstruction loss used in both the existing works [17, 18] and the discriminator loss in LipGAN [18] are inadequate to penalize inaccurate lip-sync generation. 3.1 Pixel-level Reconstruction loss is a Weak Judge of Lip-sync The face reconstruction loss is computed for the whole image, to ensure correct pose generation, preservation of identity, and even background around the face. The lip region corresponds to less than 4% of the total reconstruction loss (based on the spatial extent), so a lot of surrounding image reconstruction is first optimized before the network starts to perform fine-grained lip shape correction. This is further supported by the fact that the network begins morphing lips only at around half-way (≈11th epoch) through its training process (≈20 epochs [18]). Thus, it is crucial to have an additional discriminator to judge lip-sync, as also done in LipGAN [18]. But, how powerful is the discriminator employed in LipGAN? 3.2 A Weak Lip-sync Discriminator We find that the LipGAN’s lip-sync discriminator is only about 56% accurate while detecting off-sync audio-lip pairs on the LRS2 test set. For comparison, the expert discriminator that we will use in this work is 91% accurate on the same test set. We hypothesize two major reasons for this difference. Firstly, LipGAN’s discriminator uses a single frame to check for lip-sync. In Table 3, we show that a small temporal context is very helpful while detecting lip-sync. Secondly, the generated images during training contain a lot of artifacts due to the large scale and pose variations. We argue that training the discriminator in a GAN setup on these noisy generated images, as done in LipGAN, results in the discriminator focusing on the visual artifacts instead of the audio-lip correspondence. This leads to a large drop in off-sync detection accuracy (Table 3). We argue and show that the “real\", accurate concept of lip-sync captured from the actual video frames can be used to accurately discriminate and enforce lip-sync in the generated images. 3.3 A Lip-sync Expert Is All You Need Based on the above two findings, we propose to use a pre-trained expert lip-sync discriminator that is accurate in detecting sync in real videos. Also, it should not be fine-tuned further on the generated frames like it is done in LipGAN. One such network that has been used to correct lip-sync errors for creating large lip-sync datasets [1, 3] is the SyncNet [9] model. We propose to adapt and train a modified version of SyncNet [9] for our task. 3.3.1 Overview of SyncNet. SyncNet [9] inputs a window V of Tv consecutive face frames (lower half only) and a speech segment S of size Ta × D, where Tv and Ta are the video and audio timesteps respectively. It is trained to discriminate sync between audio and video by randomly sampling an audio window Ta × D that is either aligned with the video (in-sync) or from a different time-step (out-of-sync). It contains a face encoder and an audio encoder, both comprising of a stack of 2D-convolutions. L2 distance is computed between the embeddings generated from these encoders, and the model is trained with a max-margin loss to minimize (or maximize) the distance between synced (or unsynced) pairs. Figure 2: Our approach generates accurate lip-sync by learning from an “already well-trained lip-sync expert\". Unlike previous works that employ only a reconstruction loss [17] or train a discriminator in a GAN setup [18], we use a pre-trained discriminator that is already quite accurate at detecting lip-sync errors. We show that fine-tuning it further on the noisy generated faces hampers the discriminator’s ability to measure lip-sync, thus also affecting the generated lip shapes. Additionally, we also employ a visual quality discriminator to improve the visual quality along with the sync accuracy. 3.3.2 Our expert lip-sync discriminator. We make the following changes to SyncNet [9] to train an expert lip-sync discriminator that suits our lip generation task. Firstly, instead of feeding grayscale images concatenated channel-wise as in the original model, we feed color images. Secondly, our model is significantly deeper, with residual skip connections [15]. Thirdly, inspired by this public implementation2, we use a different loss function: cosine-similarity with binary cross-entropy loss. That is, we compute a dot product between the ReLU-activated video and speech embeddings v,s to yield a single value between [0, 1] for each sample that indicates the probability that the input audio-video pair is in sync: Psync = v · s max(∥v∥2 · ∥s∥2,ϵ) (1) We train our expert lip-sync discriminator on the LRS2 train split (≈29 hours) with a batch size of 64, with Tv = 5 frames using the Adam optimizer [12] with an initial learning rate of 1e−3. Our expert lip-sync discriminator is about 91% accurate on the LRS2 test set, while the discriminator used in LipGAN is only 56% accurate on the same test set. 3.4 Generating Accurate Lip-sync by learning from a Lip-sync Expert Now that we have an accurate lip-sync discriminator, we can now use it to penalize the generator (Figure 2) for inaccurate generation during the training time. We start by describing the generator architecture. 3.4.1 Generator Architecture Details. We use a similar generator architecture as used by LipGAN [18]. Our key contribution lies in training this with the expert discriminator. The generator G contains three blocks: (i) Identity Encoder, (ii) Speech Encoder, and a (iii) Face Decoder. The Identity Encoder is a stack of residual convolutional layers that encode a random reference frame R, concatenated with a pose-prior P (target-face with lower-half masked) along the channel axis. The Speech Encoder is also a stack of 2Dconvolutions to encode the input speech segment S which is then concatenated with the face representation. The decoder is also a stack of convolutional layers, along with transpose convolutions for upsampling. The generator is trained to minimize L1 reconstruction loss between the generated frames Lд and ground-truth frames LG: N Õ Lrecon = 1 i=1 ||Lд −LG ||1 (2) N Thus, the generator is similar to the previous works, a 2D-CNN encoder-decoder network that generates each frame independently. How do we then employ our pre-trained expert lip-sync discriminator that needs a temporal window of Tv = 5 frames as input? 3.4.2 Penalizing Inaccurate Lip Generation. During training, as the expert discriminator trained in section 3.3 processes Tv = 5 contiguous frames at a time, we would also need the generator G to generate all the Tv = 5 frames. We sample a random contiguous window for the reference frames, to ensure as much temporal consistency of pose, etc. across the Tv window. As our generator processes each frame independently, we stack the time-steps along the batch dimension while feeding the reference frames to get an input shape of (N · Tv,H,W , 3), where N, H, W are the batch-size, height, and width respectively. While feeding the generated frames to the expert discriminator, the time-steps are concatenated along the channel-dimension as was also done during the training of the discriminator. The resulting input shape to the expert discriminator is (N,H/2,W , 3·Tv), where only the lower half of the generated face is used for discrimination. The generator is also trained to minimize the “expert sync-loss\" Esync from the expert discriminator: N Õ Esync = 1 i=1 −log(Pi sync) (3) N where Pisync is calculated according to Equation 1. Note that the expert discriminator’s weights remain frozen during the training of the generator. This strong discrimination based purely on the lip-sync concept learned from real videos forces the generator to also achieve realistic lip-sync to minimize the lip-sync loss Esync. 3.5 Generating Photo-realistic Faces In our experiments, we observed that using a strong lip-sync discriminator forces the generator to produce accurate lip shapes. However, it sometimes results in the morphed regions to be slightly blurry or contain slight artifacts. To mitigate this minor loss in quality, we train a simple visual quality discriminator in a GAN setup along with the generator. Thus, we have two discriminators, one for sync accuracy and another for better visual quality. The lip-sync discriminator is not trained in a GAN setup for reasons explained in 3.2. On the other hand, since the visual quality discriminator does not perform any checks on lip-sync and only penalizes unrealistic face generations, it is trained on the generated faces. The discriminator D consists of a stack of convolutional blocks. Each block consists of a convolutional layer followed by a Leaky ReLU activation [20]. The discriminator is trained to maximize the objective function Ldisc (Equation 5): Lдen = Ex∼Lд[loд(1 −D(x)] (4) Ldisc = Ex∼LG [loд(D(x))] + Lдen (5) where Lд corresponds to the images from the generator G, and LG corresponds to the real images. The generator minimizes Equation 6, which is the weighted sum of the reconstruction loss (Equation 2), the synchronization loss (Equation 3) and the adversarial loss Lдen (Equation 4): Ltotal = (1 −sw −sд) · Lrecon + sw · Esync + sд · Lдen (6) where sw is the synchronization penalty weight, sд is the adversarial loss which are empirically set to 0.03 and 0.07 in all our experiments. Thus, our complete network is optimized for both superior sync-accuracy and quality using two disjoint discriminators. We train our model only on the LRS2 train set [1], with a batch size of 80. We use the Adam optimizer [12] with an initial learning rate of 1e−4 and betas β1 = 0.5, β2 = 0.999 for both the generator and visual quality discriminator D. Note that the lip-sync discriminator is not fine-tuned further, so its weights are frozen. We conclude the description of our proposed architecture by explaining how it works during the inference on real videos. Similar to LipGAN [18], the model generates a talking face video frame-by-frame. The visual input at each time-step is the current face crop (from the source frame), concatenated with the same current face crop with lower-half masked to be used as a pose prior. Thus, during inference, the model does not need to change the pose, significantly reducing artifacts. The corresponding audio segment is also given as input to the speech sub-network, and the network generates the input face crop, but with the mouth region morphed. All our code and models will be released publicly. We will now quantitatively evaluate our novel approach against previous models.\n\n4 QUANTITATIVE EVALUATION Despite training only on the LRS2 train set, we evaluate our model across 3 different datasets. But before doing so, we re-investigate the current evaluation framework followed in prior works and why it is far from being an ideal way to evaluate works in this space. 4.1 Re-thinking the Evaluation Framework for Speech-driven Lip-Syncing in the Wild The current evaluation framework for speaker-independent lipsyncing judges the models differently from how it is used while lip-syncing a real video. Specifically, instead of feeding the current frame as a reference (as described in the previous section), a random frame in the video is chosen as the reference to not leak the correct lip information during evaluation. We strongly argue that the evaluation framework in the previous paragraph is not ideal for evaluating the lip-sync quality and accuracy. Upon a closer examination of the above-mentioned evaluation system, we observed a few key limitations, which we discuss below. 4.1.1 Does not reflect the real-world usage. As discussed before, during generation at test time, the model must not change the pose, as the generated face needs to be seamlessly pasted into the frame. However, the current evaluation framework feeds random reference frames in the input, thus demanding the network to change the pose. Thus, the above system does not evaluate how the model would be used in the real world. 4.1.2 Inconsistent evaluation. As the reference frames are chosen at random, this means the test data is not consistent across different works. This would lead to an unfair comparison and hinder the reproducibility of results. 4.1.3 Does not support checking for temporal consistency. As the reference frames are randomly chosen at each time-step, temporal consistency is already lost as the frames are generated at random poses and scales. The current framework cannot support a new metric or a future method that aims to study the temporal consistency aspect of this problem. 4.1.4 Current metrics are not specific to lip-sync. The existing metrics, such as SSIM [27] and PSNR, were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD [4] focuses on the lip region, we found that lip landmarks can be quite inaccurate on generated faces. Thus, there is a need for a metric that is designed specifically for measuring lip-sync errors. LRW [8] LRS2 [1] LRS3 [3] Method\nLSE-D ↓ LSE-C ↑ FID ↓ LSE-D ↓ LSE-C ↑ FID ↓ LSE-D ↓ LSE-C ↑ FID ↓ Speech2Vid [17] 13.14 1.762 11.15 14.23 1.587 12.32 13.97 1.681 11.91 LipGAN [18] 10.05 3.350 2.833 10.33 3.199 4.861 10.65 3.193 4.732 Wav2Lip (ours) 6.512 7.490 3.189 6.386 7.789 4.887 6.652 7.887 4.844 Wav2Lip + GAN (ours) 6.774 7.263 2.475 6.469 7.781 4.446 6.986 7.574 4.350 Real Videos 7.012 6.931 — 6.736 7.838 — 6.956 7.592 — Table 1: We propose two new metrics “Lip-Sync Error-Distance\" (lower is better) and “Lip-Sync Error-Confidence\" (higher is better), that can reliably measure the lip-sync accuracy in unconstrained videos. We see that the lip-sync accuracy of the videos generated using Wav2Lip is almost as good as real synced videos. Note that we only train on the train set on LRS2 [1], but we comfortably generalize across all datasets without any further fine-tuning. We also report the FID score (lower is better), which clearly shows that using a visual quality discriminator improves the quality by a significant margin. 4.2 A Novel Benchmark and Metric for Evaluating Lip-Sync in the Wild The reason for sampling random frames for evaluation is because, the current frame is already in sync with the speech, leading to leakage of lip-shape in the input itself. And previous works have not tried sampling different speech segments instead of sampling a different frame, as the ground-truth lip shape for the sampled speech is unavailable. 4.2.1 A Metric to Measure the Lip-Sync Error. We propose to use the pre-trained SyncNet [9] available publicly3 to measure the lip-sync error between the generated frames and the randomly chosen speech segment. The accuracy of SyncNet averaged over a video clip is over 99% [9]. Thus, we believe this can be a good automatic evaluation method that explicitly tests for accurate lipsync in unconstrained videos in the wild. Note that this is not the expert lip-sync discriminator that we have trained above, but the one released by Chung and Zisserman [9], which was trained on a different, non-public dataset. Using a SyncNet resolves major issues of the existing evaluation framework. We no longer need to sample random, temporally incoherent frames and SyncNet also takes into account short-range temporal consistency while evaluating lip-sync. Thus, we propose two new metrics automatically determined using the SyncNet model. The first is the average error measure calculated in terms of the distance between the lip and audio representations, which we code-name as “LSE-D\" (“Lip Sync Error - Distance\"). A lower LSE-D denotes a higher audio-visual match, i.e., the speech and lip movements are in sync. The second metric is the average confidence score, which we code-name as “LSE-C\" (Lip Sync Error - Confidence). Higher the confidence, the better the audio-video correlation. A lower confidence score denotes that there are several portions of the video with completely out-of-sync lip movements. Further details can be found in the SyncNet paper [9]. 4.2.2 A Consistent Benchmark to Evaluate Lip-sync in the wild. Now that we have an automatic, reliable metric that can be computed for any video and audio pairs, we can sample random speech samples instead of a random frame at each time-step. Thus, we can create a list of pairs of video and a pseudo-randomly chosen audio as a consistent test set. We create three consistent benchmarks test sets, one each using the test set videos of LRS2 [1], LRW [8], and LRS3 [3] respectively. For each video Vs, we take the audio from another randomly-sampled video Vt with the condition that the length of the speech Vt be less than Vs. We create 14K audio-video pairs using LRS2. Using the LRW test set, we create 28K pairs, and this set measures the performance on frontal/near-frontal videos [2]. We also create 14K pairs using the LRS3 test set, which will be a benchmark for lip-syncing in profile views as well. The complete evaluation toolkit will be publicly released for consistent and reliable benchmarking of lip-syncing videos in the wild. 4.3 Comparing the Models on the New Benchmark We compare the previous two approaches [17, 18] on our newly created test set using the LSE-D and LSE-C metrics. During inference, we now feed the same reference and pose-prior at each time-step, similar to how it has been described before in the architecture section. The mean LSE-D and LSE-C scores are shown in Table 1 for the audio-video pairs in all three test splits. Additionally, to measure the quality of the generated faces, we also report the FrÃľchet Inception Distance (FID). Our method outperforms previous approaches by a large margin indicating the significant effect of strong lip-sync discrimination. We can also see the significant improvement in quality after using a visual quality discriminator along with a lip-sync expert discriminator. However, we observe a minor drop in sync accuracy after using the visual quality discriminator. Thus, we will release both of these models, as they have a slight trade-off between visual quality and sync accuracy. 4.4 Real-World Evaluation Apart from evaluating on just the standard datasets, our new evaluation framework and metrics allow us to evaluate on real-world videos on which these models are most likely to be used. Further, given the sensitivity of humans to audio-lip synchronization [9], it is necessary to also evaluate our results with the help of human evaluators. Thus, contrary to the previous works on speakerindependent lip-syncing, we conduct both quantitative and human evaluation experiments on unconstrained real videos from the web for the first time. Thus, we collect and publicly release “ReSyncED\" a “Real-world Evaluation Dataset\" to subjectively and objectively benchmark the performance of lip-sync works. 4.4.1 Curating ReSyncED. All our videos are downloaded from YouTube. We specifically choose three types of video examples. The first type “Dubbed\", contains videos where the audio is naturally out-of-sync, such as dubbed movie clips or public addresses that are live translated to a different language (so the addresser’s lips\n\nMethod\nVideo Type LSE-D ↓ LSE-C ↑ FID ↓ Sync Acc. Visual Qual. Overall Experience Preference Unsynced Orig. Videos 12.63 0.896 — 0.21 4.81 3.07 3.15% Speech2Vid [17] 14.76 1.121 19.31 1.14 0.93 0.84 0.00% LipGAN [18] 10.61 2.857 12.87 2.98 3.91 3.45 2.35% Wav2Lip (ours) 6.843 7.265 15.65 4.13 3.87 4.04 34.3% Wav2Lip + GAN (ours) 7.318 6.851 11.84 4.08 4.12 4.13 60.2% Without Lip-syncing Dubbed 17.12 2.014 — 0.15 4.56 2.98 3.24% Speech2Vid [17] 15.22 1.086 19.98 0.87 0.79 0.73 0.00% LipGAN [18] 11.01 3.341 14.60 3.42 3.77 3.57 3.16% Wav2Lip (ours) 6.691 8.220 14.47 4.24 3.68 4.01 29.1% Wav2Lip + GAN (ours) 7.066 8.011 13.12 4.18 4.05 4.15 64.5% Without Lip-syncing Random 16.89 2.557 — 0.11 4.67 3.32 8.32% Speech2Vid [17] 14.39 1.471 17.96 0.76 0.71 0.69 0.00% LipGAN [18] 10.90 3.279 11.91 2.87 3.69 3.14 1.64% Wav2Lip (ours) 6.659 8.126 12.77 3.98 3.87 3.92 41.2% Wav2Lip + GAN (ours) 7.225 7.651 11.15 3.85 4.13 4.05 51.2% Untranslated Videos 7.767 7.047 — 4.83 4.91 — — Table 2: Real world evaluation using our newly collected ReSyncED benchmark. We evaluate using both quantitative metrics and human evaluation scores across three classes of real videos. We can see that in all cases, the Wav2Lip model produces high-quality, accurate lip-syncing videos. Specifically, the metrics indicate that our lip-synced videos are as good as the real synced videos. We also note that human evaluations indicate that there is a scope for improvement when trying to lip-sync TTS generated speech. Finally, it is worth noting that our lip-synced videos are preferred over existing methods or the actual unsynced videos over 90% of the time. TTS are out-of-sync with the translated speech). The second type is “Random\", where we have a collection of videos and we create random audio-visual pairs similar to 4.2.2. The third and final type of videos, “TTS\", has been specifically chosen for benchmarking the lip-syncing performance on synthetic speech obtained from a text-to-speech system. This is essential for future works that aspire to automatically translate videos (Face-to-Face Translation [18]) or rapidly create new video content. We manually transcribe the text, use Google Translate (about 5 languages totally) and publicly available text-to-speech models to generate synthetic translated speech for the videos in this category. The task is to correct lip movements in the original videos to match this synthetic speech. 4.4.2 Real-world Evaluation on ReSyncED. We first evaluate the generated real video results using our new automatic metrics, “LSED\" and “LSE-C\" obtained from SyncNet [9]. For the human evaluation, we ask 14 evaluators to judge the different synced versions of the videos based on the following parameters: (a) Sync Accuracy (b) Visual Quality (to evaluate the extent of visual artifacts), (c) Overall Experience (to evaluate the overall experience of the audio-visual content), and (d) Preference, where the viewer chooses the version of the video that is most appealing to watch. The first three parameters are scored between 1 −5, and (d) is a single-choice voting, and we report the percentage of votes obtained by a model. We evaluate each of the three classes of videos separately and report our results in Table 2. An outcome worth noting is that the previous works [17, 18] which produce several out-of-sync segments are less preferred over the unsynced version as the latter still preserves good Visual quality. Thus, ours is the first work that provides a significant improvement over unsynced talking face videos in-thewild. We also show some qualitative comparisons in Figure 3 which contains a few generated samples from the ReSyncED test set. 4.5 Is our expert discriminator best among the alternatives? Model Fine-tuned? Off-sync Acc. LSE-D LSE-C Tv = 1 [18] ✓ 55.6% 10.33 3.19 Ours, Tv = 1 × 79.3% 8.583 4.845 Ours, Tv = 3 ✓ 72.3% 10.14 3.214 Ours, Tv = 3 × 87.4% 7.230 6.533 Ours, Tv = 5 ✓ 73.6% 9.953 3.508 Ours, Tv = 5 × 91.6% 6.386 7.789 Table 3: A larger temporal window allows for better lip-sync discrimination. On the other hand, training the lip-sync discriminator on the generated faces deteriorates its ability to detect off-sync audio-lip pairs. Consequently, training a lipsync generator using such a discriminator leads to poorly lip-synced videos. Our expert discriminator uses Tv = 5 video frames to measure the lip-sync error. It is also not fine-tuned on the generated faces in a GAN setup. We justify these two design choices in this ablation study. We can test the discriminator’s performance by randomly sampling in-sync and off-sync pairs from the LRS2 test set. We vary the size ofTv = 1, 3, 5 to understand its effect on detecting sync. We also fine-tune/freeze each of the three variants of Tv while training the Wav2Lip model. Thus, we get a total of 6 variations in Table 3 from which we can clearly make two observations. Increasing the temporal window size Tv consistently provides a better lip-sync discrimination performance. More importantly, we see that if we fine-tune the discriminator on the generated faces that contain artifacts, then the discriminator loses its ability to detect out-of-sync audio-visual pairs. We argue that this happens because the finetuned discriminator focuses on the visual artifacts in the generated Figure 3: Examples of faces generated from our proposed models (green and yellow outlines). We compare with the current best approach [18] (red outline). The text is shown for illustration to denote the utterance being spoken in the frame shown. We can see that our model produces accurate, natural lip shapes. The addition of a visual quality discriminator also significantly improves the visual quality. We strongly encourage the reader to check out the demo video on our website. faces for discrimination, rather than the fine-grained audio-lip correspondence. Thus, it classifies the real unsynced pairs as “in-sync\", since these real face images do not contain any artifacts. Down the line, using such a weak discriminator leads to poor lip-sync penalization for our generator, resulting in poorly lip-synced talking face videos.\n\n5 APPLICATIONS & FAIR USE At a time when our content consumption and social communication is becoming increasingly audio-visual, there is a dire need for large-scale video translation and creation. Wav2Lip can play a vital role in fulfilling these needs, as it is accurate for videos in the wild. For instance, online lecture videos that are typically in English can now be lip-synced to (automatically) dubbed speech in other local languages (Table 2, last block). We can also lip-sync dubbed movies making them pleasant to watch (Table 2, first block). Every day throughout the globe, press conferences and public addresses are live translated but the addresser’s lips are out of sync with the translated speech. Our model can seamlessly correct this. Automatically animating the lips of CGI characters to the voice actors’ speech can save several hours of manual effort while creating animated movies and rich, conversational game content. We demonstrate our model on all these applications and more in the demo video on our website. We believe that it is also essential to discuss and promote fair use of the increasingly capable lip-sync works. The vast applicability of our models with near-realistic lip-syncing capabilities for any identity and voice raises concerns about the potential for misuse. Thus, we strongly suggest that any result created using our code and models must unambiguously present itself as synthetic. In addition to the strong positive impact mentioned above, our intention to completely open-source our work is that it can simultaneously also encourage efforts [11, 16, 24, 25] in detecting manipulated video content and their misuse. We believe that Wav2Lip can enable several positive applications and also encourage productive discussions and research efforts regarding fair use of synthetic content. 6\n\nRELATED WORK\n2.1 Constrained Talking Face Generation from Speech We first review works on talking face generation that are either constrained by the range of identities they can generate or the range of vocabulary they are limited to. Realistic generation of talking face videos was achieved by a few recent works [19, 22] on videos of Barack Obama. They learn a mapping between the input audio and the corresponding lip landmarks. As they are trained on only a specific speaker, they cannot synthesize for new identities or voices. They also require a large amount of data of a particular speaker, typically a few hours. A recent work along this line [13] proposes to seamlessly edit videos of individual speakers by adding or removing phrases from the speech. They still require an hour of data per speaker to achieve this task. Very recently, another work [23] tries to minimize this data overhead by using a two-stage approach, where they first learn speaker-independent features and then learn a rendering mapping with ≈5 minutes of data of the desired speaker. However, they train their speaker-independent network on a significantly smaller corpus and also have an additional overhead of requiring clean training data of each target speaker to generate for that speaker. Another limitation of existing works is in terms of the vocabulary. Several works [5, 26, 28] train on datasets with a limited set of words such as GRID [10] (56 words), TIMIT [14] and LRW [8] (1000 words) which significantly hampers a model from learning the vast diversity of phoneme-viseme mappings in real videos [18]. Our work focuses on lip-syncing unconstrained talking face videos to match any target speech, not limited by identities, voices, or vocabulary. 2.2 Unconstrained Talking Face Generation from Speech Despite the rise in the number of works on speech-driven face generation, surprisingly, very few works have been designed to lip-sync videos of arbitrary identities, voices, and languages. They are not trained on a small set of identities or a small vocabulary. This allows them to, at test time, lip-sync random identities for any speech. To the best of our knowledge, only two such prominent works [17, 18] exist in the current literature. Note that [17] is an extended version of [7]. Both these works [17, 18] formulate the task of learning to lip-sync in the wild as follows: Given a short speech segment S and a random reference face image R, the task of the network is to generate a lip-synced version Lд of the input face that matches the audio. Additionally, the LipGAN model also inputs the target face with bottom-half masked to act as a pose prior. This was crucial as it allowed the generated face crops to be seamlessly pasted back into the original video without further post-processing. It also trains a discriminator in conjunction with the generator to discriminate in-sync or out-of-sync audio-video pairs. Both these works, however, suffer from a significant limitation: they work very well on static images of arbitrary identities but produce inaccurate lip generation when trying to lip-sync unconstrained videos in the wild. In contrast to the GAN setup used in LipGAN [18], we use a pre-trained, accurate lip-sync discriminator that is not trained further with the generator. We observe that this is an important design choice to achieve much better lip-sync results.\n\nCONCLUSION\nIn this work, we proposed a novel approach to generate accurate lipsynced videos in the wild. We have highlighted two major reasons why current approaches are inaccurate while lip-syncing unconstrained talking face videos. Based on this, we argued that a pretrained, accurate lip-sync “expert\" can enforce accurate, natural lip motion generation. Before evaluating our model, we re-examined the current quantitative evaluation framework and highlight several major issues. To resolve them, we proposed several new evaluation benchmarks and metrics, and also a real-world evaluation set. We believe future works can be reliably judged in this new framework. Our Wav2Lip model outperforms the current approaches by a large margin in both quantitative metrics and human evaluations. We also investigated the reasons behind our design choices in the discriminator in an ablation study. We encourage the readers to view the demo video on our website. We believe our efforts and ideas in this problem can lead to new directions such as synthesizing expressions and head-poses along with the accurate lip movements.\n\nREFERENCES\n[1] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman. 2018. Deep Audio-Visual Speech Recognition. In arXiv:1809.02108. [2] T. Afouras, J. S. Chung, and A. Zisserman. 2018. The Conversation: Deep AudioVisual Speech Enhancement. In INTERSPEECH. [3] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman. 2018. LRS3-TED: a large-scale dataset for visual speech recognition. arXiv preprint arXiv:1809.00496 (2018). [4] Lele Chen, Zhiheng Li, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. 2018. Lip movements generation at a glance. In Proceedings of the European Conference on Computer Vision (ECCV). 520–535. [5] Lele Chen, Ross K Maddox, Zhiyao Duan, and Chenliang Xu. 2019. Hierarchical cross-modal talking face generation with dynamic pixel-wise loss. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 7832–7841. [6] Lele Chen, Haitian Zheng, Ross K Maddox, Zhiyao Duan, and Chenliang Xu.\n2019. Sound to Visual: Hierarchical Cross-Modal Talking Face Video Generation.\nIn IEEE Computer Society Conference on Computer Vision and Pattern Recognition workshops. [7] Joon Son Chung, Amir Jamaludin, and Andrew Zisserman. 2017. You said that? arXiv preprint arXiv:1705.02966 (2017). [8] Joon Son Chung and Andrew Zisserman. 2016. Lip reading in the wild. In Asian Conference on Computer Vision. Springer, 87–103. [9] Joon Son Chung and Andrew Zisserman. 2016. Out of time: automated lip sync in the wild. In Workshop on Multi-view Lip-reading, ACCV. [10] Martin Cooke, Jon Barker, Stuart Cunningham, and Xu Shao. 2006. An audiovisual corpus for speech perception and automatic speech recognition. The Journal of the Acoustical Society of America 120, 5 (2006), 2421–2424. [11] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge Dataset. arXiv:2006.07397 [cs.CV] [12] John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. Journal of machine learning research 12, 7 (2011). [13] Ohad Fried, Ayush Tewari, Michael Zollhöfer, Adam Finkelstein, Eli Shechtman, Dan B Goldman, Kyle Genova, Zeyu Jin, Christian Theobalt, and Maneesh Agrawala. 2019. Text-based editing of talking-head video. ACM Transactions on Graphics (TOG) 38, 4 (2019), 1–14. [14] Naomi Harte and Eoin Gillen. 2015. TCD-TIMIT: An audio-visual corpus of continuous speech. IEEE Transactions on Multimedia 17, 5 (2015), 603–615. [15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition. 770–778. [16] Chih-Chung Hsu, Yi-Xiu Zhuang, and Chia-Yen Lee. 2020. Deep Fake Image Detection based on Pairwise Learning. Applied Sciences 10 (2020), 370. [17] Amir Jamaludin, Joon Son Chung, and Andrew Zisserman. 2019. You said that?: Synthesising talking faces from audio. International Journal of Computer Vision 127, 11-12 (2019), 1767–1779. [18] Prajwal KR, Rudrabha Mukhopadhyay, Jerin Philip, Abhishek Jha, Vinay Namboodiri, and CV Jawahar. 2019. Towards Automatic Face-to-Face Translation. In Proceedings of the 27th ACM International Conference on Multimedia. ACM, 1428–1436. [19] Rithesh Kumar, Jose Sotelo, Kundan Kumar, Alexandre de Brébisson, and Yoshua Bengio. 2017. Obamanet: Photo-realistic lip-sync from text. arXiv preprint arXiv:1801.01442 (2017). [20] Andrew L Maas, Awni Y Hannun, and Andrew Y Ng. 2013. Rectifier nonlinearities improve neural network acoustic models. In Proc. icml, Vol. 30. 3. [21] NPD. 2016. 52 Percent of Millennial Smartphone Owners Use their Device for Video Calling, According to The NPD Group. https://www.npd.com/wps/portal/npd/us/ news/press-releases/2016/52-percent-of-millennial-smartphone-owners-usetheir-device-for-video-calling-according-to-the-npd-group/ [22] Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman. 2017. Synthesizing obama: learning lip sync from audio. ACM Transactions on Graphics (TOG) 36, 4 (2017), 95. [23] Justus Thies, Mohamed Elgharib, Ayush Tewari, Christian Theobalt, and Matthias Nießner. 2019. Neural Voice Puppetry: Audio-driven Facial Reenactment. arXiv preprint arXiv:1912.05566 (2019). [24] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, Aythami Morales, and Javier Ortega-Garcia. 2020. DeepFakes and Beyond: A Survey of Face Manipulation and Fake Detection. arXiv:2001.00179 [cs.CV] [25] Eleanor Tursman, Marilyn George, Seny Kamara, and James Tompkin. 2020. Towards Untrusted Social Video Verification to Combat Deepfakes via Face Geometry Consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops. [26] Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. 2019. Realistic speech-driven facial animation with gans. International Journal of Computer Vision (2019), 1–16. [27] Zhou Wang, Alan C Bovik, Hamid R Sheikh, Eero P Simoncelli, et al. 2004. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing 13, 4 (2004), 600–612. [28] Hang Zhou, Yu Liu, Ziwei Liu, Ping Luo, and Xiaogang Wang. 2018. Talking Face Generation by Adversarially Disentangled Audio-Visual Representation. arXiv preprint arXiv:1807.07860 (2018).\n--- FOOTNOTES © 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM.", "persona": "Student", "response": "### 1. Overview\nThey continue to summarize the primary intentions of the url: Introduction (I’ll limit this section to the author’s reasons for mentioning these urls, and leave out the in-depth information regarding specifically the approaches of these works) The authors use the following terms to refer to the various sections of their network, which I will continue to use following this section: Methodology Conclusion and Further Reading Due to the demanding requirements of using such an approach in practice, one requires a model which can generalize to a variety of speaker identities. As a result, speaker-independent approaches have arose. These models are trained on thousands of speaker identities. However, even these approaches employed in prior publications fail to meet the expectations of the authors of this work. They acknowledge that prior speaker independent models, while being able to generate accurate lip-syncing on individual static images, are inapplicable to dynamic content. For applications such as the translation of television series and films, it is required that an approach is more generalizable to the varying lip shapes of the speakers in different unconstrained videos. Review of Existing Literature Identity encoder: Encodes the concatenation of the ground truth frames and a random reference segment, providing visual context for the network to adapt appropriately to any particular speaker. Speech encoder: Encodes the audio data (self-explanatory). At a high level, Wav2Lip inputs a Mel-spectrogram representation of a particular audio segment alongside a concatenation of the corresponding ground truth frames (with the bottom half masked) and a random reference segment whose speaker confirms to that of the ground truth segment. It reduces this input via convolutional layers to form a feature vector for both the audio and frames input. It then concatenates these feature representations, projecting the resulting matrix onto a segment of reconstructed frames through a series of transposed convolutional layers.\n\n### 2. Background\nUnfortunately, however, earlier approaches commonly failed to generalize to a variety of speakers identities, only performing well when evaluated on the small subset of potential speakers which comprised their training set. The authors acknowledge several discrepancies between prior approaches and the requirements for an approach to work fully in the real world: Requiring a large amount of training data for some methods. Limitations in terms of the extent of vocabulary learned by the model. Training on datasets with a limited set of vocabulary impedes upon the ability of prior approaches to learn the wide variety of phoneme-viseme mappings. Finally, the authors propose their approach, taking into considering both of the above issues in prior works. Terminology Detailed training procedure and hyperparameters used by the authors\n\n### 3. Methods\nPixel-level Reconstruction loss is a Weak Judge of Lip-sync: Loss functions incorporated in prior works inadequately penalize inaccurate lip-sync generation. A Weak Lip-sync Discriminator: The discriminator in the LipGAN model architecture only has a 56% accuracy at detecting off-sync video-audio content, while the discriminator of Wav2Lip is 91% accurate at distinguishing in-sync content from off-sync content on the same test set. Face decoder: Decoded the concatenated feature vectors into a series of reconstructed frames. Loss Functions The full expert discriminator loss is computed by taking the cross-entropy of the distribution $P_{sync}$ as follows: The Visual-Quality Discriminator is trained to maximize the following loss: where the generator loss $L_{gen}$ is formulated as follows: Accordingly, the generator attempts to minimize the weighted sum of the reconstruction loss, the synchronization loss, and the adversarial loss (recall that we are dealing with two discriminators): where $s_w$ is the a weighting value which indicates the penalty attributed to synchronization, and $s_g$ is the adversarial loss. These two disjoint discriminators allow the network to achieve superior synchronization-accuracy and visual generation quality. you can investigate further by reading the url: https://arxiv.org/pdf/2008.10010v1.pdf\n\n### 4. Results\nIdentifying the cause of prior approaches failing to generalize to a variety of speakers. Resolve said issues by incorporating a powerful lip-sync discriminator. Propose new benchmarks for evaluating the performance of approaches towards the task of lip-syncing. Use a pre-trained lip-sync discriminator that is already accurate in detecting out-of-sync video-audio content in raw, unconstrained samples. Random reference segment: A random sample of a segment of consecutive frames used to identify a particular speaker, providing the network context of the identity specific to the aforementioned speaker. The generator aims to minimize the L1 loss between the reconstructed frames $L_g$ and the ground truth frames $L_G$: For lip-syncing, they implement cosine similarity with binary cross-entropy loss, thus computing the probability that a given two frames. More specifically, loss is computed between the ReLU-activated video and speech embeddings 𝑣 and 𝑠. This results in a list of probabilities, one for each sample, indicating the probability that the corresponding sample is in sync. The proposed metric/evaluation system\n\n### 5. Takeaways\nWav2Lip: A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild Ryan Rudes 7 min read · Sep 4, 2020 -- 1 Listen Share Wav2Lip Model Architecture (https://arxiv.org/pdf/2008.10010v1.pdf) This url proposes Wav2Lip, an adaptation of the SyncNet model, which outperforms all prior speaker-independent approaches towards the task of video-audio lip-syncing. The authors note that, while prior approaches typically fail to generalize when presented with video of speakers not present in the training set, Wav2Lip is capable of producing accurate lip movements with a variety of speakers. The section concludes with a brief summary of the authors contributions: They propose Wav2Lip, which significantly outperforms prior approaches. They introduce a new set of benchmarks/metrics for evaluating the performance of models in this task. They release their own dataset to evaluate the performance of their approach when presented with unseen video-audio content sampled from the wild. Wav2Lip is the first speaker-independent approach which frequently matches the accuracy of real synced videos; according to human evaluation, their approach is preferred to existing methods approximately 90% of the time. They push the FID score on generating synchronous video frames for dubbed videos from 12.87 (LipGAN) to 11.84 (Wav2Lip + GAN), improving the average user-preference from 2.35% (LipGAN) to 60.2% (Wav2Lip + GAN). They continue to argue why prior approaches commonly fail to generate accurate lip-syncing when presented with unseen video content from the wild: A Lip-sync Expert Is All You Need Overview of the Wav2Lip Model Architecture Wav2Lip Model Architecture (https://arxiv.org/pdf/2008.10010v1.pdf) Wav2Lip attempts to fully reconstruct the ground truth frames from their masked copies. We compute L1 reconstruction loss between the reconstructed frames and the ground truth frames. Then, the reconstructed frames are fed through a pretrained “expert” lip-sync detector, while both the reconstructed frames and ground truth frames are fed through the Visual Quality Discriminator. The Visual Quality Discriminator attempts to distinguish between reconstructed frames and ground truth frames to promote the visual generation quality of the frame generator. The Lip-Sync Discriminator The Visual-Quality Discriminator That’s it for ”A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild”.", "paper_id": "21", "story_id": "21-02"}
{"paper": "Synthesizing Obama: Learning Lip Sync from Audio SUPASORN SUWAJANAKORN, STEVEN M. SEITZ, and IRA KEMELMACHER-SHLIZERMAN, University of Washington Fig. 1. Given input Obama audio and a reference video, we synthesize photorealistic, lip-synced video of Obama speaking those words. Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results. CCS Concepts: •Computing methodologies →Image-based rendering; Image manipulation; Animation; Shape modeling; Additional Key Words and Phrases: Audio, Face Synthesis, LSTM, RNN, Big data, Videos, Audiovisual Speech, Uncanny Valley, Lip Sync ACM Reference format: Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman.\n\n2017. Synthesizing Obama: Learning Lip Sync from Audio. ACM Trans.\nGraph. 36, 4, Article 95 (July 2017), 13 pages. 1\n\nINTRODUCTION\nHow much can you infer about someone’s persona from their video footage? Imagine learning how to replicate the sound and cadence of a person’s voice, how they speak, what they say, how they converse and interact, and how they appear and express themselves. With tools like Skype, FaceTime, and other video conferencing solutions, we are increasingly capturing video footage of ourselves. In the case of public ﬁgures, there is signiﬁcant video footage available online, in the form of interviews, speeches, newscasts, etc. Analyzing this video is quite challenging, however, as the faces are ofen shown in a near-proﬁle view, the face region is small, and the lighting, dress, hair, and make-up varies signiﬁcantly from one interview to the next (also, most of this video is proprietary). Tis is the author’s version of the work. It is posted here for your personal use. Not for redistribution. Te deﬁnitive Version of Record was published in ACM Transactions on Output Obama Video In this paper, we do a case study on President Barack Obama, and focus on the speciﬁc task of learning to generate video of Obama from his voice and stock footage. Barack Obama is ideally suited as an initial test subject for a number of reasons. First, there exists an abundance of video footage from his weekly presidential addresses— 17 hours, and nearly two million frames, spanning a period of eight years. Importantly, the video is online and public domain, and hence well suited for academic research and publication. Furthermore, the quality is high (HD), with the face region occupying a relatively large part of the frame. And, while lighting and composition varies a bit from week to week, and his head pose changes signiﬁcantly, the shots are relatively controlled with the subject in the center and facing the camera. Finally, Obama’s persona in this footage is consistent—it is the President addressing the nation directly, and adopting a serious and direct tone. Despite the availability of such promising data, the problem of generating mouth video from audio is extremely diﬃcult, due in part to the technical challenge of mapping from a one-dimensional signal to a (3D) time-varying image, but also due to the fact that humans are extremely atuned to subtle details in the mouth region; many previous atempts at simulating talking heads have produced results that look uncanny. In addition to generating realistic results, this paper represents the ﬁrst atempt to solve the audio speech to video speech problem by analyzing a large corpus of existing video data of a single person. As such, it opens to the door to modeling other public ﬁgures, or ourselves (through analyzing Skype footage, e.g.,). Audio to video, aside from being interesting purely from a scientiﬁc standpoint, has a range of important practical applications. Te ability to generate high quality video from audio could signiﬁcantly reduce the amount of bandwidth needed in video coding/transmission (which makes up a large percentage of current internet bandwidth). For hearing-impaired people, video synthesis could enable lip-reading from over-the-phone audio. And digital humans are central to entertainment applications like ﬁlm special effects and games. Input audio\n\n1. Recurrent Neural Network\nSparse shape … …\n\n2. Mouth Synthesis\nMouth texture … …\n\n3. Re-timing\nTarget video … …\n\n4. Final Composite\nFinal output … … Fig. 2. Our system first converts audio input to a time-varying sparse mouth shape. Based on this mouth shape, we generate photo-realistic mouth texture, that is composited into the mouth region of a target video. Before the final composite, the mouth texture sequence and the target video are matched and re-timed so that the head motion appears natural and fits the input speech. Our approach is based on synthesizing video from audio in the region around the mouth, and using compositing techniques to borrow the rest of the head and torso from other stock footage (Fig. 2). Our compositing approach builds on similar talking head techniques like Face2Face [Ties et al. 2016], although Face2Face transfers the mouth from another video sequence whereas we synthesize the mouth shape directly from audio. A main contribution is our recurrent neural network technique for synthesizing mouth shape from audio, trained on millions of video frames, that is signiﬁcantly simpler than prior methods, yet produces very convincing results. We evaluated many different network architectures to arrive at our solution, but found that a surprisingly simple approach based on standard LSTM techniques produces excellent results. In addition, our approach for generating photorealistic mouth texture preserves ﬁne detail in the lips and teeth, and reproduces time-varying wrinkles and dimples around the mouth and chin. 2\n\nEXPERIMENTS\nIn this section, we describe implementation details, evaluations, comparisons to prior work, limitations, and applications. 4.1 Running times and hardware: We report the following runtime based on NVIDIA TitanX for RNN inference, and Intel Core i7-5820K for other computations. For a source audio of 66 seconds in length, on a single CPU core, it took 45 minutes in total to produce a 30fps output video. Te breakdown is as follows: 5 seconds to run RNN inference and generate 1980 mouth shapes (30fps for 66 seconds); mouth texture synthesis took 0.1s per frame (3.3 minutes total); and the ﬁnal composite including chin correction, masking, and rendering took 0.35s per frame (11.5 minutes total). Te retiming dynamic programming solution took 0.2s for the entire sequence, with an additional 4s per repeated frame for optical ﬂow interpolation [Liu et al. 2008]. In practice, we parallelized most computations on a 24-core CPU and reduced the runtime from 45 to 3 minutes total (0.1s per frame). For network training, the time for [Suwajanakorn et al. 2014] to preprocess 17-hour Obama video (pose estimation, frontalization) took around 2 weeks on 10 cluster nodes of Intel Xeon E5530. Te network training for 300 epochs on NVIDIA TitanX and Intel Core i7-5820K took around 2 hours. 4.2 LSTM Architecture and Data For training we downloaded 300 weekly addresses available online2 spanning 2009 to 2016. Each address lasts about 3 minutes on average, resulting in total of 17 hours of video. We extracted frames at 30fps and obtained around 1.9 million video frames. We randomly split out 20% of the addresses (3 hours) for validation and used 80% (14 hours) for training. Our network consists of 60 LSTM nodes (dimension of c) and uses a 20 step time-delay d, corresponding to 200ms. We train the network with a batch size of 100 using truncated backpropagation through time with 100 time steps. We use the ADAM optimizer [Kingma and Ba 2014] with learning rate 0.001, implemented in TensorFlow [Abadi et al. 2016]. Each dimension in the input vector is normalized by its mean and variance, but the output is unnormalized to keep the relative importances of the PCA coeﬃcients. Training took 3 hours in total for 300 epochs on a NVIDIA TitanX. We found that augmenting the LSTM with a time delay was critical for improving validation loss and visual quality. Tis modiﬁcation effectively increases the receptive ﬁeld beyond that of the MFCC window (25ms) to at least 200ms of future audio context. Figure 9 shows validation losses by varying the time delay steps for our single-layer 60-node LSTM network. Without the time delay, both the training and validation losses are high and the visual lip-sync quality is poor. Table 1 shows validations losses for varying time delays as well as the number of LSTM nodes. We found that for our architecture, the time delay of 200ms gives consistently lower validation losses across different numbers of LSTM nodes. Performance decreases beyond 200ms, likely due to the need to propagate information further across time. Time Delay: 50ms 100ms 200ms 400ms L1-30 5.824 4.946 4.572 5.147 L1-60 5.782 4.877 4.400 5.089 L1-90 5.726 4.841 4.391 5.009 L1-120 5.732 4.946 4.572 5.147 Table 1. Validation losses for single-layer (L1) networks with varying (30, 60, 90, and 120) LSTM nodes and time delays. Other network architectures are evaluated in Table 2 by keeping the time delay at 200ms and varying the number of stacked layers, LSTM nodes, and the dropout probability of the standard RNN regularization [Zaremba et al. 2014]. Additionally, we explored other regularization techniques, e.g. variational RNN dropout [Gal 2015] on a few conﬁgurations but did not ﬁnd a major improvement. Tese validation losses have high Loss Loss 8 8 Training Validation Training Validation 7 7 6 6 5 5 4 4 3 3 2 2 1 1 0 0 0 10 20 30 40 50 60 70 80 0 100 200 300 Epoch Time Delay Step a) b) Fig. 9. a) shows the losses at the end of the training of networks with varying time delay steps from 0 to 80 (800ms) trained with 300 unfold time steps. b) plots loss during training of our single-layer LSTM network with 20 time delay steps. Dropout probability: 0 0.1 0.3 0.5 L2-30 4.449 4.587 4.881 5.252 L2-60 4.389 4.420 4.621 4.923 L2-90 4.403 4.347 4.498 4.754 L3-30 4.409 4.548 4.850 5.237 L3-60 4.402 4.386 4.585 4.881 L3-90 4.439 4.310 4.487 4.718 Table 2. Validation losses for two (L2) and three (L3) layers networks with various LSTM nodes and dropout probability. variance and do not necessarily translate to beter visual lip-sync quality afer reaching a certain value. One reason is that we do not have ground-truth mouth landmarks and instead rely on landmark estimates from imperfect algorithms. While we ﬁltered out clear failure cases such as when the mouth shape is irregular or not detected, other error cases remain in both the training and validation sets. As such, zero loss does not equate to perfect visual results. We believe recently developed techniques such as recurrent batch normalization can further improve the loss, but larger accuracy gains may require improving the landmark data. For our purpose, we opt for the simplest network (L1-60, with 200ms delay) that achieves a similar loss and empirically similar visual quality to the more complex models. 4.2.1 Word/phoneme overlap between target and input audio: Here we investigate the amount of phoneme and word overlap between the audio from a target video (used for mouth synthesis) and the input audio. For 4-Obama result (Video E), we compute the percentage of words in input audio that exist in each of the four target videos: Top-lef 41%, -right 45%, botom-lef 52%, -right 48%. In other words, half of the words spoken in the input audio do not exist in the target videos. In terms of phoneme, diphone, triphone, tetraphone, pentaphone overlaps (average across 4 targets): 99.9%, 82.9%, 35.7%, 12.1%, 4.9%, respectively. Tere is less than 5% chance to ﬁnd similar 5 consecutive phonemes in the target videos. (An average word in the input consists of 3.9 phonemes.) 4.2.2 Training set size: Finally, we evaluated the effect of varying the amount of training data on the quality of the output video. We trained our network with: 0.35% of the data (3 minutes total), 10% (1 hour total), 50% (7 hours), and the full test dataset (14 hours). Te supplementary video shows how quality improves with more training data. In particular, with more training data, lip-sync quality improves signiﬁcantly and mouth jiter is reduced. Tere is signiﬁcant improvement at each step, even from 7 hours to 14, indicating that having a large amount of training data is critical. 4.3 Lower Face Synthesis Evaluation Figure 10 shows comparison of our synthesis algorithm to the classic AAM approach [Cootes et al. 2001] that is prevalent in visual speech synthesis, and to a recent detail-enhancing Laplacian pyramid technique [Suwajanakorn et al. 2015]. We observe that both AAM and [Suwajanakorn et al. 2015] show signiﬁcant blurriness. Te blurriness appears due to use of data captured in uncontrolled and uncalibrated conditions, i.e., faces can be non-frontal, under different lighting, etc. Te results in the ﬁgure are computed using the same set of frontalized face images as used in our algorithm, and even if lighting-invariant optical ﬂow (i.e., collection ﬂow [KemelmacherShlizerman and Seitz 2012] is performed as in [Suwajanakorn et al. 2015]), the resulted synthesis is blurry in the teeth area due to ﬁne misalignment across photos. Figure 10 also compares weighted mean, median and mode, for generating facial texture from the same set of image candidates. Mean produces the blurriest result among all three, and mode appears noisy even when the sparse sampling (i.e., low number of candidates) is handled by using larger frequency bins or counting by merging nearby points in color space. Tis behavior can also be understood in an optimization framework where mean, median, and mode correspond to a minimization with L2, L1, L0 norms, respectively. In the case of mode, the summation of the quasi-convex L0 has multiple minima and is sensitive to image noise and misalignment, whereas L2 produces an over-smooth average. Median strikes a good balance between these two, which we’ve seen in practice translates to beter edge-preserving properties than mean and is less noisy than mode. In Figure 11 we compare to the recent Face2face algorithm [Ties et al. 2016]. We provide the same source video (a weekly presidential address) to both methods. Note that we use only the source audio as input, whereas their technique requires the source video–i.e., they effectively have access to the ground truth mouth appearance. In addition, we provide the same target video (a different weekly presidential address) to both methods. Te Face2face were produced by the authors running their original system on our videos. Te differences between the two methods are best viewed in the supplementary video. Some observations: our method tends to produce more realistic and Obama-like lip and head motion. Additionally, our method captures time-varying wrinkles and creases around the mouth whereas Face2face’s texture for the surrounding skin appears more static. Te teeth produced by Face2face sometimes appear non-rigid with occasional ghosting artifacts, whereas teeth produced by our method appear rigid and temporally smoother. Te differences perhaps expected, since our system is trained on many videos of Obama and tuned to produce high quality realistic results a) Mean b) Median c) Mode d) AAM e) [Suwajanakorn f) Ours et al. 2015] Fig. 10. Mouth synthesis comparison to weighted-mean (a), weightedmedian (b), weighted-mode (c), AAM-based techniques (d), and [Suwajanakorn et al. 2015] (e). For all results shown here, we first frontalize all training images using the same frontalization technique as our result, and for (a,b,c,e), we use identical weights to ours computed from Equation 11. Notice how other techniques produce blurry results on our training dataset that contains mouth images from a real speech with natural head motion. a) Face2face [Thies et al. 2016] b) Our results Fig. 11. Comparison to Face2face [Thies et al. 2016] for four different utterances in the same speech using the same source video. Note that [Thies et al. 2016] requires the video of the input speech to drive the animation and focuses on the real-time puppetry application whereas ours aims to synthesize a visual speech given only a recorded audio of Obama. Notice how our method can synthesize more realistic mouths with natural creases around the mouth. The differences between the two approaches are best viewed in the supplementary video. while Face2face is aimed at a real-time puppetry scenario that uses only two videos (source and target). In addition to the above comparisons, we compiled a 4 minute video showing related face animation results of the last 20 years. Te end of our supplementary video includes this compilation, as a helpful visual reference for our work. algorithm. We evaluate this on Video A: the average standard deviation of the frame indices of 100 image candidates used to produce one output texture is 931.8 frames (spanning across 31 seconds) with standard deviation 88.5 (3 seconds). Te span (or the min and max) of the frame indices are always at least 2,448 frames apart (3,648 total). 4.4 Target Video Re-timing Evaluation For all results shown, we use the following ﬁxed parameters: Te blinking detector was implemented by ﬁrst normalizing eye landmarks by making the distance between lef and right eyes a unit distance. Ten blinking is detected if the average eye area is less than 0.025. For indicating silence, we use a threshold of 0.001 on the average audio volume within a window. Te resulting binary ﬂags with 0-gap less than 0.5 seconds are connected, and then dilated with kernel of size 0.1 seconds. Te dilation and erosion kernels for removing salt and pepper noise for blinking are 3, 3 frames respectively. In the supplementary video (Video D) we show results generated with and without re-timing (Section 3.3). Since our dynamic programming approach also solves for the best starting frame of the target video, for comparison we start both target videos at this same frame. Without retiming, there are occasional head motion and expression changes during vocal pauses which appears unnatural, as indicated by the red arrow. We also evaluate consistency of the re-timing approach across different target videos (Video E in sup. video), by generating output videos with four different targets. Most of the time, all four results start moving when Obama starts a new sentence and stop during long pauses. Occasionally, Obama moves slightly during small pauses, but fast motion during pauses is avoided. 4.5 Results & Applications All of the result ﬁgures and supplementary videos are generated from input speeches that are not in the training or validation sets. In Figure 12, we run on the audio of Obama weekly address “Standing with Orlando” from YouTube with ID3 nIxM8rL5GVE and use four different target videos also from weekly addresses (E3gfMumXCjI, 3vPdtajOJfw, 25GOnaY8ZCY, k4OZOTaf3lk). Fully generated videos for each of the targets are presented in supplementary Video E. Our results are realistic with convincing, natural head motion. When compared to prior techniques (Video I), our mouth texture looks sharper than those that rely on AAM and more temporally smooth than those that use frames from original footage. Te upper teeth look sharp and realistic. We do note that the lower teeth and tongue appear blurrier and less plausible when compared to the groundtruth footage, e.g., in Video A. In Figure 13 and Video F, we compare our synthesized textures to the original video of the input speech. In Video J, we show the pixel difference map (mean of absolute RGB differences) between the groundtruth video of the input audio and our result using the same input video as the target video (no retiming). Our results show good agreement in terms of the mouth Fig. 12. Results for the same input speech using four different target videos. Results along each column are generated from the same uterance. shapes, timing, and overall mouth animation, but the mouth region appears less detailed and smoother compared to the groundtruth. To evaluate generalization to other types of input speech not from the weekly addresses on which we trained, we ran our system on the audio of Obama’s interview with Steve Harvey (qMlLjFPCO4M), 60 Minutes (F8MxP9adPO8), and the View (Hdn1iX1a528). Our method can handle casual speech and generates plausible mouth animation even during a mumble or hesitation. We also test on the voice of a quarter century younger Obama from 1990 (7XGi3FGVmA0) and on a voice impressionist (vSAA5GH6OFg). Te lipsync quality still looks reasonable although it degrades somewhat as the voice deviates from our training audio. One useful application of our method is speech summarization, inspired by [Berthouzoz et al. 2012]. I.e., given a long address speech, create a short summary version by manually selecting the desired sections from transcribed text. Our method can then be used to generate a seamless video for the summarized speech shown in Video G. While our result looks similar to [Berthouzoz et al. 2012] for this particular example, an advantage of our system is that we can produce visually seamless cuts even when the head position, lighting, or background has changed between concatenated source footage clips. Original Video for Input Audio a) Our result Original Video for Input Audio b) Our result Fig. 13. Comparison of our mouth shapes to the ground-truth footage of the input audio (note good agreement–more results in supplemental video). a) is a weekly address on climate change (cNVzN62l0Yg), and b) is on health care (deF-f0OqvQ4).\n\n5 DISCUSSION & FUTURE WORK We show that by training on a large amount of video of the same person, and designing algorithms with the goal of photorealism in mind, we can create believable video from audio with convincing lip sync. Tis work opens up a number of interesting future directions, some of which we describe below. Our pipeline includes one manual step that the user must perform for each target video: selecting and masking a teeth proxy. We believe this step could be automated by training a teeth detector (looking for a large, clear white teeth image) Our method relies on MFCC audio features, which are not designed speciﬁcally for visual synthesis. Tis suggests that an even more end-to-end network may be able to achieve even beter quality by going directly from raw audio waveforms to mouth shapes or textures. For example, [van den Oord et al. 2016] achieves beter performance in natural audio generation by replacing MFCC and RNN with dilated convolution. It would be interesting to see how such a network could be applied to our audiovisual synthesis task. Similarly, it would be interesting to see if the network could learn to predict emotional state from audio to produce corresponding visuals (e.g., happy, sad, angry speech, etc.). Training our system on another person, such as a non-celebrity, can be quite challenging due to the diﬃculty of obtaining hours of training data. However, the association between mouth shapes and uterances may be, to some extent, speaker-independent. Perhaps a network trained on Obama could be retrained for another person with much less additional training data. Going a step further, perhaps a single universal network could be trained from videos of many different people, and then conditioned on individual speakers, e.g., by giving it a small video sample of the new person, to produce accurate mouth shapes for that person. While we synthesize only the region around the mouth and borrow the rest of Obama from a target video, a more ﬂexible system would synthesize more of Obama’s face and body, and perhaps the background as well. Such a system could enable generating arbitrary length sequences, with much more control of how he moves and acts.\n\nRELATED WORK\nCreating a photorealistic talking head model – a virtual character that sounds and appears real, has long been a goal both in digital special effects and in the computer graphics research community. In their seminal paper, Bregler et al. [1997] demonstrated how to “rewrite” a person’s lip movement in a video to match a new audio track represented as a phoneme sequence. Teir approach was notable in automating all of the key components; face tracking, phoneme detection, mouth synthesis, and compositing, and produced compelling results for a few short sequences. However, the generality of the method in practice was limited due to insuﬃcient phoneme and viseme reference data; as noted by the authors, correct triphones could be found only 6% of the time, and visemes had to be present for each desired pose. Nevertheless, Video Rewrite remains important as one of the very few techniques in the literature that operate on existing video footage, e.g., President John F. Kennedy, rather than training on laboratory-captured footage. Almost all subsequent work that aims to produce photo-realistic speech from audio has required subjects captured in a controlled lab environment, e.g., [Anderson et al. 2013a; Fan et al. 2015a; Matheyses et al. 2013; Shimba et al. 2015; Wang et al. 2010]. Te advantage of the lab environment is that the pose of the subject, their lighting, and the words they uter can all be controlled. Typically, the subject is instructed to say pre-determined, phonetically-rich sentences in a neutral expression (or repeat with up to six different emotions [Anderson et al. 2013a]). In contrast to these lab-based approaches, our goal is to develop methods that can eventually be applied to online video footage, i.e., from interviews, speeches, or Skype feeds. A requirement of most prior work in this area is the need for phoneme labels with millisecond-accurate timestamps. Tese labels are either provided manually, from a speech recognition system, or from a text-to-speech module [Fan et al. 2015a; Matheyses et al. 2013]. Automatic phoneme labeling tends to be error-prone, and thus limits the quality of lip sync. Te input is ofen converted to a sequence of phonemes, or diphones and triphones that encode the surrounding phonemes [Bregler et al. 1997; Fan et al. 2015a]. Additional known contextual information such as stress or position in the sentence may also be provided [Fan et al. 2015a]. Different schemes have been used to solve this regression problem that takes phonemes as input and predicts the visual speech. One is based on Hidden-Markov Models (HMM) [Fu et al. 2005; Sako et al. 2000; Xie and Liu 2007a,b] and the other constructs the ﬁnal visual speech by concatenating visemes generated from a learned phoneme-toviseme mapping [Taylor et al. 2012]. Both approaches require a signiﬁcant amount of linguistic modeling and are quite complex. Voice Puppetry [Brand 1999] is notable as an early (HMM-based) approach that does not require phoneme labels. Recently, regression techniques based on decision tree [Kim et al. 2015], or deep bidirectional long short-term memory [Fan et al. 2015a] have been shown to outperform HMM-based approaches, although these still rely on phoneme labels. Similar to our approach, [Shimba et al. 2015] use an LSTM neural network that does not require phoneme labels and works directly on audio features. However, their network lacks a time-delay, which we have found crucial to producing good results. Tey unfortunately have no video results available online, and the images in the paper are limited to low-res face images with mocap dots that have been manually annotated. As an alternative to recurrent neural network, another recent work [Taylor et al. 2016] uses a deep neural network to regress a window of visual features from a sliding window of audio features. In this work, we show that a simple time-shif recurrent network trained on an uncontrolled, unlabeled visual speech dataset is able to produce convincing results without any dependencies on a speech recognition system. A key aspect of rendering realistic talking heads is synthesizing visual speech, i.e., the motion and appearance of the mouth and surrounding areas. While simple effects can be produced based on motion alone, i.e., using morphing techniques to warp the mouth into new poses [Brand 1999], the results ofen look cartoony, as they fail to capture important geometric and shading changes, e.g., creases, dimples, that occur as you move your mouth. Most modern methods therefore atempt to synthesize at least the mouth region of the face. Face synthesis algorithms can be categorized into methods that use 3D face models [Anderson et al. 2013a; Cao et al. 2016, 2005; Ties et al. 2016] and others that operate on 2D images. Even in most 3Dbased methods, the mouth and teeth are not explicitly modeled in 3D but are represented with 2D texture, e.g. in [Anderson et al. 2013a]. One of the most common 2D face texture synthesis techniques is Active Appearance Models (AAM) [Cootes et al. 2001] where a face is jointly modeled in a PCA-based representation for both the sparse shape and texture. Due to the low-rank PCA approximation, however, details such as mouth and teeth ofen appear blurry [Fan et al. 2015a]. Alternatively, several authors have chosen to use a teeth proxy [Anderson et al. 2013b; Garrido et al. 2015; Ties et al. 2015] or to copy teeth texture from original source frames [Ties et al. 2016; Vlasic et al. 2005; Wang et al. 2010]. Neither approach is full-proof, however, ofen appearing unnatural with artifacts near the lip boundary (see our overview of related work in the accompanying video). A third source of artifacts in prior art is temporal ﬂickering. To produce smoother results, triphones are sometimes used to ﬁnd longer subsequences [Bregler et al. 1997], or a visually-smooth path through the original frames is optimized with respect to some similarity cost function [Bregler et al. 1997; Wang et al. 2010]. Another approach is to use optical ﬂow to interpolate in-between frames [Li et al. 2012; Ties et al. 2016]. Note that some of the similarity metrics in these facial reenactment methods require a driving reference face to compare to, which is not available when only audio is given as input. Unfortunately, none of these techniques are full-proof, and some amount of ﬂickering or unnatural warping ofen remains. In contrast, our mouth synthesis approach is simple, highly realistic, and naturally exhibits temporal continuity without the need for explicit temporal smoothing or interpolation. We close our related work section by mentioning perhaps the simplest rewrite approach is to take raw video clips of a person talking, chop them up into word-long segments, and reorder the clips to ﬁt the words of any desired new sentence. Te popular website talkobamato.me does just that; the results can be fun, but also distracting, as the the background, pose, and tone changes rapidly and discontinuously. 3 AUDIO TO VIDEO Given a source audio track of President Barack Obama speaking, we seek to synthesize a corresponding video track. To achieve this capability, we propose to train on many hours of stock video footage of the President (from his weekly addresses) to learn how to map audio input to video output. Tis problem may be thought of as learning a sequence to sequence mapping, from audio to video, that is tailored for one speciﬁc individual. Tis problem is challenging both due both to the fact that mapping goes from a lower dimensional (audio) to a higher dimensional (video) signal, but also the need to avoid the uncanny valley, as humans are highly atuned to lip motion. To make the problem easier, we focus on synthesizing the parts of the face that are most correlated to speech. At least for the Presidential address footage, we have found that the content of Obama’s speech correlates most strongly to the region around the mouth (lips, cheeks, and chin), and also aspects of head motion – his head stops moving when he pauses his speech (which we model through a retiming technique). We therefore focus on synthesizing the region around his mouth, and borrow the rest of Obama (eyes, head, upper torso, background) from stock footage. We use the following terms throughout the paper: the many hours of online weekly address video is referred to as stock video footage. Stock footage will be used to train our audio-to-shape neural net. Te input audio track is the source, and the target video is a stock video clip into which we composite the synthesized mouth region. Te overall pipeline works as follows (Fig. 2): Given an audio of Obama, we ﬁrst extract audio features to use as input to a recurrent neural network that outputs, for every output video frame, a sparse mouth shape (Section 3.1). From the sparse mouth shape, we synthesize texture for the mouth and lower region of the face (Section 3.2). Te mouth texture is then blended onto a stock video that is modiﬁed so that the ﬁnal head motion appears natural and matches with the given input speech (Section 3.3). During blending, the jaw line is warped to match the chin of the new speech, and the face is composed to a target frame in the original pose. (Section 3.4). 3.1 Audio to Sparse Mouth Shape Rather than synthesize video directly from audio, we decompose the problem into two steps: 1) map from audio features to sparse shape coeﬃcients, and 2) map from shape to mouth texture. Like [Shimba et al. 2015], we skip the error-prone process of phoneme extraction, and map directly from audio to sparse shape features. In this step, we represent the audio using standard MFCC coeﬃcients, and the mouth shape by 18 lip ﬁducials, ranked reduced by a PCA basis, as described next. Audio Features. For audio features, we use Mel-frequency cepstral coeﬃcients (MFCC) which are computed as follows: (1) Given a 16KHz mono audio, we normalize the volume using RMS-based normalization in ffmpeg [Bellard et al. 2012; Robitza 2016]. (2) Take the Discrete Fourier Transform on every 25ms-length sliding window over the audio with 10ms sampling interval. (3) Apply 40 triangular Mel-scale ﬁlters onto the Fourier power spectrum, apply logarithm to the outputs. (4) Apply the Discrete Cosine Transform to reduce dimensionality to a 13-D vector. Te ﬁnal 28-D output feature vector consists of the 13-D vector plus the log mean energy to account for volume, and their ﬁrst temporal derivatives. Mouth Shape Features. To compute the mouth shape representation, we ﬁrst detect and frontalize Obama’s face in each video frame using the approach in [Suwajanakorn et al. 2014]. For each frontalized face, we detect mouth landmarks using [Xiong and De la Torre 2013] which gives 18 points along the outer and inner contours of the lip. We reshape each 18-point mouth shape into a 36-D vector, apply PCA over all frames, and represent each mouth shape by the coeﬃcients of the ﬁrst 20 PCA coeﬃcients; this step both reduces dimensionality and decorrelates the resulting feature set. Finally, we temporally upsample the mouth shape from 30Hz to sampling rate. Note that this upsampling is only used for training; we generate the ﬁnal video at 30Hz. 3.1.1 Recurrent Neural Network. We seek to learn a mapping from MFCC audio coeﬃcients to PCA mouth shape coeﬃcients. Let’s model this mapping using a Neural Network. Consider Obama saying the word “America”. He begins by making the sound Uhhh, which is a cue to the mouth synthesizer that he should start opening his mouth. Clearly our network needs the latest audio features as input to determine the mouth shape. But note also that the current mouth shape also depends on the previous shape; he will continue to say Uhhh for several milliseconds during which time the mouth will open wider, rather than reopening from a closed state. Tese considerations motivate a recurrent neural network (RNN): at each moment in time, this network takes the latest audio input xt , uses it to modify its hidden state, aka memory c, and outputs a new mouth shape vector yt for that time instant, as well as passing its memory forward in time. In this way, the memory vector (which can be as large as you want), can represent events potentially far into the past. Tis RNN technique is very popular for learning time-series problems. RNNs are similar to Hidden Markov Models (HMMs), which have been the basis for most prior talking face work [Fu et al. 2005; Sako et al. 2000; Xie and Liu 2007a,b], but RNNs provide a more general memory mechanism, nonlinear transitions, and beter performance on many problems. Many variants of RNNs have been proposed, and we use Long Short Term Memory (LSTM) models which provide a more eﬃcient mechanism for modeling long term dependencies. LSTMs work by replacing each hidden unit with a series of gates that are speciﬁcally designed to facilitate remembering and forgeting (when useful) information (see 1 for a nice tutorial). Sometimes, your mouth moves before you say something. I.e., by the time Obama says Uhhh, his mouth is already open. Hence, it’s not enough to condition your mouth shape on past audio input – the network needs to look into the future. Shimba et al. [2015] point this out as a limitation of their LSTM method. One possible solution is to make the network bidirectional. Indeed [Fan et al. 2015b] uses a bidirectional LSTM to exploit future context. However, bidirectional networks require much more compute power and memory to train, as they must be unfolded in the backpropagation process, which usually limits not only the length of training examples, but also the length of the output. Instead, a much simpler way to introduce a short future context to a unidirectional network is to add a time delay to the output by shifing the network output forward in time as explored in [Graves and Schmidhuber 2005] as “target delay.” While bidirectional LSTMs are popular for speech recognition problems [Graves and Schmidhuber 2005], we ﬁnd that the simpler time delay mechanism is suﬃcient for our task, likely due to the need to look less far in the future for audio to video, compared with speech recognition which may require looking multiple words ahead. We ﬁnd that introducing this time delay dramatically improves the quality of results (Section 4.2), compared to prior architectures like [Shimba et al. 2015] which omit it. A time-delayed RNN (for a delay of d = 2) looks like this: We opt for a simple single-layer unidirectional LSTM [Hochreiter and Schmidhuber 1997]. In Section 4.2, we show a comparison with other architectures such as multi-layer LSTMs, but we did not ﬁnd signiﬁcant improvements to merit the additional complexity. Given [x1, . . . ,xn], [y1, . . . ,yn] as input and output vector sequences, our standard LSTM network is deﬁned by the following functions: ft = σ (Wf · [ht−1,xt ] + bf ) (1) it = σ (Wi · [ht−1,xt ] + bi ) (2) ot = σ (Wo · [ht−1,xt ] + bo) (3) ct = ct−1ft + it tanh(Wj · [ht−1,xt ] + bj ) (4) ht = tanh(ct )ot (5) ˆyt−d = Wyht + by (6) where f ,i,o,c,h are forget gate, input gate, output gate, cell state, cell output as proposed in [Hochreiter and Schmidhuber 1997]. σ is the sigmoid activation. Note that the cell state and cell output are transformed with tanh. ˆyt−d represents the predicted PCA coeﬃcients at time t −d where d is the time-delay parameter. Learned parameters are weight matrices W and bias vectors b. We use a 60 dimensional cell state c and a time delay d of 20 steps (200ms). Te network is minimized using L2-loss on the coeﬃcients and trained using Adam optimizer [Kingma and Ba 2014] implemented in TensorFlow [Abadi et al. 2016], on many hours of stock Obama weekly address footage. More details can be found in Section 4.2. 3.2 Facial Texture Synthesis In this section, we describe our approach for synthesizing high detailed face textures from sparse mouth shapes. We focus on synthesizing the lower face area, i.e., mouth, chin, cheeks, and area surrounding the nose and mouth. Figure 8 shows the mask. Te rest of Obama’s appearance (eyes, head, torso, background) comes from stock footage of Obama’s weekly addresses. Our texture synthesis algorithm is designed to satisfy two key requirements: 1) sharp and realistic appearance per video frame, 2) temporally smooth texture changes across frames. We explored several approaches for synthesizing the mouth area based on prior art (see Section 2) but found that results were either too blurry (in the case of Active Appearance Models), the teeth were too non-rigid (with warping/ﬂow techniques), or the illumination was mismatched. We compare these different techniques in Figure 10 and the supplementary video. Instead, we propose an approach that combines weighted median and high frequencies from a teeth proxy. Given a sparse mouth shape sequence and a target video, we process each mouth shape independently. Te algorithm overview is as follows: per mouth PCA shape, select a ﬁxed number of target video frames that best match the given shape; apply weighted median on the candidates to synthesize a median texture; select teeth proxy frames from the target video, and transfer high-frequency teeth details from the proxy into the teeth region of the media texture. We describe those steps in detail below. 3.2.1 Candidate frame selection: Given a generated mouth shape, we seek a set of best matching frames from the target video. Candidate frames are selected as follows: we run a landmark detector [Xiong and De la Torre 2013] on the target video, estimate 3D pose, and frontalize every frame using a 3D model of Obama (Figure 3). We compute the 3D face model using [Suwajanakorn et al. 2014], and augment it with rough approximations of chin and background shape. We found that the later step signiﬁcantly improved frontalization results. Te 3D face model is extended to include the chin by assuming a planar background and solving for a smooth surface that connects the face to the background. Speciﬁcally, we minimize the surface’s second partial derivatives: suppose the initial surface parametrized on a 2D depth map f (x,y) : Ω→R is only given on the face region Ω′ ⊂Ω. We solve for a new f ∗on the entire domain Ωby: \" !2 + ∂2f ∗ !2 dx dy (7) ∂2f ∗ min f ∗ ∂x2 ∂y2 Ω subject to f ∗|Ω′ = f |Ω′ and ∇f ∗|∂Ω= 0 (8) where ∂Ωdenotes the boundary of Ω. Te objective and constraints are turned into a linear least squares problem (with sof constraints) on a discrete pixel grid by ﬁnite differences. Te extended 3D model is shown in Figure 3b. Next, we estimate drif-free 3D pose for each frame using [Suwajanakorn et al. 2014], place the model onto each frame, and back project the head to the frontal view. Even a) Reconstruction from [Suwajanakorn et al. 2014] b) Our extended model with a simple plane Fig. 3. We augment an Obama model (a) reconstructed from [Suwajanakorn et al. 2014] with a simple, near-planar background. This extension is used for frontalizing the chin and neck in addition to the face region. though this extended geometry is inaccurate away from the face area, it suﬃces as a frontal-warping proxy since the ﬁnal synthesized texture will be warped back to the original pose using the same geometry. Texture is synthesized in an area deﬁned by a manually drawn mask that includes the lower face and neck areas in frontal pose. Te mask is drawn only once. Additionally, since in some poses the neck is occluded, we automatically mask out the clothing in every video frame (by means of simple thresholding in HSV space; the same threshold is used in all results) and in-paint the masked region using [Telea 2004], and the OpenCV [Bradski 2000] implementation. Once all frames are frontalized and pose is computed, n frames that have the smallest L2 distance between frame’s mouth shape and target mouth shape are selected. 3.2.2 Weighted median texture synthesis: Given a set of frontal mouth candidate images {I1, . . . , In} with associated mouth shapes S1, . . . ,Sn where Si ∈R2×18 and a target mouth shape Starget, we ﬁrst compute the weighted median per pixel (u,v): n X i=1 wi |Ii (u,v) −c| (9) median(u,v) = arg min c subject to ∃k,c = Ik (u,v) (10) Tis is computed independently for each of the R,G,B channels. c is the output pixel intensity and wi represents how similar Si is to Starget and is computed by: −∥Si −Starget∥2 2 2σ 2 (11) wi = e Choosing the right σ is critical. Small σ will create a peak distribution on a few images which can cause temporal ﬂickering, similar to taking a single original frame, and large σ can produce a blurry result. Moreover, the optimal σ for one target shape can be suboptimal for another target shape depending on the number of good candidates, i.e., ones with small ∥Si −Starget∥. Because the optimal σ is tied to the number of good candidates, we adaptively select σ such that the weight contribution of n candidates is α-fraction of the weight of all available frames. In other words, we solve for σ Realism Temporal Smoothness Highest Lowest High Lowest Highest High n = 10 n = 20 n = 50 n = 100 Our final result a) b) Fig. 4. a) shows the visual quality with respect to the number of candidates (n). Even though averaging (through median) lower numbers produce sharper results, they are not temporally smooth when used in an animation. On the other hand, our final result shown in b) both minimizes blur and is temporally smooth. for each target shape such that n X N X i=1 wi (σ) = α i=1 wi (σ) (12) where N is the total number of video frames. Tis can be eﬃciently solved with a binary search on σ. We ﬁx α to 0.9 and tune n for the best balance between the visual quality and temporal smoothness (Figure 4). Once n is selected, only σ will vary for each output frame. With all wi computed, Equation 9 is solved eﬃciently by sorting pixel intensities and picking the intensity situated at the half of the total weight. 3.2.3 Teeth Proxy: Synthesizing realistic teeth is surprisingly challenging. Te teeth must appear rigid, sharp, pose aligned, lit correctly, and properly occluded by the lip. Our evaluation of prior methods (see Section 2 and submission video) all exhibited problems in one or more of these aspects. In particular, AAM-like models yielded blurry results, while teeth proxy approaches produced compositing artifacts. We achieved our best results with a new, hybrid technique that combines low-frequencies from the weighted median texture, and high frequency detail from a teeth proxy image. Tis idea of combining frequencies from different sources is based on [Oliva et al. 2006], which also inspires [Shih et al. 2014; Suwajanakorn et al. 2015]. Te key insight is that the median texture provides a good (but blurry) mask for the teeth region, whereas the teeth proxy does a good job of capturing sharp details. Hence, we apply the high frequencies of the teeth proxy only in the (automatically detected) teeth region of the median image. Te teeth proxy reference frame is manually chosen to be one of the target frames where teeth are frontal-facing and highly visible. We need one proxy for the upper and another for the lower set of teeth; these two proxies may be chosen from different frames. Tis is a step in approach that is manual, and must be repeated for each target sequence. Te teeth region in the median texture, to which we will transfer proxy details, is detected by applying a threshold (low-saturation, high-value) in HSV space within the mouth region given by the landmarks. c) After sharpening a) Median b) After teeth enhancement d) After only sharpening Fig. 5. The effects of proxy-based teeth enhancement. a) shows the weighted median texture computed in Section 3.2.2. b) is afer applying proxy-based teeth enhancement in Section 3.2.3. c) is afer an additional high-pass filter. d) shows the result of a high-pass filter on the median texture, without the teeth proxy. Given a teeth proxy reference frame T whose pixel values are converted to be within [0, 1]3, we apply a high-pass ﬁlter to T: Hσ,s (T ) = (T −Gσ ∗T ) × s + 0.5 (13) where Gσ is a Gaussian kernel with standard deviation σ, ∗is the convolution operator, and s is the adjustable strength. We also truncate the value of H (T ) to be within [0, 1]. Ten given a median texture I, we compute the ﬁnal texture I ′ for pixel (u,v) in the mouth region as: I ′(u,v) =  if H (u,v) < 0.5 1 −2(1 −I (u,v))(1 −H (u,v)) otherwise (14) Additionally, we enhance I with multiple Hσ,s of different σ’s to handle various frequency scales. Tis high-frequency addition, however, only works for the upper teeth, since they are stationary with respect to the face. For the lower teeth, we shif H (u,v) ←H (u+∆u,v+∆v) by ∆u, ∆v which represent the jaw difference between I and T estimated from the lower lip landmarks. Without accurate landmarks, this can cause ﬂickering. So, instead of using the landmark output from our network or running a landmark detection on I which can be noisy, we compute a weighted average of the lip landmarks of all image candidates using the weights from Equation 11 to obtain an accurate, temporally smooth jaw location. Te enhanced teeth texture afer an additional spatial sharpening (unsharp masking) is illustrated in Figure 5. 3.3 Video Re-timing for Natural Head Motion We assume the availability of a target video, into which our synthesized mouth region will be composited. Any of Obama’s weekly presidential address videos work well as targets, for example. Since the speech in the target video is different from the source (input) speech, a naive composite can appear awkward. In particular, we’ve observed that it’s important to align audio and visual pauses; if Obama pauses his speech, but his head or eyebrows keeps moving, it looks unnatural. To solve this problem, we use dynamic programming to re-time the target video. We look for the optimal monotonic mapping between N synthesized mouth animation frames and M target video frames such that: • it prefers more motion during uterance and minimal motion during silence • any target video frame may be repeated at most once but never skipped. Tis limits slow downs to at most 50% and the video cannot be sped up; otherwise a noticeable jump or freeze can occur. • it prefers sections of the target video where slowing down would be least noticeable, i.e., not during blinking or quick expression changes. To formulate the dynamic programming objective, we ﬁrst compute the motion speed for each frame j in the source video, denoted by V (j), using the ﬁrst derivative of the facial landmark positions as well as a binary ﬂag indicating a blink, denoted by B(j), by applying a threshold on the size of the eye landmarks. Ten we assign V (j) ←V (j) + αBB(j) where αB is a balancing weight. For the source speech, we compute a binary ﬂag, denoted by A(i), indicating non-silence by applying a threshold on the audio volume. Tese binary ﬂag sequences typically contain salt and pepper noise (random 0’s or 1’s), which we ﬁlter out by applying dilation followed by erosion to remove small gaps of 0’s. We additionally ﬁlter out very short consecutive sequences of 0’s or 1’s by a second threshold. Te recurrence relation is deﬁned as follows: F (i, j, 0) = min(F (i −1, j −1, 0), F (i −1, j −1, 1)) + G(i, j) (15) F (i, j, 1) = F (i −1, j, 0) + αsV (j) + G(i, j) (16) G(i, j) =  V (j) if A(i) = 0 0 otherwise (17) −  αuV (j) if A(i −2) = 1 and A(i −3) = 0 0 otherwise (18) where F (i, j,k) stores the score when the ith mouth shape frame is matched with the jth video frame and k is the number of times this mouth frame has been repeated. αs penalizes repeating frames during large motion. G(i, j) is an auxiliary function that penalizes large motion during silence and small motion during uterance with adjustable weight αu. We initialize the base cases as follows: F (0, j, 0) =  V (j) if A(i) = 0 0 otherwise (19) F (i, 0, 0) =  ∞ if i > 0 0 otherwise (20) F (i, j, 1) = ∞if i = 0 or j = 0 (21) Te optimal score is minj ( min(F (N −1, j, 0), F (N −1, j, 1)) ) and the optimal mapping is found by back-tracing the minimal path through the 3-dimensional F array with an overall O(MN ) time complexity. We set αB = 1 and αs = αu = 2 in our implementation. Finally, to avoid having a completely static motion for the ﬁnal composite when a frame is repeated, we warp the repeated frame half way between the previous and next frame. Speciﬁcally, suppose frame i is a copy of frame i −1, we compute optical ﬂows F(i−1)→(i+1) a) Without jaw correction b) With jaw correction Fig. 6. a) shows a jawline discrepancy when the mouth texture of a different speech is blended onto a target video frame. b) shows our corrected result where two jawlines are connected. Optical flow warping a) Target frame b) Mouth texture c) After jaw correction = d) Optical flow e) Mask f) Masked flow Fig. 7. To prepare the mouth texture so that the final jawline appears seamless in Figure 6, we first compute optical flow between a target video frame (a) and our mouth texture (b). This resulting flow (d) is masked by (e) to produce (f) which is used to warp our mouth texture and produce the final texture in (c). and F(i+1)→(i−1), and deﬁne the ﬁnal frame i as the average of frame i −1 warped by 0.5F(i−1)→(i+1) and frame i + 1 warped by 0.5F(i+1)→(i−1). 3.4 Composite into Target Video Compositing into the target video is the ﬁnal step of our algorithm. By this point, we have created a lower face texture for each mouth shape corresponding to the source audio. We have also re-timed the target video to naturally ﬁt silence or talking moments in the source audio. Te key part of the composition step is to create a natural, artifact-free chin motion and jawline blending of the lower face texture into the target head. Figure 6 illustrates how blending may look if jawlines are not explicitly corrected. Te artifacts are especially visible when watching a video. Terefore, we created a jaw correction approach that operates per frame. 3.4.1 Jaw correction: Te algorithm is illustrated in Figure 7. Optical ﬂow (d) is computed between the lower face texture frame (b) and target video frame (a). Next, an alpha map is created based on ﬁducials to focus only on the area of the jawline (e). Te ﬂow is masked by the alpha map and then used to warp the lower face texture to ﬁt the target frame. Mouth texture Target frame Mouth texture = a) Target frame (non-shirt part) b) Neck c) Shirt d) Mouth e) Final texture Fig. 8. The final composite is produced by pyramid blending of the following layers from back to front: a) the target frame, b) the neck region under the chin in the mouth texture, c) Obama’s shirt from the target frame, d) the mouth. 3.4.2 Final compositing: Figure 8 illustrates the ﬁnal masking and blending steps. Te blending is done using Laplacian pyramids [Burt and Adelson 1983] in a layer based fashion. Tere are four layers that are blended in the following order from front to back: 1) Lower face texture (excluding the neck), 2) torso (shirt and jacket), 3) Neck, and 4) the rest. Parts 1 and 3 come from the synthesized texture, while parts 2 and 4 come from the target frame. Te neck mask is the region under the chin in our synthesized mouth texture and the mouth mask is the region above. Te chin is determined by splining face contour landmarks estimated from DLIB library [King 2009]. In some target videos where the background is easy to segment, e.g. when it is a solid black, we create an additional mask for the background (via a color detector) and add it to the shirt mask to have the second layer include both the shirt and background. Although this is optional, it helps prevent the artifact shown in Figure 14b. Te ﬁnal texture is rendered back to the target frame pose using the 3D shape estimated in the synthesis part (Section 3.2). 4\n\nACKNOWLEDGMENTS\nWe thank Samsung, Google, Intel, and the University of Washington Animation Research Labs for supporting this research.\n\nREFERENCES\nMartın Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S Corrado, Andy Davis, Jeffrey Dean, Mathieu Devin, and others. 2016. Tensorﬂow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467 (2016). Robert Anderson, Bj¨orn Stenger, Vincent Wan, and Roberto Cipolla. 2013a. An expressive text-driven 3D talking head. In ACM SIGGRAPH 2013 Posters. ACM, 80. Robert Anderson, Bjorn Stenger, Vincent Wan, and Roberto Cipolla. 2013b. Expressive visual text-to-speech using active appearance models. In Proceedings of the IEEE Conference on Computer Vision and Patern Recognition. 3382–3389. Fabrice Bellard, M Niedermayer, and others. 2012. FFmpeg. Availabel from: htp://ffmpeg. org (2012). Floraine Berthouzoz, Wilmot Li, and Maneesh Agrawala. 2012. Tools for placing cuts and transitions in interview video. ACM Trans. Graph. 31, 4 (2012), 67–1.\nG. Bradski. 2000. Dr. Dobb’s Journal of Sofware Tools (2000).\nMathew Brand. 1999. Voice Puppetry. In Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH ’99). ACM Press/Addison- 311535.311537 Christoph Bregler, Michele Covell, and Malcolm Slaney. 1997. Video rewrite: Driving visual speech with audio. In Proceedings of the 24th annual conference on Computer graphics and interactive techniques. ACM Press/Addison-Wesley Publishing Co., 353–360. Peter J Burt and Edward H Adelson. 1983. A multiresolution spline with application to image mosaics. ACM Transactions on Graphics (TOG) 2, 4 (1983), 217–236. Chen Cao, Hongzhi Wu, Yanlin Weng, Tianjia Shao, and Kun Zhou. 2016. Real-time facial animation with image-based dynamic avatars. ACM Transactions on Graphics (TOG) 35, 4 (2016), 126. Yong Cao, Wen C Tien, Petros Faloutsos, and Fr´ed´eric Pighin. 2005. Expressive speechdriven facial animation. ACM Transactions on Graphics (TOG) 24, 4 (2005), 1283– 1302. Timothy F Cootes, Gareth J Edwards, Christopher J Taylor, and others. 2001. Active appearance models. IEEE Transactions on patern analysis and machine intelligence 23, 6 (2001), 681–685. Kevin Dale, Kalyan Sunkavalli, Micah K Johnson, Daniel Vlasic, Wojciech Matusik, and Hanspeter Pﬁster. 2011. Video face replacement. ACM Transactions on Graphics (TOG) 30, 6 (2011), 130. Tony Ezzat, Gadi Geiger, and Tomaso Poggio. 2002. Trainable videorealistic speech animation. Vol. 21. ACM. Bo Fan, Lijuan Wang, Frank K Soong, and Lei Xie. 2015a. Photo-real talking head with deep bidirectional LSTM. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 4884–4888. Bo Fan, Lei Xie, Shan Yang, Lijuan Wang, and Frank K Soong. 2015b. A deep bidirectional LSTM approach for video-realistic talking head. Multimedia Tools and Applications (2015), 1–23. Shengli Fu, Ricardo Gutierrez-Osuna, Anna Esposito, Praveen K Kakumanu, and Oscar N Garcia. 2005. Audio/visual mapping with cross-modal hidden Markov models. IEEE Transactions on Multimedia 7, 2 (2005), 243–252. Yarin Gal. 2015. A theoretically grounded application of dropout in recurrent neural networks. arXiv preprint arXiv:1512.05287 (2015). Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Torsten Tormahlen, Patrick Perez, and Christian Teobalt. 2014. Automatic face reenactment. In Proceedings of the IEEE Conference on Computer Vision and Patern Recognition. 4217–4224. Pablo Garrido, Levi Valgaerts, Hamid Sarmadi, Ingmar Steiner, Kiran Varanasi, Patrick Perez, and Christian Teobalt. 2015. Vdub: Modifying face video of actors for plausible visual alignment to a dubbed audio track. In Computer Graphics Forum, Vol. 34. Wiley Online Library, 193–204. Alex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850 (2013). Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. 2013. Hybrid speech recognition with deep bidirectional LSTM. In Automatic Speech Recognition and Understanding (ASRU), 2013 IEEE Workshop on. IEEE, 273–278. Alex Graves and J¨urgen Schmidhuber. 2005. Framewise phoneme classiﬁcation with bidirectional LSTM and other neural network architectures. Neural Networks 18, 5 (2005), 602–610. Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural computation 9, 8 (1997), 1735–1780. Masahide Kawai, Tomoyori Iwao, Daisuke Mima, Akinobu Maejima, and Shigeo Morishima. 2014. Data-driven speech animation synthesis focusing on realistic inside of the mouth. Journal of information processing 22, 2 (2014), 401–409. Ira Kemelmacher-Shlizerman and Steven M Seitz. 2012. Collection ﬂow. In Computer Vision and Patern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 1792–1799. Taehwan Kim, Yisong Yue, Sarah Taylor, and Iain Mathews. 2015. A decision tree framework for spatiotemporal sequence prediction. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 577–586. Davis E. King. 2009. Dlib-ml: A Machine Learning Toolkit. Journal of Machine Learning Research 10 (2009), 1755–1758. Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980 (2014). Kai Li, Feng Xu, Jue Wang, Qionghai Dai, and Yebin Liu. 2012. A data-driven approach for facial expression synthesis in video. In Computer Vision and Patern Recognition (CVPR), 2012 IEEE Conference on. IEEE, 57–64. Shu Liang, Linda G Shapiro, and Ira Kemelmacher-Shlizerman. 2016. Head reconstruction from internet photos. In European Conference on Computer Vision. Springer, 360–374. Ce Liu, William T Freeman, Edward H Adelson, and Yair Weiss. 2008. Human-assisted motion annotation. In Computer Vision and Patern Recognition, 2008. CVPR 2008. IEEE Conference on. IEEE, 1–8. Wesley Matheyses, Lukas Latacz, and Werner Verhelst. 2013. Comprehensive manyto-many phoneme-to-viseme mapping and its application for concatenative visual speech synthesis. Speech Communication 55, 7 (2013), 857–876. Wesley Matheyses and Werner Verhelst. 2015. Audiovisual speech synthesis: An overview of the state-of-the-art. Speech Communication 66 (2015), 182–217. Aude Oliva, Antonio Torralba, and Philippe G. Schyns. 2006. Hybrid Images. ACM Trans. Wener Robitza. 2016. ffmpeg-normalize. htps://github.com/slhck/ffmpeg-normalize. (2016). Shunsuke Saito, Tianye Li, and Hao Li. 2016. Real-Time Facial Segmentation and Performance Capture from RGB Input. arXiv preprint arXiv:1604.02647 (2016). Shinji Sako, Keiichi Tokuda, Takashi Masuko, Takao Kobayashi, and Tadashi Kitamura.\n2000. HMM-based text-to-audio-visual speech synthesis.. In INTERSPEECH. 25–28.\nYiChang Shih, Sylvain Paris, Connelly Barnes, William T Freeman, and Fr´edo Durand.\n2014. Style transfer for headshot portraits. (2014).\nTaiki Shimba, Ryuhei Sakurai, Hirotake Yamazoe, and Joo-Ho Lee. 2015. Talking heads synthesis from audio with deep neural networks. In 2015 IEEE/SICE International Symposium on System Integration (SII). IEEE, 100–105. Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, and Steven M Seitz. 2014. Total moving face reconstruction. In European Conference on Computer Vision. Springer, 796–812. Supasorn Suwajanakorn, Steven M Seitz, and Ira Kemelmacher-Shlizerman. 2015. What Makes Tom Hanks Look Like Tom Hanks. In Proceedings of the IEEE International Conference on Computer Vision. 3952–3960. Sarah Taylor, Akihiro Kato, Ben Milner, and Iain Mathews. 2016. Audio-to-Visual Speech Conversion using Deep Neural Networks. (2016). Sarah L Taylor, Moshe Mahler, Barry-John Teobald, and Iain Mathews. 2012. Dynamic units of visual speech. In Proceedings of the 11th ACM SIGGRAPH/Eurographics conference on Computer Animation. Eurographics Association, 275–284. Alexandru Telea. 2004. An image inpainting technique based on the fast marching method. Journal of graphics tools 9, 1 (2004), 23–34. Justus Ties, Michael Zollh¨ofer, Mathias Nießner, Levi Valgaerts, Marc Stamminger, and Christian Teobalt. 2015. Real-time expression transfer for facial reenactment. ACM Transactions on Graphics (TOG) 34, 6 (2015), 183. Justus Ties, Michael Zollh¨ofer, Marc Stamminger, Christian Teobalt, and Mathias Nießner. 2016. Face2face: Real-time face capture and reenactment of rgb videos. Proc. Computer Vision and Patern Recognition (CVPR), IEEE 1 (2016). A¨aron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. 2016. Wavenet: A generative model for raw audio. arXiv preprint arXiv:1609.03499 (2016). Daniel Vlasic, Mathew Brand, Hanspeter Pﬁster, and Jovan Popovi´c. 2005. Face transfer with multilinear models. In ACM Transactions on Graphics (TOG), Vol. 24. ACM, 426–433. Lijuan Wang, Wei Han, and Frank K Soong. 2012. High quality lip-sync animation for Speech and Signal Processing (ICASSP). IEEE, 4529–4532. Lijuan Wang, Xiaojun Qian, Wei Han, and Frank K Soong. 2010. Synthesizing photoreal talking head via trajectory-guided sample selection.. In INTERSPEECH, Vol. 10. 446–449. Lei Xie and Zhi-Qiang Liu. 2007a. A coupled HMM approach to video-realistic speech animation. Patern Recognition 40, 8 (2007), 2325–2340. Lei Xie and Zhi-Qiang Liu. 2007b. Realistic mouth-synching for speech-driven talking face using articulatory modelling. IEEE Transactions on Multimedia 9, 3 (2007), 500–510. Xuehan Xiong and Fernando De la Torre. 2013. Supervised descent method and its applications to face alignment. In Proceedings of the IEEE conference on computer vision and patern recognition. 532–539. Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. 2014. Recurrent neural network regularization. arXiv preprint arXiv:1409.2329 (2014). Xinjian Zhang, Lijuan Wang, Gang Li, Frank Seide, and Frank K Soong. 2013. A new language independent, photo-realistic talking head driven by voice only.. In INTERSPEECH. 2743–2747.\n--- FOOTNOTES © 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM. 100Hz by linearly interpolating PCA coeﬃcients, to match the audio 2I (u,v)H (u,v) 3D geometry errors: During the ﬁnal composite, our mouth texture is composited over a target frame that may have a different 3D photo-realistic talking head. In 2012 IEEE International Conference on Acoustics,", "persona": "Student", "response": "### 1. Overview\nLearning Lip Sync from Audio Janaki Sathish 9 min read · Apr 18, 2020 -- Listen Share This is based on the url ‘Synthesizing Obama: Learning Lip Sync from Audio’ published by Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman, University of Washington for Siggraph 2017. Given an audio of President Barack Obama, the aim of this url is to seamlessly transfer this audio along with accurate lip sync into a target video clip. Hours of his weekly address footage are used by a Recurrent Neural Network (RNN) to map raw audio features to mouth shapes. A high quality mouth texture is synthesized for the mouth shape in each frame. Then, in order to change what he appears to be saying and match our audio track, 3D pose matching is performed and combined with the mouth texture generated for each frame. Mouth shape features: In each video frame, Obama’s face is detected and frontalized, which is basically the modification of an image of a face to obtain the frontal view. For each frontal face, mouth landmarks are detected. Each 18-point mouth shape is reshaped into a 36-D vector, and Principal Component Analysis (PCA) is applied to reduce dimensionality of the resulting feature set. For training, the mouth shape is up-sampled to match the audio sampling rate. Let us consider an example of Obama beginning to say the word ‘united’. He begins to make the sound ‘Youu’ which acts as a cue to the mouth synthesizer that he should start opening his mouth to make a ‘U’ shape.\n\n### 2. Background\nSome practical applications include enabling lip-reading from over-the-phone audio for hearing-impaired people and use in film special effects and games. ‘c’ denotes output pixel intensity, and wᵢ represents how similar Sᵢ is to the target mouth shape. · There should be more movement when speaking and less during silence. the input audio. The focus should be laid upon synthesizing the parts of the face that are most correlated to speech. In Obama’s video footage, the most important parts are the region around the mouth, i.e. lips, cheeks and chin, and aspects of head motion. Candidate frame selection: Similar to what was done to detect mouth shape features, a landmark detector is run over the target video, the 3D pose estimated, and every frame frontalized (to show a frontal view of the face image) using a 3D model of Obama (Fig. 2). To improve frontalization results, the 3D model is further augmented with approximations of chin and background shapes. The 3D pose is estimated for each frame by placing the model onto each frame and bringing a rear projection effect. Finally, the frames that have the smallest L₂ distance between the frame’s mouth shape and target’s mouth shape are picked out as the candidate frames.\n\n### 3. Methods\nStep 1: Input audio features to RNN that outputs a sparse mouth shape In order to generate a sparse mouth shape for every frame, sparse shape coefficients are mapped from the audio features, which are then used to map to the mouth texture. It is to be noted that it is not enough to condition the mouth shape on the previous audio input. The network should look into the future as well. Thus, a short future context is introduced to the unidirectional LSTM network by adding a time delay to the output. This is done by shifting the output of the network forward, like a ‘target delay’. Fig. 1: The above network is the single-layer unidirectional LSTM we are using. [x1, …, xn] and [y1, …, yn] is the input and output vector sequence. ⁽¹⁾ Weight matrices W and bias vectors b are the learned parameters. The network is minimized using L₂-loss and trained using Adam optimizer. Step 2: Facial Texture Synthesis The sparse mouth shapes obtained in step 1 are used here to synthesize detailed face textures. The key focus is on bringing a sharp and realistic appearance for every video frame, and temporally smoothing the texture changes across every frame.\n\n### 4. Results\nImplementation Weighted median texture synthesis: Let [l₁, …, lₙ] be the set of candidate frames with the associated mouth shapes [S₁,…,Sₙ] . The weighted median per pixel(u, v) is computed by the formula: A small value of σ will cause temporal flickering by creating a peak distribution on a few of the images. On the other hand, a large value of σ will produce a blurry result. Therefore, it is important to choose an optimal value for σ. · A target video frame may be repeated for the sake of seamless syncing, but it can be done so at most once, but never skipped. The video frame may also be slowed down to at most 50% but not sped up as the latter can get noticeable. · It is better to map N to M where slowing down is least noticeable, eg: when he burls or changes his expression quickly. Finally, using the 3D shape estimated in step 2, the final texture is rendered back to the target frame. Compiling videos from about 300 weekly addresses, 17 hours of video was given as input for training. The training-validation split was 80–20. The training network consisted of 60 LSTM nodes, where a 20 step time-delay was used. The network was trained for 300 epochs with a learning rate of 0.001.\n\n### 5. Takeaways\nAudio shape features: The audio features extracted from the input audio are represented by MFCC coefficients. To obtain this, the volume of the audio is normalized using RMS-based normalization. Then, the Discrete Fourier Transform is taken on every 25-ms length sliding window over the audio, to whose output about 40 triangular Mel-scale filters and logarithm is applied. Finally, a Discrete Cosine Transform is applied to reduce the dimensionality to a 13-D vector. Working of RNN: We now have both the MFCC audio coefficients generated from the audio sample, and the PCA mouth shape coefficients generated from the target video. Our aim is to use the RNN to map the MFCC coefficients to the mouth shape coefficients. Generating Teeth Proxies: Another challenge is to synthesize realistic, sharp and pose aligned teeth on the mouth area in the target video. This is done by generating teeth proxy frame from the target video, to transfer the teeth details into the teeth region of the target video. One option is to use the median texture generated in the previous step to generate a mask for the teeth region, but this turns out to be blurry (Fig. 3 a). Another option is to use teeth proxies which captures sharp details (Fig. 3 d).", "paper_id": "02", "story_id": "02-02"}
{"paper": "ABSTRACT\nThis paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efﬁciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiﬁcantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal ﬁdelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ﬁnd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition. 1\n\nINTRODUCTION\nThis work explores raw audio generation techniques, inspired by recent advances in neural autoregressive generative models that model complex distributions such as images (van den Oord et al., 2016a;b) and text (J´ozefowicz et al., 2016). Modeling joint probabilities over pixels or words using neural architectures as products of conditional distributions yields state-of-the-art generation. Remarkably, these architectures are able to model distributions over thousands of random variables (e.g. 64×64 pixels as in PixelRNN (van den Oord et al., 2016a)). The question this paper addresses is whether similar approaches can succeed in generating wideband raw audio waveforms, which are signals with very high temporal resolution, at least 16,000 samples per second (see Fig. 1). Figure 1: A second of generated speech. • In order to deal with long-range temporal dependencies needed for raw audio generation, we develop new architectures based on dilated causal convolutions, which exhibit very large receptive ﬁelds. • We show that when conditioned on a speaker identity, a single model can be used to generate different voices. • The same architecture shows strong results when tested on a small speech recognition dataset, and is promising when used to generate other audio modalities such as music. We believe that WaveNets provide a generic and ﬂexible framework for tackling many applications that rely on audio generation (e.g. TTS, music, speech enhancement, voice conversion, source separation).\n\n2 WAVENET In this paper we introduce a new generative model operating directly on the raw audio waveform. The joint probability of a waveform x = {x1, . . . , xT } is factorised as a product of conditional probabilities as follows: T Y t=1 p (xt | x1, . . . , xt−1) (1) p (x) = Each audio sample xt is therefore conditioned on the samples at all previous timesteps. Similarly to PixelCNNs (van den Oord et al., 2016a;b), the conditional probability distribution is modelled by a stack of convolutional layers. There are no pooling layers in the network, and the output of the model has the same time dimensionality as the input. The model outputs a categorical distribution over the next value xt with a softmax layer and it is optimized to maximize the loglikelihood of the data w.r.t. the parameters. Because log-likelihoods are tractable, we tune hyperparameters on a validation set and can easily measure if the model is overﬁtting or underﬁtting. 2.1 DILATED CAUSAL CONVOLUTIONS Figure 2: Visualization of a stack of causal convolutional layers. The main ingredient of WaveNet are causal convolutions. By using causal convolutions, we make sure the model cannot violate the ordering in which we model the data: the prediction p (xt+1 | x1, ..., xt) emitted by the model at timestep t cannot depend on any of the future timesteps xt+1, xt+2, . . . , xT as shown in Fig. 2. For images, the equivalent of a causal convolution is a masked convolution (van den Oord et al., 2016a) which can be implemented by constructing a mask tensor and doing an elementwise multiplication of this mask with the convolution kernel before applying it. For 1-D data such as audio one can more easily implement this by shifting the output of a normal convolution by a few timesteps. Output Hidden Layer Hidden Layer Hidden Layer Input Because models with causal convolutions do not have recurrent connections, they are typically faster to train than RNNs, especially when applied to very long sequences. One of the problems of causal convolutions is that they require many layers, or large ﬁlters to increase the receptive ﬁeld. For example, in Fig. 2 the receptive ﬁeld is only 5 (= #layers + ﬁlter length - 1). In this paper we use dilated convolutions to increase the receptive ﬁeld by orders of magnitude, without greatly increasing computational cost. A dilated convolution (also called `a trous, or convolution with holes) is a convolution where the ﬁlter is applied over an area larger than its length by skipping input values with a certain step. It is equivalent to a convolution with a larger ﬁlter derived from the original ﬁlter by dilating it with zeros, but is signiﬁcantly more efﬁcient. A dilated convolution effectively allows the network to operate on a coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but here the output has the same size as the input. As a special case, dilated convolution with dilation 1 yields the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1, 2, 4, and 8. Dilated convolutions have previously been used in various contexts, e.g. signal processing (Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu & Koltun, 2016). Figure 3: Visualization of a stack of dilated causal convolutional layers. Stacked dilated convolutions enable networks to have very large receptive ﬁelds with just a few layers, while preserving the input resolution throughout the network as well as computational efﬁciency. In this paper, the dilation is doubled for every layer up to a limit and then repeated: e.g. 1, 2, 4, . . . , 512, 1, 2, 4, . . . , 512, 1, 2, 4, . . . , 512. The intuition behind this conﬁguration is two-fold. First, exponentially increasing the dilation factor results in exponential receptive ﬁeld growth with depth (Yu & Koltun, 2016). For example each 1, 2, 4, . . . , 512 block has receptive ﬁeld of size 1024, and can be seen as a more efﬁcient and discriminative (non-linear) counterpart of a 1×1024 convolution. Second, stacking these blocks further increases the model capacity and the receptive ﬁeld size. 2.2 SOFTMAX DISTRIBUTIONS One approach to modeling the conditional distributions p (xt | x1, . . . , xt−1) over the individual audio samples would be to use a mixture model such as a mixture density network (Bishop, 1994) or mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However, van den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the data is implicitly continuous (as is the case for image pixel intensities or audio sample values). One of the reasons is that a categorical distribution is more ﬂexible and can more easily model arbitrary distributions because it makes no assumptions about their shape. Because raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a softmax layer would need to output 65,536 probabilities per timestep to model all possible values. To make this more tractable, we ﬁrst apply a µ-law companding transformation (ITU-T, 1988) to the data, and then quantize it to 256 possible values: Output Dilation = 8 Hidden Layer Dilation = 4 Hidden Layer Dilation = 2 Hidden Layer Dilation = 1 Input where −1 < xt < 1 and µ = 255. This non-linear quantization produces a signiﬁcantly better reconstruction than a simple linear quantization scheme. Especially for speech, we found that the reconstructed signal after quantization sounded very similar to the original. 2.3 GATED ACTIVATION UNITS We use the same gated activation unit as used in the gated PixelCNN (van den Oord et al., 2016b): z = tanh (Wf,k ∗x) ⊙σ (Wg,k ∗x) , (2) where ∗denotes a convolution operator, ⊙denotes an element-wise multiplication operator, σ(·) is a sigmoid function, k is the layer index, f and g denote ﬁlter and gate, respectively, and W is a learnable convolution ﬁlter. In our initial experiments, we observed that this non-linearity worked signiﬁcantly better than the rectiﬁed linear activation function (Nair & Hinton, 2010) for modeling audio signals. 2.4 RESIDUAL AND SKIP CONNECTIONS Residual + 1 ⇥1 ReLU ReLU 1 ⇥1 1 ⇥1 + Softmax ⇥ Skip-connections σ tanh Dilated Conv k Layers Causal Conv Input Figure 4: Overview of the residual block and the entire architecture. Both residual (He et al., 2015) and parameterised skip connections are used throughout the network, to speed up convergence and enable training of much deeper models. In Fig. 4 we show a residual block of our model, which is stacked many times in the network. 2.5 CONDITIONAL WAVENETS Given an additional input h, WaveNets can model the conditional distribution p (x | h) of the audio given this input. Eq. (1) now becomes T Y p (x | h) = t=1 p (xt | x1, . . . , xt−1, h) . (3) By conditioning the model on other input variables, we can guide WaveNet’s generation to produce audio with the required characteristics. For example, in a multi-speaker setting we can choose the speaker by feeding the speaker identity to the model as an extra input. Similarly, for TTS we need to feed information about the text as an extra input. We condition the model on other inputs in two different ways: global conditioning and local conditioning. Global conditioning is characterised by a single latent representation h that inﬂuences the output distribution across all timesteps, e.g. a speaker embedding in a TTS model. The activation function from Eq. (2) now becomes: Output where V∗,k is a learnable linear projection, and the vector V T ∗,kh is broadcast over the time dimension. For local conditioning we have a second timeseries ht, possibly with a lower sampling frequency than the audio signal, e.g. linguistic features in a TTS model. We ﬁrst transform this time series using a transposed convolutional network (learned upsampling) that maps it to a new time series y = f(h) with the same resolution as the audio signal, which is then used in the activation unit as follows: z = tanh (Wf,k ∗x + Vf,k ∗y) ⊙σ (Wg,k ∗x + Vg,k ∗y) , where Vf,k ∗y is now a 1×1 convolution. As an alternative to the transposed convolutional network, it is also possible to use Vf,k∗h and repeat these values across time. We saw that this worked slightly worse in our experiments. 2.6 CONTEXT STACKS We have already mentioned several different ways to increase the receptive ﬁeld size of a WaveNet: increasing the number of dilation stages, using more layers, larger ﬁlters, greater dilation factors, or a combination thereof. A complementary approach is to use a separate, smaller context stack that processes a long part of the audio signal and locally conditions a larger WaveNet that processes only a smaller part of the audio signal (cropped at the end). One can use multiple context stacks with varying lengths and numbers of hidden units. Stacks with larger receptive ﬁelds have fewer units per layer. Context stacks can also have pooling layers to run at a lower frequency. This keeps the computational requirements at a reasonable level and is consistent with the intuition that less capacity is required to model temporal correlations at longer timescales. 3\n\nEXPERIMENTS\nTo measure WaveNet’s audio modelling performance, we evaluate it on three different tasks: multispeaker speech generation (not conditioned on text), TTS, and music audio modelling. We provide samples drawn from WaveNet for these experiments on the accompanying webpage: https://www.deepmind.com/blog/wavenet-generative-model-raw-audio/. 3.1 MULTI-SPEAKER SPEECH GENERATION For the ﬁrst experiment we looked at free-form speech generation (not conditioned on text). We used the English multi-speaker corpus from CSTR voice cloning toolkit (VCTK) (Yamagishi, 2012) and conditioned WaveNet only on the speaker. The conditioning was applied by feeding the speaker ID to the model in the form of a one-hot vector. The dataset consisted of 44 hours of data from 109 different speakers. Because the model is not conditioned on text, it generates non-existent but human language-like words in a smooth way with realistic sounding intonations. This is similar to generative models of language or images, where samples look realistic at ﬁrst glance, but are clearly unnatural upon closer inspection. The lack of long range coherence is partly due to the limited size of the model’s receptive ﬁeld (about 300 milliseconds), which means it can only remember the last 2–3 phonemes it produced. A single WaveNet was able to model speech from any of the speakers by conditioning it on a onehot encoding of a speaker. This conﬁrms that it is powerful enough to capture the characteristics of all 109 speakers from the dataset in a single model. We observed that adding speakers resulted in better validation set performance compared to training solely on a single speaker. This suggests that WaveNet’s internal representation was shared among multiple speakers. 3.2 TEXT-TO-SPEECH For the second experiment we looked at TTS. We used the same single-speaker speech databases from which Google’s North American English and Mandarin Chinese TTS systems are built. The North American English dataset contains 24.6 hours of speech data, and the Mandarin Chinese dataset contains 34.8 hours; both were spoken by professional female speakers. WaveNets for the TTS task were locally conditioned on linguistic features which were derived from input texts. We also trained WaveNets conditioned on the logarithmic fundamental frequency (log F0) values in addition to the linguistic features. External models predicting log F0 values and phone durations from linguistic features were also trained for each language. The receptive ﬁeld size of the WaveNets was 240 milliseconds. As example-based and model-based speech synthesis baselines, hidden Markov model (HMM)-driven unit selection concatenative (Gonzalvo et al., 2016) and long short-term memory recurrent neural network (LSTM-RNN)-based statistical parametric (Zen et al., 2016) speech synthesizers were built. Since the same datasets and linguistic features were used to train both the baselines and WaveNets, these speech synthesizers could be fairly compared. To evaluate the performance of WaveNets for the TTS task, subjective paired comparison tests and mean opinion score (MOS) tests were conducted. In the paired comparison tests, after listening to each pair of samples, the subjects were asked to choose which they preferred, though they could choose “neutral” if they did not have any preference. In the MOS tests, after listening to each stimulus, the subjects were asked to rate the naturalness of the stimulus in a ﬁve-point Likert scale score (1: Bad, 2: Poor, 3: Fair, 4: Good, 5: Excellent). Please refer to Appendix B for details. Fig. 5 shows a selection of the subjective paired comparison test results (see Appendix B for the complete table). It can be seen from the results that WaveNet outperformed the baseline statistical parametric and concatenative speech synthesizers in both languages. We found that WaveNet conditioned on linguistic features could synthesize speech samples with natural segmental quality but sometimes it had unnatural prosody by stressing wrong words in a sentence. This could be due to the long-term dependency of F0 contours: the size of the receptive ﬁeld of the WaveNet, 240 milliseconds, was not long enough to capture such long-term dependency. WaveNet conditioned on both linguistic features and F0 values did not have this problem: the external F0 prediction model runs at a lower frequency (200 Hz) so it can learn long-range dependencies that exist in F0 contours. Table 1 show the MOS test results. It can be seen from the table that WaveNets achieved 5-scale MOSs in naturalness above 4.0, which were signiﬁcantly better than those from the baseline systems. They were the highest ever reported MOS values with these training datasets and test sentences. The gap in the MOSs from the best synthetic speech to the natural ones decreased from 0.69 to 0.34 (51%) in US English and 0.42 to 0.13 (69%) in Mandarin Chinese. Speech samples North American English Mandarin Chinese LSTM-RNN parametric 3.67 ± 0.098 3.79 ± 0.084 HMM-driven concatenative 3.86 ± 0.137 3.47 ± 0.108 WaveNet (L+F) 4.21 ± 0.081 4.08 ± 0.085 Natural (8-bit µ-law) 4.46 ± 0.067 4.25 ± 0.082 Natural (16-bit linear PCM) 4.55 ± 0.075 4.21 ± 0.071 Table 1: Subjective 5-scale mean opinion scores of speech samples from LSTM-RNN-based statistical parametric, HMM-driven unit selection concatenative, and proposed WaveNet-based speech synthesizers, 8-bit µ-law encoded natural speech, and 16-bit linear pulse-code modulation (PCM) natural speech. WaveNet improved the previous state of the art signiﬁcantly, reducing the gap between natural speech and best previous model by more than 50%. Subjective 5-scale MOS in naturalness 100 No pref. Concat LSTM Preference scores (%) 80 60 63.6 40 20 23.3 13.1 0 Mandarin Chinese North American English 100 No pref. WaveNet (L+F) WaveNet (L) Preference scores (%) 80 60 40 44.3 37.9 20 17.8 0 Mandarin Chinese North American English 100 No pref. WaveNet (L+F) Best baseline Preference scores (%) 80 60 49.3 40 30.6 20 20.1 0 Mandarin Chinese North American English Figure 5: Subjective preference scores (%) of speech samples between (top) two baselines, (middle) two WaveNets, and (bottom) the best baseline and WaveNet. Note that LSTM and Concat correspond to LSTM-RNN-based statistical parametric and HMM-driven unit selection concatenative baseline synthesizers, and WaveNet (L) and WaveNet (L+F) correspond to the WaveNet conditioned on linguistic features only and that conditioned on both linguistic features and log F0 values. 50.6 33.8 15.6 64.5 25.5 10.0 58.2 29.3 12.5 • the MagnaTagATune dataset (Law & Von Ahn, 2009), which consists of about 200 hours of music audio. Each 29-second clip is annotated with tags from a set of 188, which describe the genre, instrumentation, tempo, volume and mood of the music. • the YouTube piano dataset, which consists of about 60 hours of solo piano music obtained from YouTube videos. Because it is constrained to a single instrument, it is considerably easier to model. Although it is difﬁcult to quantitatively evaluate these models, a subjective evaluation is possible by listening to the samples they produce. We found that enlarging the receptive ﬁeld was crucial to obtain samples that sounded musical. Even with a receptive ﬁeld of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, volume and sound quality. Nevertheless, the samples were often harmonic and aesthetically pleasing, even when produced by unconditional models. Of particular interest are conditional music models, which can generate music given a set of tags specifying e.g. genre or instruments. Similarly to conditional speech models, we insert biases that depend on a binary vector representation of the tags associated with each training clip. This makes it possible to control various aspects of the output of the model when sampling, by feeding in a binary vector that encodes the desired properties of the samples. We have trained such models on the MagnaTagATune dataset; although the tag data bundled with the dataset was relatively noisy and had many omissions, after cleaning it up by merging similar tags and removing those with too few associated clips, we found this approach to work reasonably well. 3.4 SPEECH RECOGNITION Although WaveNet was designed as a generative model, it can straightforwardly be adapted to discriminative audio tasks such as speech recognition. Traditionally, speech recognition research has largely focused on using log mel-ﬁlterbank energies or mel-frequency cepstral coefﬁcients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T¨uske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015). Recurrent neural networks such as LSTM-RNNs (Hochreiter & Schmidhuber, 1997) have been a key component in these new speech classiﬁcation pipelines, because they allow for building models with long range contexts. With WaveNets we have shown that layers of dilated convolutions allow the receptive ﬁeld to grow longer in a much cheaper way than using LSTM units. As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160× downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT. 4\n\nCONCLUSION\nThis paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal ﬁlters with dilated convolutions to allow their receptive ﬁelds to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition.\n\nThe authors would like to thank Lasse Espeholt, Jeffrey De Fauw and Grzegorz Swirszcz for their inputs, Adam Cain, Max Cant and Adrian Bolton for their help with artwork, Helen King, Steven Gaffney and Steve Crossan for helping to manage the project, Faith Mackinder for help with preparing the blogpost, James Besley for legal support and Demis Hassabis for managing the project and his inputs.\n\nREFERENCES\nAgiomyrgiannakis, Yannis. Vocaine the vocoder and applications is speech synthesis. In ICASSP, pp. 4230–4234, 2015. Bishop, Christopher M. Mixture density networks. Technical Report NCRG/94/004, Neural Computing Research Group, Aston University, 1994. Chen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Semantic image segmentation with deep convolutional nets and fully connected CRFs. In ICLR,\n2015. URL http://arxiv.org/abs/1412.7062.\nChiba, Tsutomu and Kajiyama, Masato. The Vowel: Its Nature and Structure. Tokyo-Kaiseikan, 1942. Dudley, Homer. Remaking speech. The Journal of the Acoustical Society of America, 11(2):169– 177, 1939. Dutilleux, Pierre. An implementation of the “algorithme `a trous” to compute the wavelet transform. In Combes, Jean-Michel, Grossmann, Alexander, and Tchamitchian, Philippe (eds.), Wavelets: Time-Frequency Methods and Phase Space, pp. 298–304. Springer Berlin Heidelberg, 1989. Fan, Yuchen, Qian, Yao, and Xie, Feng-Long, Soong Frank K. TTS synthesis with bidirectional LSTM based recurrent neural networks. In Interspeech, pp. 1964–1968, 2014. Fant, Gunnar. Acoustic Theory of Speech Production. Mouton De Gruyter, 1970. Garofolo, John S., Lamel, Lori F., Fisher, William M., Fiscus, Jonathon G., and Pallett, David S. DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM. NIST speech disc 1-1.1. NASA STI/Recon technical report, 93, 1993. Gonzalvo, Xavi, Tazari, Siamak, Chan, Chun-an, Becker, Markus, Gutkin, Alexander, and Silen, Hanna. Recent advances in Google real-time HMM-driven unit selection synthesizer. In Interspeech, 2016. URL http://research.google.com/pubs/pub45564.html. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735–1780, 1997. Holschneider, Matthias, Kronland-Martinet, Richard, Morlet, Jean, and Tchamitchian, Philippe. A real-time algorithm for signal analysis with the help of the wavelet transform. In Combes, JeanMichel, Grossmann, Alexander, and Tchamitchian, Philippe (eds.), Wavelets: Time-Frequency Methods and Phase Space, pp. 286–297. Springer Berlin Heidelberg, 1989. Hoshen, Yedid, Weiss, Ron J., and Wilson, Kevin W. Speech acoustic modeling from raw multichannel waveforms. In ICASSP, pp. 4624–4628. IEEE, 2015. Hunt, Andrew J. and Black, Alan W. Unit selection in a concatenative speech synthesis system using a large speech database. In ICASSP, pp. 373–376, 1996. Imai, Satoshi and Furuichi, Chieko. Unbiased estimation of log spectrum. In EURASIP, pp. 203– 206, 1988. ITU-T. Recommendation G. 711. Pulse Code Modulation (PCM) of voice frequencies, 1988. J´ozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer, Noam, and Wu, Yonghui. Exploring the limits of language modeling. CoRR, abs/1602.02410, 2016. URL http://arxiv.org/abs/ 1602.02410. Juang, Biing-Hwang and Rabiner, Lawrence. Mixture autoregressive hidden Markov models for speech signals. IEEE Trans. Acoust. Speech Signal Process., pp. 1404–1413, 1985. Kameoka, Hirokazu, Ohishi, Yasunori, Mochihashi, Daichi, and Le Roux, Jonathan. Speech analysis with multi-kernel linear prediction. In Spring Conference of ASJ, pp. 499–502, 2010. (in Japanese). Karaali, Orhan, Corrigan, Gerald, Gerson, Ira, and Massey, Noel. Text-to-speech conversion with neural networks: A recurrent TDNN approach. In Eurospeech, pp. 561–564, 1997. Kawahara, Hideki, Masuda-Katsuse, Ikuyo, and de Cheveign´e, Alain. Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequencybased f0 extraction: possible role of a repetitive structure in sounds. Speech Commn., 27:187– 207, 1999. Kawahara, Hideki, Estill, Jo, and Fujimura, Osamu. Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modiﬁcation and synthesis system STRAIGHT. In MAVEBA, pp. 13–15, 2001. Law, Edith and Von Ahn, Luis. Input-agreement: a new mechanism for collecting data using human computation games. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 1197–1206. ACM, 2009. Maia, Ranniery, Zen, Heiga, and Gales, Mark J. F. Statistical parametric speech synthesis with joint estimation of acoustic and excitation model parameters. In ISCA SSW7, pp. 88–93, 2010. Morise, Masanori, Yokomori, Fumiya, and Ozawa, Kenji. WORLD: A vocoder-based high-quality speech synthesis system for real-time applications. IEICE Trans. Inf. Syst., E99-D(7):1877–1884, 2016. Moulines, Eric and Charpentier, Francis. Pitch synchronous waveform processing techniques for text-to-speech synthesis using diphones. Speech Commn., 9:453–467, 1990. Muthukumar, P. and Black, Alan W. A deep learning approach to data-driven parameterizations for statistical parametric speech synthesis. arXiv:1409.8558, 2014. Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted Boltzmann machines. In ICML, pp. 807–814, 2010. Nakamura, Kazuhiro, Hashimoto, Kei, Nankaku, Yoshihiko, and Tokuda, Keiichi. Integration of spectral feature extraction and modeling for HMM-based speech synthesis. IEICE Trans. Inf. Syst., E97-D(6):1438–1448, 2014. Palaz, Dimitri, Collobert, Ronan, and Magimai-Doss, Mathew. Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks. In Interspeech, pp. 1766–1770, 2013. Peltonen, Sari, Gabbouj, Moncef, and Astola, Jaakko. Nonlinear ﬁlter design: methodologies and challenges. In IEEE ISPA, pp. 102–107, 2001. Poritz, Alan B. Linear predictive hidden Markov models and the speech signal. In ICASSP, pp. 1291–1294, 1982. Sainath, Tara N., Weiss, Ron J., Senior, Andrew, Wilson, Kevin W., and Vinyals, Oriol. Learning the speech front-end with raw waveform CLDNNs. In Interspeech, pp. 1–5, 2015. Takaki, Shinji and Yamagishi, Junichi. A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis. In ICASSP, pp. 5535–5539, 2016. Takamichi, Shinnosuke, Toda, Tomoki, Black, Alan W., Neubig, Graham, Sakriani, Sakti, and Nakamura, Satoshi. Postﬁlters to modify the modulation spectrum for statistical parametric speech synthesis. IEEE/ACM Trans. Audio Speech Lang. Process., 24(4):755–767, 2016. Theis, Lucas and Bethge, Matthias. Generative image modeling using spatial LSTMs. In NIPS, pp. 1927–1935, 2015. Toda, Tomoki and Tokuda, Keiichi. A speech parameter generation algorithm considering global variance for HMM-based speech synthesis. IEICE Trans. Inf. Syst., E90-D(5):816–824, 2007. Toda, Tomoki and Tokuda, Keiichi. Statistical approach to vocal tract transfer function estimation based on factor analyzed trajectory hmm. In ICASSP, pp. 3925–3928, 2008. Tokuda, Keiichi. Speech synthesis as a statistical machine learning problem. http://www.sp. nitech.ac.jp/˜tokuda/tokuda_asru2011_for_pdf.pdf, 2011. Invited talk given at ASRU. Tokuda, Keiichi and Zen, Heiga. Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis. In ICASSP, pp. 4215–4219, 2015. Tokuda, Keiichi and Zen, Heiga. Directly modeling voiced and unvoiced components in speech waveforms by neural networks. In ICASSP, pp. 5640–5644, 2016. Tuerk, Christine and Robinson, Tony. Speech synthesis using artiﬁcial neural networks trained on cepstral coefﬁcients. In Proc. Eurospeech, pp. 1713–1716, 1993. T¨uske, Zolt´an, Golik, Pavel, Schl¨uter, Ralf, and Ney, Hermann. Acoustic modeling with deep neural networks using raw time signal for LVCSR. In Interspeech, pp. 890–894, 2014. Uria, Benigno, Murray, Iain, Renals, Steve, Valentini-Botinhao, Cassia, and Bridle, John. Modelling acoustic feature dependencies with artiﬁcial neural networks: Trajectory-RNADE. In ICASSP, pp. 4465–4469, 2015. van den Oord, A¨aron, Kalchbrenner, Nal, and Kavukcuoglu, Koray. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016a. van den Oord, A¨aron, Kalchbrenner, Nal, Vinyals, Oriol, Espeholt, Lasse, Graves, Alex, and Kavukcuoglu, Koray. Conditional image generation with PixelCNN decoders. CoRR, abs/1606.05328, 2016b. URL http://arxiv.org/abs/1606.05328. Wu, Yi-Jian and Tokuda, Keiichi. Minimum generation error training with direct log spectral distortion on LSPs for HMM-based speech synthesis. In Interspeech, pp. 577–580, 2008. Yamagishi, Junichi. English multi-speaker corpus for CSTR voice cloning toolkit, 2012. URL http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html. Technology, 2002. o Vocoder analysis Model training Speech Speech Acoustic model Text analysis Text Text l Λˆ l Training Synthesis Figure 6: Outline of statistical parametric speech synthesis. Zen, Heiga, Tokuda, Keiichi, and Kitamura, Tadashi. Reformulating the HMM as a trajectory model by imposing explicit relationships between static and dynamic features. Comput. Speech Lang., 21(1):153–173, 2007. Zen, Heiga, Tokuda, Keiichi, and Black, Alan W. Statistical parametric speech synthesis. Speech Commn., 51(11):1039–1064, 2009. Zen, Heiga, Senior, Andrew, and Schuster, Mike. Statistical parametric speech synthesis using deep neural networks. In Proc. ICASSP, pp. 7962–7966, 2013. Zen, Heiga, Agiomyrgiannakis, Yannis, Egberts, Niels, Henderson, Fergus, and Szczepaniak, Przemysław. Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices. In Interspeech, 2016. URL https://arxiv.org/abs/1606. 06061.\n\nA TEXT-TO-SPEECH BACKGROUND The goal of TTS synthesis is to render naturally sounding speech signals given a text to be synthesized. Human speech production process ﬁrst translates a text (or concept) into movements of muscles associated with articulators and speech production-related organs. Then using air-ﬂow from lung, vocal source excitation signals, which contain both periodic (by vocal cord vibration) and aperiodic (by turbulent noise) components, are generated. By ﬁltering the vocal source excitation signals by time-varying vocal tract transfer functions controlled by the articulators, their frequency characteristics are modulated. Finally, the generated speech signals are emitted. The aim of TTS is to mimic this process by computers in some way. TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. This part typically includes prosody prediction and speech waveform generation. There are two main approaches to realize the speech synthesis part; non-parametric, example-based approach known as concatenative speech synthesis (Moulines & Charpentier, 1990; Sagisaka et al., 1992; Hunt & Black, 1996), and parametric, model-based approach known as statistical parametric speech synthesis (Yoshimura, 2002; Zen et al., 2009). The concatenative approach builds up the utterance from units of recorded speech, whereas the statistical parametric approach uses a generative model to synthesize the speech. The statistical parametric approach ﬁrst extracts a sequence of vocoder parameters (Dudley, 1939) o = {o1, . . . , oN} from speech signals x = {x1, . . . , xT } and linguistic features l from the text W, where N and T correspond to the numbers of vocoder parameter vectors and speech signals. Typically a vocoder parameter vector on is extracted at every 5 milliseconds. It often includes cepstra (Imai & Furuichi, 1988) or line spectral pairs (Itakura, 1975), which represent vocal tract transfer function, and fundamental frequency (F0) and aperiodicity (Kawahara et al., 2001), which represent characteristics of vocal source excitation signals. Then a set of generative models, such as hidden Markov models (HMMs) (Yoshimura, 2002), feed-forward neural networks (Zen et al., 2013), and recurrent neural networks (Tuerk & Robinson, 1993; Karaali et al., 1997; Fan et al., 2014), is trained from the extracted vocoder parameters and linguistic features ˆo Feature prediction Vocoder synthesis Text analysis as ˆΛ = arg max Λ p (o | l, Λ) , (4) where Λ denotes the set of parameters of the generative model. At the synthesis stage, the most probable vocoder parameters are generated given linguistic features extracted from a text to be synthesized as ˆo = arg max o p(o | l, ˆΛ). (5) Then a speech waveform is reconstructed from ˆo using a vocoder. The statistical parametric approach offers various advantages over the concatenative one such as small footprint and ﬂexibility to change its voice characteristics. However, its subjective naturalness is often signiﬁcantly worse than that of the concatenative approach; synthesized speech often sounds mufﬂed and has artifacts. Zen et al. (2009) reported three major factors that can degrade the subjective naturalness; quality of vocoders, accuracy of generative models, and effect of oversmoothing. The ﬁrst factor causes the artifacts and the second and third factors lead to the mufﬂeness in the synthesized speech. There have been a number of attempts to address these issues individually, such as developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016). Zen et al. (2016) showed that state-of-the-art statistical parametric speech syntheziers matched state-of-the-art concatenative ones in some languages. However, its vocoded sound quality is still a major issue. Extracting vocoder parameters can be viewed as estimation of a generative model parameters given speech signals (Itakura & Saito, 1970; Imai & Furuichi, 1988). For example, linear predictive analysis (Itakura & Saito, 1970), which has been used in speech coding, assumes that the generative model of speech signals is a linear auto-regressive (AR) zero-mean Gaussian process; where ap is a p-th order linear predictive coefﬁcient (LPC) and G2 is a variance of modeling error. These parameters are estimated based on the maximum likelihood (ML) criterion. In this sense, the training part of the statistical parametric approach can be viewed as a two-step optimization and sub-optimal: extract vocoder parameters by ﬁtting a generative model of speech signals then model trajectories of the extracted vocoder parameters by a separate generative model for time series (Tokuda, 2011). There have been attempts to integrate these two steps into a single one (Toda & Tokuda, 2008; Wu & Tokuda, 2008; Maia et al., 2010; Nakamura et al., 2014; Muthukumar & Black, 2014; Tokuda & Zen, 2015; 2016; Takaki & Yamagishi, 2016). For example, Tokuda & Zen (2016) integrated non-stationary, nonzero-mean Gaussian process generative model of speech signals and LSTM-RNN-based sequence generative model to a single one and jointly optimized them by back-propagation. Although they showed that this model could approximate natural speech signals, its segmental naturalness was signiﬁcantly worse than the non-integrated model due to overgeneralization and over-estimation of noise components in speech signals. The conventional generative models of raw audio signals have a number of assumptions which are inspired from the speech production, such as • Use of ﬁxed-length analysis window; They are typically based on a stationary stochastic process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010). To model time-varying speech signals by a stationary stochastic process, parameters of these generative models are estimated within a ﬁxed-length, overlapping and shifting analysis window (typically its length is 20 to 30 milliseconds, and shift is 5 to 10 milliseconds). However, some phones such as stops are time-limited by less than 20 milliseconds (Rabiner & Juang, 1993). Therefore, using such ﬁxed-size analysis window has limitations. • Linear ﬁlter; These generative models are typically realized as a linear time-invariant ﬁlter (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010) within a windowed frame. However, the relationship between successive audio samples can be highly non-linear. P X xt = p=1 apxt−p + ϵt (6) ϵt ∼N(0, G2) (7) • Gaussian process assumption; The conventional generative models are based on Gaussian process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016). From the source-ﬁlter model of speech production (Chiba & Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Tokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016). Together with the linear assumption above, it results in assuming that speech signals are normally distributed. However, distributions of real speech signals can be signiﬁcantly different from Gaussian. Although these assumptions are convenient, samples from these generative models tend to be noisy and lose important details to make these audio signals sounding natural. WaveNet, which was described in Section 2, has none of the above-mentioned assumptions. It incorporates almost no prior knowledge about audio signals, except the choice of the receptive ﬁeld and µ-law encoding of the signal. It can also be viewed as a non-linear causal ﬁlter for quantized signals. Although such non-linear ﬁlter can represent complicated signals while preserving the details, designing such ﬁlters is usually difﬁcult (Peltonen et al., 2001). WaveNets give a way to train them from data. B DETAILS OF TTS EXPERIMENT The HMM-driven unit selection and WaveNet TTS systems were built from speech at 16 kHz sampling. Although LSTM-RNNs were trained from speech at 22.05 kHz sampling, speech at 16 kHz sampling was synthesized at runtime using a resampling functionality in the Vocaine vocoder (Agiomyrgiannakis, 2015). Both the LSTM-RNN-based statistical parametric and HMM-driven unit selection speech synthesizers were built from the speech datasets in the 16-bit linear PCM, whereas the WaveNet-based ones were trained from the same speech datasets in the 8-bit µ-law encoding. The linguistic features include phone, syllable, word, phrase, and utterance-level features (Zen, 2006) (e.g. phone identities, syllable stress, the number of syllables in a word, and position of the current syllable in a phrase) with additional frame position and phone duration features (Zen et al., 2013). These features were derived and associated with speech every 5 milliseconds by phone-level forced alignment at the training stage. We used LSTM-RNN-based phone duration and autoregressive CNN-based log F0 prediction models. They were trained so as to minimize the mean squared errors (MSE). It is important to note that no post-processing was applied to the audio signals generated from the WaveNets. The subjective listening tests were blind and crowdsourced. 100 sentences not included in the training data were used for evaluation. Each subject could evaluate up to 8 and 63 stimuli for North American English and Mandarin Chinese, respectively. Test stimuli were randomly chosen and presented for each subject. In the paired comparison test, each pair of speech samples was the same text synthesized by the different models. In the MOS test, each stimulus was presented to subjects in isolation. Each pair was evaluated by eight subjects in the paired comparison test, and each stimulus was evaluated by eight subjects in the MOS test. The subjects were paid and native speakers performing the task. Those ratings (about 40%) where headphones were not used were excluded when computing the preference and mean opinion scores. Table 2 shows the full details of the paired comparison test shown in Fig. 5. Subjective preference (%) in naturalness WaveNet WaveNet No Language LSTM Concat (L) (L+F) preference p value North 23.3 63.6 13.1 ≪10−9 American 18.7 69.3 12.0 ≪10−9 English 7.6 82.0 10.4 ≪10−9 32.4 41.2 26.4 0.003 20.1 49.3 30.6 ≪10−9 Mandarin 50.6 15.6 33.8 ≪10−9 Chinese 25.0 23.3 51.8 0.476 12.5 29.3 58.2 ≪10−9 17.6 43.1 39.3 ≪10−9 7.6 55.9 36.5 ≪10−9 Table 2: Subjective preference scores of speech samples between LSTM-RNN-based statistical parametric (LSTM), HMM-driven unit selection concatenative (Concat), and proposed WaveNet-based speech synthesizers. Each row of the table denotes scores of a paired comparison test between two synthesizers. Scores of the synthesizers which were signiﬁcantly better than their competing ones at p < 0.01 level were shown in the bold type. Note that WaveNet (L) and WaveNet (L+F) correspond to WaveNet conditioned on linguistic features only and that conditioned on both linguistic features and F0 values. 17.8 37.9 44.3 ≪10−9 10.0 25.5 64.5 ≪10−9\n--- FOOTNOTES Yoshimura, Takayoshi. Simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for HMM-based text-to-speech systems. PhD thesis, Nagoya Institute of", "persona": "Student", "response": "### 1. Overview\nWaveNet is proposed, which can be efficiently trained on data with tens of thousands of samples per second of audio. The corresponding audio can be listened from official website: https://deepmind.google/discover/blog/wavenet-a-generative-model-for-raw-audio/ 2 music datasets are used: The MagnaTagATune dataset, which consists of about 200 hours of music audio. Each 29-second clip is annotated with tags from a set of 188, and the YouTube piano dataset, which consists of about 60 hours of solo piano music obtained from YouTube videos. Of particular interest are conditional music models, after cleaning it up by merging similar tags and removing those with too few associated clips, we found this approach to work reasonably well. Dilated Convolution (Image from Google DeepMind) The above figure depicts dilated causal convolutions for dilations 1, 2, 4, and 8.\n\n### 2. Background\nThe joint probability of a waveform x = {x1, …, xT} is factorised as a product of conditional probabilities as follows: Normally, the conditional probability distribution is modelled by a stack of convolutional layers: 2.1. Dilated Convolution In this paper, the dilation is doubled for every layer up to a limit and then repeated. 2.2. Softmax Distribution Because raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a softmax layer would need to output 65,536 probabilities per timestep to model all possible values. To make this more tractable, a μ-law companding transformation (ITU-T, 1988) is first applied to the data, and it is quantized to 256 possible values: Gated Activation Units are used, which is better than ReLU: 2.4.\n\n### 3. Methods\nResidual and Skip Connections Overview of the residual block and the entire architecture. Both residual (ResNet) and parameterised skip connections are used throughout the network, to speed up convergence and enable training of much deeper models. 2.5. Conditional WaveNets Given an additional input h, WaveNets can model the conditional distribution p(x|h) of the audio given this input. For this task, a mean-pooling layer is added after the dilated convolutions.\n\n### 4. Results\nGlobal conditioning is characterised by a single latent representation h that influences the output distribution across all timesteps, e.g. a speaker embedding in a TTS model. For local conditioning we have a second timeseries ht, possibly with a lower sampling frequency than the audio signal, e.g. linguistic features in a TTS model. 3.1. Multi-Speaker Speech Generation The English multi-speaker corpus from CSTR voice cloning toolkit (VCTK) is used. The dataset consisted of 44 hours of data from 109 different speakers. It generates non-existent but human language-like words in a smooth way with realistic sounding intonations. This is similar to generative models of language or images, where samples look realistic at first glance, but are clearly unnatural upon closer inspection. The lack of long range coherence is partly due to the limited size of the model’s receptive field (about 300 milliseconds), which means it can only remember the last 2–3 phonemes it produced. 3.2. Text-To-Speech (TTS) The single-speaker speech databases from Google’s North American English and Mandarin Chinese TTS systems are used.\n\n### 5. Takeaways\nPreliminaries WaveNet However, the receptive field is small even having numerous layers. 2. WaveNet Dilated convolutions, which uses alternative-spaced neuron at the previous layer for information propagation, have previously been used in DeepLab and DilatedNet. This non-linear quantization produces a significantly better reconstruction than a simple linear quantization scheme. Enter your email Subscribe By conditioning the model on other input variables, we can guide WaveNet’s generation to produce audio with the required characteristics. For example, in a multi-speaker setting we can choose the speaker by feeding the speaker identity to the model as an extra input. Similarly, for TTS we need to feed information about the text as an extra input. The receptive field size of the WaveNets was 240 milliseconds. WaveNets achieved 5-scale MOSs in naturalness above 4.0, which were significantly better than those from the baseline systems. 3.3. Music WaveNets are tested on the TIMIT Dataset. WaveNet is trained with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to the best score (at that moment) obtained from a model trained directly on raw audio on TIMIT.", "paper_id": "06", "story_id": "06-05"}
{"paper": "ABSTRACT\nThis paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efﬁciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiﬁcantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal ﬁdelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ﬁnd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition. 1\n\nINTRODUCTION\nThis work explores raw audio generation techniques, inspired by recent advances in neural autoregressive generative models that model complex distributions such as images (van den Oord et al., 2016a;b) and text (J´ozefowicz et al., 2016). Modeling joint probabilities over pixels or words using neural architectures as products of conditional distributions yields state-of-the-art generation. Remarkably, these architectures are able to model distributions over thousands of random variables (e.g. 64×64 pixels as in PixelRNN (van den Oord et al., 2016a)). The question this paper addresses is whether similar approaches can succeed in generating wideband raw audio waveforms, which are signals with very high temporal resolution, at least 16,000 samples per second (see Fig. 1). Figure 1: A second of generated speech. • In order to deal with long-range temporal dependencies needed for raw audio generation, we develop new architectures based on dilated causal convolutions, which exhibit very large receptive ﬁelds. • We show that when conditioned on a speaker identity, a single model can be used to generate different voices. • The same architecture shows strong results when tested on a small speech recognition dataset, and is promising when used to generate other audio modalities such as music. We believe that WaveNets provide a generic and ﬂexible framework for tackling many applications that rely on audio generation (e.g. TTS, music, speech enhancement, voice conversion, source separation).\n\n2 WAVENET In this paper we introduce a new generative model operating directly on the raw audio waveform. The joint probability of a waveform x = {x1, . . . , xT } is factorised as a product of conditional probabilities as follows: T Y t=1 p (xt | x1, . . . , xt−1) (1) p (x) = Each audio sample xt is therefore conditioned on the samples at all previous timesteps. Similarly to PixelCNNs (van den Oord et al., 2016a;b), the conditional probability distribution is modelled by a stack of convolutional layers. There are no pooling layers in the network, and the output of the model has the same time dimensionality as the input. The model outputs a categorical distribution over the next value xt with a softmax layer and it is optimized to maximize the loglikelihood of the data w.r.t. the parameters. Because log-likelihoods are tractable, we tune hyperparameters on a validation set and can easily measure if the model is overﬁtting or underﬁtting. 2.1 DILATED CAUSAL CONVOLUTIONS Figure 2: Visualization of a stack of causal convolutional layers. The main ingredient of WaveNet are causal convolutions. By using causal convolutions, we make sure the model cannot violate the ordering in which we model the data: the prediction p (xt+1 | x1, ..., xt) emitted by the model at timestep t cannot depend on any of the future timesteps xt+1, xt+2, . . . , xT as shown in Fig. 2. For images, the equivalent of a causal convolution is a masked convolution (van den Oord et al., 2016a) which can be implemented by constructing a mask tensor and doing an elementwise multiplication of this mask with the convolution kernel before applying it. For 1-D data such as audio one can more easily implement this by shifting the output of a normal convolution by a few timesteps. Output Hidden Layer Hidden Layer Hidden Layer Input Because models with causal convolutions do not have recurrent connections, they are typically faster to train than RNNs, especially when applied to very long sequences. One of the problems of causal convolutions is that they require many layers, or large ﬁlters to increase the receptive ﬁeld. For example, in Fig. 2 the receptive ﬁeld is only 5 (= #layers + ﬁlter length - 1). In this paper we use dilated convolutions to increase the receptive ﬁeld by orders of magnitude, without greatly increasing computational cost. A dilated convolution (also called `a trous, or convolution with holes) is a convolution where the ﬁlter is applied over an area larger than its length by skipping input values with a certain step. It is equivalent to a convolution with a larger ﬁlter derived from the original ﬁlter by dilating it with zeros, but is signiﬁcantly more efﬁcient. A dilated convolution effectively allows the network to operate on a coarser scale than with a normal convolution. This is similar to pooling or strided convolutions, but here the output has the same size as the input. As a special case, dilated convolution with dilation 1 yields the standard convolution. Fig. 3 depicts dilated causal convolutions for dilations 1, 2, 4, and 8. Dilated convolutions have previously been used in various contexts, e.g. signal processing (Holschneider et al., 1989; Dutilleux, 1989), and image segmentation (Chen et al., 2015; Yu & Koltun, 2016). Figure 3: Visualization of a stack of dilated causal convolutional layers. Stacked dilated convolutions enable networks to have very large receptive ﬁelds with just a few layers, while preserving the input resolution throughout the network as well as computational efﬁciency. In this paper, the dilation is doubled for every layer up to a limit and then repeated: e.g. 1, 2, 4, . . . , 512, 1, 2, 4, . . . , 512, 1, 2, 4, . . . , 512. The intuition behind this conﬁguration is two-fold. First, exponentially increasing the dilation factor results in exponential receptive ﬁeld growth with depth (Yu & Koltun, 2016). For example each 1, 2, 4, . . . , 512 block has receptive ﬁeld of size 1024, and can be seen as a more efﬁcient and discriminative (non-linear) counterpart of a 1×1024 convolution. Second, stacking these blocks further increases the model capacity and the receptive ﬁeld size. 2.2 SOFTMAX DISTRIBUTIONS One approach to modeling the conditional distributions p (xt | x1, . . . , xt−1) over the individual audio samples would be to use a mixture model such as a mixture density network (Bishop, 1994) or mixture of conditional Gaussian scale mixtures (MCGSM) (Theis & Bethge, 2015). However, van den Oord et al. (2016a) showed that a softmax distribution tends to work better, even when the data is implicitly continuous (as is the case for image pixel intensities or audio sample values). One of the reasons is that a categorical distribution is more ﬂexible and can more easily model arbitrary distributions because it makes no assumptions about their shape. Because raw audio is typically stored as a sequence of 16-bit integer values (one per timestep), a softmax layer would need to output 65,536 probabilities per timestep to model all possible values. To make this more tractable, we ﬁrst apply a µ-law companding transformation (ITU-T, 1988) to the data, and then quantize it to 256 possible values: Output Dilation = 8 Hidden Layer Dilation = 4 Hidden Layer Dilation = 2 Hidden Layer Dilation = 1 Input where −1 < xt < 1 and µ = 255. This non-linear quantization produces a signiﬁcantly better reconstruction than a simple linear quantization scheme. Especially for speech, we found that the reconstructed signal after quantization sounded very similar to the original. 2.3 GATED ACTIVATION UNITS We use the same gated activation unit as used in the gated PixelCNN (van den Oord et al., 2016b): z = tanh (Wf,k ∗x) ⊙σ (Wg,k ∗x) , (2) where ∗denotes a convolution operator, ⊙denotes an element-wise multiplication operator, σ(·) is a sigmoid function, k is the layer index, f and g denote ﬁlter and gate, respectively, and W is a learnable convolution ﬁlter. In our initial experiments, we observed that this non-linearity worked signiﬁcantly better than the rectiﬁed linear activation function (Nair & Hinton, 2010) for modeling audio signals. 2.4 RESIDUAL AND SKIP CONNECTIONS Residual + 1 ⇥1 ReLU ReLU 1 ⇥1 1 ⇥1 + Softmax ⇥ Skip-connections σ tanh Dilated Conv k Layers Causal Conv Input Figure 4: Overview of the residual block and the entire architecture. Both residual (He et al., 2015) and parameterised skip connections are used throughout the network, to speed up convergence and enable training of much deeper models. In Fig. 4 we show a residual block of our model, which is stacked many times in the network. 2.5 CONDITIONAL WAVENETS Given an additional input h, WaveNets can model the conditional distribution p (x | h) of the audio given this input. Eq. (1) now becomes T Y p (x | h) = t=1 p (xt | x1, . . . , xt−1, h) . (3) By conditioning the model on other input variables, we can guide WaveNet’s generation to produce audio with the required characteristics. For example, in a multi-speaker setting we can choose the speaker by feeding the speaker identity to the model as an extra input. Similarly, for TTS we need to feed information about the text as an extra input. We condition the model on other inputs in two different ways: global conditioning and local conditioning. Global conditioning is characterised by a single latent representation h that inﬂuences the output distribution across all timesteps, e.g. a speaker embedding in a TTS model. The activation function from Eq. (2) now becomes: Output where V∗,k is a learnable linear projection, and the vector V T ∗,kh is broadcast over the time dimension. For local conditioning we have a second timeseries ht, possibly with a lower sampling frequency than the audio signal, e.g. linguistic features in a TTS model. We ﬁrst transform this time series using a transposed convolutional network (learned upsampling) that maps it to a new time series y = f(h) with the same resolution as the audio signal, which is then used in the activation unit as follows: z = tanh (Wf,k ∗x + Vf,k ∗y) ⊙σ (Wg,k ∗x + Vg,k ∗y) , where Vf,k ∗y is now a 1×1 convolution. As an alternative to the transposed convolutional network, it is also possible to use Vf,k∗h and repeat these values across time. We saw that this worked slightly worse in our experiments. 2.6 CONTEXT STACKS We have already mentioned several different ways to increase the receptive ﬁeld size of a WaveNet: increasing the number of dilation stages, using more layers, larger ﬁlters, greater dilation factors, or a combination thereof. A complementary approach is to use a separate, smaller context stack that processes a long part of the audio signal and locally conditions a larger WaveNet that processes only a smaller part of the audio signal (cropped at the end). One can use multiple context stacks with varying lengths and numbers of hidden units. Stacks with larger receptive ﬁelds have fewer units per layer. Context stacks can also have pooling layers to run at a lower frequency. This keeps the computational requirements at a reasonable level and is consistent with the intuition that less capacity is required to model temporal correlations at longer timescales. 3\n\nEXPERIMENTS\nTo measure WaveNet’s audio modelling performance, we evaluate it on three different tasks: multispeaker speech generation (not conditioned on text), TTS, and music audio modelling. We provide samples drawn from WaveNet for these experiments on the accompanying webpage: https://www.deepmind.com/blog/wavenet-generative-model-raw-audio/. 3.1 MULTI-SPEAKER SPEECH GENERATION For the ﬁrst experiment we looked at free-form speech generation (not conditioned on text). We used the English multi-speaker corpus from CSTR voice cloning toolkit (VCTK) (Yamagishi, 2012) and conditioned WaveNet only on the speaker. The conditioning was applied by feeding the speaker ID to the model in the form of a one-hot vector. The dataset consisted of 44 hours of data from 109 different speakers. Because the model is not conditioned on text, it generates non-existent but human language-like words in a smooth way with realistic sounding intonations. This is similar to generative models of language or images, where samples look realistic at ﬁrst glance, but are clearly unnatural upon closer inspection. The lack of long range coherence is partly due to the limited size of the model’s receptive ﬁeld (about 300 milliseconds), which means it can only remember the last 2–3 phonemes it produced. A single WaveNet was able to model speech from any of the speakers by conditioning it on a onehot encoding of a speaker. This conﬁrms that it is powerful enough to capture the characteristics of all 109 speakers from the dataset in a single model. We observed that adding speakers resulted in better validation set performance compared to training solely on a single speaker. This suggests that WaveNet’s internal representation was shared among multiple speakers. 3.2 TEXT-TO-SPEECH For the second experiment we looked at TTS. We used the same single-speaker speech databases from which Google’s North American English and Mandarin Chinese TTS systems are built. The North American English dataset contains 24.6 hours of speech data, and the Mandarin Chinese dataset contains 34.8 hours; both were spoken by professional female speakers. WaveNets for the TTS task were locally conditioned on linguistic features which were derived from input texts. We also trained WaveNets conditioned on the logarithmic fundamental frequency (log F0) values in addition to the linguistic features. External models predicting log F0 values and phone durations from linguistic features were also trained for each language. The receptive ﬁeld size of the WaveNets was 240 milliseconds. As example-based and model-based speech synthesis baselines, hidden Markov model (HMM)-driven unit selection concatenative (Gonzalvo et al., 2016) and long short-term memory recurrent neural network (LSTM-RNN)-based statistical parametric (Zen et al., 2016) speech synthesizers were built. Since the same datasets and linguistic features were used to train both the baselines and WaveNets, these speech synthesizers could be fairly compared. To evaluate the performance of WaveNets for the TTS task, subjective paired comparison tests and mean opinion score (MOS) tests were conducted. In the paired comparison tests, after listening to each pair of samples, the subjects were asked to choose which they preferred, though they could choose “neutral” if they did not have any preference. In the MOS tests, after listening to each stimulus, the subjects were asked to rate the naturalness of the stimulus in a ﬁve-point Likert scale score (1: Bad, 2: Poor, 3: Fair, 4: Good, 5: Excellent). Please refer to Appendix B for details. Fig. 5 shows a selection of the subjective paired comparison test results (see Appendix B for the complete table). It can be seen from the results that WaveNet outperformed the baseline statistical parametric and concatenative speech synthesizers in both languages. We found that WaveNet conditioned on linguistic features could synthesize speech samples with natural segmental quality but sometimes it had unnatural prosody by stressing wrong words in a sentence. This could be due to the long-term dependency of F0 contours: the size of the receptive ﬁeld of the WaveNet, 240 milliseconds, was not long enough to capture such long-term dependency. WaveNet conditioned on both linguistic features and F0 values did not have this problem: the external F0 prediction model runs at a lower frequency (200 Hz) so it can learn long-range dependencies that exist in F0 contours. Table 1 show the MOS test results. It can be seen from the table that WaveNets achieved 5-scale MOSs in naturalness above 4.0, which were signiﬁcantly better than those from the baseline systems. They were the highest ever reported MOS values with these training datasets and test sentences. The gap in the MOSs from the best synthetic speech to the natural ones decreased from 0.69 to 0.34 (51%) in US English and 0.42 to 0.13 (69%) in Mandarin Chinese. Speech samples North American English Mandarin Chinese LSTM-RNN parametric 3.67 ± 0.098 3.79 ± 0.084 HMM-driven concatenative 3.86 ± 0.137 3.47 ± 0.108 WaveNet (L+F) 4.21 ± 0.081 4.08 ± 0.085 Natural (8-bit µ-law) 4.46 ± 0.067 4.25 ± 0.082 Natural (16-bit linear PCM) 4.55 ± 0.075 4.21 ± 0.071 Table 1: Subjective 5-scale mean opinion scores of speech samples from LSTM-RNN-based statistical parametric, HMM-driven unit selection concatenative, and proposed WaveNet-based speech synthesizers, 8-bit µ-law encoded natural speech, and 16-bit linear pulse-code modulation (PCM) natural speech. WaveNet improved the previous state of the art signiﬁcantly, reducing the gap between natural speech and best previous model by more than 50%. Subjective 5-scale MOS in naturalness 100 No pref. Concat LSTM Preference scores (%) 80 60 63.6 40 20 23.3 13.1 0 Mandarin Chinese North American English 100 No pref. WaveNet (L+F) WaveNet (L) Preference scores (%) 80 60 40 44.3 37.9 20 17.8 0 Mandarin Chinese North American English 100 No pref. WaveNet (L+F) Best baseline Preference scores (%) 80 60 49.3 40 30.6 20 20.1 0 Mandarin Chinese North American English Figure 5: Subjective preference scores (%) of speech samples between (top) two baselines, (middle) two WaveNets, and (bottom) the best baseline and WaveNet. Note that LSTM and Concat correspond to LSTM-RNN-based statistical parametric and HMM-driven unit selection concatenative baseline synthesizers, and WaveNet (L) and WaveNet (L+F) correspond to the WaveNet conditioned on linguistic features only and that conditioned on both linguistic features and log F0 values. 50.6 33.8 15.6 64.5 25.5 10.0 58.2 29.3 12.5 • the MagnaTagATune dataset (Law & Von Ahn, 2009), which consists of about 200 hours of music audio. Each 29-second clip is annotated with tags from a set of 188, which describe the genre, instrumentation, tempo, volume and mood of the music. • the YouTube piano dataset, which consists of about 60 hours of solo piano music obtained from YouTube videos. Because it is constrained to a single instrument, it is considerably easier to model. Although it is difﬁcult to quantitatively evaluate these models, a subjective evaluation is possible by listening to the samples they produce. We found that enlarging the receptive ﬁeld was crucial to obtain samples that sounded musical. Even with a receptive ﬁeld of several seconds, the models did not enforce long-range consistency which resulted in second-to-second variations in genre, instrumentation, volume and sound quality. Nevertheless, the samples were often harmonic and aesthetically pleasing, even when produced by unconditional models. Of particular interest are conditional music models, which can generate music given a set of tags specifying e.g. genre or instruments. Similarly to conditional speech models, we insert biases that depend on a binary vector representation of the tags associated with each training clip. This makes it possible to control various aspects of the output of the model when sampling, by feeding in a binary vector that encodes the desired properties of the samples. We have trained such models on the MagnaTagATune dataset; although the tag data bundled with the dataset was relatively noisy and had many omissions, after cleaning it up by merging similar tags and removing those with too few associated clips, we found this approach to work reasonably well. 3.4 SPEECH RECOGNITION Although WaveNet was designed as a generative model, it can straightforwardly be adapted to discriminative audio tasks such as speech recognition. Traditionally, speech recognition research has largely focused on using log mel-ﬁlterbank energies or mel-frequency cepstral coefﬁcients (MFCCs), but has been moving to raw audio recently (Palaz et al., 2013; T¨uske et al., 2014; Hoshen et al., 2015; Sainath et al., 2015). Recurrent neural networks such as LSTM-RNNs (Hochreiter & Schmidhuber, 1997) have been a key component in these new speech classiﬁcation pipelines, because they allow for building models with long range contexts. With WaveNets we have shown that layers of dilated convolutions allow the receptive ﬁeld to grow longer in a much cheaper way than using LSTM units. As a last experiment we looked at speech recognition with WaveNets on the TIMIT (Garofolo et al., 1993) dataset. For this task we added a mean-pooling layer after the dilated convolutions that aggregated the activations to coarser frames spanning 10 milliseconds (160× downsampling). The pooling layer was followed by a few non-causal convolutions. We trained WaveNet with two loss terms, one to predict the next sample and one to classify the frame, the model generalized better than with a single loss and achieved 18.8 PER on the test set, which is to our knowledge the best score obtained from a model trained directly on raw audio on TIMIT. 4\n\nCONCLUSION\nThis paper has presented WaveNet, a deep generative model of audio data that operates directly at the waveform level. WaveNets are autoregressive and combine causal ﬁlters with dilated convolutions to allow their receptive ﬁelds to grow exponentially with depth, which is important to model the long-range temporal dependencies in audio signals. We have shown how WaveNets can be conditioned on other inputs in a global (e.g. speaker identity) or local way (e.g. linguistic features). When applied to TTS, WaveNets produced samples that outperform the current best TTS systems in subjective naturalness. Finally, WaveNets showed very promising results when applied to music audio modeling and speech recognition.\n\nThe authors would like to thank Lasse Espeholt, Jeffrey De Fauw and Grzegorz Swirszcz for their inputs, Adam Cain, Max Cant and Adrian Bolton for their help with artwork, Helen King, Steven Gaffney and Steve Crossan for helping to manage the project, Faith Mackinder for help with preparing the blogpost, James Besley for legal support and Demis Hassabis for managing the project and his inputs.\n\nREFERENCES\nAgiomyrgiannakis, Yannis. Vocaine the vocoder and applications is speech synthesis. In ICASSP, pp. 4230–4234, 2015. Bishop, Christopher M. Mixture density networks. Technical Report NCRG/94/004, Neural Computing Research Group, Aston University, 1994. Chen, Liang-Chieh, Papandreou, George, Kokkinos, Iasonas, Murphy, Kevin, and Yuille, Alan L. Semantic image segmentation with deep convolutional nets and fully connected CRFs. In ICLR,\n2015. URL http://arxiv.org/abs/1412.7062.\nChiba, Tsutomu and Kajiyama, Masato. The Vowel: Its Nature and Structure. Tokyo-Kaiseikan, 1942. Dudley, Homer. Remaking speech. The Journal of the Acoustical Society of America, 11(2):169– 177, 1939. Dutilleux, Pierre. An implementation of the “algorithme `a trous” to compute the wavelet transform. In Combes, Jean-Michel, Grossmann, Alexander, and Tchamitchian, Philippe (eds.), Wavelets: Time-Frequency Methods and Phase Space, pp. 298–304. Springer Berlin Heidelberg, 1989. Fan, Yuchen, Qian, Yao, and Xie, Feng-Long, Soong Frank K. TTS synthesis with bidirectional LSTM based recurrent neural networks. In Interspeech, pp. 1964–1968, 2014. Fant, Gunnar. Acoustic Theory of Speech Production. Mouton De Gruyter, 1970. Garofolo, John S., Lamel, Lori F., Fisher, William M., Fiscus, Jonathon G., and Pallett, David S. DARPA TIMIT acoustic-phonetic continuous speech corpus CD-ROM. NIST speech disc 1-1.1. NASA STI/Recon technical report, 93, 1993. Gonzalvo, Xavi, Tazari, Siamak, Chan, Chun-an, Becker, Markus, Gutkin, Alexander, and Silen, Hanna. Recent advances in Google real-time HMM-driven unit selection synthesizer. In Interspeech, 2016. URL http://research.google.com/pubs/pub45564.html. He, Kaiming, Zhang, Xiangyu, Ren, Shaoqing, and Sun, Jian. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735–1780, 1997. Holschneider, Matthias, Kronland-Martinet, Richard, Morlet, Jean, and Tchamitchian, Philippe. A real-time algorithm for signal analysis with the help of the wavelet transform. In Combes, JeanMichel, Grossmann, Alexander, and Tchamitchian, Philippe (eds.), Wavelets: Time-Frequency Methods and Phase Space, pp. 286–297. Springer Berlin Heidelberg, 1989. Hoshen, Yedid, Weiss, Ron J., and Wilson, Kevin W. Speech acoustic modeling from raw multichannel waveforms. In ICASSP, pp. 4624–4628. IEEE, 2015. Hunt, Andrew J. and Black, Alan W. Unit selection in a concatenative speech synthesis system using a large speech database. In ICASSP, pp. 373–376, 1996. Imai, Satoshi and Furuichi, Chieko. Unbiased estimation of log spectrum. In EURASIP, pp. 203– 206, 1988. ITU-T. Recommendation G. 711. Pulse Code Modulation (PCM) of voice frequencies, 1988. J´ozefowicz, Rafal, Vinyals, Oriol, Schuster, Mike, Shazeer, Noam, and Wu, Yonghui. Exploring the limits of language modeling. CoRR, abs/1602.02410, 2016. URL http://arxiv.org/abs/ 1602.02410. Juang, Biing-Hwang and Rabiner, Lawrence. Mixture autoregressive hidden Markov models for speech signals. IEEE Trans. Acoust. Speech Signal Process., pp. 1404–1413, 1985. Kameoka, Hirokazu, Ohishi, Yasunori, Mochihashi, Daichi, and Le Roux, Jonathan. Speech analysis with multi-kernel linear prediction. In Spring Conference of ASJ, pp. 499–502, 2010. (in Japanese). Karaali, Orhan, Corrigan, Gerald, Gerson, Ira, and Massey, Noel. Text-to-speech conversion with neural networks: A recurrent TDNN approach. In Eurospeech, pp. 561–564, 1997. Kawahara, Hideki, Masuda-Katsuse, Ikuyo, and de Cheveign´e, Alain. Restructuring speech representations using a pitch-adaptive time-frequency smoothing and an instantaneous-frequencybased f0 extraction: possible role of a repetitive structure in sounds. Speech Commn., 27:187– 207, 1999. Kawahara, Hideki, Estill, Jo, and Fujimura, Osamu. Aperiodicity extraction and control using mixed mode excitation and group delay manipulation for a high quality speech analysis, modiﬁcation and synthesis system STRAIGHT. In MAVEBA, pp. 13–15, 2001. Law, Edith and Von Ahn, Luis. Input-agreement: a new mechanism for collecting data using human computation games. In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 1197–1206. ACM, 2009. Maia, Ranniery, Zen, Heiga, and Gales, Mark J. F. Statistical parametric speech synthesis with joint estimation of acoustic and excitation model parameters. In ISCA SSW7, pp. 88–93, 2010. Morise, Masanori, Yokomori, Fumiya, and Ozawa, Kenji. WORLD: A vocoder-based high-quality speech synthesis system for real-time applications. IEICE Trans. Inf. Syst., E99-D(7):1877–1884, 2016. Moulines, Eric and Charpentier, Francis. Pitch synchronous waveform processing techniques for text-to-speech synthesis using diphones. Speech Commn., 9:453–467, 1990. Muthukumar, P. and Black, Alan W. A deep learning approach to data-driven parameterizations for statistical parametric speech synthesis. arXiv:1409.8558, 2014. Nair, Vinod and Hinton, Geoffrey E. Rectiﬁed linear units improve restricted Boltzmann machines. In ICML, pp. 807–814, 2010. Nakamura, Kazuhiro, Hashimoto, Kei, Nankaku, Yoshihiko, and Tokuda, Keiichi. Integration of spectral feature extraction and modeling for HMM-based speech synthesis. IEICE Trans. Inf. Syst., E97-D(6):1438–1448, 2014. Palaz, Dimitri, Collobert, Ronan, and Magimai-Doss, Mathew. Estimating phoneme class conditional probabilities from raw speech signal using convolutional neural networks. In Interspeech, pp. 1766–1770, 2013. Peltonen, Sari, Gabbouj, Moncef, and Astola, Jaakko. Nonlinear ﬁlter design: methodologies and challenges. In IEEE ISPA, pp. 102–107, 2001. Poritz, Alan B. Linear predictive hidden Markov models and the speech signal. In ICASSP, pp. 1291–1294, 1982. Sainath, Tara N., Weiss, Ron J., Senior, Andrew, Wilson, Kevin W., and Vinyals, Oriol. Learning the speech front-end with raw waveform CLDNNs. In Interspeech, pp. 1–5, 2015. Takaki, Shinji and Yamagishi, Junichi. A deep auto-encoder based low-dimensional feature extraction from FFT spectral envelopes for statistical parametric speech synthesis. In ICASSP, pp. 5535–5539, 2016. Takamichi, Shinnosuke, Toda, Tomoki, Black, Alan W., Neubig, Graham, Sakriani, Sakti, and Nakamura, Satoshi. Postﬁlters to modify the modulation spectrum for statistical parametric speech synthesis. IEEE/ACM Trans. Audio Speech Lang. Process., 24(4):755–767, 2016. Theis, Lucas and Bethge, Matthias. Generative image modeling using spatial LSTMs. In NIPS, pp. 1927–1935, 2015. Toda, Tomoki and Tokuda, Keiichi. A speech parameter generation algorithm considering global variance for HMM-based speech synthesis. IEICE Trans. Inf. Syst., E90-D(5):816–824, 2007. Toda, Tomoki and Tokuda, Keiichi. Statistical approach to vocal tract transfer function estimation based on factor analyzed trajectory hmm. In ICASSP, pp. 3925–3928, 2008. Tokuda, Keiichi. Speech synthesis as a statistical machine learning problem. http://www.sp. nitech.ac.jp/˜tokuda/tokuda_asru2011_for_pdf.pdf, 2011. Invited talk given at ASRU. Tokuda, Keiichi and Zen, Heiga. Directly modeling speech waveforms by neural networks for statistical parametric speech synthesis. In ICASSP, pp. 4215–4219, 2015. Tokuda, Keiichi and Zen, Heiga. Directly modeling voiced and unvoiced components in speech waveforms by neural networks. In ICASSP, pp. 5640–5644, 2016. Tuerk, Christine and Robinson, Tony. Speech synthesis using artiﬁcial neural networks trained on cepstral coefﬁcients. In Proc. Eurospeech, pp. 1713–1716, 1993. T¨uske, Zolt´an, Golik, Pavel, Schl¨uter, Ralf, and Ney, Hermann. Acoustic modeling with deep neural networks using raw time signal for LVCSR. In Interspeech, pp. 890–894, 2014. Uria, Benigno, Murray, Iain, Renals, Steve, Valentini-Botinhao, Cassia, and Bridle, John. Modelling acoustic feature dependencies with artiﬁcial neural networks: Trajectory-RNADE. In ICASSP, pp. 4465–4469, 2015. van den Oord, A¨aron, Kalchbrenner, Nal, and Kavukcuoglu, Koray. Pixel recurrent neural networks. arXiv preprint arXiv:1601.06759, 2016a. van den Oord, A¨aron, Kalchbrenner, Nal, Vinyals, Oriol, Espeholt, Lasse, Graves, Alex, and Kavukcuoglu, Koray. Conditional image generation with PixelCNN decoders. CoRR, abs/1606.05328, 2016b. URL http://arxiv.org/abs/1606.05328. Wu, Yi-Jian and Tokuda, Keiichi. Minimum generation error training with direct log spectral distortion on LSPs for HMM-based speech synthesis. In Interspeech, pp. 577–580, 2008. Yamagishi, Junichi. English multi-speaker corpus for CSTR voice cloning toolkit, 2012. URL http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html. Technology, 2002. o Vocoder analysis Model training Speech Speech Acoustic model Text analysis Text Text l Λˆ l Training Synthesis Figure 6: Outline of statistical parametric speech synthesis. Zen, Heiga, Tokuda, Keiichi, and Kitamura, Tadashi. Reformulating the HMM as a trajectory model by imposing explicit relationships between static and dynamic features. Comput. Speech Lang., 21(1):153–173, 2007. Zen, Heiga, Tokuda, Keiichi, and Black, Alan W. Statistical parametric speech synthesis. Speech Commn., 51(11):1039–1064, 2009. Zen, Heiga, Senior, Andrew, and Schuster, Mike. Statistical parametric speech synthesis using deep neural networks. In Proc. ICASSP, pp. 7962–7966, 2013. Zen, Heiga, Agiomyrgiannakis, Yannis, Egberts, Niels, Henderson, Fergus, and Szczepaniak, Przemysław. Fast, compact, and high quality LSTM-RNN based statistical parametric speech synthesizers for mobile devices. In Interspeech, 2016. URL https://arxiv.org/abs/1606. 06061.\n\nA TEXT-TO-SPEECH BACKGROUND The goal of TTS synthesis is to render naturally sounding speech signals given a text to be synthesized. Human speech production process ﬁrst translates a text (or concept) into movements of muscles associated with articulators and speech production-related organs. Then using air-ﬂow from lung, vocal source excitation signals, which contain both periodic (by vocal cord vibration) and aperiodic (by turbulent noise) components, are generated. By ﬁltering the vocal source excitation signals by time-varying vocal tract transfer functions controlled by the articulators, their frequency characteristics are modulated. Finally, the generated speech signals are emitted. The aim of TTS is to mimic this process by computers in some way. TTS can be viewed as a sequence-to-sequence mapping problem; from a sequence of discrete symbols (text) to a real-valued time series (speech signals). A typical TTS pipeline has two parts; 1) text analysis and 2) speech synthesis. The text analysis part typically includes a number of natural language processing (NLP) steps, such as sentence segmentation, word segmentation, text normalization, part-of-speech (POS) tagging, and grapheme-to-phoneme (G2P) conversion. It takes a word sequence as input and outputs a phoneme sequence with a variety of linguistic contexts. The speech synthesis part takes the context-dependent phoneme sequence as its input and outputs a synthesized speech waveform. This part typically includes prosody prediction and speech waveform generation. There are two main approaches to realize the speech synthesis part; non-parametric, example-based approach known as concatenative speech synthesis (Moulines & Charpentier, 1990; Sagisaka et al., 1992; Hunt & Black, 1996), and parametric, model-based approach known as statistical parametric speech synthesis (Yoshimura, 2002; Zen et al., 2009). The concatenative approach builds up the utterance from units of recorded speech, whereas the statistical parametric approach uses a generative model to synthesize the speech. The statistical parametric approach ﬁrst extracts a sequence of vocoder parameters (Dudley, 1939) o = {o1, . . . , oN} from speech signals x = {x1, . . . , xT } and linguistic features l from the text W, where N and T correspond to the numbers of vocoder parameter vectors and speech signals. Typically a vocoder parameter vector on is extracted at every 5 milliseconds. It often includes cepstra (Imai & Furuichi, 1988) or line spectral pairs (Itakura, 1975), which represent vocal tract transfer function, and fundamental frequency (F0) and aperiodicity (Kawahara et al., 2001), which represent characteristics of vocal source excitation signals. Then a set of generative models, such as hidden Markov models (HMMs) (Yoshimura, 2002), feed-forward neural networks (Zen et al., 2013), and recurrent neural networks (Tuerk & Robinson, 1993; Karaali et al., 1997; Fan et al., 2014), is trained from the extracted vocoder parameters and linguistic features ˆo Feature prediction Vocoder synthesis Text analysis as ˆΛ = arg max Λ p (o | l, Λ) , (4) where Λ denotes the set of parameters of the generative model. At the synthesis stage, the most probable vocoder parameters are generated given linguistic features extracted from a text to be synthesized as ˆo = arg max o p(o | l, ˆΛ). (5) Then a speech waveform is reconstructed from ˆo using a vocoder. The statistical parametric approach offers various advantages over the concatenative one such as small footprint and ﬂexibility to change its voice characteristics. However, its subjective naturalness is often signiﬁcantly worse than that of the concatenative approach; synthesized speech often sounds mufﬂed and has artifacts. Zen et al. (2009) reported three major factors that can degrade the subjective naturalness; quality of vocoders, accuracy of generative models, and effect of oversmoothing. The ﬁrst factor causes the artifacts and the second and third factors lead to the mufﬂeness in the synthesized speech. There have been a number of attempts to address these issues individually, such as developing high-quality vocoders (Kawahara et al., 1999; Agiomyrgiannakis, 2015; Morise et al., 2016), improving the accuracy of generative models (Zen et al., 2007; 2013; Fan et al., 2014; Uria et al., 2015), and compensating the oversmoothing effect (Toda & Tokuda, 2007; Takamichi et al., 2016). Zen et al. (2016) showed that state-of-the-art statistical parametric speech syntheziers matched state-of-the-art concatenative ones in some languages. However, its vocoded sound quality is still a major issue. Extracting vocoder parameters can be viewed as estimation of a generative model parameters given speech signals (Itakura & Saito, 1970; Imai & Furuichi, 1988). For example, linear predictive analysis (Itakura & Saito, 1970), which has been used in speech coding, assumes that the generative model of speech signals is a linear auto-regressive (AR) zero-mean Gaussian process; where ap is a p-th order linear predictive coefﬁcient (LPC) and G2 is a variance of modeling error. These parameters are estimated based on the maximum likelihood (ML) criterion. In this sense, the training part of the statistical parametric approach can be viewed as a two-step optimization and sub-optimal: extract vocoder parameters by ﬁtting a generative model of speech signals then model trajectories of the extracted vocoder parameters by a separate generative model for time series (Tokuda, 2011). There have been attempts to integrate these two steps into a single one (Toda & Tokuda, 2008; Wu & Tokuda, 2008; Maia et al., 2010; Nakamura et al., 2014; Muthukumar & Black, 2014; Tokuda & Zen, 2015; 2016; Takaki & Yamagishi, 2016). For example, Tokuda & Zen (2016) integrated non-stationary, nonzero-mean Gaussian process generative model of speech signals and LSTM-RNN-based sequence generative model to a single one and jointly optimized them by back-propagation. Although they showed that this model could approximate natural speech signals, its segmental naturalness was signiﬁcantly worse than the non-integrated model due to overgeneralization and over-estimation of noise components in speech signals. The conventional generative models of raw audio signals have a number of assumptions which are inspired from the speech production, such as • Use of ﬁxed-length analysis window; They are typically based on a stationary stochastic process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010). To model time-varying speech signals by a stationary stochastic process, parameters of these generative models are estimated within a ﬁxed-length, overlapping and shifting analysis window (typically its length is 20 to 30 milliseconds, and shift is 5 to 10 milliseconds). However, some phones such as stops are time-limited by less than 20 milliseconds (Rabiner & Juang, 1993). Therefore, using such ﬁxed-size analysis window has limitations. • Linear ﬁlter; These generative models are typically realized as a linear time-invariant ﬁlter (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010) within a windowed frame. However, the relationship between successive audio samples can be highly non-linear. P X xt = p=1 apxt−p + ϵt (6) ϵt ∼N(0, G2) (7) • Gaussian process assumption; The conventional generative models are based on Gaussian process (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Kameoka et al., 2010; Tokuda & Zen, 2015; 2016). From the source-ﬁlter model of speech production (Chiba & Kajiyama, 1942; Fant, 1970) point of view, this is equivalent to assuming that a vocal source excitation signal is a sample from a Gaussian distribution (Itakura & Saito, 1970; Imai & Furuichi, 1988; Poritz, 1982; Juang & Rabiner, 1985; Tokuda & Zen, 2015; Kameoka et al., 2010; Tokuda & Zen, 2016). Together with the linear assumption above, it results in assuming that speech signals are normally distributed. However, distributions of real speech signals can be signiﬁcantly different from Gaussian. Although these assumptions are convenient, samples from these generative models tend to be noisy and lose important details to make these audio signals sounding natural. WaveNet, which was described in Section 2, has none of the above-mentioned assumptions. It incorporates almost no prior knowledge about audio signals, except the choice of the receptive ﬁeld and µ-law encoding of the signal. It can also be viewed as a non-linear causal ﬁlter for quantized signals. Although such non-linear ﬁlter can represent complicated signals while preserving the details, designing such ﬁlters is usually difﬁcult (Peltonen et al., 2001). WaveNets give a way to train them from data. B DETAILS OF TTS EXPERIMENT The HMM-driven unit selection and WaveNet TTS systems were built from speech at 16 kHz sampling. Although LSTM-RNNs were trained from speech at 22.05 kHz sampling, speech at 16 kHz sampling was synthesized at runtime using a resampling functionality in the Vocaine vocoder (Agiomyrgiannakis, 2015). Both the LSTM-RNN-based statistical parametric and HMM-driven unit selection speech synthesizers were built from the speech datasets in the 16-bit linear PCM, whereas the WaveNet-based ones were trained from the same speech datasets in the 8-bit µ-law encoding. The linguistic features include phone, syllable, word, phrase, and utterance-level features (Zen, 2006) (e.g. phone identities, syllable stress, the number of syllables in a word, and position of the current syllable in a phrase) with additional frame position and phone duration features (Zen et al., 2013). These features were derived and associated with speech every 5 milliseconds by phone-level forced alignment at the training stage. We used LSTM-RNN-based phone duration and autoregressive CNN-based log F0 prediction models. They were trained so as to minimize the mean squared errors (MSE). It is important to note that no post-processing was applied to the audio signals generated from the WaveNets. The subjective listening tests were blind and crowdsourced. 100 sentences not included in the training data were used for evaluation. Each subject could evaluate up to 8 and 63 stimuli for North American English and Mandarin Chinese, respectively. Test stimuli were randomly chosen and presented for each subject. In the paired comparison test, each pair of speech samples was the same text synthesized by the different models. In the MOS test, each stimulus was presented to subjects in isolation. Each pair was evaluated by eight subjects in the paired comparison test, and each stimulus was evaluated by eight subjects in the MOS test. The subjects were paid and native speakers performing the task. Those ratings (about 40%) where headphones were not used were excluded when computing the preference and mean opinion scores. Table 2 shows the full details of the paired comparison test shown in Fig. 5. Subjective preference (%) in naturalness WaveNet WaveNet No Language LSTM Concat (L) (L+F) preference p value North 23.3 63.6 13.1 ≪10−9 American 18.7 69.3 12.0 ≪10−9 English 7.6 82.0 10.4 ≪10−9 32.4 41.2 26.4 0.003 20.1 49.3 30.6 ≪10−9 Mandarin 50.6 15.6 33.8 ≪10−9 Chinese 25.0 23.3 51.8 0.476 12.5 29.3 58.2 ≪10−9 17.6 43.1 39.3 ≪10−9 7.6 55.9 36.5 ≪10−9 Table 2: Subjective preference scores of speech samples between LSTM-RNN-based statistical parametric (LSTM), HMM-driven unit selection concatenative (Concat), and proposed WaveNet-based speech synthesizers. Each row of the table denotes scores of a paired comparison test between two synthesizers. Scores of the synthesizers which were signiﬁcantly better than their competing ones at p < 0.01 level were shown in the bold type. Note that WaveNet (L) and WaveNet (L+F) correspond to WaveNet conditioned on linguistic features only and that conditioned on both linguistic features and F0 values. 17.8 37.9 44.3 ≪10−9 10.0 25.5 64.5 ≪10−9\n--- FOOTNOTES Yoshimura, Takayoshi. Simultaneous modeling of phonetic and prosodic parameters, and characteristic conversion for HMM-based text-to-speech systems. PhD thesis, Nagoya Institute of", "persona": "Student", "response": "### 1. Overview\nThis post presents WaveNet, a deep generative model of raw audio waveforms. We show that WaveNets are able to generate speech which mimics any human voice and which sounds more natural than the best existing Text-to-Speech systems, reducing the gap with human performance by over 50%. We also demonstrate that the same network can be used to synthesize other audio signals such as music, and present some striking samples of automatically generated piano pieces. Talking Machines Allowing people to converse with machines is a long-standing dream of human-computer interaction. The ability of computers to understand natural speech has been revolutionised in the last few years by the application of deep neural networks (e.g., Google Voice Search). However, generating speech with computers — a process usually referred to as speech synthesis or text-to-speech (TTS) — is still largely based on so-called concatenative TTS, where a very large database of short speech fragments are recorded from a single speaker and then recombined to form complete utterances. This makes it difficult to modify the voice (for example switching to a different speaker, or altering the emphasis or emotion of their speech) without recording a whole new database.\n\n### 2. Background\nThis has led to a great demand for parametric TTS, where all the information required to generate the data is stored in the parameters of the model, and the contents and characteristics of the speech can be controlled via the inputs to the model. So far, however, parametric TTS has tended to sound less natural than concatenative. Existing parametric models typically generate audio signals by passing their outputs through signal processing algorithms known as vocoders. WaveNet changes this paradigm by directly modelling the raw waveform of the audio signal, one sample at a time. As well as yielding more natural-sounding speech, using raw waveforms means that WaveNet can model any kind of audio, including music.\n\n### 3. Methods\nHere are some samples from all three systems so you can listen and compare yourself: US English Parametric Knowing What to Say In order to use WaveNet to turn text into speech, we have to tell it what the text is. We do this by transforming the text into a sequence of linguistic and phonetic features (which contain information about the current phoneme, syllable, word, etc.) and by feeding it into WaveNet. This means the network’s predictions are conditioned not only on the previous audio samples, but also on the text we want it to say. If we train the network without the text sequence, it still generates speech, but now it has to make up what to say.\n\n### 4. Results\nAs you can hear from the samples below, this results in a kind of babbling, where real words are interspersed with made-up word-like sounds: Notice that non-speech sounds, such as breathing and mouth movements, are also sometimes generated by WaveNet; this reflects the greater flexibility of a raw-audio model. As you can hear from these samples, a single WaveNet is able to learn the characteristics of many different voices, male and female. To make sure it knew which voice to use for any given utterance, we conditioned the network on the identity of the speaker. Interestingly, we found that training on many speakers made it better at modelling a single speaker than training on that speaker alone, suggesting a form of transfer learning. By changing the speaker identity, we can use WaveNet to say the same thing in different voices:\n\n### 5. Takeaways\nResearch WaveNet: A generative model for raw audio Share The above animation shows how a WaveNet is structured. It is a fully convolutional neural network, where the convolutional layers have various dilation factors that allow its receptive field to grow exponentially with depth and cover thousands of timesteps. At training time, the input sequences are real waveforms recorded from human speakers. After training, we can sample the network to generate synthetic utterances. At each step during sampling a value is drawn from the probability distribution computed by the network. This value is then fed back into the input and a new prediction for the next step is made. Building up samples one step at a time like this is computationally expensive, but we have found it essential for generating complex, realistic-sounding audio. Improving the State of the Art We trained WaveNet using some of Google’s TTS datasets so we could evaluate its performance. The following figure shows the quality of WaveNets on a scale from 1 to 5, compared with Google’s current best TTS systems (parametric and concatenative), and with human speech using Mean Opinion Scores (MOS). MOS are a standard measure for subjective sound quality tests, and were obtained in blind tests with human subjects (from over 500 ratings on 100 test sentences). As we can see, WaveNets reduce the gap between the state of the art and human-level performance by over 50% for both US English and Mandarin Chinese. For both Chinese and English, Google’s current TTS systems are considered among the best worldwide, so improving on both with a single model is a major achievement.", "paper_id": "06", "story_id": "06-01"}
{"paper": "Translatotron 2: High-quality direct speech-to-speech translation with voice preservation Ye Jia 1 Michelle Tadmor Ramanovich 1 Tal Remez 1 Roi Pomerantz 1\n\nAbstract\narXiv:2107.08661v5  [cs.CL]  17 May 2022 We present Translatotron 2, a neural direct speechto-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module that connects them together. Experimental results on three datasets consistently show that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade systems. In addition, we propose a simple method for preserving speakers’ voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker’s voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker’s privacy and mitigates potential misuse of voice cloning for creating spoofing audio artifacts.\n\n1. Introduction\nSpeech-to-speech translation (S2ST) is highly beneﬁcial for breaking down communication barriers between people not sharing a common language. Conventional automatic S2ST systems are composed of a cascade of three components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis (Lavie et al., 1997; Wahlster, 2000; Nakamura et al., 2006). In the past few years, direct speech-to-text translation (ST) is rapidly emerging, and has outperformed the cascade of ASR and MT (Weiss et al., 2017; Jia et al., 2019a; Di Gangi et al., 2019; McCarthy et al., 2020; Ansari et al., 2020; Wang et al., 2021b; Anastasopoulos et al., 2021), which makes the Ye Jia <jiaye@google.com>. cascade of ST and TTS as S2ST feasible (Jia et al., 2019b). Recently, works on S2ST without relying on intermediate text representation are emerging, such as end-to-end direct S2ST (Jia et al., 2019b; Kano et al., 2021) and cascade S2ST based on discrete speech representation (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022; 2021a; Ma et al., 2021). Compared to text-centric cascaded systems, although such approaches require parallel S2ST data, which is scarce, they have the potential advantages of: 1) Preserving paralinguistic and non-linguistic information during translation, such as speaker’s voice (Jia et al., 2019b), emotion and prosody; 2) Supporting languages without written form, or being able to be trained without transcription of speech (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022; 2021a); 3) Reduced computational requirements and lower inference latency (Lee et al., 2022); 4) Avoiding error compounding across sub-systems (Jia et al., 2022); 5) Easier on handling contents that do not need to be translated, such as names and proper nouns (Jia et al., 2019b). Among these works, Translatotron (Jia et al., 2019b) is the ﬁrst model that is able to directly translate speech in one language to speech in another language. It obtained reasonable translation quality and high naturalness in the predicted translation speech, and is able to preserve speakers’ voices during the speech translation. However, the translation quality from Translatotron still underperforms cascade baselines by a large margin, and the translation speech it produces suffers from over-generation issues, such as babbling and long pause. Such weaknesses make this model not yet practical for production. Nevertheless, it remains the state-of-the-art of end-to-end direct S2ST. In this paper, we ﬁrst tackle the performance gap between end-to-end direct S2ST and cascade S2ST. We propose Translatotron 2, a novel direct S2ST model that is able to be trained end-to-end. We conduct experiments on three S2ST datasets, including multilingual S2ST. The results consistently suggest that Translatotron 2 signiﬁcantly outperforms Translatotron in terms of both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade S2ST. When a simple data augmentation ConcatAug is used, the translation quality gap on the Fisher Spanish-English corpus (Post et al., 2013) is reduced from 16.4 to 0.4 BLEU. These results are the ﬁrst time that end-to-end direct S2ST approaches cascade S2ST. In addition, we propose a simple method for preserving speakers’ voices during S2ST without relying on any speaker representation (ID or embedding). The proposed method enables Translatotron 2 to preserve each speaker’s voice on speaker turns without requiring for speaker separation, which is the ﬁrst of its kind. Furthermore, compared to existing approaches of voice preservation, the proposed method better preserves speaker’s privacy (Pathak & Raj, 2012) and mitigates potential misuse of voice cloning for creating spooﬁng audio artifacts. Audio samples from Translatotron 2 are available online.1\n\n3. Translatotron 2\nWe designed the architecture of Translatotron 2 to address three performance bottlenecks existing in the original Translatotron: 1) The utilization of the auxiliary textual supervision during training is suboptimal, namely, the attention alignment learned by the auxiliary ST task does not directly contribute to the main S2ST task; 2) The challenge posed by modeling the translation alignment between two very (English) spectrogram phoneme (English) Synthesizer Decoder autoregression Attention Encoder spectrogram (Spanish) (a) Overview of Translatotron 2. Figure 1: A Translatotron 2 model that translates Spanish speech into English speech. long spectrogram sequences using the attention mechanism; 3) Attention-based speech generation is known to suffer from robustness issues such as over-generation and undergeneration (Shen et al., 2020; Ren et al., 2019; He et al., 2019; Zheng et al., 2019; Battenberg et al., 2020). We addressed these bottlenecks by designing a novel S2ST model architecture composed of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module connecting them together (Figure 1a). The model is jointly trained with a speech-to-speech translation objective and a speech-to-phoneme translation objective. The following subsections describe each component of Translatotron 2. Note that as shown in the ablation studies in Sec. 5.5.1, while the speciﬁc architectural choices of these components help the performance of Translatotron 2, the primary improvement comes from the high-level architecture rather than the choice of each individual component. 3.1. Speech encoder The encoder of Translatotron 2 takes the mel-spectrogram of the source speech as the input, and produces a hidden representation which encodes both linguistic and acoustic information from the source speech. We use Conformer (Gulati et al., 2020) as the architecture of the encoder. It ﬁrst subsamples the input mel-spectrogram with a convolutional layer, and then processes it with a stack of Conformer blocks. Each Conformer block is composed of a feed-forward layer, a self-attention layer, a convolution layer, and a second feedforward layer. SpecAugment (Park et al., 2019) is applied at the training time as data augmentation. spectrogram RNN + Conv autoregression durations Upsampler Duration Predictor concat(hidden, attention) (b) Synthesizer of Translatotron 2. 3.2. Linguistic decoder The autoregressive decoder is responsible for producing linguistic information in the translation speech. It takes the encoder output through the attention module, and predicts a phoneme sequence corresponding to the translation speech. We use an LSTM stack (Hochreiter & Schmidhuber, 1997) as the decoder, assisted with regularization including Zoneout (Krueger et al., 2017) and label smoothing (Szegedy et al., 2016). The combination of the encoder, the decoder, and the attention module is similar to a typical ST model, except that it predicts phonemes instead of subword tokens. 3.3. Acoustic synthesizer The synthesizer is responsible for acoustic generation of the translation speech. It takes the intermediate output from the decoder (before a ﬁnal projection and softmax for phoneme prediction), as well as the context output from the attention as its input, and generates a mel-spectrogram corresponding to the translation speech. It is similar to the decoders in typical neural TTS models. The predicted mel-spectrogram can be converted to waveform using an estimation algorithm such as Grifﬁn & Lim (1984) or a neural vocoder such as WaveRNN (Kalchbrenner et al., 2018). We use the duration-based autoregressive synthesizer from the NAT (Shen et al., 2020) TTS model (Figure 1b). It ﬁrst predicts durations for each elements in the input sequence, then temporally upsamples the input sequence based on the predicted durations. After that, an LSTM stack is used for generating the target spectrogram without altering the sequence length. A ﬁnal residual convolutional block further reﬁnes the generated spectrogram. Unlike in NAT, we do not supervise the duration prediction on per-phoneme duration labels, to avoid additional requirement on the training data. Instead, an L2 loss on the total predicted duration of the entire utterance is used (similar to the “na¨ıve approach” of unsupervised duration modeling in Shen et al. (2020)). 3.4. A single attention It is critical that Translatotron 2 utilizes a single attention module for both the linguistic decoder and the acoustic synthesizer. This attention models both linguistic and acoustic alignments between the source and the target speeches. A multi-head attention (Vaswani et al., 2017) is used. The queries to this attention are from the linguistic decoder. As a result, unlike in the original Translatotron, this attention does not directly model the translation alignment between two very long spectrogram sequences. Instead, it models the alignment between a source spectrogram sequence and a shorter target phoneme sequence, which is signiﬁcantly easier to learn. In the meantime, the attention provides acoustic information from the source speech to the synthesizer, summarized at per-phoneme level. Such summarized acoustic information is not only usually sufﬁcient for speech generation but also eases the duration prediction per-phoneme because it is of the same granularity. Because a single attention is used, the linguistic and acoustic information seen by the synthesizer is synchronized temporally. Such synchronization enables Translatotron 2 to preserve paralinguistic and nonlinguistic information at ﬁne granularity, such as preserving each speaker’s voice on speaker turns (Sec. 4.2). Although the synthesizer takes attention output as part of its input, the attention is not driven (i.e. queried) by the synthesizer. As a result, while it beneﬁts from the attention on obtaining aligned acoustic information from the source speech, it does not suffer from robustness issues as in typical attention-based speech synthesis models.\n\n4. Voice preserving\nThe original Translatotron (Jia et al., 2019b) demonstrated the capacity of preserving source speakers’ voices in the translation speech, by conditioning its synthesizer on a speaker embedding generated from a separately trained speaker encoder. In fact, it is capable of generating the translation speech in a different speaker’s voice, as long as a clip of the target speaker’s recording is used as the reference audio to the speaker encoder, or the embedding of the target speaker is directly available. While this is impressively powerful, it can potentially be misused for generating spooﬁng audio with arbitrary content, posing a concern for production deployment. Source: \"¿Por qué elegiste hacer esto?\", \"de 24 fuentes en 12 países\" 120 10 100 5 80 60 0 40 20 5 0 2 4 6 8 0 Reference: \"Why did you choose to do this?\", \"from 24 sources in 12 countries\" 7.5 120 5.0 100 2.5 80 60 0.0 40 2.5 20 5.0 0 1 2 3 4 5 0 Prediction: \"why did you choose to do this from twenty four sources in twelve countries\" 120 5.0 100 2.5 80 0.0 60 2.5 40 5.0 20 0 7.5 0 1 2 3 4 5 Time (sec) Figure 2: Sample mel-spectrograms on input with speaker turns. The input speech includes an utterance from a male speaker followed by another utterance from a female speaker. Translatotron 2 preserves the voices of each speaker in the translation speech. preserving speaker’s voice during S2ST, so that the trained models are restricted to preserving the source speaker’s voice, but not able to generate speech in a different speaker’s voice. In addition, this approach enables S2ST models to preserve each speaker’s voice on input speech with speaker turns, without requiring for speaker segmentation. 4.1. Training-time voice transferring In our approach, the key to restrict S2ST models to voice preservation but not arbitrary voice cloning (from a different speaker) is to move the powerful voice transferring to only happen at the training time (or the training data preparation time) but not the inference time. In contrast, it happens at both the training time and the inference time in the original Translatotron. To preserve speakers’ voices across translation, we train S2ST models on parallel utterances with the same speaker’s voice on both sides. Such a dataset with human recordings on both sides is extremely difﬁcult to collect, because it requires a large number of ﬂuent bilingual speakers. Instead, we use a TTS model capable of cross-lingual voice cloning to synthesize such training targets. We modiﬁed the PnG NAT (Jia et al., 2021; Shen et al., 2020) TTS model by incorporating a separately trained speaker encoder (Wan et al., 2018) in the same way as in Jia et al. (2018), and trained it on the LibriTTS corpus (Zen et al., 2019). The resulting TTS model is capable of zero-shot voice transferring, but synthesizes in a better quality and Table 1: Datasets for experiments with translation speech in a single-speaker’s voice. Conversational (Jia et al., 2019a) Fisher Es-En (Post et al., 2013) CoVoST 2 (Wang et al., 2021a) Languages es→en es→en es, fr, de, ca →en Domain Read, short-form Telephone conversation Read, short-form Source sample rate 16-48 kHz 8 kHz 48 kHz Utterance pairs 979k 120k 321k Source hours 1,400 127 476 Target hours 619 96 296 Target synthesized by Tacotron 2 + Grifﬁn-Lim Parallel WaveNet PnG NAT + WaveRNN more robust than Jia et al. (2018).2 We used this model to synthesize translation speech in the source speaker’s voice as the training targets in our experiments. Other TTS models capable of cross-lingual voice modeling, such as Zhang et al. (2019); Chen et al. (2019); Xin et al. (2021), could also be utilized. 4.2. Speaker turns Because the single attention module provides linguistic and acoustic information temporally synchronized (Sec. 3.4), Translatotron 2 is theoretically capable of voice preservation in complicated scenarios such as speaker turns. However, proper training data with speaker turns is required to demonstrate such capacity, which is difﬁcult to obtain. We propose a simple data augmentation to enable such training. ConcatAug To enable direct S2ST models to preserve each speaker’s voice for input with speaker turns, we augmented the training data by randomly sampling pairs of training examples and concatenating the source speech, the target speech, and the target phoneme sequences to construct new training examples. The resulting new examples contain two speakers’ voices in both the source and the target speech, which enables the model to learn on examples with speaker turns. See Figure 2 for an example of such concatenation and the prediction from Translatotron 2 on it. Such augmentation does not only enable the model to learn voice retention on speaker turns, but also increases the diversity of the speech content as well as the complexity of the acoustic conditions in the training examples, which may further improve the translation quality of the model, especially on small datasets (Sec. 5.1). Narayanan et al. (2019) uses a similar augmentation but in a more complicated fashion, for improving ASR performance on multi-speaker inputs.\n\n5. Experiments\nWe conducted experiments on three datasets, including two Spanish→English datasets and a multilingual→English dataset. All datasets use TTS synthesized target speech in 24 kHz sample rate. The phonemes used at training time were converted from the transcripts using a proprietary G2P system. See Table 1 for the details of each dataset. We evaluated the translation quality, naturalness and robustness of the produced translation speech, as well as speaker similarity for voice preservation. All models were implemented using the Lingvo framework (Shen et al., 2019). A comprehensive table of hyper-parameters is available in Appendix A. 5.1. Translation quality To evaluate the translation quality, we used the same two datasets as in Jia et al. (2019b), both of which have translation speech in a single female speaker’s voice. Following Jia et al. (2019b), the translation quality is measured by BLEU on ASR transcription from the translation speech (in lowercase, excluding punctuation marks except for apostrophes), compared to reference translation text. Because ASR makes errors, such BLEU can be thought of as a lower bound of the translation quality. We used an ASR model from Park et al. (2020), trained on LibriSpeech (Panayotov et al., 2015) and LibriLight (Kahn et al., 2020) corpora. For a fair comparison, we retrained the baseline Translatotron models and evaluated them using the same ASR model. The same ST→TTS cascade S2ST baselines from Jia et al. (2019b) were used and re-evaluated, which were composed of strong ST models and a Tacotron 2 TTS model. The predicted mel-spectrogram is converted to waveform using the Grifﬁn-Lim algorithm for all models. As shown in Table 2, the translation quality from Translatotron 2 outperformed the original Translatotron by +15.5 BLEU on Fisher Es-En and +5.2 BLEU on Conversational. Applying ConcatAug further improved the performance on the smaller Fisher Es-En dataset by +0.5 BLEU. These improvements narrowed down the performance gap between end-to-end direct S2ST and cascade S2ST from 16.4 / 8.4 down to 0.4 / 3.7 BLEU on the two datasets respectively. Table 2: Performance of S2ST in a single speaker’s voice. BLEU were computed with 1 reference for the Conversational test set, and with 4 references for the Fisher test set. End-to-end direct S2ST: Translatotron 2 55.6 4.21 ± 0.06 0.16 42.4 3.98 ± 0.08 0.07 + ConcatAug 55.1 4.19 ± 0.06 0.13 42.9 3.79 ± 0.09 0.14 Translatotron 50.4 4.15 ± 0.07 0.69 26.9 3.70 ± 0.08 0.48 Cascade (ST →TTS) 58.8 4.31 ± 0.06 0.21 43.3 4.04 ± 0.08 0.13 Reference (synthetic) 81.9 3.37 ± 0.09 0.43 88.6 3.95 ± 0.07 0.07 Discrete representation-based cascade S2ST: Zhang et al. (2021) (trained w/o text) - - - 9.4 - - Lee et al. (2022) (trained w/ text) - - - 39.9 3.41 ± 0.14 - ion scores (MOS) with 95% conﬁdence interval on 1,000 randomly sampled predictions. A WaveRNN-based neural vocoder was used for converting the mel-spectrograms predicted from S2ST models to waveforms. As shown in Table 2, the naturalness of the translation speech predicted from Translatotron 2 is signiﬁcantly better than from the original Translatotron, and is on-par with or very close to the cascade systems which used one of the state-of-the-art TTS models, Tacotron 2, for synthesizing translation speech from text. Consistent with Jia et al. (2019b), despite that the training targets in the Conversational dataset is synthesized with a lower quality Grifﬁn-Lim vocoder, the trained S2ST model is able to produce translation speech in signiﬁcantly higher naturalness when a higher quality neural vocoder is used at inference time. 5.3. Speech robustness We speciﬁcally evaluated the robustness issue of overgeneration in the predicted translation speech, such as babbling or long pause, measured by unaligned duration ratio (UDR) (Shen et al., 2020) with a 1-second threshold.3 The ASR transcription from the translation speech is used for alignment, using a conﬁdence islands-based forced alignment model (Chiu et al., 2018). As can be seen from Table 2, the UDR from Translatotron 2 is about 7 and 4 times lower than from Translatotron on the Fisher Es-En and Conversational datasets, respectively. It is even about 3 times lower than the training targets from the Conversational set, while is about the same as the training targets from Fisher Es-En. This can be explained by the fact that the training targets in the Conversational set were synthesized by the Tacotron 2 TTS model, which by itself Conversational Fisher Es-En BLEU MOS UDR (%) BLEU MOS UDR (%) Table 3: S2ST performance with voice preservation on Conversational dataset. Speaker similarity MOS is evaluated between Spanish source speech and English translation speech. (Numbers not directly comparable to Table 2 because of dataset differences.) BLEU Naturalness Similarity Proposed: Translatotron 2 57.3 3.24 ± 0.08 2.33 ± 0.08 + ConcatAug 56.8 2.94 ± 0.08 2.12 ± 0.07 Translatotron 48.5 2.55 ± 0.09 2.30 ± 0.07 + ConcatAug 51.3 2.76 ± 0.09 2.19 ± 0.07 Reference (synthetic) 81.3 3.40 ± 0.08 2.55 ± 0.07 Jia et al. (2019b): Translatotron 36.2 3.15 ± 0.08 1.85 ± 0.06 Reference (human) 59.9 4.10 ± 0.06 - suffered from over-generation, while the same in Fisher Es-En were synthesized by a more robust Parallel WaveNet (Oord et al., 2018) TTS model (see Table 1). The results suggest that Translatotron 2 drastically improves robustness than Translatotron, and is also robust to a small ratio of disﬂuency in the training targets. 5.4. Voice preservation To evaluate the ability of preserving speakers’ voices while translating their speeches from one language to another, we augmented the Conversational dataset by synthesizing target speech using a voice-transferring TTS model as described in Sec. 4.1. Examples with source speech shorter than 1 second were discarded for the stability of voice transferring. The result dataset contains parallel utterances with similar voices on both sides. S2ST models were trained on this dataset without any explicit conditioning on speaker embeddings or IDs (i.e. no speaker encoder for the original Translatotron). Following Jia et al. (2019b), we reduced the pre-net dimension of the synthesizer to 16 to encourage it to infer voice information from the encoder output instead of from the teacher-forcing inputs. Table 4: Voice preservation performance on speaker turns. Speaker similarity MOS between the leading/trailing 1.6-second segment from the English translation speech and the entire 1st/2nd source speaker’s Spanish speech is reported. (↑/ ↓: higher/lower values are better.) 1st source speaker 2nd source speaker Leading seg. ↑ Trailing seg. ↓ Leading seg. ↓ Trailing seg. ↑ Translatotron 2 2.22 ± 0.07 2.15 ± 0.07 2.04 ± 0.07 2.00 ± 0.07 + ConcatAug 2.44 ± 0.07 1.82 ± 0.07 1.76 ± 0.07 2.51 ± 0.08 Translatotron 1.87 ± 0.06 1.90 ± 0.07 2.06 ± 0.07 2.05 ± 0.07 + ConcatAug 2.18 ± 0.07 1.71 ± 0.06 1.93 ± 0.07 2.35 ± 0.07 Reference (synthetic) 2.58 ± 0.08 1.62 ± 0.06 1.83 ± 0.07 2.44 ± 0.07 5-point subjective MOS on both naturalness and speaker similarity was evaluated with 1,000 random samples or pairs of samples from the test set, reported with 95% conﬁdence interval. As Table 3 shows, when the proposed approach for voice preservation was used, both Translatotron 2 and Translatotron obtained about the same speaker similarity MOS as the original Translatotron but signiﬁcantly better translation quality. Translatotron 2 further outperformed Translatotron in terms of translation quality and speech naturalness, which is consistent with the experimental results for translating in a single speaker’s voice (Sec. 5.1, 5.2). It is worth to note that the speaker similarity from S2ST models is capped by the same of the training targets, which by itself is limited. This can be partially due to the performance of the voice-transferring TTS model used for synthesizing the training targets, and partially due to the fact that crosslingual speaker similarity evaluation is more challenging to raters (some rating comments are purely based on language difference), as also observed in Zhang et al. (2019). Obtaining better quality training targets, such as human recordings instead of synthesized speech, may further improve the performance of voice preservation with the proposed approach. 5.4.1. SPEAKER TURNS Speaker similarity evaluation with speaker turns on entire translation speech is challenging because it would require speaker separation on both source and target speeches. The content re-ordering during translation and translation errors would also add extra difﬁculty. We approximated by considering the leading/trailing short segments in the translation speech as corresponding to each of the two speakers in the source speech with a single speaker turn. We trained Translatotron 2 and Translatotron on the dataset described in Sec. 5.4, with half of the training examples augmented by ConcatAug. The evaluation set was artiﬁcially constructed in a similar way by applying ConcatAug, so that each utterance contains two speakers’ voices. We evaluated subjective speaker similarity MOS between the two entire source utterances before ConcatAug and the leading/trailing 1.6-second segments from the translation speech. Evaluation examples with target speech shorter than 2 seconds before ConcatAug were discarded. As can be seen from Table 4, the impact of ConcatAug is consistent on Translatotron 2 and Translatotron. When ConcatAug was not used during training, for each source speaker, the similarity compared to the leading/trailing segment from the translation speech was about the same; and for each segment in the translation speech, the speaker similarity compared to the ﬁrst/second source speaker was also close. This suggests that the translation speech imitated both source speakers at the same time regardless of the speaker turn. When ConcatAug was used, both models obtained signiﬁcantly higher speaker similarity on matched pairs than mismatched pairs, indicating that the models successfully separated two speakers and preserved voices for each of them respectively. It can also be seen that Translatotron 2 obtained signiﬁcantly higher speaker similarity than Translatotron on matched pairs, indicating the effectiveness of Translatotron 2. Such quantitative evaluation cannot reﬂect how the predicted translation speech transits from one speaker’s voice to another speaker’s. Listening to audio samples (available online) veriﬁed that the voice changed instantly on sentence boundaries without blurry, rather than a smoothed transition. A sample of S2ST on such a speaker turn from Translatotron 2 is visualized in Figure 2. While ConcatAug enables S2ST models to preserve speakers’ voices on speaker turns and improves translation quality on small datasets, it may negatively impact the speech naturalness and speaker similarity on models with strong performance, as shown in Table 2 and Table 3. It may be explained by the fact that the augmented utterances sound less natural and may involve abrupt change in volume and background noise on the artiﬁcial speaker turns. This suggests headroom for improvement. 5.5. Multilingual S2ST We also conducted experiments to evaluate the performance of multilingual X→En S2ST. We trained Translatotron 2 and Translatotron on the 4 high-resource language pairs from the CoVoST 2 corpus (Wang et al., 2021a), using Table 5: Ablation studies of multilingual X→En S2ST on 4 highresource language pairs from CoVoST 2, measured by BLEU on ASR transcription from the translation speech. + / −indicates using or replacing a component (see Sec. 5.5.1). Translatotron 2 27.0 18.8 27.7 22.5 −SpecAugment 25.9 17.9 25.9 21.8 −Conformer encoder 26.4 18.1 26.4 21.8 −NAT synthesizer 26.9 18.3 27.0 22.0 Translatotron (w/ SpecAugment) 17.7 9.9 17.7 13.1 + Conformer encoder 18.9 10.8 18.8 13.9 + NAT synthesizer 4.0 2.1 3.5 2.5 ST (Wang et al., 2021a) 27.0 18.9 28.0 23.9 Reference (synthetic) 82.1 86.0 85.1 89.3 TTS synthesized target speech in a single female speaker’s voice.4 The original Common Voice (Ardila et al., 2020) data split instead of the CoVoST 2 data split was followed. The models were not explicitly conditioned on languages. For a fair comparison, both models used SpecAugment, but did not use auxiliary supervision from the source phonemes. The translation quality as measured by BLEU on ASR transcription from the translation speech is shown in the ﬁrst rows of each block in Table 5. Translatotron 2 outperformed Translatotron by +9.4 BLEU on average on the 4 language pairs. Although the BLEU scores are not directly comparable between S2ST and ST (because of ASR transcription and BLEU calculation difference), the close numbers suggest that Translatotron 2 obtained translation quality comparable to the baseline ST model. 5.5.1. ABLATION STUDIES To understand the importance of each component in Translatotron 2, we conducted ablation studies on this multilingual X→En dataset. All models in the ablation used the same input and output features, SpecAugment settings, and learning rate schedules (detailed in Appendix A). No auxiliary supervision from source text was used. For models not using a Conformer encoder, we ﬁrst applied the same 4× temporal subsampling as in the Conformer encoder, then used a 256×8 bidirectional LSTM stack to encoder the subsampled features. The number of parameters in this LSTM encoder is close to the same in the Conformer encoder. For the Translatotron model using a NAT synthesizer, the same hyperparameters as in Translatotron 2 were used. For Translatotron 2 not using a NAT synthesizer, a non-autoregressive Conformer synthesizer (Sec. 5.6) was used. All the rest hyperparameters followed Appendix A for Translatotron 2, and Table 6: Ablation studies on Conversational dataset using an autoregressive RNN + Conv synthesizer and a non-autoregressive Conformer synthesizer. Synthesizer BLEU Naturalness fr de es ca RNN + Conv 55.6 4.21 ± 0.06 Conformer 54.5 3.61 ± 0.09 followed the Conversational model from Jia et al. (2019b) for Translatotron. All models were trained for 200K steps with a batch size of 768. The checkpoints for evaluation were picked by the best average BLEU on 4 language pairs on the validation set. The results are shown in Table 5. As can be seen, while the use of Conformer, SpecAugment, and NAT synthesizer helps the performance of Translatotron 2, replacing them with alternative architectural choices or removing SpecAugment only reduced the performance by a small degree (<2 BLEU). Similarly, directly using these components in Translatotron does not bring its performance close to Translatotron 2. These results suggest that the improvements of Translatotron 2 primarily comes from the high-level architectural design which addressed the performance bottlenecks existing in Translatotron (Sec. 3), rather than the choices of each individual component. 5.6. Non-autoregressive synthesizer It is tempting to use a non-autoregressive architecture for the synthesizer of Translatotron 2, which may signiﬁcantly reduce its inference latency, similar to recent works on nonautoregressive TTS (Ren et al., 2019; 2021; Guo et al., 2021; Lee et al., 2021b; Elias et al., 2021b;a). We experimented with using a 6-layer Conformer synthesizer (Guo et al., 2021) with a dimension of 512 and 8 attention heads on both Conversational and CoVoST 2 datasets. As can be seen from Table 5 and 6, using a Conformer-based non-autoregressive synthesizer obtained comparable translation quality to using an autoregressive NAT synthesizer (with BLEU on ASR transcription up to 1.1 BLEU lower). However, it caused a signiﬁcant regression on the naturalness of the predicted translation speech, which is consistent with the observation in TTS in Shen et al. (2020); Peng et al. (2020); Hwang et al. (2021), etc., suggesting more exploration is needed on this direction.\n\n2. Related works\nS2ST Until very recently, automatic S2ST systems are typically composed of a cascade of ASR, MT, and TTS components (Lavie et al., 1997; Wahlster, 2000; Nakamura et al., 2006; ITU, 2016). Translatotron (Jia et al., 2019b) is the ﬁrst direct S2ST model, which is a sequence-to-sequence model trained end-to-end in a multi-objective task. It has shown reasonable translation quality and speech naturalness, but still underperformed a baseline of ST →TTS cascade by a large margin. It also demonstrated the capacity of preserving speakers’ voices during the translation, by leveraging a speaker encoder separately trained in a speaker veriﬁcation task (Wan et al., 2018; Jia et al., 2018). A few recent works proposed cascade S2ST systems using learned discrete speech representation as the intermediate representation instead of text or phoneme. Tjandra et al. (2019) introduced such an S2ST system that ﬁrst translated the source speech into a discrete representation of the target speech which was predicted from a separately trained VQVAE (Oord et al., 2017), then constructed the target speech spectrogram from the discrete representation using the VQVAE decoder. Zhang et al. (2021) additionally trained the VQ-VAE jointly with a supervised phoneme recognition objective in different languages. Lee et al. (2022; 2021a) used a separately trained vocoder to directly predict waveform from the discrete representation without relying on spectrogram; for the best performance, this vocoder included a duration predictor, akin to a generative TTS model. All these works require multiple components being trained in multiple steps, but are not able to be trained end-to-end. Another potential limitation of such an approach is that it may not be effective in preserving paralinguistic and nonlinguistic information. Oppositely, it can be desired that such variation be removed in the discrete representation (Lee et al., 2021a). Kano et al. (2021) introduced an end-to-end S2ST model with a cascade of three autoregressive decoders, and used pre-trained MT and TTS models as teacher models to facilitate the training of the end-to-end model. It requires pre-trained ASR, MT, and TTS models, and the end-to-end model itself has to be trained in multiple steps. While most of these works conducted experiments using synthetic datasets with translation speech in a clean single speaker’s voice, Jia et al. (2019b); Lee et al. (2021a) conducted experiments using multi-speaker human recordings. Although these recent works generated speech translation in novel ways without relying on TTS subsystems, only a few of them (Jia et al., 2019b; Lee et al., 2022) have evaluated the perceptual quality (e.g. naturalness) of the produced speech translation, which is critical to S2ST (Wagner et al., 2019; Salesky et al., 2021), with the rest focused only on the translation quality. TTS Translatotron uses a decoder similar to the Tacotron 2 TTS model (Shen et al., 2018), which is an attention-based autoregressive decoder. Due to the ﬂexibility of the attention mechanism, they both suffer from robustness issues such as over-generation. Recent TTS models such as FastSpeech (Ren et al., 2019; 2021) and Non-Attentive Tacotron (NAT) (Shen et al., 2020) demonstrated that replacing the attention module with a duration-based upsampler yields more robust synthesized speech, as quantitatively evaluated at a large scale in Shen et al. (2020). The synthesizer component in this work resembles these works. Voice conversion and anti-spooﬁng The performance of voice conversion has progressed rapidly in the recent years, and is reaching a quality that is hard for automatic speaker veriﬁcation (ASV) systems to detect (Yi et al., 2020). ASVspoof 2019 (Todisco et al., 2019; Wang et al., 2020) found that it was challenging to detect spoof audios generated from a zero-shot voice cloning TTS model (Jia et al., 2018), which was followed by the original Translatotron for preserving speakers’ voices during S2ST. Such progress poses concerns on related techniques being misused for creating spooﬁng artifacts. We propose a new voice preservation method for S2ST with the motivation of avoiding such potential misuse.\n\n6. Conclusion\nWe proposed Translatotron 2, a neural direct S2ST model that can be trained end-to-end. Experimental results on three datasets consistently suggest that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches cascade S2ST. In addition, we proposed a simple method for preserving speakers’ voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker’s voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker’s privacy and mitigates potential misuse of voice cloning for creating spooﬁng audio artifacts. Future works include extending Translatotron 2 to support simultaneous translation, cross-lingual prosody transfer, unwritten languages, and further quality improvement by utilizing self-supervised pre-training (Baevski et al., 2020; Wang et al., 2021b) and weakly supervised data (Jia et al., 2019a).\n\nAcknowledgements The authors would like to thank Chung-Cheng Chiu, Quan Wang, Heiga Zen, Ron J. Weiss, Wolfgang Macherey, Yu Zhang, Yonghui Wu, Hadar Shemtov, Ruoming Pang, Nadav Bar, Michael Hassid, and the rest of the Google Research team for helpful discussions and previous work on data preparation.\n\nReferences\nAnastasopoulos, A., Bojar, O., Bremerman, J., Cattoni, R., Elbayad, M., Federico, M., Ma, X., Nakamura, S., Negri, M., Niehues, J., et al. Findings of the IWSLT 2021 evaluation campaign. In International Conference on Spoken Language Translation (IWSLT), 2021. Ansari, E., Axelrod, A., Bach, N., Bojar, O., Cattoni, R., Dalvi, F., Durrani, N., Federico, M., Federmann, C., Gu, J., et al. Findings of the IWSLT 2020 evaluation campaign. In International Conference on Spoken Language Translation (IWSLT), 2020. Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common Voice: A massively-multilingual speech corpus. In Proceedings of Language Resources and Evaluation Conference (LREC), 2020. Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Battenberg, E., Skerry-Ryan, R., Mariooryad, S., Stanton, D., Kao, D., Shannon, M., and Bagby, T. Locationrelative attention mechanisms for robust long-form speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Chen, M., Chen, M., Liang, S., Ma, J., Chen, L., Wang, S., and Xiao, J. Cross-lingual, multi-speaker text-tospeech synthesis using neural speaker embedding. In Proc. Interspeech, 2019. Chiu, C.-C., Tripathi, A., Chou, K., Co, C., Jaitly, N., Jaunzeikare, D., Kannan, A., Nguyen, P., Sak, H., Sankar, A., Tansuwan, J., Wan, N., Wu, Y., and Zhang, X. Speech recognition for medical conversations. In Proc. Interspeech, 2018. Di Gangi, M. A., Negri, M., and Turchi, M. One-to-many multilingual end-to-end speech translation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Skerry-Ryan, R., and Wu, Y. Parallel Tacotron 2: A non-autoregressive neural TTS model with differentiable duration modeling. In Proc. Interspeech, 2021a. Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Weiss, R., and Wu, Y. Parallel Tacotron: Non-autoregressive and controllable TTS. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021b. Grifﬁn, D. and Lim, J. Signal estimation from modiﬁed short-time Fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(2):236–243, 1984. Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang,\nR. Conformer: Convolution-augmented transformer for\nspeech recognition. In Proc. Interspeech, 2020. Guo, P., Boyer, F., Chang, X., Hayashi, T., Higuchi, Y., Inaguma, H., Kamo, N., Li, C., Garcia-Romero, D., Shi, J., et al. Recent developments on espnet toolkit boosted by conformer. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5874– 5878, 2021. He, M., Deng, Y., and He, L. Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural TTS. In Proc. Interspeech, 2019. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. Hwang, M.-J., Yamamoto, R., Song, E., and Kim, J.-M. TTS-by-TTS: TTS-driven data augmentation for fast and high-quality speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6598–6602, 2021. ITU. ITU-T F.745: Functional requirements for networkbased speech-to-speech translation services, 2016. International Telecommunication Union. Jia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., Ren, F., Chen, Z., Nguyen, P., Pang, R., Moreno, I. L., and Wu, Y. Transfer learning from speaker veriﬁcation to multispeaker text-to-speech synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Jia, Y., Johnson, M., Macherey, W., Weiss, R. J., Cao, Y., Chiu, C.-C., Ari, N., Laurenzo, S., and Wu, Y. Leveraging weakly supervised data to improve end-to-end speechto-text translation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019a. Jia, Y., Weiss, R. J., Biadsy, F., Macherey, W., Johnson, M., Chen, Z., and Wu, Y. Direct speech-to-speech translation with a sequence-to-sequence model. In Proc. Interspeech, 2019b. Jia, Y., Zen, H., Shen, J., Zhang, Y., and Wu, Y. PnG BERT: Augmented BERT on phonemes and graphemes for neural TTS. In Proc. Interspeech, 2021. Jia, Y., Tadmor Ramanovich, M., Wang, Q., and Zen, H. CVSS corpus and massively multilingual speechto-speech translation. In Proceedings of Language Resources and Evaluation Conference (LREC), 2022. Kahn, J., Rivi`ere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazar´e, P.-E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. Libri-light: A benchmark for ASR with limited or no supervision. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A. v. d., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural audio synthesis. In Proceedings of International Conference on Machine Learning (ICML), 2018. Kano, T., Sakti, S., and Nakamura, S. Transformer-based direct speech-to-speech translation with transcoder. In IEEE Spoken Language Technology Workshop (SLT), 2021. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. Krueger, D., Maharaj, T., Kram´ar, J., Pezeshki, M., Ballas, N., Ke, N. R., Goyal, A., Bengio, Y., Courville, A., and Pal, C. Zoneout: Regularizing RNNs by randomly preserving hidden activations. In International Conference on Learning Representations (ICLR), 2017. Lavie, A., Waibel, A., Levin, L., Finke, M., Gates, D., Gavalda, M., Zeppenfeld, T., and Zhan, P. JANUS-III: Speech-to-speech translation in multiple languages. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1997. Lee, A., Gong, H., Duquenne, P.-A., Schwenk, H., Chen, P.-J., Wang, C., Popuri, S., Pino, J., Gu, J., and Hsu, W.-N. Textless speech-to-speech translation on real data. arXiv preprint arXiv:2112.08352, 2021a. Lee, A., Chen, P.-J., Wang, C., Gu, J., Ma, X., Polyak, A., Adi, Y., He, Q., Tang, Y., Pino, J., et al. Direct speechto-speech translation with discrete units. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), 2022. Lee, Y., Shin, J., and Jung, K. Bidirectional variational inference for non-autoregressive text-to-speech. In International Conference on Learning Representations (ICLR), 2021b. Ma, X., Gong, H., Liu, D., Lee, A., Tang, Y., Chen, P.-J., Hsu, W.-N., Heaﬁeld, K., Koehn, P., and Pino, J. Direct simultaneous speech to speech translation. arXiv preprint arXiv:2110.08250, 2021. McCarthy, A. D., Puzon, L., and Pino, J. SkinAugment: Auto-encoding speaker conversions for automatic speech translation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Nakamura, S., Markov, K., Nakaiwa, H., Kikui, G., Kawai, H., Jitsuhiro, T., Zhang, J.-S., Yamamoto, H., Sumita, E., and Yamamoto, S. The ATR multilingual speech-tospeech translation system. IEEE Transactions on Audio, Speech, and Language Processing, 2006. Narayanan, A., Prabhavalkar, R., Chiu, C.-C., Rybach, D., Sainath, T. N., and Strohman, T. Recognizing long-form speech using streaming end-to-end models. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., Driessche, G., Lockhart, E., Cobo, L., Stimberg, F., et al. Parallel WaveNet: Fast highﬁdelity speech synthesis. In Proceedings of International Conference on Machine Learning (ICML), 2018. Oord, A. v. d., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. LibriSpeech: an ASR corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. SpecAugment: A simple data augmentation method for automatic speech recognition. In Proc. Interspeech, 2019. Park, D. S., Zhang, Y., Jia, Y., Han, W., Chiu, C.-C., Li, B., Wu, Y., and Le, Q. V. Improved noisy student training for automatic speech recognition. In Proc. Interspeech, 2020. Pathak, M. A. and Raj, B. Privacy-preserving speaker veriﬁcation and identiﬁcation using Gaussian mixture models. IEEE Transactions on Audio, Speech, and Language Processing, 21(2):397–406, 2012. Peng, K., Ping, W., Song, Z., and Zhao, K. Nonautoregressive neural text-to-speech. In Proceedings of International Conference on Machine Learning (ICML), 2020. Post, M., Kumar, G., Lopez, A., Karakos, D., CallisonBurch, C., and Khudanpur, S. Improved speech-to-text translation with the Fisher and Callhome Spanish–English speech translation corpus. In International Conference on Spoken Language Translation (IWSLT), 2013. Ren, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. FastSpeech: Fast, robust and controllable text to speech. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Ren, Y., Hu, C., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. FastSpeech 2: Fast and high-quality end-to-end text-tospeech. In International Conference on Learning Representations (ICLR), 2021. Salesky, E., M¨ader, J., and Klinger, S. Assessing evaluation metrics for speech-to-speech translation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021. Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomyrgiannakis, Y., and Wu, Y. Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. Shen, J., Nguyen, P., Wu, Y., Chen, Z., et al. Lingvo: A modular and scalable framework for sequence-to-sequence modeling. arXiv preprint arXiv:1902.08295, 2019. Shen, J., Jia, Y., Chrzanowski, M., Zhang, Y., Elias, I., Zen, H., and Wu, Y. Non-Attentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling. arXiv preprint arXiv:2010.04301, 2020. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818–2826, 2016. Tjandra, A., Sakti, S., and Nakamura, S. Speech-to-speech translation between untranscribed unknown languages. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Todisco, M., Wang, X., Vestman, V., Sahidullah, M., Delgado, H., Nautsch, A., Yamagishi, J., Evans, N., Kinnunen, T., and Lee, K. A. ASVspoof 2019: Future horizons in spoofed and fake audio detection. In Proc. Interspeech, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Wagner, P., Beskow, J., Betz, S., Edlund, J., Gustafson, J., Eje Henter, G., Le Maguer, S., Malisz, Z., Sz´ekely, ´E., T˚annander, C., et al. Speech synthesis evaluation – stateof-the-art assessment and suggestion for a novel research program. In Proceedings of the 10th Speech Synthesis Workshop (SSW10), 2019. Wahlster, W. Verbmobil: Foundations of speech-to-speech translation. Springer, 2000. Wan, L., Wang, Q., Papir, A., and Moreno, I. L. Generalized end-to-end loss for speaker veriﬁcation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. Wang, C., Wu, A., and Pino, J. CoVoST 2: A massively multilingual speech-to-text translation corpus. In Proc. Interspeech, 2021a. Wang, C., Wu, A., Pino, J., Baevski, A., Auli, M., and Conneau, A. Large-scale self-and semi-supervised learning for speech translation. In Proc. Interspeech, 2021b. Wang, X., Yamagishi, J., Todisco, M., Delgado, H., Nautsch, A., Evans, N., Sahidullah, M., Vestman, V., Kinnunen, T., Lee, K. A., et al. ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech. Computer Speech & Language, 64:101114, 2020. Weiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., and Chen,\nZ. Sequence-to-sequence models can directly translate\nforeign speech. In Proc. Interspeech, 2017. for cross-lingual TTS. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021. Yi, Z., Huang, W.-C., Tian, X., Yamagishi, J., Das, R. K., Kinnunen, T., Ling, Z., and Toda, T. Voice conversion challenge 2020: Intra-lingual semi-parallel and crosslingual voice conversion. In Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge, 2020. Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. Interspeech, 2019. Zhang, C., Tan, X., Ren, Y., Qin, T., Zhang, K., and Liu, T.-\nY. UWSpeech: Speech to speech translation for unwritten\nlanguages. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021. Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Chen, Z., SkerryRyan, R., Jia, Y., Rosenberg, A., and Ramabhadran, B. Learning to speak ﬂuently in a foreign language: Multilingual speech synthesis and cross-language voice cloning. In Proc. Interspeech, 2019. Zheng, Y., Wang, X., He, L., Pan, S., Soong, F. K., Wen, Z., and Tao, J. Forward-backward decoding for regularizing end-to-end TTS. In Proc. Interspeech, 2019.\n\nA. Table of hyper-parameters\nTable 7: Model hyper-parameters used in the experiments. (“×n”: n layers; †: 128-dim pre-net is used for translating in a single voice; 16-dim pre-net is used for voice preservation.) Input Sample rate (Hz) 8,000 48,000 16,000 – 48,000 Mel channels 80 Mel lower band (Hz) 125 Mel upper band (Hz) 3,800 7,600 7,600 Frame size (ms) 25.0 Frame step (ms) 10.0 Output Sample rate (Hz) 24,000 Mel channels 128 Mel lower band (Hz) 20 Mel upper band (Hz) 12,000 Frame size (ms) 50.0 Frame step (ms) 12.5 SpecAugment Freq blocks 2 Time blocks 10 Freq block max length ratio 0.33 Time block max length ratio 0.05 Encoder Conformer dims 144 × 16 Attention heads 4 Conv kernal size 32 Subsample factor 4 Attention Output dim 256 512 512 Hidden dim 512 512 512 Attention heads 4 8 8 Dropout prob 0.1 0.2 0.2 Decoder LSTM dims 256 × 4 512 × 6 512 × 4 Zoneout prob 0.1 0.1 0.1 Phoneme embedding dim 96 256 256 Label smoothing uncertainty 0.1 0.1 0.1 Loss weight 10.0 10.0 10.0 Duration predictor Bi-LSTM (dim × layers) 64 × 2 128 × 2 128 × 2 Loss weight 1.0 1.0 1.0 Synthesizer LSTM dims 1,024 × 2 LSTM zoneout prob 0.1 Pre-net dims 128 × 2 128 × 2 128 / 16 † × 2 Pre-net dropout prob 0.5 Post-net (kernel, channels) × layers (5, 512) × 4 + (5, 128) Loss weight 1.0 Training Optimizer Adam (Kingma & Ba, 2015) Learning rate schedule Vaswani et al. (2017) Learning rate (peak) 4.2×10−3 2.2×10−3 3.3×10−3 Warm-up steps Batch size 1,024 768 768 L2 regularization weight 10−6 10−6 10−6 Fisher Es-En CoVoST 2 Conversational src vs src tgt vs src tgt vs tgt s2st vs src s2st vs tgt Figure 3: Afﬁnity matrices of d-vector similarity among 100 random examples. (“s2st” refers to the predictions from Translatotron 2.) Table 8: Objective d-vector similarity between the predicted translated speech (English) and the source human speech (Spanish) on speaker turns. The similarity between the leading/trailing 1.6-second segment from the predicted speech and the entire 1st/2nd source speaker’s speech is measured. (↑/ ↓means higher/lower values are better.) 1st source speaker 2nd source speaker Leading seg. ↑ Trailing seg. ↓ Leading seg. ↓ Trailing seg. ↑ Translatotron 2 0.21 0.19 0.21 0.19 + ConcatAug 0.20 0.14 0.14 0.21 Translatotron 0.20 0.22 0.27 0.29 + ConcatAug 0.32 0.16 0.14 0.35 Reference (synthetic) 0.48 0.17 0.15 0.48\nB. Objective speaker similarity analysis\nSubjective speaker similarity evaluation is costly and has a long turnaround. We explored alternative objective evaluation using separately trained speaker encoders, such as d-vector (Wan et al., 2018). We evaluated the voice retention performance using the cosine similarity of the d-vectors. We ﬁrst checked the scenario that each input contains a single speaker’s recording. Figure 3 visualizes the afﬁnity matrices of d-vector similarity among different input utterances for a Translatotron 2 model. The outstanding higher similarity values on the diagonals indicate that the model is able to preserve the source speaker’s voice in the predicted translation speech. We then conducted a detailed evaluation for the voice retention performance for speaker turns. The experiment setting up was identical to Section 5.4.1, except that the speaker similarity was measured by d-vector similarity instead of subjective MOS evaluation. The d-vectors for each source speaker were computed on the entire original utterance before concatenation; the d-vectors for each speaker in the prediction is approximated by computing on the leading/trailing 1.6 seconds of predicted speech. The results are shown in Table 8. Consistent with the MOS evaluation results in Table 4, when the concatenation augmentation was not used, the d-vector similarity to each source speaker is about the same regardless if it was compared to the leading or trailing segments, indicating that the predicted speech was in a single speaker’s voice and the model was unable to separate different speakers in the input, but rather optimized for both source speakers at the same time. When the concatenation augmentation was used, the d-vector similarity was signiﬁcantly higher between matched pairs than between unmatched pairs, indicating that the models were able to separate different speakers in the input and preserve their voices in the predicted translation speech respectively. However, when these similarities are compared among different models, it seems to suggest that Translatotron performed better than Translatotron 2, which is contradictory to the subjective evaluation results in Table 4. By carefully listening to the audio samples, we found that such discrepancy may be due to the fact that the d-vector model was also sensitive to non-voice related acoustic characteristics, such as reverb and channel noise in the audios. This is likely a consequence of the fact that in the large-scale training set for the d-vector model used in the evaluation, each speaker is typically associated with a particular recording condition, e.g. recording device and room. Because the encoder output from the Translatotron model was of signiﬁcantly larger dimension than from the Translatotron 2 model (2048 vs 144), it was capable of carrying more non-voice acoustic information and thus obtained better d-vector similarity, which not necessarily indicating higher speaker similarity. These results suggest that while such speaker encoder-based objective analysis reveals insightful indications about the performance of the S2ST models, it can be less reliable compared to subjective MOS evaluation. Such reliability also highly depends on the training details of the speaker encoder model being used, especially the training corpus.\n--- FOOTNOTES 1Google Research. Correspondence to: 10K 20K 10K", "persona": "Student", "response": "### 1. Overview\nThe speech-to-speech translation system translates the input audio from one language to another. These are abbreviated as S2ST (Speech to Speech Translation) systems or S2S(Speech to Speech) systems in general. The primary objective of this system is to enable communication among people who speak different languages. The main drawback of such a system was Cascading of error : The error introduced in AST caused compounding effects in MT and TTS. In 2019, researchers at Google came up with direct speech-to-speech translation with a sequence-to-sequence model, which was the first end-to-end sequence-to-sequence model for S2ST. Translatotron 2 This was followed by a modified architecture in 2022 called Translatotron 2: high-quality direct speech-to-speech translation with voice preservation. In this article, we will understand the Translatotron-2 architecture in detail. Feed Forward Module Convolution Module Speech Synthesizer Speech Synthesizer of Translatotron 2 The speech synthesizer in Translatotron 2 is adopted from NAT (Non-Attentive Tacotron). NAT first predicts the duration and range of influence for each token in the input sequence. Using these two values it uses Gaussian upsampling to upsample the input. After that, an LSTM stack is used for generating the target spectrogram. A final residual convolutional block further refines the generated spectrogram Vocoder The spectrogram is subsequently input into a Vocoder, an abbreviation for \"Voice\" and \"Encoder.\" The Vocoder serves the purpose of both analyzing and synthesizing the human voice signal based on the information contained in the spectrogram.\n\n### 2. Background\nSeq2Seq Model A Seq2Seq model consists of the following three types of systems: ASR (Automatic Speech Recognition): This system converts the recorded voice to text in the same audio language. For our example, it will take the audio file as input and try to produce the sentence, ' I like watching cricket.' TTS (text-to-speech synthesis) : This will take the converted output text from step 2 and convert it back to audio. Encoder The encoder uses a conformer as the architecture. The conformer is a combination of transformer and CNN, hence the name 'conformer'. Conformer was devised to combine the advantages of Transformer and convolution capturing the global contexts and local contexts, respectively. Decoder The autoregressive decoder is responsible for producing linguistic information in the translation speech. It takes the attention module, and predicts a phoneme sequence corresponding to the translation speech. It uses LSTM stack . The dimension of LSTM is same as encoder_dim. The number of stack is 6 to 4.The output from the LSTM stack is passed through a projection layer to convert it to phoneme embedding dimension which is typically 256 R rahulsm27 Follow Improve Article Tags : Deep Learning\n\n### 3. Methods\nTranslatotron 2 Architecture You can refer to the image for the Translatotron 2 architecture for better understanding. Translatotron 2 Architecture The target audio spectrogram The target language phoneme will be used as input for the target audio spectrogram as well. Multi-head Self-attention Module Attention Module The attention module serves as the bridge that connects all elements of the Translatotron architecture, including the encoder, decoder, and synthesizer. This attention mechanism plays a dual role by modeling both linguistic and acoustic alignments between the source and target speeches. It employs a multi-head attention mechanism, with queries originating from the linguistic decoder. Its primary function is to capture the alignment relationship between a sequence of source spectrograms and a shorter sequence of target phonemes. Furthermore, the attention module provides valuable acoustic information from the source speech to the synthesizer, presenting it in a summarized form at the per-phoneme level. This summarized acoustic information not only proves to be generally adequate for the speech generation process but also simplifies the task of predicting phoneme durations since it aligns with the same granularity. The synthesizer assumes the role of acoustically generating the translated speech. It accepts two inputs: the intermediate output from the decoder (prior to the final projection and softmax for phoneme prediction) and the contextual output derived from the attention mechanism. These inputs are concatenated, and the synthesizer utilizes this combined information to produce a Mel-Spectrogram that corresponds to the translated speech.\n\n### 4. Results\nMT (machine translation): This will take the converted sentence from step 1 and translate it into the target language. In our case, it will give the output as 'मुझे क्रिकेट देखना पसंद है' in Hindi. Phonemes are the basic sound units in any given language that have become incorporated into formal language systems. For many of the world's languages, phonemes consist of various combinations of consonants (C) and vowels (V). Conclusion When Google introduced Translatotorn 1 for end-to-end S2ST, though it performed well it was not able to match the performance of cascade S2ST.\n\n### 5. Takeaways\nVoice Preservation In order to mitigate this risk Google used a speech encoder only during the training to make the model learn voice preservation by training the model on parallel utterances with the same speakers' voice on both sides. Since obtaining such a dataset is very difficult it used TTS (Text to Speech model) with a speech encoder to generate training examples. In June 2023 Google released Translatotron 3: Speech-to-Speech Translation with Monolingual Data. The core architecture of Translatotron 3 was the same as that of Translatotron. However, the major highlight of this url was the ability to apply unsupervised training for speech-to-speech translation. It meant that even if we do not have a corpus of translated data between two languages, but if we have individual datasets for each language we can train Translatotron 3 to learn the mapping between these two languages !! We will explore Translatotron 3 in our future article.", "paper_id": "29", "story_id": "29-03"}
{"paper": "Translatotron 2: High-quality direct speech-to-speech translation with voice preservation Ye Jia 1 Michelle Tadmor Ramanovich 1 Tal Remez 1 Roi Pomerantz 1\n\nAbstract\narXiv:2107.08661v5  [cs.CL]  17 May 2022 We present Translatotron 2, a neural direct speechto-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module that connects them together. Experimental results on three datasets consistently show that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade systems. In addition, we propose a simple method for preserving speakers’ voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker’s voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker’s privacy and mitigates potential misuse of voice cloning for creating spoofing audio artifacts.\n\n1. Introduction\nSpeech-to-speech translation (S2ST) is highly beneﬁcial for breaking down communication barriers between people not sharing a common language. Conventional automatic S2ST systems are composed of a cascade of three components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis (Lavie et al., 1997; Wahlster, 2000; Nakamura et al., 2006). In the past few years, direct speech-to-text translation (ST) is rapidly emerging, and has outperformed the cascade of ASR and MT (Weiss et al., 2017; Jia et al., 2019a; Di Gangi et al., 2019; McCarthy et al., 2020; Ansari et al., 2020; Wang et al., 2021b; Anastasopoulos et al., 2021), which makes the Ye Jia <jiaye@google.com>. cascade of ST and TTS as S2ST feasible (Jia et al., 2019b). Recently, works on S2ST without relying on intermediate text representation are emerging, such as end-to-end direct S2ST (Jia et al., 2019b; Kano et al., 2021) and cascade S2ST based on discrete speech representation (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022; 2021a; Ma et al., 2021). Compared to text-centric cascaded systems, although such approaches require parallel S2ST data, which is scarce, they have the potential advantages of: 1) Preserving paralinguistic and non-linguistic information during translation, such as speaker’s voice (Jia et al., 2019b), emotion and prosody; 2) Supporting languages without written form, or being able to be trained without transcription of speech (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022; 2021a); 3) Reduced computational requirements and lower inference latency (Lee et al., 2022); 4) Avoiding error compounding across sub-systems (Jia et al., 2022); 5) Easier on handling contents that do not need to be translated, such as names and proper nouns (Jia et al., 2019b). Among these works, Translatotron (Jia et al., 2019b) is the ﬁrst model that is able to directly translate speech in one language to speech in another language. It obtained reasonable translation quality and high naturalness in the predicted translation speech, and is able to preserve speakers’ voices during the speech translation. However, the translation quality from Translatotron still underperforms cascade baselines by a large margin, and the translation speech it produces suffers from over-generation issues, such as babbling and long pause. Such weaknesses make this model not yet practical for production. Nevertheless, it remains the state-of-the-art of end-to-end direct S2ST. In this paper, we ﬁrst tackle the performance gap between end-to-end direct S2ST and cascade S2ST. We propose Translatotron 2, a novel direct S2ST model that is able to be trained end-to-end. We conduct experiments on three S2ST datasets, including multilingual S2ST. The results consistently suggest that Translatotron 2 signiﬁcantly outperforms Translatotron in terms of both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade S2ST. When a simple data augmentation ConcatAug is used, the translation quality gap on the Fisher Spanish-English corpus (Post et al., 2013) is reduced from 16.4 to 0.4 BLEU. These results are the ﬁrst time that end-to-end direct S2ST approaches cascade S2ST. In addition, we propose a simple method for preserving speakers’ voices during S2ST without relying on any speaker representation (ID or embedding). The proposed method enables Translatotron 2 to preserve each speaker’s voice on speaker turns without requiring for speaker separation, which is the ﬁrst of its kind. Furthermore, compared to existing approaches of voice preservation, the proposed method better preserves speaker’s privacy (Pathak & Raj, 2012) and mitigates potential misuse of voice cloning for creating spooﬁng audio artifacts. Audio samples from Translatotron 2 are available online.1\n\n3. Translatotron 2\nWe designed the architecture of Translatotron 2 to address three performance bottlenecks existing in the original Translatotron: 1) The utilization of the auxiliary textual supervision during training is suboptimal, namely, the attention alignment learned by the auxiliary ST task does not directly contribute to the main S2ST task; 2) The challenge posed by modeling the translation alignment between two very (English) spectrogram phoneme (English) Synthesizer Decoder autoregression Attention Encoder spectrogram (Spanish) (a) Overview of Translatotron 2. Figure 1: A Translatotron 2 model that translates Spanish speech into English speech. long spectrogram sequences using the attention mechanism; 3) Attention-based speech generation is known to suffer from robustness issues such as over-generation and undergeneration (Shen et al., 2020; Ren et al., 2019; He et al., 2019; Zheng et al., 2019; Battenberg et al., 2020). We addressed these bottlenecks by designing a novel S2ST model architecture composed of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module connecting them together (Figure 1a). The model is jointly trained with a speech-to-speech translation objective and a speech-to-phoneme translation objective. The following subsections describe each component of Translatotron 2. Note that as shown in the ablation studies in Sec. 5.5.1, while the speciﬁc architectural choices of these components help the performance of Translatotron 2, the primary improvement comes from the high-level architecture rather than the choice of each individual component. 3.1. Speech encoder The encoder of Translatotron 2 takes the mel-spectrogram of the source speech as the input, and produces a hidden representation which encodes both linguistic and acoustic information from the source speech. We use Conformer (Gulati et al., 2020) as the architecture of the encoder. It ﬁrst subsamples the input mel-spectrogram with a convolutional layer, and then processes it with a stack of Conformer blocks. Each Conformer block is composed of a feed-forward layer, a self-attention layer, a convolution layer, and a second feedforward layer. SpecAugment (Park et al., 2019) is applied at the training time as data augmentation. spectrogram RNN + Conv autoregression durations Upsampler Duration Predictor concat(hidden, attention) (b) Synthesizer of Translatotron 2. 3.2. Linguistic decoder The autoregressive decoder is responsible for producing linguistic information in the translation speech. It takes the encoder output through the attention module, and predicts a phoneme sequence corresponding to the translation speech. We use an LSTM stack (Hochreiter & Schmidhuber, 1997) as the decoder, assisted with regularization including Zoneout (Krueger et al., 2017) and label smoothing (Szegedy et al., 2016). The combination of the encoder, the decoder, and the attention module is similar to a typical ST model, except that it predicts phonemes instead of subword tokens. 3.3. Acoustic synthesizer The synthesizer is responsible for acoustic generation of the translation speech. It takes the intermediate output from the decoder (before a ﬁnal projection and softmax for phoneme prediction), as well as the context output from the attention as its input, and generates a mel-spectrogram corresponding to the translation speech. It is similar to the decoders in typical neural TTS models. The predicted mel-spectrogram can be converted to waveform using an estimation algorithm such as Grifﬁn & Lim (1984) or a neural vocoder such as WaveRNN (Kalchbrenner et al., 2018). We use the duration-based autoregressive synthesizer from the NAT (Shen et al., 2020) TTS model (Figure 1b). It ﬁrst predicts durations for each elements in the input sequence, then temporally upsamples the input sequence based on the predicted durations. After that, an LSTM stack is used for generating the target spectrogram without altering the sequence length. A ﬁnal residual convolutional block further reﬁnes the generated spectrogram. Unlike in NAT, we do not supervise the duration prediction on per-phoneme duration labels, to avoid additional requirement on the training data. Instead, an L2 loss on the total predicted duration of the entire utterance is used (similar to the “na¨ıve approach” of unsupervised duration modeling in Shen et al. (2020)). 3.4. A single attention It is critical that Translatotron 2 utilizes a single attention module for both the linguistic decoder and the acoustic synthesizer. This attention models both linguistic and acoustic alignments between the source and the target speeches. A multi-head attention (Vaswani et al., 2017) is used. The queries to this attention are from the linguistic decoder. As a result, unlike in the original Translatotron, this attention does not directly model the translation alignment between two very long spectrogram sequences. Instead, it models the alignment between a source spectrogram sequence and a shorter target phoneme sequence, which is signiﬁcantly easier to learn. In the meantime, the attention provides acoustic information from the source speech to the synthesizer, summarized at per-phoneme level. Such summarized acoustic information is not only usually sufﬁcient for speech generation but also eases the duration prediction per-phoneme because it is of the same granularity. Because a single attention is used, the linguistic and acoustic information seen by the synthesizer is synchronized temporally. Such synchronization enables Translatotron 2 to preserve paralinguistic and nonlinguistic information at ﬁne granularity, such as preserving each speaker’s voice on speaker turns (Sec. 4.2). Although the synthesizer takes attention output as part of its input, the attention is not driven (i.e. queried) by the synthesizer. As a result, while it beneﬁts from the attention on obtaining aligned acoustic information from the source speech, it does not suffer from robustness issues as in typical attention-based speech synthesis models.\n\n4. Voice preserving\nThe original Translatotron (Jia et al., 2019b) demonstrated the capacity of preserving source speakers’ voices in the translation speech, by conditioning its synthesizer on a speaker embedding generated from a separately trained speaker encoder. In fact, it is capable of generating the translation speech in a different speaker’s voice, as long as a clip of the target speaker’s recording is used as the reference audio to the speaker encoder, or the embedding of the target speaker is directly available. While this is impressively powerful, it can potentially be misused for generating spooﬁng audio with arbitrary content, posing a concern for production deployment. Source: \"¿Por qué elegiste hacer esto?\", \"de 24 fuentes en 12 países\" 120 10 100 5 80 60 0 40 20 5 0 2 4 6 8 0 Reference: \"Why did you choose to do this?\", \"from 24 sources in 12 countries\" 7.5 120 5.0 100 2.5 80 60 0.0 40 2.5 20 5.0 0 1 2 3 4 5 0 Prediction: \"why did you choose to do this from twenty four sources in twelve countries\" 120 5.0 100 2.5 80 0.0 60 2.5 40 5.0 20 0 7.5 0 1 2 3 4 5 Time (sec) Figure 2: Sample mel-spectrograms on input with speaker turns. The input speech includes an utterance from a male speaker followed by another utterance from a female speaker. Translatotron 2 preserves the voices of each speaker in the translation speech. preserving speaker’s voice during S2ST, so that the trained models are restricted to preserving the source speaker’s voice, but not able to generate speech in a different speaker’s voice. In addition, this approach enables S2ST models to preserve each speaker’s voice on input speech with speaker turns, without requiring for speaker segmentation. 4.1. Training-time voice transferring In our approach, the key to restrict S2ST models to voice preservation but not arbitrary voice cloning (from a different speaker) is to move the powerful voice transferring to only happen at the training time (or the training data preparation time) but not the inference time. In contrast, it happens at both the training time and the inference time in the original Translatotron. To preserve speakers’ voices across translation, we train S2ST models on parallel utterances with the same speaker’s voice on both sides. Such a dataset with human recordings on both sides is extremely difﬁcult to collect, because it requires a large number of ﬂuent bilingual speakers. Instead, we use a TTS model capable of cross-lingual voice cloning to synthesize such training targets. We modiﬁed the PnG NAT (Jia et al., 2021; Shen et al., 2020) TTS model by incorporating a separately trained speaker encoder (Wan et al., 2018) in the same way as in Jia et al. (2018), and trained it on the LibriTTS corpus (Zen et al., 2019). The resulting TTS model is capable of zero-shot voice transferring, but synthesizes in a better quality and Table 1: Datasets for experiments with translation speech in a single-speaker’s voice. Conversational (Jia et al., 2019a) Fisher Es-En (Post et al., 2013) CoVoST 2 (Wang et al., 2021a) Languages es→en es→en es, fr, de, ca →en Domain Read, short-form Telephone conversation Read, short-form Source sample rate 16-48 kHz 8 kHz 48 kHz Utterance pairs 979k 120k 321k Source hours 1,400 127 476 Target hours 619 96 296 Target synthesized by Tacotron 2 + Grifﬁn-Lim Parallel WaveNet PnG NAT + WaveRNN more robust than Jia et al. (2018).2 We used this model to synthesize translation speech in the source speaker’s voice as the training targets in our experiments. Other TTS models capable of cross-lingual voice modeling, such as Zhang et al. (2019); Chen et al. (2019); Xin et al. (2021), could also be utilized. 4.2. Speaker turns Because the single attention module provides linguistic and acoustic information temporally synchronized (Sec. 3.4), Translatotron 2 is theoretically capable of voice preservation in complicated scenarios such as speaker turns. However, proper training data with speaker turns is required to demonstrate such capacity, which is difﬁcult to obtain. We propose a simple data augmentation to enable such training. ConcatAug To enable direct S2ST models to preserve each speaker’s voice for input with speaker turns, we augmented the training data by randomly sampling pairs of training examples and concatenating the source speech, the target speech, and the target phoneme sequences to construct new training examples. The resulting new examples contain two speakers’ voices in both the source and the target speech, which enables the model to learn on examples with speaker turns. See Figure 2 for an example of such concatenation and the prediction from Translatotron 2 on it. Such augmentation does not only enable the model to learn voice retention on speaker turns, but also increases the diversity of the speech content as well as the complexity of the acoustic conditions in the training examples, which may further improve the translation quality of the model, especially on small datasets (Sec. 5.1). Narayanan et al. (2019) uses a similar augmentation but in a more complicated fashion, for improving ASR performance on multi-speaker inputs.\n\n5. Experiments\nWe conducted experiments on three datasets, including two Spanish→English datasets and a multilingual→English dataset. All datasets use TTS synthesized target speech in 24 kHz sample rate. The phonemes used at training time were converted from the transcripts using a proprietary G2P system. See Table 1 for the details of each dataset. We evaluated the translation quality, naturalness and robustness of the produced translation speech, as well as speaker similarity for voice preservation. All models were implemented using the Lingvo framework (Shen et al., 2019). A comprehensive table of hyper-parameters is available in Appendix A. 5.1. Translation quality To evaluate the translation quality, we used the same two datasets as in Jia et al. (2019b), both of which have translation speech in a single female speaker’s voice. Following Jia et al. (2019b), the translation quality is measured by BLEU on ASR transcription from the translation speech (in lowercase, excluding punctuation marks except for apostrophes), compared to reference translation text. Because ASR makes errors, such BLEU can be thought of as a lower bound of the translation quality. We used an ASR model from Park et al. (2020), trained on LibriSpeech (Panayotov et al., 2015) and LibriLight (Kahn et al., 2020) corpora. For a fair comparison, we retrained the baseline Translatotron models and evaluated them using the same ASR model. The same ST→TTS cascade S2ST baselines from Jia et al. (2019b) were used and re-evaluated, which were composed of strong ST models and a Tacotron 2 TTS model. The predicted mel-spectrogram is converted to waveform using the Grifﬁn-Lim algorithm for all models. As shown in Table 2, the translation quality from Translatotron 2 outperformed the original Translatotron by +15.5 BLEU on Fisher Es-En and +5.2 BLEU on Conversational. Applying ConcatAug further improved the performance on the smaller Fisher Es-En dataset by +0.5 BLEU. These improvements narrowed down the performance gap between end-to-end direct S2ST and cascade S2ST from 16.4 / 8.4 down to 0.4 / 3.7 BLEU on the two datasets respectively. Table 2: Performance of S2ST in a single speaker’s voice. BLEU were computed with 1 reference for the Conversational test set, and with 4 references for the Fisher test set. End-to-end direct S2ST: Translatotron 2 55.6 4.21 ± 0.06 0.16 42.4 3.98 ± 0.08 0.07 + ConcatAug 55.1 4.19 ± 0.06 0.13 42.9 3.79 ± 0.09 0.14 Translatotron 50.4 4.15 ± 0.07 0.69 26.9 3.70 ± 0.08 0.48 Cascade (ST →TTS) 58.8 4.31 ± 0.06 0.21 43.3 4.04 ± 0.08 0.13 Reference (synthetic) 81.9 3.37 ± 0.09 0.43 88.6 3.95 ± 0.07 0.07 Discrete representation-based cascade S2ST: Zhang et al. (2021) (trained w/o text) - - - 9.4 - - Lee et al. (2022) (trained w/ text) - - - 39.9 3.41 ± 0.14 - ion scores (MOS) with 95% conﬁdence interval on 1,000 randomly sampled predictions. A WaveRNN-based neural vocoder was used for converting the mel-spectrograms predicted from S2ST models to waveforms. As shown in Table 2, the naturalness of the translation speech predicted from Translatotron 2 is signiﬁcantly better than from the original Translatotron, and is on-par with or very close to the cascade systems which used one of the state-of-the-art TTS models, Tacotron 2, for synthesizing translation speech from text. Consistent with Jia et al. (2019b), despite that the training targets in the Conversational dataset is synthesized with a lower quality Grifﬁn-Lim vocoder, the trained S2ST model is able to produce translation speech in signiﬁcantly higher naturalness when a higher quality neural vocoder is used at inference time. 5.3. Speech robustness We speciﬁcally evaluated the robustness issue of overgeneration in the predicted translation speech, such as babbling or long pause, measured by unaligned duration ratio (UDR) (Shen et al., 2020) with a 1-second threshold.3 The ASR transcription from the translation speech is used for alignment, using a conﬁdence islands-based forced alignment model (Chiu et al., 2018). As can be seen from Table 2, the UDR from Translatotron 2 is about 7 and 4 times lower than from Translatotron on the Fisher Es-En and Conversational datasets, respectively. It is even about 3 times lower than the training targets from the Conversational set, while is about the same as the training targets from Fisher Es-En. This can be explained by the fact that the training targets in the Conversational set were synthesized by the Tacotron 2 TTS model, which by itself Conversational Fisher Es-En BLEU MOS UDR (%) BLEU MOS UDR (%) Table 3: S2ST performance with voice preservation on Conversational dataset. Speaker similarity MOS is evaluated between Spanish source speech and English translation speech. (Numbers not directly comparable to Table 2 because of dataset differences.) BLEU Naturalness Similarity Proposed: Translatotron 2 57.3 3.24 ± 0.08 2.33 ± 0.08 + ConcatAug 56.8 2.94 ± 0.08 2.12 ± 0.07 Translatotron 48.5 2.55 ± 0.09 2.30 ± 0.07 + ConcatAug 51.3 2.76 ± 0.09 2.19 ± 0.07 Reference (synthetic) 81.3 3.40 ± 0.08 2.55 ± 0.07 Jia et al. (2019b): Translatotron 36.2 3.15 ± 0.08 1.85 ± 0.06 Reference (human) 59.9 4.10 ± 0.06 - suffered from over-generation, while the same in Fisher Es-En were synthesized by a more robust Parallel WaveNet (Oord et al., 2018) TTS model (see Table 1). The results suggest that Translatotron 2 drastically improves robustness than Translatotron, and is also robust to a small ratio of disﬂuency in the training targets. 5.4. Voice preservation To evaluate the ability of preserving speakers’ voices while translating their speeches from one language to another, we augmented the Conversational dataset by synthesizing target speech using a voice-transferring TTS model as described in Sec. 4.1. Examples with source speech shorter than 1 second were discarded for the stability of voice transferring. The result dataset contains parallel utterances with similar voices on both sides. S2ST models were trained on this dataset without any explicit conditioning on speaker embeddings or IDs (i.e. no speaker encoder for the original Translatotron). Following Jia et al. (2019b), we reduced the pre-net dimension of the synthesizer to 16 to encourage it to infer voice information from the encoder output instead of from the teacher-forcing inputs. Table 4: Voice preservation performance on speaker turns. Speaker similarity MOS between the leading/trailing 1.6-second segment from the English translation speech and the entire 1st/2nd source speaker’s Spanish speech is reported. (↑/ ↓: higher/lower values are better.) 1st source speaker 2nd source speaker Leading seg. ↑ Trailing seg. ↓ Leading seg. ↓ Trailing seg. ↑ Translatotron 2 2.22 ± 0.07 2.15 ± 0.07 2.04 ± 0.07 2.00 ± 0.07 + ConcatAug 2.44 ± 0.07 1.82 ± 0.07 1.76 ± 0.07 2.51 ± 0.08 Translatotron 1.87 ± 0.06 1.90 ± 0.07 2.06 ± 0.07 2.05 ± 0.07 + ConcatAug 2.18 ± 0.07 1.71 ± 0.06 1.93 ± 0.07 2.35 ± 0.07 Reference (synthetic) 2.58 ± 0.08 1.62 ± 0.06 1.83 ± 0.07 2.44 ± 0.07 5-point subjective MOS on both naturalness and speaker similarity was evaluated with 1,000 random samples or pairs of samples from the test set, reported with 95% conﬁdence interval. As Table 3 shows, when the proposed approach for voice preservation was used, both Translatotron 2 and Translatotron obtained about the same speaker similarity MOS as the original Translatotron but signiﬁcantly better translation quality. Translatotron 2 further outperformed Translatotron in terms of translation quality and speech naturalness, which is consistent with the experimental results for translating in a single speaker’s voice (Sec. 5.1, 5.2). It is worth to note that the speaker similarity from S2ST models is capped by the same of the training targets, which by itself is limited. This can be partially due to the performance of the voice-transferring TTS model used for synthesizing the training targets, and partially due to the fact that crosslingual speaker similarity evaluation is more challenging to raters (some rating comments are purely based on language difference), as also observed in Zhang et al. (2019). Obtaining better quality training targets, such as human recordings instead of synthesized speech, may further improve the performance of voice preservation with the proposed approach. 5.4.1. SPEAKER TURNS Speaker similarity evaluation with speaker turns on entire translation speech is challenging because it would require speaker separation on both source and target speeches. The content re-ordering during translation and translation errors would also add extra difﬁculty. We approximated by considering the leading/trailing short segments in the translation speech as corresponding to each of the two speakers in the source speech with a single speaker turn. We trained Translatotron 2 and Translatotron on the dataset described in Sec. 5.4, with half of the training examples augmented by ConcatAug. The evaluation set was artiﬁcially constructed in a similar way by applying ConcatAug, so that each utterance contains two speakers’ voices. We evaluated subjective speaker similarity MOS between the two entire source utterances before ConcatAug and the leading/trailing 1.6-second segments from the translation speech. Evaluation examples with target speech shorter than 2 seconds before ConcatAug were discarded. As can be seen from Table 4, the impact of ConcatAug is consistent on Translatotron 2 and Translatotron. When ConcatAug was not used during training, for each source speaker, the similarity compared to the leading/trailing segment from the translation speech was about the same; and for each segment in the translation speech, the speaker similarity compared to the ﬁrst/second source speaker was also close. This suggests that the translation speech imitated both source speakers at the same time regardless of the speaker turn. When ConcatAug was used, both models obtained signiﬁcantly higher speaker similarity on matched pairs than mismatched pairs, indicating that the models successfully separated two speakers and preserved voices for each of them respectively. It can also be seen that Translatotron 2 obtained signiﬁcantly higher speaker similarity than Translatotron on matched pairs, indicating the effectiveness of Translatotron 2. Such quantitative evaluation cannot reﬂect how the predicted translation speech transits from one speaker’s voice to another speaker’s. Listening to audio samples (available online) veriﬁed that the voice changed instantly on sentence boundaries without blurry, rather than a smoothed transition. A sample of S2ST on such a speaker turn from Translatotron 2 is visualized in Figure 2. While ConcatAug enables S2ST models to preserve speakers’ voices on speaker turns and improves translation quality on small datasets, it may negatively impact the speech naturalness and speaker similarity on models with strong performance, as shown in Table 2 and Table 3. It may be explained by the fact that the augmented utterances sound less natural and may involve abrupt change in volume and background noise on the artiﬁcial speaker turns. This suggests headroom for improvement. 5.5. Multilingual S2ST We also conducted experiments to evaluate the performance of multilingual X→En S2ST. We trained Translatotron 2 and Translatotron on the 4 high-resource language pairs from the CoVoST 2 corpus (Wang et al., 2021a), using Table 5: Ablation studies of multilingual X→En S2ST on 4 highresource language pairs from CoVoST 2, measured by BLEU on ASR transcription from the translation speech. + / −indicates using or replacing a component (see Sec. 5.5.1). Translatotron 2 27.0 18.8 27.7 22.5 −SpecAugment 25.9 17.9 25.9 21.8 −Conformer encoder 26.4 18.1 26.4 21.8 −NAT synthesizer 26.9 18.3 27.0 22.0 Translatotron (w/ SpecAugment) 17.7 9.9 17.7 13.1 + Conformer encoder 18.9 10.8 18.8 13.9 + NAT synthesizer 4.0 2.1 3.5 2.5 ST (Wang et al., 2021a) 27.0 18.9 28.0 23.9 Reference (synthetic) 82.1 86.0 85.1 89.3 TTS synthesized target speech in a single female speaker’s voice.4 The original Common Voice (Ardila et al., 2020) data split instead of the CoVoST 2 data split was followed. The models were not explicitly conditioned on languages. For a fair comparison, both models used SpecAugment, but did not use auxiliary supervision from the source phonemes. The translation quality as measured by BLEU on ASR transcription from the translation speech is shown in the ﬁrst rows of each block in Table 5. Translatotron 2 outperformed Translatotron by +9.4 BLEU on average on the 4 language pairs. Although the BLEU scores are not directly comparable between S2ST and ST (because of ASR transcription and BLEU calculation difference), the close numbers suggest that Translatotron 2 obtained translation quality comparable to the baseline ST model. 5.5.1. ABLATION STUDIES To understand the importance of each component in Translatotron 2, we conducted ablation studies on this multilingual X→En dataset. All models in the ablation used the same input and output features, SpecAugment settings, and learning rate schedules (detailed in Appendix A). No auxiliary supervision from source text was used. For models not using a Conformer encoder, we ﬁrst applied the same 4× temporal subsampling as in the Conformer encoder, then used a 256×8 bidirectional LSTM stack to encoder the subsampled features. The number of parameters in this LSTM encoder is close to the same in the Conformer encoder. For the Translatotron model using a NAT synthesizer, the same hyperparameters as in Translatotron 2 were used. For Translatotron 2 not using a NAT synthesizer, a non-autoregressive Conformer synthesizer (Sec. 5.6) was used. All the rest hyperparameters followed Appendix A for Translatotron 2, and Table 6: Ablation studies on Conversational dataset using an autoregressive RNN + Conv synthesizer and a non-autoregressive Conformer synthesizer. Synthesizer BLEU Naturalness fr de es ca RNN + Conv 55.6 4.21 ± 0.06 Conformer 54.5 3.61 ± 0.09 followed the Conversational model from Jia et al. (2019b) for Translatotron. All models were trained for 200K steps with a batch size of 768. The checkpoints for evaluation were picked by the best average BLEU on 4 language pairs on the validation set. The results are shown in Table 5. As can be seen, while the use of Conformer, SpecAugment, and NAT synthesizer helps the performance of Translatotron 2, replacing them with alternative architectural choices or removing SpecAugment only reduced the performance by a small degree (<2 BLEU). Similarly, directly using these components in Translatotron does not bring its performance close to Translatotron 2. These results suggest that the improvements of Translatotron 2 primarily comes from the high-level architectural design which addressed the performance bottlenecks existing in Translatotron (Sec. 3), rather than the choices of each individual component. 5.6. Non-autoregressive synthesizer It is tempting to use a non-autoregressive architecture for the synthesizer of Translatotron 2, which may signiﬁcantly reduce its inference latency, similar to recent works on nonautoregressive TTS (Ren et al., 2019; 2021; Guo et al., 2021; Lee et al., 2021b; Elias et al., 2021b;a). We experimented with using a 6-layer Conformer synthesizer (Guo et al., 2021) with a dimension of 512 and 8 attention heads on both Conversational and CoVoST 2 datasets. As can be seen from Table 5 and 6, using a Conformer-based non-autoregressive synthesizer obtained comparable translation quality to using an autoregressive NAT synthesizer (with BLEU on ASR transcription up to 1.1 BLEU lower). However, it caused a signiﬁcant regression on the naturalness of the predicted translation speech, which is consistent with the observation in TTS in Shen et al. (2020); Peng et al. (2020); Hwang et al. (2021), etc., suggesting more exploration is needed on this direction.\n\n2. Related works\nS2ST Until very recently, automatic S2ST systems are typically composed of a cascade of ASR, MT, and TTS components (Lavie et al., 1997; Wahlster, 2000; Nakamura et al., 2006; ITU, 2016). Translatotron (Jia et al., 2019b) is the ﬁrst direct S2ST model, which is a sequence-to-sequence model trained end-to-end in a multi-objective task. It has shown reasonable translation quality and speech naturalness, but still underperformed a baseline of ST →TTS cascade by a large margin. It also demonstrated the capacity of preserving speakers’ voices during the translation, by leveraging a speaker encoder separately trained in a speaker veriﬁcation task (Wan et al., 2018; Jia et al., 2018). A few recent works proposed cascade S2ST systems using learned discrete speech representation as the intermediate representation instead of text or phoneme. Tjandra et al. (2019) introduced such an S2ST system that ﬁrst translated the source speech into a discrete representation of the target speech which was predicted from a separately trained VQVAE (Oord et al., 2017), then constructed the target speech spectrogram from the discrete representation using the VQVAE decoder. Zhang et al. (2021) additionally trained the VQ-VAE jointly with a supervised phoneme recognition objective in different languages. Lee et al. (2022; 2021a) used a separately trained vocoder to directly predict waveform from the discrete representation without relying on spectrogram; for the best performance, this vocoder included a duration predictor, akin to a generative TTS model. All these works require multiple components being trained in multiple steps, but are not able to be trained end-to-end. Another potential limitation of such an approach is that it may not be effective in preserving paralinguistic and nonlinguistic information. Oppositely, it can be desired that such variation be removed in the discrete representation (Lee et al., 2021a). Kano et al. (2021) introduced an end-to-end S2ST model with a cascade of three autoregressive decoders, and used pre-trained MT and TTS models as teacher models to facilitate the training of the end-to-end model. It requires pre-trained ASR, MT, and TTS models, and the end-to-end model itself has to be trained in multiple steps. While most of these works conducted experiments using synthetic datasets with translation speech in a clean single speaker’s voice, Jia et al. (2019b); Lee et al. (2021a) conducted experiments using multi-speaker human recordings. Although these recent works generated speech translation in novel ways without relying on TTS subsystems, only a few of them (Jia et al., 2019b; Lee et al., 2022) have evaluated the perceptual quality (e.g. naturalness) of the produced speech translation, which is critical to S2ST (Wagner et al., 2019; Salesky et al., 2021), with the rest focused only on the translation quality. TTS Translatotron uses a decoder similar to the Tacotron 2 TTS model (Shen et al., 2018), which is an attention-based autoregressive decoder. Due to the ﬂexibility of the attention mechanism, they both suffer from robustness issues such as over-generation. Recent TTS models such as FastSpeech (Ren et al., 2019; 2021) and Non-Attentive Tacotron (NAT) (Shen et al., 2020) demonstrated that replacing the attention module with a duration-based upsampler yields more robust synthesized speech, as quantitatively evaluated at a large scale in Shen et al. (2020). The synthesizer component in this work resembles these works. Voice conversion and anti-spooﬁng The performance of voice conversion has progressed rapidly in the recent years, and is reaching a quality that is hard for automatic speaker veriﬁcation (ASV) systems to detect (Yi et al., 2020). ASVspoof 2019 (Todisco et al., 2019; Wang et al., 2020) found that it was challenging to detect spoof audios generated from a zero-shot voice cloning TTS model (Jia et al., 2018), which was followed by the original Translatotron for preserving speakers’ voices during S2ST. Such progress poses concerns on related techniques being misused for creating spooﬁng artifacts. We propose a new voice preservation method for S2ST with the motivation of avoiding such potential misuse.\n\n6. Conclusion\nWe proposed Translatotron 2, a neural direct S2ST model that can be trained end-to-end. Experimental results on three datasets consistently suggest that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches cascade S2ST. In addition, we proposed a simple method for preserving speakers’ voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker’s voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker’s privacy and mitigates potential misuse of voice cloning for creating spooﬁng audio artifacts. Future works include extending Translatotron 2 to support simultaneous translation, cross-lingual prosody transfer, unwritten languages, and further quality improvement by utilizing self-supervised pre-training (Baevski et al., 2020; Wang et al., 2021b) and weakly supervised data (Jia et al., 2019a).\n\nAcknowledgements The authors would like to thank Chung-Cheng Chiu, Quan Wang, Heiga Zen, Ron J. Weiss, Wolfgang Macherey, Yu Zhang, Yonghui Wu, Hadar Shemtov, Ruoming Pang, Nadav Bar, Michael Hassid, and the rest of the Google Research team for helpful discussions and previous work on data preparation.\n\nReferences\nAnastasopoulos, A., Bojar, O., Bremerman, J., Cattoni, R., Elbayad, M., Federico, M., Ma, X., Nakamura, S., Negri, M., Niehues, J., et al. Findings of the IWSLT 2021 evaluation campaign. In International Conference on Spoken Language Translation (IWSLT), 2021. Ansari, E., Axelrod, A., Bach, N., Bojar, O., Cattoni, R., Dalvi, F., Durrani, N., Federico, M., Federmann, C., Gu, J., et al. Findings of the IWSLT 2020 evaluation campaign. In International Conference on Spoken Language Translation (IWSLT), 2020. Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common Voice: A massively-multilingual speech corpus. In Proceedings of Language Resources and Evaluation Conference (LREC), 2020. Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Battenberg, E., Skerry-Ryan, R., Mariooryad, S., Stanton, D., Kao, D., Shannon, M., and Bagby, T. Locationrelative attention mechanisms for robust long-form speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Chen, M., Chen, M., Liang, S., Ma, J., Chen, L., Wang, S., and Xiao, J. Cross-lingual, multi-speaker text-tospeech synthesis using neural speaker embedding. In Proc. Interspeech, 2019. Chiu, C.-C., Tripathi, A., Chou, K., Co, C., Jaitly, N., Jaunzeikare, D., Kannan, A., Nguyen, P., Sak, H., Sankar, A., Tansuwan, J., Wan, N., Wu, Y., and Zhang, X. Speech recognition for medical conversations. In Proc. Interspeech, 2018. Di Gangi, M. A., Negri, M., and Turchi, M. One-to-many multilingual end-to-end speech translation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Skerry-Ryan, R., and Wu, Y. Parallel Tacotron 2: A non-autoregressive neural TTS model with differentiable duration modeling. In Proc. Interspeech, 2021a. Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Weiss, R., and Wu, Y. Parallel Tacotron: Non-autoregressive and controllable TTS. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021b. Grifﬁn, D. and Lim, J. Signal estimation from modiﬁed short-time Fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(2):236–243, 1984. Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang,\nR. Conformer: Convolution-augmented transformer for\nspeech recognition. In Proc. Interspeech, 2020. Guo, P., Boyer, F., Chang, X., Hayashi, T., Higuchi, Y., Inaguma, H., Kamo, N., Li, C., Garcia-Romero, D., Shi, J., et al. Recent developments on espnet toolkit boosted by conformer. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5874– 5878, 2021. He, M., Deng, Y., and He, L. Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural TTS. In Proc. Interspeech, 2019. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. Hwang, M.-J., Yamamoto, R., Song, E., and Kim, J.-M. TTS-by-TTS: TTS-driven data augmentation for fast and high-quality speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6598–6602, 2021. ITU. ITU-T F.745: Functional requirements for networkbased speech-to-speech translation services, 2016. International Telecommunication Union. Jia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., Ren, F., Chen, Z., Nguyen, P., Pang, R., Moreno, I. L., and Wu, Y. Transfer learning from speaker veriﬁcation to multispeaker text-to-speech synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Jia, Y., Johnson, M., Macherey, W., Weiss, R. J., Cao, Y., Chiu, C.-C., Ari, N., Laurenzo, S., and Wu, Y. Leveraging weakly supervised data to improve end-to-end speechto-text translation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019a. Jia, Y., Weiss, R. J., Biadsy, F., Macherey, W., Johnson, M., Chen, Z., and Wu, Y. Direct speech-to-speech translation with a sequence-to-sequence model. In Proc. Interspeech, 2019b. Jia, Y., Zen, H., Shen, J., Zhang, Y., and Wu, Y. PnG BERT: Augmented BERT on phonemes and graphemes for neural TTS. In Proc. Interspeech, 2021. Jia, Y., Tadmor Ramanovich, M., Wang, Q., and Zen, H. CVSS corpus and massively multilingual speechto-speech translation. In Proceedings of Language Resources and Evaluation Conference (LREC), 2022. Kahn, J., Rivi`ere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazar´e, P.-E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. Libri-light: A benchmark for ASR with limited or no supervision. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A. v. d., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural audio synthesis. In Proceedings of International Conference on Machine Learning (ICML), 2018. Kano, T., Sakti, S., and Nakamura, S. Transformer-based direct speech-to-speech translation with transcoder. In IEEE Spoken Language Technology Workshop (SLT), 2021. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. Krueger, D., Maharaj, T., Kram´ar, J., Pezeshki, M., Ballas, N., Ke, N. R., Goyal, A., Bengio, Y., Courville, A., and Pal, C. Zoneout: Regularizing RNNs by randomly preserving hidden activations. In International Conference on Learning Representations (ICLR), 2017. Lavie, A., Waibel, A., Levin, L., Finke, M., Gates, D., Gavalda, M., Zeppenfeld, T., and Zhan, P. JANUS-III: Speech-to-speech translation in multiple languages. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1997. Lee, A., Gong, H., Duquenne, P.-A., Schwenk, H., Chen, P.-J., Wang, C., Popuri, S., Pino, J., Gu, J., and Hsu, W.-N. Textless speech-to-speech translation on real data. arXiv preprint arXiv:2112.08352, 2021a. Lee, A., Chen, P.-J., Wang, C., Gu, J., Ma, X., Polyak, A., Adi, Y., He, Q., Tang, Y., Pino, J., et al. Direct speechto-speech translation with discrete units. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), 2022. Lee, Y., Shin, J., and Jung, K. Bidirectional variational inference for non-autoregressive text-to-speech. In International Conference on Learning Representations (ICLR), 2021b. Ma, X., Gong, H., Liu, D., Lee, A., Tang, Y., Chen, P.-J., Hsu, W.-N., Heaﬁeld, K., Koehn, P., and Pino, J. Direct simultaneous speech to speech translation. arXiv preprint arXiv:2110.08250, 2021. McCarthy, A. D., Puzon, L., and Pino, J. SkinAugment: Auto-encoding speaker conversions for automatic speech translation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Nakamura, S., Markov, K., Nakaiwa, H., Kikui, G., Kawai, H., Jitsuhiro, T., Zhang, J.-S., Yamamoto, H., Sumita, E., and Yamamoto, S. The ATR multilingual speech-tospeech translation system. IEEE Transactions on Audio, Speech, and Language Processing, 2006. Narayanan, A., Prabhavalkar, R., Chiu, C.-C., Rybach, D., Sainath, T. N., and Strohman, T. Recognizing long-form speech using streaming end-to-end models. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., Driessche, G., Lockhart, E., Cobo, L., Stimberg, F., et al. Parallel WaveNet: Fast highﬁdelity speech synthesis. In Proceedings of International Conference on Machine Learning (ICML), 2018. Oord, A. v. d., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. LibriSpeech: an ASR corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. SpecAugment: A simple data augmentation method for automatic speech recognition. In Proc. Interspeech, 2019. Park, D. S., Zhang, Y., Jia, Y., Han, W., Chiu, C.-C., Li, B., Wu, Y., and Le, Q. V. Improved noisy student training for automatic speech recognition. In Proc. Interspeech, 2020. Pathak, M. A. and Raj, B. Privacy-preserving speaker veriﬁcation and identiﬁcation using Gaussian mixture models. IEEE Transactions on Audio, Speech, and Language Processing, 21(2):397–406, 2012. Peng, K., Ping, W., Song, Z., and Zhao, K. Nonautoregressive neural text-to-speech. In Proceedings of International Conference on Machine Learning (ICML), 2020. Post, M., Kumar, G., Lopez, A., Karakos, D., CallisonBurch, C., and Khudanpur, S. Improved speech-to-text translation with the Fisher and Callhome Spanish–English speech translation corpus. In International Conference on Spoken Language Translation (IWSLT), 2013. Ren, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. FastSpeech: Fast, robust and controllable text to speech. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Ren, Y., Hu, C., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. FastSpeech 2: Fast and high-quality end-to-end text-tospeech. In International Conference on Learning Representations (ICLR), 2021. Salesky, E., M¨ader, J., and Klinger, S. Assessing evaluation metrics for speech-to-speech translation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021. Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomyrgiannakis, Y., and Wu, Y. Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. Shen, J., Nguyen, P., Wu, Y., Chen, Z., et al. Lingvo: A modular and scalable framework for sequence-to-sequence modeling. arXiv preprint arXiv:1902.08295, 2019. Shen, J., Jia, Y., Chrzanowski, M., Zhang, Y., Elias, I., Zen, H., and Wu, Y. Non-Attentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling. arXiv preprint arXiv:2010.04301, 2020. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818–2826, 2016. Tjandra, A., Sakti, S., and Nakamura, S. Speech-to-speech translation between untranscribed unknown languages. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Todisco, M., Wang, X., Vestman, V., Sahidullah, M., Delgado, H., Nautsch, A., Yamagishi, J., Evans, N., Kinnunen, T., and Lee, K. A. ASVspoof 2019: Future horizons in spoofed and fake audio detection. In Proc. Interspeech, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Wagner, P., Beskow, J., Betz, S., Edlund, J., Gustafson, J., Eje Henter, G., Le Maguer, S., Malisz, Z., Sz´ekely, ´E., T˚annander, C., et al. Speech synthesis evaluation – stateof-the-art assessment and suggestion for a novel research program. In Proceedings of the 10th Speech Synthesis Workshop (SSW10), 2019. Wahlster, W. Verbmobil: Foundations of speech-to-speech translation. Springer, 2000. Wan, L., Wang, Q., Papir, A., and Moreno, I. L. Generalized end-to-end loss for speaker veriﬁcation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. Wang, C., Wu, A., and Pino, J. CoVoST 2: A massively multilingual speech-to-text translation corpus. In Proc. Interspeech, 2021a. Wang, C., Wu, A., Pino, J., Baevski, A., Auli, M., and Conneau, A. Large-scale self-and semi-supervised learning for speech translation. In Proc. Interspeech, 2021b. Wang, X., Yamagishi, J., Todisco, M., Delgado, H., Nautsch, A., Evans, N., Sahidullah, M., Vestman, V., Kinnunen, T., Lee, K. A., et al. ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech. Computer Speech & Language, 64:101114, 2020. Weiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., and Chen,\nZ. Sequence-to-sequence models can directly translate\nforeign speech. In Proc. Interspeech, 2017. for cross-lingual TTS. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021. Yi, Z., Huang, W.-C., Tian, X., Yamagishi, J., Das, R. K., Kinnunen, T., Ling, Z., and Toda, T. Voice conversion challenge 2020: Intra-lingual semi-parallel and crosslingual voice conversion. In Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge, 2020. Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. Interspeech, 2019. Zhang, C., Tan, X., Ren, Y., Qin, T., Zhang, K., and Liu, T.-\nY. UWSpeech: Speech to speech translation for unwritten\nlanguages. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021. Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Chen, Z., SkerryRyan, R., Jia, Y., Rosenberg, A., and Ramabhadran, B. Learning to speak ﬂuently in a foreign language: Multilingual speech synthesis and cross-language voice cloning. In Proc. Interspeech, 2019. Zheng, Y., Wang, X., He, L., Pan, S., Soong, F. K., Wen, Z., and Tao, J. Forward-backward decoding for regularizing end-to-end TTS. In Proc. Interspeech, 2019.\n\nA. Table of hyper-parameters\nTable 7: Model hyper-parameters used in the experiments. (“×n”: n layers; †: 128-dim pre-net is used for translating in a single voice; 16-dim pre-net is used for voice preservation.) Input Sample rate (Hz) 8,000 48,000 16,000 – 48,000 Mel channels 80 Mel lower band (Hz) 125 Mel upper band (Hz) 3,800 7,600 7,600 Frame size (ms) 25.0 Frame step (ms) 10.0 Output Sample rate (Hz) 24,000 Mel channels 128 Mel lower band (Hz) 20 Mel upper band (Hz) 12,000 Frame size (ms) 50.0 Frame step (ms) 12.5 SpecAugment Freq blocks 2 Time blocks 10 Freq block max length ratio 0.33 Time block max length ratio 0.05 Encoder Conformer dims 144 × 16 Attention heads 4 Conv kernal size 32 Subsample factor 4 Attention Output dim 256 512 512 Hidden dim 512 512 512 Attention heads 4 8 8 Dropout prob 0.1 0.2 0.2 Decoder LSTM dims 256 × 4 512 × 6 512 × 4 Zoneout prob 0.1 0.1 0.1 Phoneme embedding dim 96 256 256 Label smoothing uncertainty 0.1 0.1 0.1 Loss weight 10.0 10.0 10.0 Duration predictor Bi-LSTM (dim × layers) 64 × 2 128 × 2 128 × 2 Loss weight 1.0 1.0 1.0 Synthesizer LSTM dims 1,024 × 2 LSTM zoneout prob 0.1 Pre-net dims 128 × 2 128 × 2 128 / 16 † × 2 Pre-net dropout prob 0.5 Post-net (kernel, channels) × layers (5, 512) × 4 + (5, 128) Loss weight 1.0 Training Optimizer Adam (Kingma & Ba, 2015) Learning rate schedule Vaswani et al. (2017) Learning rate (peak) 4.2×10−3 2.2×10−3 3.3×10−3 Warm-up steps Batch size 1,024 768 768 L2 regularization weight 10−6 10−6 10−6 Fisher Es-En CoVoST 2 Conversational src vs src tgt vs src tgt vs tgt s2st vs src s2st vs tgt Figure 3: Afﬁnity matrices of d-vector similarity among 100 random examples. (“s2st” refers to the predictions from Translatotron 2.) Table 8: Objective d-vector similarity between the predicted translated speech (English) and the source human speech (Spanish) on speaker turns. The similarity between the leading/trailing 1.6-second segment from the predicted speech and the entire 1st/2nd source speaker’s speech is measured. (↑/ ↓means higher/lower values are better.) 1st source speaker 2nd source speaker Leading seg. ↑ Trailing seg. ↓ Leading seg. ↓ Trailing seg. ↑ Translatotron 2 0.21 0.19 0.21 0.19 + ConcatAug 0.20 0.14 0.14 0.21 Translatotron 0.20 0.22 0.27 0.29 + ConcatAug 0.32 0.16 0.14 0.35 Reference (synthetic) 0.48 0.17 0.15 0.48\nB. Objective speaker similarity analysis\nSubjective speaker similarity evaluation is costly and has a long turnaround. We explored alternative objective evaluation using separately trained speaker encoders, such as d-vector (Wan et al., 2018). We evaluated the voice retention performance using the cosine similarity of the d-vectors. We ﬁrst checked the scenario that each input contains a single speaker’s recording. Figure 3 visualizes the afﬁnity matrices of d-vector similarity among different input utterances for a Translatotron 2 model. The outstanding higher similarity values on the diagonals indicate that the model is able to preserve the source speaker’s voice in the predicted translation speech. We then conducted a detailed evaluation for the voice retention performance for speaker turns. The experiment setting up was identical to Section 5.4.1, except that the speaker similarity was measured by d-vector similarity instead of subjective MOS evaluation. The d-vectors for each source speaker were computed on the entire original utterance before concatenation; the d-vectors for each speaker in the prediction is approximated by computing on the leading/trailing 1.6 seconds of predicted speech. The results are shown in Table 8. Consistent with the MOS evaluation results in Table 4, when the concatenation augmentation was not used, the d-vector similarity to each source speaker is about the same regardless if it was compared to the leading or trailing segments, indicating that the predicted speech was in a single speaker’s voice and the model was unable to separate different speakers in the input, but rather optimized for both source speakers at the same time. When the concatenation augmentation was used, the d-vector similarity was signiﬁcantly higher between matched pairs than between unmatched pairs, indicating that the models were able to separate different speakers in the input and preserve their voices in the predicted translation speech respectively. However, when these similarities are compared among different models, it seems to suggest that Translatotron performed better than Translatotron 2, which is contradictory to the subjective evaluation results in Table 4. By carefully listening to the audio samples, we found that such discrepancy may be due to the fact that the d-vector model was also sensitive to non-voice related acoustic characteristics, such as reverb and channel noise in the audios. This is likely a consequence of the fact that in the large-scale training set for the d-vector model used in the evaluation, each speaker is typically associated with a particular recording condition, e.g. recording device and room. Because the encoder output from the Translatotron model was of signiﬁcantly larger dimension than from the Translatotron 2 model (2048 vs 144), it was capable of carrying more non-voice acoustic information and thus obtained better d-vector similarity, which not necessarily indicating higher speaker similarity. These results suggest that while such speaker encoder-based objective analysis reveals insightful indications about the performance of the S2ST models, it can be less reliable compared to subjective MOS evaluation. Such reliability also highly depends on the training details of the speaker encoder model being used, especially the training corpus.\n--- FOOTNOTES 1Google Research. Correspondence to: 10K 20K 10K", "persona": "Developer", "response": "### 1. Overview\nAudio samples from \"Translatotron 2: High-quality direct speech-to-speech translation with voice preservation\" url: arXiv Abstract: We present Translatotron 2, a neural direct speech-to-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module that connects them together. Experimental results on three datasets consistently show that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade systems. In addition, we propose a simple method for preserving speakers' voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker's voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker's privacy and mitigates potential misuse of voice cloning for creating spoofing audio artifacts. Authors: Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, Roi Pomerantz.\n\n### 2. Background\nContents In some cases (e.g. group 2), the TTS-synthesized references (and training targets) fail to transfer the source speakers' voices. As a result, the trained S2ST models also make similar mistakes. See also the samples in the next section. The source audios are the concatenation of two randomly sampled human recordings; the reference audios are the concatenation of the corresponding TTS synthesized reference audios. The model predictions are the direct outputs from the models on the concatenated source input, without extra pre-/post-processing. The transcripts for the source and reference are concatenation of the ground truth from the dataset (each segment in a pair of quotation marks); the transcripts on the model predictions were transcribed by an ASR model used for evaluation.\n\n### 3. Methods\nSource (Spanish) Reference (English) Translatotron 2 (w/ ConcatAug) Translatotron 2 Translatotron (w/ ConcatAug) Translatotron These audio samples were randomly sampled from the evaluation in Table 5, corresponding to Section 5.5 in the url. The S2ST models were trained on the dataset derived from CoVoST 2, and are able to translate French, German, Spanish and Catalan speech into English speech in a canonical voice. Reference audios were synthesized with a TTS model. Transcripts for the sources and references are ground truth from the dataset; transcripts for the model predictions were transcribed by an ASR model used for evaluation.\n\n### 4. Results\nSpanish-to-English (on Conversational dataset) Reference audios were synthesized using a TTS model with zero-shot crosslingual voice transfer capacity (see Section 4.1 in the url). Transcripts for the source and reference are ground truth from the dataset; transcripts for the model prediction were transcribed by an ASR model used for evaluation (see Section 5.1 in the url). Multilingual X-to-English (on CoVoST 2 dataset) These audio samples were randomly sampled from the evaluation corresponding to Table 2 and Table 3 in the url. The S2ST models were trained on the Conversational Spanish-to-English dataset.\n\n### 5. Takeaways\nFor both Translatotron 2 and Translatotron, we include one variant that outputs in a canonical female speaker's voice, and another variant that preserves the source speaker's voice to the translation speech. Ground truth Translatotron 2 Translatotron Cascade (ST → TTS) Source (Spanish) Reference (English) Canonical voice Voice preserved Canonical voice Voice preserved Canonical voice Voice preservation on speaker turns (on Conversational dataset) These samples show that when ConcatAug is used during training, both Translatotron 2 and Translatotron are able to preserve each speaker's voice on inputs with speaker turns; in contrast, when ConcatAug is not used, the predicted audio is typically in one input speaker's voice, and some times have trouble on handling the entire input for translation (e.g. group 3 and 4). In either case, the prediction from Translatotron 2 is significantly more natural, more fluent, and more complete than the same from Translatotron. It is interesting that in group 5, despite that the TTS synthesized reference makes mistake on the first speaker's voice (incorrect gender), Translatotron 2 (w/ ConcatAug) is able to predict in voices more similar to the source (correct gender).", "paper_id": "29", "story_id": "29-02"}
{"paper": "Translatotron 2: High-quality direct speech-to-speech translation with voice preservation Ye Jia 1 Michelle Tadmor Ramanovich 1 Tal Remez 1 Roi Pomerantz 1\n\nAbstract\narXiv:2107.08661v5  [cs.CL]  17 May 2022 We present Translatotron 2, a neural direct speechto-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module that connects them together. Experimental results on three datasets consistently show that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade systems. In addition, we propose a simple method for preserving speakers’ voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker’s voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker’s privacy and mitigates potential misuse of voice cloning for creating spoofing audio artifacts.\n\n1. Introduction\nSpeech-to-speech translation (S2ST) is highly beneﬁcial for breaking down communication barriers between people not sharing a common language. Conventional automatic S2ST systems are composed of a cascade of three components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis (Lavie et al., 1997; Wahlster, 2000; Nakamura et al., 2006). In the past few years, direct speech-to-text translation (ST) is rapidly emerging, and has outperformed the cascade of ASR and MT (Weiss et al., 2017; Jia et al., 2019a; Di Gangi et al., 2019; McCarthy et al., 2020; Ansari et al., 2020; Wang et al., 2021b; Anastasopoulos et al., 2021), which makes the Ye Jia <jiaye@google.com>. cascade of ST and TTS as S2ST feasible (Jia et al., 2019b). Recently, works on S2ST without relying on intermediate text representation are emerging, such as end-to-end direct S2ST (Jia et al., 2019b; Kano et al., 2021) and cascade S2ST based on discrete speech representation (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022; 2021a; Ma et al., 2021). Compared to text-centric cascaded systems, although such approaches require parallel S2ST data, which is scarce, they have the potential advantages of: 1) Preserving paralinguistic and non-linguistic information during translation, such as speaker’s voice (Jia et al., 2019b), emotion and prosody; 2) Supporting languages without written form, or being able to be trained without transcription of speech (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022; 2021a); 3) Reduced computational requirements and lower inference latency (Lee et al., 2022); 4) Avoiding error compounding across sub-systems (Jia et al., 2022); 5) Easier on handling contents that do not need to be translated, such as names and proper nouns (Jia et al., 2019b). Among these works, Translatotron (Jia et al., 2019b) is the ﬁrst model that is able to directly translate speech in one language to speech in another language. It obtained reasonable translation quality and high naturalness in the predicted translation speech, and is able to preserve speakers’ voices during the speech translation. However, the translation quality from Translatotron still underperforms cascade baselines by a large margin, and the translation speech it produces suffers from over-generation issues, such as babbling and long pause. Such weaknesses make this model not yet practical for production. Nevertheless, it remains the state-of-the-art of end-to-end direct S2ST. In this paper, we ﬁrst tackle the performance gap between end-to-end direct S2ST and cascade S2ST. We propose Translatotron 2, a novel direct S2ST model that is able to be trained end-to-end. We conduct experiments on three S2ST datasets, including multilingual S2ST. The results consistently suggest that Translatotron 2 signiﬁcantly outperforms Translatotron in terms of both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade S2ST. When a simple data augmentation ConcatAug is used, the translation quality gap on the Fisher Spanish-English corpus (Post et al., 2013) is reduced from 16.4 to 0.4 BLEU. These results are the ﬁrst time that end-to-end direct S2ST approaches cascade S2ST. In addition, we propose a simple method for preserving speakers’ voices during S2ST without relying on any speaker representation (ID or embedding). The proposed method enables Translatotron 2 to preserve each speaker’s voice on speaker turns without requiring for speaker separation, which is the ﬁrst of its kind. Furthermore, compared to existing approaches of voice preservation, the proposed method better preserves speaker’s privacy (Pathak & Raj, 2012) and mitigates potential misuse of voice cloning for creating spooﬁng audio artifacts. Audio samples from Translatotron 2 are available online.1\n\n3. Translatotron 2\nWe designed the architecture of Translatotron 2 to address three performance bottlenecks existing in the original Translatotron: 1) The utilization of the auxiliary textual supervision during training is suboptimal, namely, the attention alignment learned by the auxiliary ST task does not directly contribute to the main S2ST task; 2) The challenge posed by modeling the translation alignment between two very (English) spectrogram phoneme (English) Synthesizer Decoder autoregression Attention Encoder spectrogram (Spanish) (a) Overview of Translatotron 2. Figure 1: A Translatotron 2 model that translates Spanish speech into English speech. long spectrogram sequences using the attention mechanism; 3) Attention-based speech generation is known to suffer from robustness issues such as over-generation and undergeneration (Shen et al., 2020; Ren et al., 2019; He et al., 2019; Zheng et al., 2019; Battenberg et al., 2020). We addressed these bottlenecks by designing a novel S2ST model architecture composed of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module connecting them together (Figure 1a). The model is jointly trained with a speech-to-speech translation objective and a speech-to-phoneme translation objective. The following subsections describe each component of Translatotron 2. Note that as shown in the ablation studies in Sec. 5.5.1, while the speciﬁc architectural choices of these components help the performance of Translatotron 2, the primary improvement comes from the high-level architecture rather than the choice of each individual component. 3.1. Speech encoder The encoder of Translatotron 2 takes the mel-spectrogram of the source speech as the input, and produces a hidden representation which encodes both linguistic and acoustic information from the source speech. We use Conformer (Gulati et al., 2020) as the architecture of the encoder. It ﬁrst subsamples the input mel-spectrogram with a convolutional layer, and then processes it with a stack of Conformer blocks. Each Conformer block is composed of a feed-forward layer, a self-attention layer, a convolution layer, and a second feedforward layer. SpecAugment (Park et al., 2019) is applied at the training time as data augmentation. spectrogram RNN + Conv autoregression durations Upsampler Duration Predictor concat(hidden, attention) (b) Synthesizer of Translatotron 2. 3.2. Linguistic decoder The autoregressive decoder is responsible for producing linguistic information in the translation speech. It takes the encoder output through the attention module, and predicts a phoneme sequence corresponding to the translation speech. We use an LSTM stack (Hochreiter & Schmidhuber, 1997) as the decoder, assisted with regularization including Zoneout (Krueger et al., 2017) and label smoothing (Szegedy et al., 2016). The combination of the encoder, the decoder, and the attention module is similar to a typical ST model, except that it predicts phonemes instead of subword tokens. 3.3. Acoustic synthesizer The synthesizer is responsible for acoustic generation of the translation speech. It takes the intermediate output from the decoder (before a ﬁnal projection and softmax for phoneme prediction), as well as the context output from the attention as its input, and generates a mel-spectrogram corresponding to the translation speech. It is similar to the decoders in typical neural TTS models. The predicted mel-spectrogram can be converted to waveform using an estimation algorithm such as Grifﬁn & Lim (1984) or a neural vocoder such as WaveRNN (Kalchbrenner et al., 2018). We use the duration-based autoregressive synthesizer from the NAT (Shen et al., 2020) TTS model (Figure 1b). It ﬁrst predicts durations for each elements in the input sequence, then temporally upsamples the input sequence based on the predicted durations. After that, an LSTM stack is used for generating the target spectrogram without altering the sequence length. A ﬁnal residual convolutional block further reﬁnes the generated spectrogram. Unlike in NAT, we do not supervise the duration prediction on per-phoneme duration labels, to avoid additional requirement on the training data. Instead, an L2 loss on the total predicted duration of the entire utterance is used (similar to the “na¨ıve approach” of unsupervised duration modeling in Shen et al. (2020)). 3.4. A single attention It is critical that Translatotron 2 utilizes a single attention module for both the linguistic decoder and the acoustic synthesizer. This attention models both linguistic and acoustic alignments between the source and the target speeches. A multi-head attention (Vaswani et al., 2017) is used. The queries to this attention are from the linguistic decoder. As a result, unlike in the original Translatotron, this attention does not directly model the translation alignment between two very long spectrogram sequences. Instead, it models the alignment between a source spectrogram sequence and a shorter target phoneme sequence, which is signiﬁcantly easier to learn. In the meantime, the attention provides acoustic information from the source speech to the synthesizer, summarized at per-phoneme level. Such summarized acoustic information is not only usually sufﬁcient for speech generation but also eases the duration prediction per-phoneme because it is of the same granularity. Because a single attention is used, the linguistic and acoustic information seen by the synthesizer is synchronized temporally. Such synchronization enables Translatotron 2 to preserve paralinguistic and nonlinguistic information at ﬁne granularity, such as preserving each speaker’s voice on speaker turns (Sec. 4.2). Although the synthesizer takes attention output as part of its input, the attention is not driven (i.e. queried) by the synthesizer. As a result, while it beneﬁts from the attention on obtaining aligned acoustic information from the source speech, it does not suffer from robustness issues as in typical attention-based speech synthesis models.\n\n4. Voice preserving\nThe original Translatotron (Jia et al., 2019b) demonstrated the capacity of preserving source speakers’ voices in the translation speech, by conditioning its synthesizer on a speaker embedding generated from a separately trained speaker encoder. In fact, it is capable of generating the translation speech in a different speaker’s voice, as long as a clip of the target speaker’s recording is used as the reference audio to the speaker encoder, or the embedding of the target speaker is directly available. While this is impressively powerful, it can potentially be misused for generating spooﬁng audio with arbitrary content, posing a concern for production deployment. Source: \"¿Por qué elegiste hacer esto?\", \"de 24 fuentes en 12 países\" 120 10 100 5 80 60 0 40 20 5 0 2 4 6 8 0 Reference: \"Why did you choose to do this?\", \"from 24 sources in 12 countries\" 7.5 120 5.0 100 2.5 80 60 0.0 40 2.5 20 5.0 0 1 2 3 4 5 0 Prediction: \"why did you choose to do this from twenty four sources in twelve countries\" 120 5.0 100 2.5 80 0.0 60 2.5 40 5.0 20 0 7.5 0 1 2 3 4 5 Time (sec) Figure 2: Sample mel-spectrograms on input with speaker turns. The input speech includes an utterance from a male speaker followed by another utterance from a female speaker. Translatotron 2 preserves the voices of each speaker in the translation speech. preserving speaker’s voice during S2ST, so that the trained models are restricted to preserving the source speaker’s voice, but not able to generate speech in a different speaker’s voice. In addition, this approach enables S2ST models to preserve each speaker’s voice on input speech with speaker turns, without requiring for speaker segmentation. 4.1. Training-time voice transferring In our approach, the key to restrict S2ST models to voice preservation but not arbitrary voice cloning (from a different speaker) is to move the powerful voice transferring to only happen at the training time (or the training data preparation time) but not the inference time. In contrast, it happens at both the training time and the inference time in the original Translatotron. To preserve speakers’ voices across translation, we train S2ST models on parallel utterances with the same speaker’s voice on both sides. Such a dataset with human recordings on both sides is extremely difﬁcult to collect, because it requires a large number of ﬂuent bilingual speakers. Instead, we use a TTS model capable of cross-lingual voice cloning to synthesize such training targets. We modiﬁed the PnG NAT (Jia et al., 2021; Shen et al., 2020) TTS model by incorporating a separately trained speaker encoder (Wan et al., 2018) in the same way as in Jia et al. (2018), and trained it on the LibriTTS corpus (Zen et al., 2019). The resulting TTS model is capable of zero-shot voice transferring, but synthesizes in a better quality and Table 1: Datasets for experiments with translation speech in a single-speaker’s voice. Conversational (Jia et al., 2019a) Fisher Es-En (Post et al., 2013) CoVoST 2 (Wang et al., 2021a) Languages es→en es→en es, fr, de, ca →en Domain Read, short-form Telephone conversation Read, short-form Source sample rate 16-48 kHz 8 kHz 48 kHz Utterance pairs 979k 120k 321k Source hours 1,400 127 476 Target hours 619 96 296 Target synthesized by Tacotron 2 + Grifﬁn-Lim Parallel WaveNet PnG NAT + WaveRNN more robust than Jia et al. (2018).2 We used this model to synthesize translation speech in the source speaker’s voice as the training targets in our experiments. Other TTS models capable of cross-lingual voice modeling, such as Zhang et al. (2019); Chen et al. (2019); Xin et al. (2021), could also be utilized. 4.2. Speaker turns Because the single attention module provides linguistic and acoustic information temporally synchronized (Sec. 3.4), Translatotron 2 is theoretically capable of voice preservation in complicated scenarios such as speaker turns. However, proper training data with speaker turns is required to demonstrate such capacity, which is difﬁcult to obtain. We propose a simple data augmentation to enable such training. ConcatAug To enable direct S2ST models to preserve each speaker’s voice for input with speaker turns, we augmented the training data by randomly sampling pairs of training examples and concatenating the source speech, the target speech, and the target phoneme sequences to construct new training examples. The resulting new examples contain two speakers’ voices in both the source and the target speech, which enables the model to learn on examples with speaker turns. See Figure 2 for an example of such concatenation and the prediction from Translatotron 2 on it. Such augmentation does not only enable the model to learn voice retention on speaker turns, but also increases the diversity of the speech content as well as the complexity of the acoustic conditions in the training examples, which may further improve the translation quality of the model, especially on small datasets (Sec. 5.1). Narayanan et al. (2019) uses a similar augmentation but in a more complicated fashion, for improving ASR performance on multi-speaker inputs.\n\n5. Experiments\nWe conducted experiments on three datasets, including two Spanish→English datasets and a multilingual→English dataset. All datasets use TTS synthesized target speech in 24 kHz sample rate. The phonemes used at training time were converted from the transcripts using a proprietary G2P system. See Table 1 for the details of each dataset. We evaluated the translation quality, naturalness and robustness of the produced translation speech, as well as speaker similarity for voice preservation. All models were implemented using the Lingvo framework (Shen et al., 2019). A comprehensive table of hyper-parameters is available in Appendix A. 5.1. Translation quality To evaluate the translation quality, we used the same two datasets as in Jia et al. (2019b), both of which have translation speech in a single female speaker’s voice. Following Jia et al. (2019b), the translation quality is measured by BLEU on ASR transcription from the translation speech (in lowercase, excluding punctuation marks except for apostrophes), compared to reference translation text. Because ASR makes errors, such BLEU can be thought of as a lower bound of the translation quality. We used an ASR model from Park et al. (2020), trained on LibriSpeech (Panayotov et al., 2015) and LibriLight (Kahn et al., 2020) corpora. For a fair comparison, we retrained the baseline Translatotron models and evaluated them using the same ASR model. The same ST→TTS cascade S2ST baselines from Jia et al. (2019b) were used and re-evaluated, which were composed of strong ST models and a Tacotron 2 TTS model. The predicted mel-spectrogram is converted to waveform using the Grifﬁn-Lim algorithm for all models. As shown in Table 2, the translation quality from Translatotron 2 outperformed the original Translatotron by +15.5 BLEU on Fisher Es-En and +5.2 BLEU on Conversational. Applying ConcatAug further improved the performance on the smaller Fisher Es-En dataset by +0.5 BLEU. These improvements narrowed down the performance gap between end-to-end direct S2ST and cascade S2ST from 16.4 / 8.4 down to 0.4 / 3.7 BLEU on the two datasets respectively. Table 2: Performance of S2ST in a single speaker’s voice. BLEU were computed with 1 reference for the Conversational test set, and with 4 references for the Fisher test set. End-to-end direct S2ST: Translatotron 2 55.6 4.21 ± 0.06 0.16 42.4 3.98 ± 0.08 0.07 + ConcatAug 55.1 4.19 ± 0.06 0.13 42.9 3.79 ± 0.09 0.14 Translatotron 50.4 4.15 ± 0.07 0.69 26.9 3.70 ± 0.08 0.48 Cascade (ST →TTS) 58.8 4.31 ± 0.06 0.21 43.3 4.04 ± 0.08 0.13 Reference (synthetic) 81.9 3.37 ± 0.09 0.43 88.6 3.95 ± 0.07 0.07 Discrete representation-based cascade S2ST: Zhang et al. (2021) (trained w/o text) - - - 9.4 - - Lee et al. (2022) (trained w/ text) - - - 39.9 3.41 ± 0.14 - ion scores (MOS) with 95% conﬁdence interval on 1,000 randomly sampled predictions. A WaveRNN-based neural vocoder was used for converting the mel-spectrograms predicted from S2ST models to waveforms. As shown in Table 2, the naturalness of the translation speech predicted from Translatotron 2 is signiﬁcantly better than from the original Translatotron, and is on-par with or very close to the cascade systems which used one of the state-of-the-art TTS models, Tacotron 2, for synthesizing translation speech from text. Consistent with Jia et al. (2019b), despite that the training targets in the Conversational dataset is synthesized with a lower quality Grifﬁn-Lim vocoder, the trained S2ST model is able to produce translation speech in signiﬁcantly higher naturalness when a higher quality neural vocoder is used at inference time. 5.3. Speech robustness We speciﬁcally evaluated the robustness issue of overgeneration in the predicted translation speech, such as babbling or long pause, measured by unaligned duration ratio (UDR) (Shen et al., 2020) with a 1-second threshold.3 The ASR transcription from the translation speech is used for alignment, using a conﬁdence islands-based forced alignment model (Chiu et al., 2018). As can be seen from Table 2, the UDR from Translatotron 2 is about 7 and 4 times lower than from Translatotron on the Fisher Es-En and Conversational datasets, respectively. It is even about 3 times lower than the training targets from the Conversational set, while is about the same as the training targets from Fisher Es-En. This can be explained by the fact that the training targets in the Conversational set were synthesized by the Tacotron 2 TTS model, which by itself Conversational Fisher Es-En BLEU MOS UDR (%) BLEU MOS UDR (%) Table 3: S2ST performance with voice preservation on Conversational dataset. Speaker similarity MOS is evaluated between Spanish source speech and English translation speech. (Numbers not directly comparable to Table 2 because of dataset differences.) BLEU Naturalness Similarity Proposed: Translatotron 2 57.3 3.24 ± 0.08 2.33 ± 0.08 + ConcatAug 56.8 2.94 ± 0.08 2.12 ± 0.07 Translatotron 48.5 2.55 ± 0.09 2.30 ± 0.07 + ConcatAug 51.3 2.76 ± 0.09 2.19 ± 0.07 Reference (synthetic) 81.3 3.40 ± 0.08 2.55 ± 0.07 Jia et al. (2019b): Translatotron 36.2 3.15 ± 0.08 1.85 ± 0.06 Reference (human) 59.9 4.10 ± 0.06 - suffered from over-generation, while the same in Fisher Es-En were synthesized by a more robust Parallel WaveNet (Oord et al., 2018) TTS model (see Table 1). The results suggest that Translatotron 2 drastically improves robustness than Translatotron, and is also robust to a small ratio of disﬂuency in the training targets. 5.4. Voice preservation To evaluate the ability of preserving speakers’ voices while translating their speeches from one language to another, we augmented the Conversational dataset by synthesizing target speech using a voice-transferring TTS model as described in Sec. 4.1. Examples with source speech shorter than 1 second were discarded for the stability of voice transferring. The result dataset contains parallel utterances with similar voices on both sides. S2ST models were trained on this dataset without any explicit conditioning on speaker embeddings or IDs (i.e. no speaker encoder for the original Translatotron). Following Jia et al. (2019b), we reduced the pre-net dimension of the synthesizer to 16 to encourage it to infer voice information from the encoder output instead of from the teacher-forcing inputs. Table 4: Voice preservation performance on speaker turns. Speaker similarity MOS between the leading/trailing 1.6-second segment from the English translation speech and the entire 1st/2nd source speaker’s Spanish speech is reported. (↑/ ↓: higher/lower values are better.) 1st source speaker 2nd source speaker Leading seg. ↑ Trailing seg. ↓ Leading seg. ↓ Trailing seg. ↑ Translatotron 2 2.22 ± 0.07 2.15 ± 0.07 2.04 ± 0.07 2.00 ± 0.07 + ConcatAug 2.44 ± 0.07 1.82 ± 0.07 1.76 ± 0.07 2.51 ± 0.08 Translatotron 1.87 ± 0.06 1.90 ± 0.07 2.06 ± 0.07 2.05 ± 0.07 + ConcatAug 2.18 ± 0.07 1.71 ± 0.06 1.93 ± 0.07 2.35 ± 0.07 Reference (synthetic) 2.58 ± 0.08 1.62 ± 0.06 1.83 ± 0.07 2.44 ± 0.07 5-point subjective MOS on both naturalness and speaker similarity was evaluated with 1,000 random samples or pairs of samples from the test set, reported with 95% conﬁdence interval. As Table 3 shows, when the proposed approach for voice preservation was used, both Translatotron 2 and Translatotron obtained about the same speaker similarity MOS as the original Translatotron but signiﬁcantly better translation quality. Translatotron 2 further outperformed Translatotron in terms of translation quality and speech naturalness, which is consistent with the experimental results for translating in a single speaker’s voice (Sec. 5.1, 5.2). It is worth to note that the speaker similarity from S2ST models is capped by the same of the training targets, which by itself is limited. This can be partially due to the performance of the voice-transferring TTS model used for synthesizing the training targets, and partially due to the fact that crosslingual speaker similarity evaluation is more challenging to raters (some rating comments are purely based on language difference), as also observed in Zhang et al. (2019). Obtaining better quality training targets, such as human recordings instead of synthesized speech, may further improve the performance of voice preservation with the proposed approach. 5.4.1. SPEAKER TURNS Speaker similarity evaluation with speaker turns on entire translation speech is challenging because it would require speaker separation on both source and target speeches. The content re-ordering during translation and translation errors would also add extra difﬁculty. We approximated by considering the leading/trailing short segments in the translation speech as corresponding to each of the two speakers in the source speech with a single speaker turn. We trained Translatotron 2 and Translatotron on the dataset described in Sec. 5.4, with half of the training examples augmented by ConcatAug. The evaluation set was artiﬁcially constructed in a similar way by applying ConcatAug, so that each utterance contains two speakers’ voices. We evaluated subjective speaker similarity MOS between the two entire source utterances before ConcatAug and the leading/trailing 1.6-second segments from the translation speech. Evaluation examples with target speech shorter than 2 seconds before ConcatAug were discarded. As can be seen from Table 4, the impact of ConcatAug is consistent on Translatotron 2 and Translatotron. When ConcatAug was not used during training, for each source speaker, the similarity compared to the leading/trailing segment from the translation speech was about the same; and for each segment in the translation speech, the speaker similarity compared to the ﬁrst/second source speaker was also close. This suggests that the translation speech imitated both source speakers at the same time regardless of the speaker turn. When ConcatAug was used, both models obtained signiﬁcantly higher speaker similarity on matched pairs than mismatched pairs, indicating that the models successfully separated two speakers and preserved voices for each of them respectively. It can also be seen that Translatotron 2 obtained signiﬁcantly higher speaker similarity than Translatotron on matched pairs, indicating the effectiveness of Translatotron 2. Such quantitative evaluation cannot reﬂect how the predicted translation speech transits from one speaker’s voice to another speaker’s. Listening to audio samples (available online) veriﬁed that the voice changed instantly on sentence boundaries without blurry, rather than a smoothed transition. A sample of S2ST on such a speaker turn from Translatotron 2 is visualized in Figure 2. While ConcatAug enables S2ST models to preserve speakers’ voices on speaker turns and improves translation quality on small datasets, it may negatively impact the speech naturalness and speaker similarity on models with strong performance, as shown in Table 2 and Table 3. It may be explained by the fact that the augmented utterances sound less natural and may involve abrupt change in volume and background noise on the artiﬁcial speaker turns. This suggests headroom for improvement. 5.5. Multilingual S2ST We also conducted experiments to evaluate the performance of multilingual X→En S2ST. We trained Translatotron 2 and Translatotron on the 4 high-resource language pairs from the CoVoST 2 corpus (Wang et al., 2021a), using Table 5: Ablation studies of multilingual X→En S2ST on 4 highresource language pairs from CoVoST 2, measured by BLEU on ASR transcription from the translation speech. + / −indicates using or replacing a component (see Sec. 5.5.1). Translatotron 2 27.0 18.8 27.7 22.5 −SpecAugment 25.9 17.9 25.9 21.8 −Conformer encoder 26.4 18.1 26.4 21.8 −NAT synthesizer 26.9 18.3 27.0 22.0 Translatotron (w/ SpecAugment) 17.7 9.9 17.7 13.1 + Conformer encoder 18.9 10.8 18.8 13.9 + NAT synthesizer 4.0 2.1 3.5 2.5 ST (Wang et al., 2021a) 27.0 18.9 28.0 23.9 Reference (synthetic) 82.1 86.0 85.1 89.3 TTS synthesized target speech in a single female speaker’s voice.4 The original Common Voice (Ardila et al., 2020) data split instead of the CoVoST 2 data split was followed. The models were not explicitly conditioned on languages. For a fair comparison, both models used SpecAugment, but did not use auxiliary supervision from the source phonemes. The translation quality as measured by BLEU on ASR transcription from the translation speech is shown in the ﬁrst rows of each block in Table 5. Translatotron 2 outperformed Translatotron by +9.4 BLEU on average on the 4 language pairs. Although the BLEU scores are not directly comparable between S2ST and ST (because of ASR transcription and BLEU calculation difference), the close numbers suggest that Translatotron 2 obtained translation quality comparable to the baseline ST model. 5.5.1. ABLATION STUDIES To understand the importance of each component in Translatotron 2, we conducted ablation studies on this multilingual X→En dataset. All models in the ablation used the same input and output features, SpecAugment settings, and learning rate schedules (detailed in Appendix A). No auxiliary supervision from source text was used. For models not using a Conformer encoder, we ﬁrst applied the same 4× temporal subsampling as in the Conformer encoder, then used a 256×8 bidirectional LSTM stack to encoder the subsampled features. The number of parameters in this LSTM encoder is close to the same in the Conformer encoder. For the Translatotron model using a NAT synthesizer, the same hyperparameters as in Translatotron 2 were used. For Translatotron 2 not using a NAT synthesizer, a non-autoregressive Conformer synthesizer (Sec. 5.6) was used. All the rest hyperparameters followed Appendix A for Translatotron 2, and Table 6: Ablation studies on Conversational dataset using an autoregressive RNN + Conv synthesizer and a non-autoregressive Conformer synthesizer. Synthesizer BLEU Naturalness fr de es ca RNN + Conv 55.6 4.21 ± 0.06 Conformer 54.5 3.61 ± 0.09 followed the Conversational model from Jia et al. (2019b) for Translatotron. All models were trained for 200K steps with a batch size of 768. The checkpoints for evaluation were picked by the best average BLEU on 4 language pairs on the validation set. The results are shown in Table 5. As can be seen, while the use of Conformer, SpecAugment, and NAT synthesizer helps the performance of Translatotron 2, replacing them with alternative architectural choices or removing SpecAugment only reduced the performance by a small degree (<2 BLEU). Similarly, directly using these components in Translatotron does not bring its performance close to Translatotron 2. These results suggest that the improvements of Translatotron 2 primarily comes from the high-level architectural design which addressed the performance bottlenecks existing in Translatotron (Sec. 3), rather than the choices of each individual component. 5.6. Non-autoregressive synthesizer It is tempting to use a non-autoregressive architecture for the synthesizer of Translatotron 2, which may signiﬁcantly reduce its inference latency, similar to recent works on nonautoregressive TTS (Ren et al., 2019; 2021; Guo et al., 2021; Lee et al., 2021b; Elias et al., 2021b;a). We experimented with using a 6-layer Conformer synthesizer (Guo et al., 2021) with a dimension of 512 and 8 attention heads on both Conversational and CoVoST 2 datasets. As can be seen from Table 5 and 6, using a Conformer-based non-autoregressive synthesizer obtained comparable translation quality to using an autoregressive NAT synthesizer (with BLEU on ASR transcription up to 1.1 BLEU lower). However, it caused a signiﬁcant regression on the naturalness of the predicted translation speech, which is consistent with the observation in TTS in Shen et al. (2020); Peng et al. (2020); Hwang et al. (2021), etc., suggesting more exploration is needed on this direction.\n\n2. Related works\nS2ST Until very recently, automatic S2ST systems are typically composed of a cascade of ASR, MT, and TTS components (Lavie et al., 1997; Wahlster, 2000; Nakamura et al., 2006; ITU, 2016). Translatotron (Jia et al., 2019b) is the ﬁrst direct S2ST model, which is a sequence-to-sequence model trained end-to-end in a multi-objective task. It has shown reasonable translation quality and speech naturalness, but still underperformed a baseline of ST →TTS cascade by a large margin. It also demonstrated the capacity of preserving speakers’ voices during the translation, by leveraging a speaker encoder separately trained in a speaker veriﬁcation task (Wan et al., 2018; Jia et al., 2018). A few recent works proposed cascade S2ST systems using learned discrete speech representation as the intermediate representation instead of text or phoneme. Tjandra et al. (2019) introduced such an S2ST system that ﬁrst translated the source speech into a discrete representation of the target speech which was predicted from a separately trained VQVAE (Oord et al., 2017), then constructed the target speech spectrogram from the discrete representation using the VQVAE decoder. Zhang et al. (2021) additionally trained the VQ-VAE jointly with a supervised phoneme recognition objective in different languages. Lee et al. (2022; 2021a) used a separately trained vocoder to directly predict waveform from the discrete representation without relying on spectrogram; for the best performance, this vocoder included a duration predictor, akin to a generative TTS model. All these works require multiple components being trained in multiple steps, but are not able to be trained end-to-end. Another potential limitation of such an approach is that it may not be effective in preserving paralinguistic and nonlinguistic information. Oppositely, it can be desired that such variation be removed in the discrete representation (Lee et al., 2021a). Kano et al. (2021) introduced an end-to-end S2ST model with a cascade of three autoregressive decoders, and used pre-trained MT and TTS models as teacher models to facilitate the training of the end-to-end model. It requires pre-trained ASR, MT, and TTS models, and the end-to-end model itself has to be trained in multiple steps. While most of these works conducted experiments using synthetic datasets with translation speech in a clean single speaker’s voice, Jia et al. (2019b); Lee et al. (2021a) conducted experiments using multi-speaker human recordings. Although these recent works generated speech translation in novel ways without relying on TTS subsystems, only a few of them (Jia et al., 2019b; Lee et al., 2022) have evaluated the perceptual quality (e.g. naturalness) of the produced speech translation, which is critical to S2ST (Wagner et al., 2019; Salesky et al., 2021), with the rest focused only on the translation quality. TTS Translatotron uses a decoder similar to the Tacotron 2 TTS model (Shen et al., 2018), which is an attention-based autoregressive decoder. Due to the ﬂexibility of the attention mechanism, they both suffer from robustness issues such as over-generation. Recent TTS models such as FastSpeech (Ren et al., 2019; 2021) and Non-Attentive Tacotron (NAT) (Shen et al., 2020) demonstrated that replacing the attention module with a duration-based upsampler yields more robust synthesized speech, as quantitatively evaluated at a large scale in Shen et al. (2020). The synthesizer component in this work resembles these works. Voice conversion and anti-spooﬁng The performance of voice conversion has progressed rapidly in the recent years, and is reaching a quality that is hard for automatic speaker veriﬁcation (ASV) systems to detect (Yi et al., 2020). ASVspoof 2019 (Todisco et al., 2019; Wang et al., 2020) found that it was challenging to detect spoof audios generated from a zero-shot voice cloning TTS model (Jia et al., 2018), which was followed by the original Translatotron for preserving speakers’ voices during S2ST. Such progress poses concerns on related techniques being misused for creating spooﬁng artifacts. We propose a new voice preservation method for S2ST with the motivation of avoiding such potential misuse.\n\n6. Conclusion\nWe proposed Translatotron 2, a neural direct S2ST model that can be trained end-to-end. Experimental results on three datasets consistently suggest that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches cascade S2ST. In addition, we proposed a simple method for preserving speakers’ voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker’s voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker’s privacy and mitigates potential misuse of voice cloning for creating spooﬁng audio artifacts. Future works include extending Translatotron 2 to support simultaneous translation, cross-lingual prosody transfer, unwritten languages, and further quality improvement by utilizing self-supervised pre-training (Baevski et al., 2020; Wang et al., 2021b) and weakly supervised data (Jia et al., 2019a).\n\nAcknowledgements The authors would like to thank Chung-Cheng Chiu, Quan Wang, Heiga Zen, Ron J. Weiss, Wolfgang Macherey, Yu Zhang, Yonghui Wu, Hadar Shemtov, Ruoming Pang, Nadav Bar, Michael Hassid, and the rest of the Google Research team for helpful discussions and previous work on data preparation.\n\nReferences\nAnastasopoulos, A., Bojar, O., Bremerman, J., Cattoni, R., Elbayad, M., Federico, M., Ma, X., Nakamura, S., Negri, M., Niehues, J., et al. Findings of the IWSLT 2021 evaluation campaign. In International Conference on Spoken Language Translation (IWSLT), 2021. Ansari, E., Axelrod, A., Bach, N., Bojar, O., Cattoni, R., Dalvi, F., Durrani, N., Federico, M., Federmann, C., Gu, J., et al. Findings of the IWSLT 2020 evaluation campaign. In International Conference on Spoken Language Translation (IWSLT), 2020. Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common Voice: A massively-multilingual speech corpus. In Proceedings of Language Resources and Evaluation Conference (LREC), 2020. Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Battenberg, E., Skerry-Ryan, R., Mariooryad, S., Stanton, D., Kao, D., Shannon, M., and Bagby, T. Locationrelative attention mechanisms for robust long-form speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Chen, M., Chen, M., Liang, S., Ma, J., Chen, L., Wang, S., and Xiao, J. Cross-lingual, multi-speaker text-tospeech synthesis using neural speaker embedding. In Proc. Interspeech, 2019. Chiu, C.-C., Tripathi, A., Chou, K., Co, C., Jaitly, N., Jaunzeikare, D., Kannan, A., Nguyen, P., Sak, H., Sankar, A., Tansuwan, J., Wan, N., Wu, Y., and Zhang, X. Speech recognition for medical conversations. In Proc. Interspeech, 2018. Di Gangi, M. A., Negri, M., and Turchi, M. One-to-many multilingual end-to-end speech translation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Skerry-Ryan, R., and Wu, Y. Parallel Tacotron 2: A non-autoregressive neural TTS model with differentiable duration modeling. In Proc. Interspeech, 2021a. Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Weiss, R., and Wu, Y. Parallel Tacotron: Non-autoregressive and controllable TTS. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021b. Grifﬁn, D. and Lim, J. Signal estimation from modiﬁed short-time Fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(2):236–243, 1984. Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang,\nR. Conformer: Convolution-augmented transformer for\nspeech recognition. In Proc. Interspeech, 2020. Guo, P., Boyer, F., Chang, X., Hayashi, T., Higuchi, Y., Inaguma, H., Kamo, N., Li, C., Garcia-Romero, D., Shi, J., et al. Recent developments on espnet toolkit boosted by conformer. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5874– 5878, 2021. He, M., Deng, Y., and He, L. Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural TTS. In Proc. Interspeech, 2019. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. Hwang, M.-J., Yamamoto, R., Song, E., and Kim, J.-M. TTS-by-TTS: TTS-driven data augmentation for fast and high-quality speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6598–6602, 2021. ITU. ITU-T F.745: Functional requirements for networkbased speech-to-speech translation services, 2016. International Telecommunication Union. Jia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., Ren, F., Chen, Z., Nguyen, P., Pang, R., Moreno, I. L., and Wu, Y. Transfer learning from speaker veriﬁcation to multispeaker text-to-speech synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Jia, Y., Johnson, M., Macherey, W., Weiss, R. J., Cao, Y., Chiu, C.-C., Ari, N., Laurenzo, S., and Wu, Y. Leveraging weakly supervised data to improve end-to-end speechto-text translation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019a. Jia, Y., Weiss, R. J., Biadsy, F., Macherey, W., Johnson, M., Chen, Z., and Wu, Y. Direct speech-to-speech translation with a sequence-to-sequence model. In Proc. Interspeech, 2019b. Jia, Y., Zen, H., Shen, J., Zhang, Y., and Wu, Y. PnG BERT: Augmented BERT on phonemes and graphemes for neural TTS. In Proc. Interspeech, 2021. Jia, Y., Tadmor Ramanovich, M., Wang, Q., and Zen, H. CVSS corpus and massively multilingual speechto-speech translation. In Proceedings of Language Resources and Evaluation Conference (LREC), 2022. Kahn, J., Rivi`ere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazar´e, P.-E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. Libri-light: A benchmark for ASR with limited or no supervision. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A. v. d., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural audio synthesis. In Proceedings of International Conference on Machine Learning (ICML), 2018. Kano, T., Sakti, S., and Nakamura, S. Transformer-based direct speech-to-speech translation with transcoder. In IEEE Spoken Language Technology Workshop (SLT), 2021. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. Krueger, D., Maharaj, T., Kram´ar, J., Pezeshki, M., Ballas, N., Ke, N. R., Goyal, A., Bengio, Y., Courville, A., and Pal, C. Zoneout: Regularizing RNNs by randomly preserving hidden activations. In International Conference on Learning Representations (ICLR), 2017. Lavie, A., Waibel, A., Levin, L., Finke, M., Gates, D., Gavalda, M., Zeppenfeld, T., and Zhan, P. JANUS-III: Speech-to-speech translation in multiple languages. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1997. Lee, A., Gong, H., Duquenne, P.-A., Schwenk, H., Chen, P.-J., Wang, C., Popuri, S., Pino, J., Gu, J., and Hsu, W.-N. Textless speech-to-speech translation on real data. arXiv preprint arXiv:2112.08352, 2021a. Lee, A., Chen, P.-J., Wang, C., Gu, J., Ma, X., Polyak, A., Adi, Y., He, Q., Tang, Y., Pino, J., et al. Direct speechto-speech translation with discrete units. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), 2022. Lee, Y., Shin, J., and Jung, K. Bidirectional variational inference for non-autoregressive text-to-speech. In International Conference on Learning Representations (ICLR), 2021b. Ma, X., Gong, H., Liu, D., Lee, A., Tang, Y., Chen, P.-J., Hsu, W.-N., Heaﬁeld, K., Koehn, P., and Pino, J. Direct simultaneous speech to speech translation. arXiv preprint arXiv:2110.08250, 2021. McCarthy, A. D., Puzon, L., and Pino, J. SkinAugment: Auto-encoding speaker conversions for automatic speech translation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Nakamura, S., Markov, K., Nakaiwa, H., Kikui, G., Kawai, H., Jitsuhiro, T., Zhang, J.-S., Yamamoto, H., Sumita, E., and Yamamoto, S. The ATR multilingual speech-tospeech translation system. IEEE Transactions on Audio, Speech, and Language Processing, 2006. Narayanan, A., Prabhavalkar, R., Chiu, C.-C., Rybach, D., Sainath, T. N., and Strohman, T. Recognizing long-form speech using streaming end-to-end models. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., Driessche, G., Lockhart, E., Cobo, L., Stimberg, F., et al. Parallel WaveNet: Fast highﬁdelity speech synthesis. In Proceedings of International Conference on Machine Learning (ICML), 2018. Oord, A. v. d., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. LibriSpeech: an ASR corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. SpecAugment: A simple data augmentation method for automatic speech recognition. In Proc. Interspeech, 2019. Park, D. S., Zhang, Y., Jia, Y., Han, W., Chiu, C.-C., Li, B., Wu, Y., and Le, Q. V. Improved noisy student training for automatic speech recognition. In Proc. Interspeech, 2020. Pathak, M. A. and Raj, B. Privacy-preserving speaker veriﬁcation and identiﬁcation using Gaussian mixture models. IEEE Transactions on Audio, Speech, and Language Processing, 21(2):397–406, 2012. Peng, K., Ping, W., Song, Z., and Zhao, K. Nonautoregressive neural text-to-speech. In Proceedings of International Conference on Machine Learning (ICML), 2020. Post, M., Kumar, G., Lopez, A., Karakos, D., CallisonBurch, C., and Khudanpur, S. Improved speech-to-text translation with the Fisher and Callhome Spanish–English speech translation corpus. In International Conference on Spoken Language Translation (IWSLT), 2013. Ren, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. FastSpeech: Fast, robust and controllable text to speech. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Ren, Y., Hu, C., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. FastSpeech 2: Fast and high-quality end-to-end text-tospeech. In International Conference on Learning Representations (ICLR), 2021. Salesky, E., M¨ader, J., and Klinger, S. Assessing evaluation metrics for speech-to-speech translation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021. Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomyrgiannakis, Y., and Wu, Y. Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. Shen, J., Nguyen, P., Wu, Y., Chen, Z., et al. Lingvo: A modular and scalable framework for sequence-to-sequence modeling. arXiv preprint arXiv:1902.08295, 2019. Shen, J., Jia, Y., Chrzanowski, M., Zhang, Y., Elias, I., Zen, H., and Wu, Y. Non-Attentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling. arXiv preprint arXiv:2010.04301, 2020. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818–2826, 2016. Tjandra, A., Sakti, S., and Nakamura, S. Speech-to-speech translation between untranscribed unknown languages. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Todisco, M., Wang, X., Vestman, V., Sahidullah, M., Delgado, H., Nautsch, A., Yamagishi, J., Evans, N., Kinnunen, T., and Lee, K. A. ASVspoof 2019: Future horizons in spoofed and fake audio detection. In Proc. Interspeech, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Wagner, P., Beskow, J., Betz, S., Edlund, J., Gustafson, J., Eje Henter, G., Le Maguer, S., Malisz, Z., Sz´ekely, ´E., T˚annander, C., et al. Speech synthesis evaluation – stateof-the-art assessment and suggestion for a novel research program. In Proceedings of the 10th Speech Synthesis Workshop (SSW10), 2019. Wahlster, W. Verbmobil: Foundations of speech-to-speech translation. Springer, 2000. Wan, L., Wang, Q., Papir, A., and Moreno, I. L. Generalized end-to-end loss for speaker veriﬁcation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. Wang, C., Wu, A., and Pino, J. CoVoST 2: A massively multilingual speech-to-text translation corpus. In Proc. Interspeech, 2021a. Wang, C., Wu, A., Pino, J., Baevski, A., Auli, M., and Conneau, A. Large-scale self-and semi-supervised learning for speech translation. In Proc. Interspeech, 2021b. Wang, X., Yamagishi, J., Todisco, M., Delgado, H., Nautsch, A., Evans, N., Sahidullah, M., Vestman, V., Kinnunen, T., Lee, K. A., et al. ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech. Computer Speech & Language, 64:101114, 2020. Weiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., and Chen,\nZ. Sequence-to-sequence models can directly translate\nforeign speech. In Proc. Interspeech, 2017. for cross-lingual TTS. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021. Yi, Z., Huang, W.-C., Tian, X., Yamagishi, J., Das, R. K., Kinnunen, T., Ling, Z., and Toda, T. Voice conversion challenge 2020: Intra-lingual semi-parallel and crosslingual voice conversion. In Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge, 2020. Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. Interspeech, 2019. Zhang, C., Tan, X., Ren, Y., Qin, T., Zhang, K., and Liu, T.-\nY. UWSpeech: Speech to speech translation for unwritten\nlanguages. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021. Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Chen, Z., SkerryRyan, R., Jia, Y., Rosenberg, A., and Ramabhadran, B. Learning to speak ﬂuently in a foreign language: Multilingual speech synthesis and cross-language voice cloning. In Proc. Interspeech, 2019. Zheng, Y., Wang, X., He, L., Pan, S., Soong, F. K., Wen, Z., and Tao, J. Forward-backward decoding for regularizing end-to-end TTS. In Proc. Interspeech, 2019.\n\nA. Table of hyper-parameters\nTable 7: Model hyper-parameters used in the experiments. (“×n”: n layers; †: 128-dim pre-net is used for translating in a single voice; 16-dim pre-net is used for voice preservation.) Input Sample rate (Hz) 8,000 48,000 16,000 – 48,000 Mel channels 80 Mel lower band (Hz) 125 Mel upper band (Hz) 3,800 7,600 7,600 Frame size (ms) 25.0 Frame step (ms) 10.0 Output Sample rate (Hz) 24,000 Mel channels 128 Mel lower band (Hz) 20 Mel upper band (Hz) 12,000 Frame size (ms) 50.0 Frame step (ms) 12.5 SpecAugment Freq blocks 2 Time blocks 10 Freq block max length ratio 0.33 Time block max length ratio 0.05 Encoder Conformer dims 144 × 16 Attention heads 4 Conv kernal size 32 Subsample factor 4 Attention Output dim 256 512 512 Hidden dim 512 512 512 Attention heads 4 8 8 Dropout prob 0.1 0.2 0.2 Decoder LSTM dims 256 × 4 512 × 6 512 × 4 Zoneout prob 0.1 0.1 0.1 Phoneme embedding dim 96 256 256 Label smoothing uncertainty 0.1 0.1 0.1 Loss weight 10.0 10.0 10.0 Duration predictor Bi-LSTM (dim × layers) 64 × 2 128 × 2 128 × 2 Loss weight 1.0 1.0 1.0 Synthesizer LSTM dims 1,024 × 2 LSTM zoneout prob 0.1 Pre-net dims 128 × 2 128 × 2 128 / 16 † × 2 Pre-net dropout prob 0.5 Post-net (kernel, channels) × layers (5, 512) × 4 + (5, 128) Loss weight 1.0 Training Optimizer Adam (Kingma & Ba, 2015) Learning rate schedule Vaswani et al. (2017) Learning rate (peak) 4.2×10−3 2.2×10−3 3.3×10−3 Warm-up steps Batch size 1,024 768 768 L2 regularization weight 10−6 10−6 10−6 Fisher Es-En CoVoST 2 Conversational src vs src tgt vs src tgt vs tgt s2st vs src s2st vs tgt Figure 3: Afﬁnity matrices of d-vector similarity among 100 random examples. (“s2st” refers to the predictions from Translatotron 2.) Table 8: Objective d-vector similarity between the predicted translated speech (English) and the source human speech (Spanish) on speaker turns. The similarity between the leading/trailing 1.6-second segment from the predicted speech and the entire 1st/2nd source speaker’s speech is measured. (↑/ ↓means higher/lower values are better.) 1st source speaker 2nd source speaker Leading seg. ↑ Trailing seg. ↓ Leading seg. ↓ Trailing seg. ↑ Translatotron 2 0.21 0.19 0.21 0.19 + ConcatAug 0.20 0.14 0.14 0.21 Translatotron 0.20 0.22 0.27 0.29 + ConcatAug 0.32 0.16 0.14 0.35 Reference (synthetic) 0.48 0.17 0.15 0.48\nB. Objective speaker similarity analysis\nSubjective speaker similarity evaluation is costly and has a long turnaround. We explored alternative objective evaluation using separately trained speaker encoders, such as d-vector (Wan et al., 2018). We evaluated the voice retention performance using the cosine similarity of the d-vectors. We ﬁrst checked the scenario that each input contains a single speaker’s recording. Figure 3 visualizes the afﬁnity matrices of d-vector similarity among different input utterances for a Translatotron 2 model. The outstanding higher similarity values on the diagonals indicate that the model is able to preserve the source speaker’s voice in the predicted translation speech. We then conducted a detailed evaluation for the voice retention performance for speaker turns. The experiment setting up was identical to Section 5.4.1, except that the speaker similarity was measured by d-vector similarity instead of subjective MOS evaluation. The d-vectors for each source speaker were computed on the entire original utterance before concatenation; the d-vectors for each speaker in the prediction is approximated by computing on the leading/trailing 1.6 seconds of predicted speech. The results are shown in Table 8. Consistent with the MOS evaluation results in Table 4, when the concatenation augmentation was not used, the d-vector similarity to each source speaker is about the same regardless if it was compared to the leading or trailing segments, indicating that the predicted speech was in a single speaker’s voice and the model was unable to separate different speakers in the input, but rather optimized for both source speakers at the same time. When the concatenation augmentation was used, the d-vector similarity was signiﬁcantly higher between matched pairs than between unmatched pairs, indicating that the models were able to separate different speakers in the input and preserve their voices in the predicted translation speech respectively. However, when these similarities are compared among different models, it seems to suggest that Translatotron performed better than Translatotron 2, which is contradictory to the subjective evaluation results in Table 4. By carefully listening to the audio samples, we found that such discrepancy may be due to the fact that the d-vector model was also sensitive to non-voice related acoustic characteristics, such as reverb and channel noise in the audios. This is likely a consequence of the fact that in the large-scale training set for the d-vector model used in the evaluation, each speaker is typically associated with a particular recording condition, e.g. recording device and room. Because the encoder output from the Translatotron model was of signiﬁcantly larger dimension than from the Translatotron 2 model (2048 vs 144), it was capable of carrying more non-voice acoustic information and thus obtained better d-vector similarity, which not necessarily indicating higher speaker similarity. These results suggest that while such speaker encoder-based objective analysis reveals insightful indications about the performance of the S2ST models, it can be less reliable compared to subjective MOS evaluation. Such reliability also highly depends on the training details of the speaker encoder model being used, especially the training corpus.\n--- FOOTNOTES 1Google Research. Correspondence to: 10K 20K 10K", "persona": "Student", "response": "### 1. Overview\n3 Key Improvements made in Translatotron 2 Used only as an auxiliary loss in Translatotron Duration-based (like Non-Attentive Tacotron) in Translatotron 2, and thus the robustness of the synthesized speech is improved. Phoneme decoder in Translatotron 2. Thus, the acoustic information that the spectrogram synthesizer sees is aligned with the translated content that it’s synthesizing and hence each speaker’s voice is retained across speaker turns. Translatotron 2 outperforms Translatotron by large margins, such as :\n\n### 2. Background\nTo retain speakers' voices across translation, researchers usually train S2ST models on parallel utterances with the same speaker's voice on both sides. This type of dataset with human recordings on both sides requires a large number of fluent bilingual speakers & hence difficult to collect For input speech with multiple speakers speaking in turns, each speaker’s voice in the translated speech can be retained by the S2ST models by a novel technique called ConcatAug. 2. Forming new training examples by concatenating the The resulting samples contain two speakers’ voices in both the source and the target speech.Thus the model can learn on examples with speaker turns.\n\n### 3. Methods\nUsed as one of the inputs to the spectrogram synthesizer in Translatotron 2 making it easier to train with better performance Attention-based (like Tacotron 2 TTS model) in Translatotron and thus suffers from the robustness issues exhibited by Tacotron 2. 3. Attention-based connection to the encoded source speech is driven by the: Spectrogram synthesizer in Translatotron target phoneme sequences\n\n### 4. Results\nVoice Retention Dataset 1.Randomly sampling pairs of training examples and VOICE RETENTION Responsible Voice Retention Translatotron could retain the source speaker's voice in the translated speech, by conditioning its decoder on a speaker embedding, generated from a separately trained speaker encoder. This enabled Translatotron to generate the translated speech in a different speaker's voice (if a clip of the target speaker's recording were used as the reference audio to the speaker encoder, or if the embedding of the target speaker were directly available). Though this was a powerful approach, it could be misused to spoof audio with arbitrary content. Hence it was a major concern to deploy the model in production.\n\n### 5. Takeaways\nBut Translatotron 2 uses only a single speech encoder, which is responsible for both linguistic understanding and voice capture. Thus, the model cannot be misused to reproduce non-source voices. This approach can also be applied to Translatotron. PnG NAT is a TTS (Text to Speech Synthesis) model which can do cross-lingual voice transferring to synthesize such training targets. The modified PnG NAT model used in Translatotron 2 incorporates a separately trained speaker encoder in the same way as in Translatotron and hence capable of zero-shot voice transference and thus the bilingual dataset problem is solved. Individual Voice Retention with ConcatAug source speech", "paper_id": "29", "story_id": "29-04"}
{"paper": "Translatotron 2: High-quality direct speech-to-speech translation with voice preservation Ye Jia 1 Michelle Tadmor Ramanovich 1 Tal Remez 1 Roi Pomerantz 1\n\nAbstract\narXiv:2107.08661v5  [cs.CL]  17 May 2022 We present Translatotron 2, a neural direct speechto-speech translation model that can be trained end-to-end. Translatotron 2 consists of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module that connects them together. Experimental results on three datasets consistently show that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade systems. In addition, we propose a simple method for preserving speakers’ voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker’s voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker’s privacy and mitigates potential misuse of voice cloning for creating spoofing audio artifacts.\n\n1. Introduction\nSpeech-to-speech translation (S2ST) is highly beneﬁcial for breaking down communication barriers between people not sharing a common language. Conventional automatic S2ST systems are composed of a cascade of three components: automatic speech recognition (ASR), text-to-text machine translation (MT), and text-to-speech (TTS) synthesis (Lavie et al., 1997; Wahlster, 2000; Nakamura et al., 2006). In the past few years, direct speech-to-text translation (ST) is rapidly emerging, and has outperformed the cascade of ASR and MT (Weiss et al., 2017; Jia et al., 2019a; Di Gangi et al., 2019; McCarthy et al., 2020; Ansari et al., 2020; Wang et al., 2021b; Anastasopoulos et al., 2021), which makes the Ye Jia <jiaye@google.com>. cascade of ST and TTS as S2ST feasible (Jia et al., 2019b). Recently, works on S2ST without relying on intermediate text representation are emerging, such as end-to-end direct S2ST (Jia et al., 2019b; Kano et al., 2021) and cascade S2ST based on discrete speech representation (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022; 2021a; Ma et al., 2021). Compared to text-centric cascaded systems, although such approaches require parallel S2ST data, which is scarce, they have the potential advantages of: 1) Preserving paralinguistic and non-linguistic information during translation, such as speaker’s voice (Jia et al., 2019b), emotion and prosody; 2) Supporting languages without written form, or being able to be trained without transcription of speech (Tjandra et al., 2019; Zhang et al., 2021; Lee et al., 2022; 2021a); 3) Reduced computational requirements and lower inference latency (Lee et al., 2022); 4) Avoiding error compounding across sub-systems (Jia et al., 2022); 5) Easier on handling contents that do not need to be translated, such as names and proper nouns (Jia et al., 2019b). Among these works, Translatotron (Jia et al., 2019b) is the ﬁrst model that is able to directly translate speech in one language to speech in another language. It obtained reasonable translation quality and high naturalness in the predicted translation speech, and is able to preserve speakers’ voices during the speech translation. However, the translation quality from Translatotron still underperforms cascade baselines by a large margin, and the translation speech it produces suffers from over-generation issues, such as babbling and long pause. Such weaknesses make this model not yet practical for production. Nevertheless, it remains the state-of-the-art of end-to-end direct S2ST. In this paper, we ﬁrst tackle the performance gap between end-to-end direct S2ST and cascade S2ST. We propose Translatotron 2, a novel direct S2ST model that is able to be trained end-to-end. We conduct experiments on three S2ST datasets, including multilingual S2ST. The results consistently suggest that Translatotron 2 signiﬁcantly outperforms Translatotron in terms of both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches the same of cascade S2ST. When a simple data augmentation ConcatAug is used, the translation quality gap on the Fisher Spanish-English corpus (Post et al., 2013) is reduced from 16.4 to 0.4 BLEU. These results are the ﬁrst time that end-to-end direct S2ST approaches cascade S2ST. In addition, we propose a simple method for preserving speakers’ voices during S2ST without relying on any speaker representation (ID or embedding). The proposed method enables Translatotron 2 to preserve each speaker’s voice on speaker turns without requiring for speaker separation, which is the ﬁrst of its kind. Furthermore, compared to existing approaches of voice preservation, the proposed method better preserves speaker’s privacy (Pathak & Raj, 2012) and mitigates potential misuse of voice cloning for creating spooﬁng audio artifacts. Audio samples from Translatotron 2 are available online.1\n\n3. Translatotron 2\nWe designed the architecture of Translatotron 2 to address three performance bottlenecks existing in the original Translatotron: 1) The utilization of the auxiliary textual supervision during training is suboptimal, namely, the attention alignment learned by the auxiliary ST task does not directly contribute to the main S2ST task; 2) The challenge posed by modeling the translation alignment between two very (English) spectrogram phoneme (English) Synthesizer Decoder autoregression Attention Encoder spectrogram (Spanish) (a) Overview of Translatotron 2. Figure 1: A Translatotron 2 model that translates Spanish speech into English speech. long spectrogram sequences using the attention mechanism; 3) Attention-based speech generation is known to suffer from robustness issues such as over-generation and undergeneration (Shen et al., 2020; Ren et al., 2019; He et al., 2019; Zheng et al., 2019; Battenberg et al., 2020). We addressed these bottlenecks by designing a novel S2ST model architecture composed of a speech encoder, a linguistic decoder, an acoustic synthesizer, and a single attention module connecting them together (Figure 1a). The model is jointly trained with a speech-to-speech translation objective and a speech-to-phoneme translation objective. The following subsections describe each component of Translatotron 2. Note that as shown in the ablation studies in Sec. 5.5.1, while the speciﬁc architectural choices of these components help the performance of Translatotron 2, the primary improvement comes from the high-level architecture rather than the choice of each individual component. 3.1. Speech encoder The encoder of Translatotron 2 takes the mel-spectrogram of the source speech as the input, and produces a hidden representation which encodes both linguistic and acoustic information from the source speech. We use Conformer (Gulati et al., 2020) as the architecture of the encoder. It ﬁrst subsamples the input mel-spectrogram with a convolutional layer, and then processes it with a stack of Conformer blocks. Each Conformer block is composed of a feed-forward layer, a self-attention layer, a convolution layer, and a second feedforward layer. SpecAugment (Park et al., 2019) is applied at the training time as data augmentation. spectrogram RNN + Conv autoregression durations Upsampler Duration Predictor concat(hidden, attention) (b) Synthesizer of Translatotron 2. 3.2. Linguistic decoder The autoregressive decoder is responsible for producing linguistic information in the translation speech. It takes the encoder output through the attention module, and predicts a phoneme sequence corresponding to the translation speech. We use an LSTM stack (Hochreiter & Schmidhuber, 1997) as the decoder, assisted with regularization including Zoneout (Krueger et al., 2017) and label smoothing (Szegedy et al., 2016). The combination of the encoder, the decoder, and the attention module is similar to a typical ST model, except that it predicts phonemes instead of subword tokens. 3.3. Acoustic synthesizer The synthesizer is responsible for acoustic generation of the translation speech. It takes the intermediate output from the decoder (before a ﬁnal projection and softmax for phoneme prediction), as well as the context output from the attention as its input, and generates a mel-spectrogram corresponding to the translation speech. It is similar to the decoders in typical neural TTS models. The predicted mel-spectrogram can be converted to waveform using an estimation algorithm such as Grifﬁn & Lim (1984) or a neural vocoder such as WaveRNN (Kalchbrenner et al., 2018). We use the duration-based autoregressive synthesizer from the NAT (Shen et al., 2020) TTS model (Figure 1b). It ﬁrst predicts durations for each elements in the input sequence, then temporally upsamples the input sequence based on the predicted durations. After that, an LSTM stack is used for generating the target spectrogram without altering the sequence length. A ﬁnal residual convolutional block further reﬁnes the generated spectrogram. Unlike in NAT, we do not supervise the duration prediction on per-phoneme duration labels, to avoid additional requirement on the training data. Instead, an L2 loss on the total predicted duration of the entire utterance is used (similar to the “na¨ıve approach” of unsupervised duration modeling in Shen et al. (2020)). 3.4. A single attention It is critical that Translatotron 2 utilizes a single attention module for both the linguistic decoder and the acoustic synthesizer. This attention models both linguistic and acoustic alignments between the source and the target speeches. A multi-head attention (Vaswani et al., 2017) is used. The queries to this attention are from the linguistic decoder. As a result, unlike in the original Translatotron, this attention does not directly model the translation alignment between two very long spectrogram sequences. Instead, it models the alignment between a source spectrogram sequence and a shorter target phoneme sequence, which is signiﬁcantly easier to learn. In the meantime, the attention provides acoustic information from the source speech to the synthesizer, summarized at per-phoneme level. Such summarized acoustic information is not only usually sufﬁcient for speech generation but also eases the duration prediction per-phoneme because it is of the same granularity. Because a single attention is used, the linguistic and acoustic information seen by the synthesizer is synchronized temporally. Such synchronization enables Translatotron 2 to preserve paralinguistic and nonlinguistic information at ﬁne granularity, such as preserving each speaker’s voice on speaker turns (Sec. 4.2). Although the synthesizer takes attention output as part of its input, the attention is not driven (i.e. queried) by the synthesizer. As a result, while it beneﬁts from the attention on obtaining aligned acoustic information from the source speech, it does not suffer from robustness issues as in typical attention-based speech synthesis models.\n\n4. Voice preserving\nThe original Translatotron (Jia et al., 2019b) demonstrated the capacity of preserving source speakers’ voices in the translation speech, by conditioning its synthesizer on a speaker embedding generated from a separately trained speaker encoder. In fact, it is capable of generating the translation speech in a different speaker’s voice, as long as a clip of the target speaker’s recording is used as the reference audio to the speaker encoder, or the embedding of the target speaker is directly available. While this is impressively powerful, it can potentially be misused for generating spooﬁng audio with arbitrary content, posing a concern for production deployment. Source: \"¿Por qué elegiste hacer esto?\", \"de 24 fuentes en 12 países\" 120 10 100 5 80 60 0 40 20 5 0 2 4 6 8 0 Reference: \"Why did you choose to do this?\", \"from 24 sources in 12 countries\" 7.5 120 5.0 100 2.5 80 60 0.0 40 2.5 20 5.0 0 1 2 3 4 5 0 Prediction: \"why did you choose to do this from twenty four sources in twelve countries\" 120 5.0 100 2.5 80 0.0 60 2.5 40 5.0 20 0 7.5 0 1 2 3 4 5 Time (sec) Figure 2: Sample mel-spectrograms on input with speaker turns. The input speech includes an utterance from a male speaker followed by another utterance from a female speaker. Translatotron 2 preserves the voices of each speaker in the translation speech. preserving speaker’s voice during S2ST, so that the trained models are restricted to preserving the source speaker’s voice, but not able to generate speech in a different speaker’s voice. In addition, this approach enables S2ST models to preserve each speaker’s voice on input speech with speaker turns, without requiring for speaker segmentation. 4.1. Training-time voice transferring In our approach, the key to restrict S2ST models to voice preservation but not arbitrary voice cloning (from a different speaker) is to move the powerful voice transferring to only happen at the training time (or the training data preparation time) but not the inference time. In contrast, it happens at both the training time and the inference time in the original Translatotron. To preserve speakers’ voices across translation, we train S2ST models on parallel utterances with the same speaker’s voice on both sides. Such a dataset with human recordings on both sides is extremely difﬁcult to collect, because it requires a large number of ﬂuent bilingual speakers. Instead, we use a TTS model capable of cross-lingual voice cloning to synthesize such training targets. We modiﬁed the PnG NAT (Jia et al., 2021; Shen et al., 2020) TTS model by incorporating a separately trained speaker encoder (Wan et al., 2018) in the same way as in Jia et al. (2018), and trained it on the LibriTTS corpus (Zen et al., 2019). The resulting TTS model is capable of zero-shot voice transferring, but synthesizes in a better quality and Table 1: Datasets for experiments with translation speech in a single-speaker’s voice. Conversational (Jia et al., 2019a) Fisher Es-En (Post et al., 2013) CoVoST 2 (Wang et al., 2021a) Languages es→en es→en es, fr, de, ca →en Domain Read, short-form Telephone conversation Read, short-form Source sample rate 16-48 kHz 8 kHz 48 kHz Utterance pairs 979k 120k 321k Source hours 1,400 127 476 Target hours 619 96 296 Target synthesized by Tacotron 2 + Grifﬁn-Lim Parallel WaveNet PnG NAT + WaveRNN more robust than Jia et al. (2018).2 We used this model to synthesize translation speech in the source speaker’s voice as the training targets in our experiments. Other TTS models capable of cross-lingual voice modeling, such as Zhang et al. (2019); Chen et al. (2019); Xin et al. (2021), could also be utilized. 4.2. Speaker turns Because the single attention module provides linguistic and acoustic information temporally synchronized (Sec. 3.4), Translatotron 2 is theoretically capable of voice preservation in complicated scenarios such as speaker turns. However, proper training data with speaker turns is required to demonstrate such capacity, which is difﬁcult to obtain. We propose a simple data augmentation to enable such training. ConcatAug To enable direct S2ST models to preserve each speaker’s voice for input with speaker turns, we augmented the training data by randomly sampling pairs of training examples and concatenating the source speech, the target speech, and the target phoneme sequences to construct new training examples. The resulting new examples contain two speakers’ voices in both the source and the target speech, which enables the model to learn on examples with speaker turns. See Figure 2 for an example of such concatenation and the prediction from Translatotron 2 on it. Such augmentation does not only enable the model to learn voice retention on speaker turns, but also increases the diversity of the speech content as well as the complexity of the acoustic conditions in the training examples, which may further improve the translation quality of the model, especially on small datasets (Sec. 5.1). Narayanan et al. (2019) uses a similar augmentation but in a more complicated fashion, for improving ASR performance on multi-speaker inputs.\n\n5. Experiments\nWe conducted experiments on three datasets, including two Spanish→English datasets and a multilingual→English dataset. All datasets use TTS synthesized target speech in 24 kHz sample rate. The phonemes used at training time were converted from the transcripts using a proprietary G2P system. See Table 1 for the details of each dataset. We evaluated the translation quality, naturalness and robustness of the produced translation speech, as well as speaker similarity for voice preservation. All models were implemented using the Lingvo framework (Shen et al., 2019). A comprehensive table of hyper-parameters is available in Appendix A. 5.1. Translation quality To evaluate the translation quality, we used the same two datasets as in Jia et al. (2019b), both of which have translation speech in a single female speaker’s voice. Following Jia et al. (2019b), the translation quality is measured by BLEU on ASR transcription from the translation speech (in lowercase, excluding punctuation marks except for apostrophes), compared to reference translation text. Because ASR makes errors, such BLEU can be thought of as a lower bound of the translation quality. We used an ASR model from Park et al. (2020), trained on LibriSpeech (Panayotov et al., 2015) and LibriLight (Kahn et al., 2020) corpora. For a fair comparison, we retrained the baseline Translatotron models and evaluated them using the same ASR model. The same ST→TTS cascade S2ST baselines from Jia et al. (2019b) were used and re-evaluated, which were composed of strong ST models and a Tacotron 2 TTS model. The predicted mel-spectrogram is converted to waveform using the Grifﬁn-Lim algorithm for all models. As shown in Table 2, the translation quality from Translatotron 2 outperformed the original Translatotron by +15.5 BLEU on Fisher Es-En and +5.2 BLEU on Conversational. Applying ConcatAug further improved the performance on the smaller Fisher Es-En dataset by +0.5 BLEU. These improvements narrowed down the performance gap between end-to-end direct S2ST and cascade S2ST from 16.4 / 8.4 down to 0.4 / 3.7 BLEU on the two datasets respectively. Table 2: Performance of S2ST in a single speaker’s voice. BLEU were computed with 1 reference for the Conversational test set, and with 4 references for the Fisher test set. End-to-end direct S2ST: Translatotron 2 55.6 4.21 ± 0.06 0.16 42.4 3.98 ± 0.08 0.07 + ConcatAug 55.1 4.19 ± 0.06 0.13 42.9 3.79 ± 0.09 0.14 Translatotron 50.4 4.15 ± 0.07 0.69 26.9 3.70 ± 0.08 0.48 Cascade (ST →TTS) 58.8 4.31 ± 0.06 0.21 43.3 4.04 ± 0.08 0.13 Reference (synthetic) 81.9 3.37 ± 0.09 0.43 88.6 3.95 ± 0.07 0.07 Discrete representation-based cascade S2ST: Zhang et al. (2021) (trained w/o text) - - - 9.4 - - Lee et al. (2022) (trained w/ text) - - - 39.9 3.41 ± 0.14 - ion scores (MOS) with 95% conﬁdence interval on 1,000 randomly sampled predictions. A WaveRNN-based neural vocoder was used for converting the mel-spectrograms predicted from S2ST models to waveforms. As shown in Table 2, the naturalness of the translation speech predicted from Translatotron 2 is signiﬁcantly better than from the original Translatotron, and is on-par with or very close to the cascade systems which used one of the state-of-the-art TTS models, Tacotron 2, for synthesizing translation speech from text. Consistent with Jia et al. (2019b), despite that the training targets in the Conversational dataset is synthesized with a lower quality Grifﬁn-Lim vocoder, the trained S2ST model is able to produce translation speech in signiﬁcantly higher naturalness when a higher quality neural vocoder is used at inference time. 5.3. Speech robustness We speciﬁcally evaluated the robustness issue of overgeneration in the predicted translation speech, such as babbling or long pause, measured by unaligned duration ratio (UDR) (Shen et al., 2020) with a 1-second threshold.3 The ASR transcription from the translation speech is used for alignment, using a conﬁdence islands-based forced alignment model (Chiu et al., 2018). As can be seen from Table 2, the UDR from Translatotron 2 is about 7 and 4 times lower than from Translatotron on the Fisher Es-En and Conversational datasets, respectively. It is even about 3 times lower than the training targets from the Conversational set, while is about the same as the training targets from Fisher Es-En. This can be explained by the fact that the training targets in the Conversational set were synthesized by the Tacotron 2 TTS model, which by itself Conversational Fisher Es-En BLEU MOS UDR (%) BLEU MOS UDR (%) Table 3: S2ST performance with voice preservation on Conversational dataset. Speaker similarity MOS is evaluated between Spanish source speech and English translation speech. (Numbers not directly comparable to Table 2 because of dataset differences.) BLEU Naturalness Similarity Proposed: Translatotron 2 57.3 3.24 ± 0.08 2.33 ± 0.08 + ConcatAug 56.8 2.94 ± 0.08 2.12 ± 0.07 Translatotron 48.5 2.55 ± 0.09 2.30 ± 0.07 + ConcatAug 51.3 2.76 ± 0.09 2.19 ± 0.07 Reference (synthetic) 81.3 3.40 ± 0.08 2.55 ± 0.07 Jia et al. (2019b): Translatotron 36.2 3.15 ± 0.08 1.85 ± 0.06 Reference (human) 59.9 4.10 ± 0.06 - suffered from over-generation, while the same in Fisher Es-En were synthesized by a more robust Parallel WaveNet (Oord et al., 2018) TTS model (see Table 1). The results suggest that Translatotron 2 drastically improves robustness than Translatotron, and is also robust to a small ratio of disﬂuency in the training targets. 5.4. Voice preservation To evaluate the ability of preserving speakers’ voices while translating their speeches from one language to another, we augmented the Conversational dataset by synthesizing target speech using a voice-transferring TTS model as described in Sec. 4.1. Examples with source speech shorter than 1 second were discarded for the stability of voice transferring. The result dataset contains parallel utterances with similar voices on both sides. S2ST models were trained on this dataset without any explicit conditioning on speaker embeddings or IDs (i.e. no speaker encoder for the original Translatotron). Following Jia et al. (2019b), we reduced the pre-net dimension of the synthesizer to 16 to encourage it to infer voice information from the encoder output instead of from the teacher-forcing inputs. Table 4: Voice preservation performance on speaker turns. Speaker similarity MOS between the leading/trailing 1.6-second segment from the English translation speech and the entire 1st/2nd source speaker’s Spanish speech is reported. (↑/ ↓: higher/lower values are better.) 1st source speaker 2nd source speaker Leading seg. ↑ Trailing seg. ↓ Leading seg. ↓ Trailing seg. ↑ Translatotron 2 2.22 ± 0.07 2.15 ± 0.07 2.04 ± 0.07 2.00 ± 0.07 + ConcatAug 2.44 ± 0.07 1.82 ± 0.07 1.76 ± 0.07 2.51 ± 0.08 Translatotron 1.87 ± 0.06 1.90 ± 0.07 2.06 ± 0.07 2.05 ± 0.07 + ConcatAug 2.18 ± 0.07 1.71 ± 0.06 1.93 ± 0.07 2.35 ± 0.07 Reference (synthetic) 2.58 ± 0.08 1.62 ± 0.06 1.83 ± 0.07 2.44 ± 0.07 5-point subjective MOS on both naturalness and speaker similarity was evaluated with 1,000 random samples or pairs of samples from the test set, reported with 95% conﬁdence interval. As Table 3 shows, when the proposed approach for voice preservation was used, both Translatotron 2 and Translatotron obtained about the same speaker similarity MOS as the original Translatotron but signiﬁcantly better translation quality. Translatotron 2 further outperformed Translatotron in terms of translation quality and speech naturalness, which is consistent with the experimental results for translating in a single speaker’s voice (Sec. 5.1, 5.2). It is worth to note that the speaker similarity from S2ST models is capped by the same of the training targets, which by itself is limited. This can be partially due to the performance of the voice-transferring TTS model used for synthesizing the training targets, and partially due to the fact that crosslingual speaker similarity evaluation is more challenging to raters (some rating comments are purely based on language difference), as also observed in Zhang et al. (2019). Obtaining better quality training targets, such as human recordings instead of synthesized speech, may further improve the performance of voice preservation with the proposed approach. 5.4.1. SPEAKER TURNS Speaker similarity evaluation with speaker turns on entire translation speech is challenging because it would require speaker separation on both source and target speeches. The content re-ordering during translation and translation errors would also add extra difﬁculty. We approximated by considering the leading/trailing short segments in the translation speech as corresponding to each of the two speakers in the source speech with a single speaker turn. We trained Translatotron 2 and Translatotron on the dataset described in Sec. 5.4, with half of the training examples augmented by ConcatAug. The evaluation set was artiﬁcially constructed in a similar way by applying ConcatAug, so that each utterance contains two speakers’ voices. We evaluated subjective speaker similarity MOS between the two entire source utterances before ConcatAug and the leading/trailing 1.6-second segments from the translation speech. Evaluation examples with target speech shorter than 2 seconds before ConcatAug were discarded. As can be seen from Table 4, the impact of ConcatAug is consistent on Translatotron 2 and Translatotron. When ConcatAug was not used during training, for each source speaker, the similarity compared to the leading/trailing segment from the translation speech was about the same; and for each segment in the translation speech, the speaker similarity compared to the ﬁrst/second source speaker was also close. This suggests that the translation speech imitated both source speakers at the same time regardless of the speaker turn. When ConcatAug was used, both models obtained signiﬁcantly higher speaker similarity on matched pairs than mismatched pairs, indicating that the models successfully separated two speakers and preserved voices for each of them respectively. It can also be seen that Translatotron 2 obtained signiﬁcantly higher speaker similarity than Translatotron on matched pairs, indicating the effectiveness of Translatotron 2. Such quantitative evaluation cannot reﬂect how the predicted translation speech transits from one speaker’s voice to another speaker’s. Listening to audio samples (available online) veriﬁed that the voice changed instantly on sentence boundaries without blurry, rather than a smoothed transition. A sample of S2ST on such a speaker turn from Translatotron 2 is visualized in Figure 2. While ConcatAug enables S2ST models to preserve speakers’ voices on speaker turns and improves translation quality on small datasets, it may negatively impact the speech naturalness and speaker similarity on models with strong performance, as shown in Table 2 and Table 3. It may be explained by the fact that the augmented utterances sound less natural and may involve abrupt change in volume and background noise on the artiﬁcial speaker turns. This suggests headroom for improvement. 5.5. Multilingual S2ST We also conducted experiments to evaluate the performance of multilingual X→En S2ST. We trained Translatotron 2 and Translatotron on the 4 high-resource language pairs from the CoVoST 2 corpus (Wang et al., 2021a), using Table 5: Ablation studies of multilingual X→En S2ST on 4 highresource language pairs from CoVoST 2, measured by BLEU on ASR transcription from the translation speech. + / −indicates using or replacing a component (see Sec. 5.5.1). Translatotron 2 27.0 18.8 27.7 22.5 −SpecAugment 25.9 17.9 25.9 21.8 −Conformer encoder 26.4 18.1 26.4 21.8 −NAT synthesizer 26.9 18.3 27.0 22.0 Translatotron (w/ SpecAugment) 17.7 9.9 17.7 13.1 + Conformer encoder 18.9 10.8 18.8 13.9 + NAT synthesizer 4.0 2.1 3.5 2.5 ST (Wang et al., 2021a) 27.0 18.9 28.0 23.9 Reference (synthetic) 82.1 86.0 85.1 89.3 TTS synthesized target speech in a single female speaker’s voice.4 The original Common Voice (Ardila et al., 2020) data split instead of the CoVoST 2 data split was followed. The models were not explicitly conditioned on languages. For a fair comparison, both models used SpecAugment, but did not use auxiliary supervision from the source phonemes. The translation quality as measured by BLEU on ASR transcription from the translation speech is shown in the ﬁrst rows of each block in Table 5. Translatotron 2 outperformed Translatotron by +9.4 BLEU on average on the 4 language pairs. Although the BLEU scores are not directly comparable between S2ST and ST (because of ASR transcription and BLEU calculation difference), the close numbers suggest that Translatotron 2 obtained translation quality comparable to the baseline ST model. 5.5.1. ABLATION STUDIES To understand the importance of each component in Translatotron 2, we conducted ablation studies on this multilingual X→En dataset. All models in the ablation used the same input and output features, SpecAugment settings, and learning rate schedules (detailed in Appendix A). No auxiliary supervision from source text was used. For models not using a Conformer encoder, we ﬁrst applied the same 4× temporal subsampling as in the Conformer encoder, then used a 256×8 bidirectional LSTM stack to encoder the subsampled features. The number of parameters in this LSTM encoder is close to the same in the Conformer encoder. For the Translatotron model using a NAT synthesizer, the same hyperparameters as in Translatotron 2 were used. For Translatotron 2 not using a NAT synthesizer, a non-autoregressive Conformer synthesizer (Sec. 5.6) was used. All the rest hyperparameters followed Appendix A for Translatotron 2, and Table 6: Ablation studies on Conversational dataset using an autoregressive RNN + Conv synthesizer and a non-autoregressive Conformer synthesizer. Synthesizer BLEU Naturalness fr de es ca RNN + Conv 55.6 4.21 ± 0.06 Conformer 54.5 3.61 ± 0.09 followed the Conversational model from Jia et al. (2019b) for Translatotron. All models were trained for 200K steps with a batch size of 768. The checkpoints for evaluation were picked by the best average BLEU on 4 language pairs on the validation set. The results are shown in Table 5. As can be seen, while the use of Conformer, SpecAugment, and NAT synthesizer helps the performance of Translatotron 2, replacing them with alternative architectural choices or removing SpecAugment only reduced the performance by a small degree (<2 BLEU). Similarly, directly using these components in Translatotron does not bring its performance close to Translatotron 2. These results suggest that the improvements of Translatotron 2 primarily comes from the high-level architectural design which addressed the performance bottlenecks existing in Translatotron (Sec. 3), rather than the choices of each individual component. 5.6. Non-autoregressive synthesizer It is tempting to use a non-autoregressive architecture for the synthesizer of Translatotron 2, which may signiﬁcantly reduce its inference latency, similar to recent works on nonautoregressive TTS (Ren et al., 2019; 2021; Guo et al., 2021; Lee et al., 2021b; Elias et al., 2021b;a). We experimented with using a 6-layer Conformer synthesizer (Guo et al., 2021) with a dimension of 512 and 8 attention heads on both Conversational and CoVoST 2 datasets. As can be seen from Table 5 and 6, using a Conformer-based non-autoregressive synthesizer obtained comparable translation quality to using an autoregressive NAT synthesizer (with BLEU on ASR transcription up to 1.1 BLEU lower). However, it caused a signiﬁcant regression on the naturalness of the predicted translation speech, which is consistent with the observation in TTS in Shen et al. (2020); Peng et al. (2020); Hwang et al. (2021), etc., suggesting more exploration is needed on this direction.\n\n2. Related works\nS2ST Until very recently, automatic S2ST systems are typically composed of a cascade of ASR, MT, and TTS components (Lavie et al., 1997; Wahlster, 2000; Nakamura et al., 2006; ITU, 2016). Translatotron (Jia et al., 2019b) is the ﬁrst direct S2ST model, which is a sequence-to-sequence model trained end-to-end in a multi-objective task. It has shown reasonable translation quality and speech naturalness, but still underperformed a baseline of ST →TTS cascade by a large margin. It also demonstrated the capacity of preserving speakers’ voices during the translation, by leveraging a speaker encoder separately trained in a speaker veriﬁcation task (Wan et al., 2018; Jia et al., 2018). A few recent works proposed cascade S2ST systems using learned discrete speech representation as the intermediate representation instead of text or phoneme. Tjandra et al. (2019) introduced such an S2ST system that ﬁrst translated the source speech into a discrete representation of the target speech which was predicted from a separately trained VQVAE (Oord et al., 2017), then constructed the target speech spectrogram from the discrete representation using the VQVAE decoder. Zhang et al. (2021) additionally trained the VQ-VAE jointly with a supervised phoneme recognition objective in different languages. Lee et al. (2022; 2021a) used a separately trained vocoder to directly predict waveform from the discrete representation without relying on spectrogram; for the best performance, this vocoder included a duration predictor, akin to a generative TTS model. All these works require multiple components being trained in multiple steps, but are not able to be trained end-to-end. Another potential limitation of such an approach is that it may not be effective in preserving paralinguistic and nonlinguistic information. Oppositely, it can be desired that such variation be removed in the discrete representation (Lee et al., 2021a). Kano et al. (2021) introduced an end-to-end S2ST model with a cascade of three autoregressive decoders, and used pre-trained MT and TTS models as teacher models to facilitate the training of the end-to-end model. It requires pre-trained ASR, MT, and TTS models, and the end-to-end model itself has to be trained in multiple steps. While most of these works conducted experiments using synthetic datasets with translation speech in a clean single speaker’s voice, Jia et al. (2019b); Lee et al. (2021a) conducted experiments using multi-speaker human recordings. Although these recent works generated speech translation in novel ways without relying on TTS subsystems, only a few of them (Jia et al., 2019b; Lee et al., 2022) have evaluated the perceptual quality (e.g. naturalness) of the produced speech translation, which is critical to S2ST (Wagner et al., 2019; Salesky et al., 2021), with the rest focused only on the translation quality. TTS Translatotron uses a decoder similar to the Tacotron 2 TTS model (Shen et al., 2018), which is an attention-based autoregressive decoder. Due to the ﬂexibility of the attention mechanism, they both suffer from robustness issues such as over-generation. Recent TTS models such as FastSpeech (Ren et al., 2019; 2021) and Non-Attentive Tacotron (NAT) (Shen et al., 2020) demonstrated that replacing the attention module with a duration-based upsampler yields more robust synthesized speech, as quantitatively evaluated at a large scale in Shen et al. (2020). The synthesizer component in this work resembles these works. Voice conversion and anti-spooﬁng The performance of voice conversion has progressed rapidly in the recent years, and is reaching a quality that is hard for automatic speaker veriﬁcation (ASV) systems to detect (Yi et al., 2020). ASVspoof 2019 (Todisco et al., 2019; Wang et al., 2020) found that it was challenging to detect spoof audios generated from a zero-shot voice cloning TTS model (Jia et al., 2018), which was followed by the original Translatotron for preserving speakers’ voices during S2ST. Such progress poses concerns on related techniques being misused for creating spooﬁng artifacts. We propose a new voice preservation method for S2ST with the motivation of avoiding such potential misuse.\n\n6. Conclusion\nWe proposed Translatotron 2, a neural direct S2ST model that can be trained end-to-end. Experimental results on three datasets consistently suggest that Translatotron 2 outperforms the original Translatotron by a large margin on both translation quality (up to +15.5 BLEU) and speech generation quality, and approaches cascade S2ST. In addition, we proposed a simple method for preserving speakers’ voices from the source speech to the translation speech in a different language. Unlike existing approaches, the proposed method is able to preserve each speaker’s voice on speaker turns without requiring for speaker segmentation. Furthermore, compared to existing approaches, it better preserves speaker’s privacy and mitigates potential misuse of voice cloning for creating spooﬁng audio artifacts. Future works include extending Translatotron 2 to support simultaneous translation, cross-lingual prosody transfer, unwritten languages, and further quality improvement by utilizing self-supervised pre-training (Baevski et al., 2020; Wang et al., 2021b) and weakly supervised data (Jia et al., 2019a).\n\nAcknowledgements The authors would like to thank Chung-Cheng Chiu, Quan Wang, Heiga Zen, Ron J. Weiss, Wolfgang Macherey, Yu Zhang, Yonghui Wu, Hadar Shemtov, Ruoming Pang, Nadav Bar, Michael Hassid, and the rest of the Google Research team for helpful discussions and previous work on data preparation.\n\nReferences\nAnastasopoulos, A., Bojar, O., Bremerman, J., Cattoni, R., Elbayad, M., Federico, M., Ma, X., Nakamura, S., Negri, M., Niehues, J., et al. Findings of the IWSLT 2021 evaluation campaign. In International Conference on Spoken Language Translation (IWSLT), 2021. Ansari, E., Axelrod, A., Bach, N., Bojar, O., Cattoni, R., Dalvi, F., Durrani, N., Federico, M., Federmann, C., Gu, J., et al. Findings of the IWSLT 2020 evaluation campaign. In International Conference on Spoken Language Translation (IWSLT), 2020. Ardila, R., Branson, M., Davis, K., Henretty, M., Kohler, M., Meyer, J., Morais, R., Saunders, L., Tyers, F. M., and Weber, G. Common Voice: A massively-multilingual speech corpus. In Proceedings of Language Resources and Evaluation Conference (LREC), 2020. Baevski, A., Zhou, H., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. In Advances in Neural Information Processing Systems (NeurIPS), 2020. Battenberg, E., Skerry-Ryan, R., Mariooryad, S., Stanton, D., Kao, D., Shannon, M., and Bagby, T. Locationrelative attention mechanisms for robust long-form speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Chen, M., Chen, M., Liang, S., Ma, J., Chen, L., Wang, S., and Xiao, J. Cross-lingual, multi-speaker text-tospeech synthesis using neural speaker embedding. In Proc. Interspeech, 2019. Chiu, C.-C., Tripathi, A., Chou, K., Co, C., Jaitly, N., Jaunzeikare, D., Kannan, A., Nguyen, P., Sak, H., Sankar, A., Tansuwan, J., Wan, N., Wu, Y., and Zhang, X. Speech recognition for medical conversations. In Proc. Interspeech, 2018. Di Gangi, M. A., Negri, M., and Turchi, M. One-to-many multilingual end-to-end speech translation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Skerry-Ryan, R., and Wu, Y. Parallel Tacotron 2: A non-autoregressive neural TTS model with differentiable duration modeling. In Proc. Interspeech, 2021a. Elias, I., Zen, H., Shen, J., Zhang, Y., Jia, Y., Weiss, R., and Wu, Y. Parallel Tacotron: Non-autoregressive and controllable TTS. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021b. Grifﬁn, D. and Lim, J. Signal estimation from modiﬁed short-time Fourier transform. IEEE Transactions on Acoustics, Speech, and Signal Processing, 32(2):236–243, 1984. Gulati, A., Qin, J., Chiu, C.-C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., and Pang,\nR. Conformer: Convolution-augmented transformer for\nspeech recognition. In Proc. Interspeech, 2020. Guo, P., Boyer, F., Chang, X., Hayashi, T., Higuchi, Y., Inaguma, H., Kamo, N., Li, C., Garcia-Romero, D., Shi, J., et al. Recent developments on espnet toolkit boosted by conformer. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5874– 5878, 2021. He, M., Deng, Y., and He, L. Robust sequence-to-sequence acoustic modeling with stepwise monotonic attention for neural TTS. In Proc. Interspeech, 2019. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735–1780, 1997. Hwang, M.-J., Yamamoto, R., Song, E., and Kim, J.-M. TTS-by-TTS: TTS-driven data augmentation for fast and high-quality speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6598–6602, 2021. ITU. ITU-T F.745: Functional requirements for networkbased speech-to-speech translation services, 2016. International Telecommunication Union. Jia, Y., Zhang, Y., Weiss, R. J., Wang, Q., Shen, J., Ren, F., Chen, Z., Nguyen, P., Pang, R., Moreno, I. L., and Wu, Y. Transfer learning from speaker veriﬁcation to multispeaker text-to-speech synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Jia, Y., Johnson, M., Macherey, W., Weiss, R. J., Cao, Y., Chiu, C.-C., Ari, N., Laurenzo, S., and Wu, Y. Leveraging weakly supervised data to improve end-to-end speechto-text translation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019a. Jia, Y., Weiss, R. J., Biadsy, F., Macherey, W., Johnson, M., Chen, Z., and Wu, Y. Direct speech-to-speech translation with a sequence-to-sequence model. In Proc. Interspeech, 2019b. Jia, Y., Zen, H., Shen, J., Zhang, Y., and Wu, Y. PnG BERT: Augmented BERT on phonemes and graphemes for neural TTS. In Proc. Interspeech, 2021. Jia, Y., Tadmor Ramanovich, M., Wang, Q., and Zen, H. CVSS corpus and massively multilingual speechto-speech translation. In Proceedings of Language Resources and Evaluation Conference (LREC), 2022. Kahn, J., Rivi`ere, M., Zheng, W., Kharitonov, E., Xu, Q., Mazar´e, P.-E., Karadayi, J., Liptchinsky, V., Collobert, R., Fuegen, C., Likhomanenko, T., Synnaeve, G., Joulin, A., Mohamed, A., and Dupoux, E. Libri-light: A benchmark for ASR with limited or no supervision. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A. v. d., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural audio synthesis. In Proceedings of International Conference on Machine Learning (ICML), 2018. Kano, T., Sakti, S., and Nakamura, S. Transformer-based direct speech-to-speech translation with transcoder. In IEEE Spoken Language Technology Workshop (SLT), 2021. Kingma, D. P. and Ba, J. Adam: A method for stochastic optimization. In International Conference on Learning Representations (ICLR), 2015. Krueger, D., Maharaj, T., Kram´ar, J., Pezeshki, M., Ballas, N., Ke, N. R., Goyal, A., Bengio, Y., Courville, A., and Pal, C. Zoneout: Regularizing RNNs by randomly preserving hidden activations. In International Conference on Learning Representations (ICLR), 2017. Lavie, A., Waibel, A., Levin, L., Finke, M., Gates, D., Gavalda, M., Zeppenfeld, T., and Zhan, P. JANUS-III: Speech-to-speech translation in multiple languages. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1997. Lee, A., Gong, H., Duquenne, P.-A., Schwenk, H., Chen, P.-J., Wang, C., Popuri, S., Pino, J., Gu, J., and Hsu, W.-N. Textless speech-to-speech translation on real data. arXiv preprint arXiv:2112.08352, 2021a. Lee, A., Chen, P.-J., Wang, C., Gu, J., Ma, X., Polyak, A., Adi, Y., He, Q., Tang, Y., Pino, J., et al. Direct speechto-speech translation with discrete units. In Proceedings of Annual Meeting of the Association for Computational Linguistics (ACL), 2022. Lee, Y., Shin, J., and Jung, K. Bidirectional variational inference for non-autoregressive text-to-speech. In International Conference on Learning Representations (ICLR), 2021b. Ma, X., Gong, H., Liu, D., Lee, A., Tang, Y., Chen, P.-J., Hsu, W.-N., Heaﬁeld, K., Koehn, P., and Pino, J. Direct simultaneous speech to speech translation. arXiv preprint arXiv:2110.08250, 2021. McCarthy, A. D., Puzon, L., and Pino, J. SkinAugment: Auto-encoding speaker conversions for automatic speech translation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. Nakamura, S., Markov, K., Nakaiwa, H., Kikui, G., Kawai, H., Jitsuhiro, T., Zhang, J.-S., Yamamoto, H., Sumita, E., and Yamamoto, S. The ATR multilingual speech-tospeech translation system. IEEE Transactions on Audio, Speech, and Language Processing, 2006. Narayanan, A., Prabhavalkar, R., Chiu, C.-C., Rybach, D., Sainath, T. N., and Strohman, T. Recognizing long-form speech using streaming end-to-end models. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., Driessche, G., Lockhart, E., Cobo, L., Stimberg, F., et al. Parallel WaveNet: Fast highﬁdelity speech synthesis. In Proceedings of International Conference on Machine Learning (ICML), 2018. Oord, A. v. d., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. LibriSpeech: an ASR corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015. Park, D. S., Chan, W., Zhang, Y., Chiu, C.-C., Zoph, B., Cubuk, E. D., and Le, Q. V. SpecAugment: A simple data augmentation method for automatic speech recognition. In Proc. Interspeech, 2019. Park, D. S., Zhang, Y., Jia, Y., Han, W., Chiu, C.-C., Li, B., Wu, Y., and Le, Q. V. Improved noisy student training for automatic speech recognition. In Proc. Interspeech, 2020. Pathak, M. A. and Raj, B. Privacy-preserving speaker veriﬁcation and identiﬁcation using Gaussian mixture models. IEEE Transactions on Audio, Speech, and Language Processing, 21(2):397–406, 2012. Peng, K., Ping, W., Song, Z., and Zhao, K. Nonautoregressive neural text-to-speech. In Proceedings of International Conference on Machine Learning (ICML), 2020. Post, M., Kumar, G., Lopez, A., Karakos, D., CallisonBurch, C., and Khudanpur, S. Improved speech-to-text translation with the Fisher and Callhome Spanish–English speech translation corpus. In International Conference on Spoken Language Translation (IWSLT), 2013. Ren, Y., Ruan, Y., Tan, X., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. FastSpeech: Fast, robust and controllable text to speech. In Advances in Neural Information Processing Systems (NeurIPS), 2019. Ren, Y., Hu, C., Qin, T., Zhao, S., Zhao, Z., and Liu, T.-Y. FastSpeech 2: Fast and high-quality end-to-end text-tospeech. In International Conference on Learning Representations (ICLR), 2021. Salesky, E., M¨ader, J., and Klinger, S. Assessing evaluation metrics for speech-to-speech translation. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021. Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomyrgiannakis, Y., and Wu, Y. Natural TTS synthesis by conditioning WaveNet on Mel spectrogram predictions. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. Shen, J., Nguyen, P., Wu, Y., Chen, Z., et al. Lingvo: A modular and scalable framework for sequence-to-sequence modeling. arXiv preprint arXiv:1902.08295, 2019. Shen, J., Jia, Y., Chrzanowski, M., Zhang, Y., Elias, I., Zen, H., and Wu, Y. Non-Attentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling. arXiv preprint arXiv:2010.04301, 2020. Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., and Wojna, Z. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2818–2826, 2016. Tjandra, A., Sakti, S., and Nakamura, S. Speech-to-speech translation between untranscribed unknown languages. In IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019. Todisco, M., Wang, X., Vestman, V., Sahidullah, M., Delgado, H., Nautsch, A., Yamagishi, J., Evans, N., Kinnunen, T., and Lee, K. A. ASVspoof 2019: Future horizons in spoofed and fake audio detection. In Proc. Interspeech, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. Wagner, P., Beskow, J., Betz, S., Edlund, J., Gustafson, J., Eje Henter, G., Le Maguer, S., Malisz, Z., Sz´ekely, ´E., T˚annander, C., et al. Speech synthesis evaluation – stateof-the-art assessment and suggestion for a novel research program. In Proceedings of the 10th Speech Synthesis Workshop (SSW10), 2019. Wahlster, W. Verbmobil: Foundations of speech-to-speech translation. Springer, 2000. Wan, L., Wang, Q., Papir, A., and Moreno, I. L. Generalized end-to-end loss for speaker veriﬁcation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. Wang, C., Wu, A., and Pino, J. CoVoST 2: A massively multilingual speech-to-text translation corpus. In Proc. Interspeech, 2021a. Wang, C., Wu, A., Pino, J., Baevski, A., Auli, M., and Conneau, A. Large-scale self-and semi-supervised learning for speech translation. In Proc. Interspeech, 2021b. Wang, X., Yamagishi, J., Todisco, M., Delgado, H., Nautsch, A., Evans, N., Sahidullah, M., Vestman, V., Kinnunen, T., Lee, K. A., et al. ASVspoof 2019: A large-scale public database of synthesized, converted and replayed speech. Computer Speech & Language, 64:101114, 2020. Weiss, R. J., Chorowski, J., Jaitly, N., Wu, Y., and Chen,\nZ. Sequence-to-sequence models can directly translate\nforeign speech. In Proc. Interspeech, 2017. for cross-lingual TTS. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021. Yi, Z., Huang, W.-C., Tian, X., Yamagishi, J., Das, R. K., Kinnunen, T., Ling, Z., and Toda, T. Voice conversion challenge 2020: Intra-lingual semi-parallel and crosslingual voice conversion. In Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge, 2020. Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. Interspeech, 2019. Zhang, C., Tan, X., Ren, Y., Qin, T., Zhang, K., and Liu, T.-\nY. UWSpeech: Speech to speech translation for unwritten\nlanguages. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (AAAI), 2021. Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Chen, Z., SkerryRyan, R., Jia, Y., Rosenberg, A., and Ramabhadran, B. Learning to speak ﬂuently in a foreign language: Multilingual speech synthesis and cross-language voice cloning. In Proc. Interspeech, 2019. Zheng, Y., Wang, X., He, L., Pan, S., Soong, F. K., Wen, Z., and Tao, J. Forward-backward decoding for regularizing end-to-end TTS. In Proc. Interspeech, 2019.\n\nA. Table of hyper-parameters\nTable 7: Model hyper-parameters used in the experiments. (“×n”: n layers; †: 128-dim pre-net is used for translating in a single voice; 16-dim pre-net is used for voice preservation.) Input Sample rate (Hz) 8,000 48,000 16,000 – 48,000 Mel channels 80 Mel lower band (Hz) 125 Mel upper band (Hz) 3,800 7,600 7,600 Frame size (ms) 25.0 Frame step (ms) 10.0 Output Sample rate (Hz) 24,000 Mel channels 128 Mel lower band (Hz) 20 Mel upper band (Hz) 12,000 Frame size (ms) 50.0 Frame step (ms) 12.5 SpecAugment Freq blocks 2 Time blocks 10 Freq block max length ratio 0.33 Time block max length ratio 0.05 Encoder Conformer dims 144 × 16 Attention heads 4 Conv kernal size 32 Subsample factor 4 Attention Output dim 256 512 512 Hidden dim 512 512 512 Attention heads 4 8 8 Dropout prob 0.1 0.2 0.2 Decoder LSTM dims 256 × 4 512 × 6 512 × 4 Zoneout prob 0.1 0.1 0.1 Phoneme embedding dim 96 256 256 Label smoothing uncertainty 0.1 0.1 0.1 Loss weight 10.0 10.0 10.0 Duration predictor Bi-LSTM (dim × layers) 64 × 2 128 × 2 128 × 2 Loss weight 1.0 1.0 1.0 Synthesizer LSTM dims 1,024 × 2 LSTM zoneout prob 0.1 Pre-net dims 128 × 2 128 × 2 128 / 16 † × 2 Pre-net dropout prob 0.5 Post-net (kernel, channels) × layers (5, 512) × 4 + (5, 128) Loss weight 1.0 Training Optimizer Adam (Kingma & Ba, 2015) Learning rate schedule Vaswani et al. (2017) Learning rate (peak) 4.2×10−3 2.2×10−3 3.3×10−3 Warm-up steps Batch size 1,024 768 768 L2 regularization weight 10−6 10−6 10−6 Fisher Es-En CoVoST 2 Conversational src vs src tgt vs src tgt vs tgt s2st vs src s2st vs tgt Figure 3: Afﬁnity matrices of d-vector similarity among 100 random examples. (“s2st” refers to the predictions from Translatotron 2.) Table 8: Objective d-vector similarity between the predicted translated speech (English) and the source human speech (Spanish) on speaker turns. The similarity between the leading/trailing 1.6-second segment from the predicted speech and the entire 1st/2nd source speaker’s speech is measured. (↑/ ↓means higher/lower values are better.) 1st source speaker 2nd source speaker Leading seg. ↑ Trailing seg. ↓ Leading seg. ↓ Trailing seg. ↑ Translatotron 2 0.21 0.19 0.21 0.19 + ConcatAug 0.20 0.14 0.14 0.21 Translatotron 0.20 0.22 0.27 0.29 + ConcatAug 0.32 0.16 0.14 0.35 Reference (synthetic) 0.48 0.17 0.15 0.48\nB. Objective speaker similarity analysis\nSubjective speaker similarity evaluation is costly and has a long turnaround. We explored alternative objective evaluation using separately trained speaker encoders, such as d-vector (Wan et al., 2018). We evaluated the voice retention performance using the cosine similarity of the d-vectors. We ﬁrst checked the scenario that each input contains a single speaker’s recording. Figure 3 visualizes the afﬁnity matrices of d-vector similarity among different input utterances for a Translatotron 2 model. The outstanding higher similarity values on the diagonals indicate that the model is able to preserve the source speaker’s voice in the predicted translation speech. We then conducted a detailed evaluation for the voice retention performance for speaker turns. The experiment setting up was identical to Section 5.4.1, except that the speaker similarity was measured by d-vector similarity instead of subjective MOS evaluation. The d-vectors for each source speaker were computed on the entire original utterance before concatenation; the d-vectors for each speaker in the prediction is approximated by computing on the leading/trailing 1.6 seconds of predicted speech. The results are shown in Table 8. Consistent with the MOS evaluation results in Table 4, when the concatenation augmentation was not used, the d-vector similarity to each source speaker is about the same regardless if it was compared to the leading or trailing segments, indicating that the predicted speech was in a single speaker’s voice and the model was unable to separate different speakers in the input, but rather optimized for both source speakers at the same time. When the concatenation augmentation was used, the d-vector similarity was signiﬁcantly higher between matched pairs than between unmatched pairs, indicating that the models were able to separate different speakers in the input and preserve their voices in the predicted translation speech respectively. However, when these similarities are compared among different models, it seems to suggest that Translatotron performed better than Translatotron 2, which is contradictory to the subjective evaluation results in Table 4. By carefully listening to the audio samples, we found that such discrepancy may be due to the fact that the d-vector model was also sensitive to non-voice related acoustic characteristics, such as reverb and channel noise in the audios. This is likely a consequence of the fact that in the large-scale training set for the d-vector model used in the evaluation, each speaker is typically associated with a particular recording condition, e.g. recording device and room. Because the encoder output from the Translatotron model was of signiﬁcantly larger dimension than from the Translatotron 2 model (2048 vs 144), it was capable of carrying more non-voice acoustic information and thus obtained better d-vector similarity, which not necessarily indicating higher speaker similarity. These results suggest that while such speaker encoder-based objective analysis reveals insightful indications about the performance of the S2ST models, it can be less reliable compared to subjective MOS evaluation. Such reliability also highly depends on the training details of the speaker encoder model being used, especially the training corpus.\n--- FOOTNOTES 1Google Research. Correspondence to: 10K 20K 10K", "persona": "Student", "response": "### 1. Overview\nSpeech-to-speech translation (S2ST) is key to breaking down language barriers between people all over the world. Automatic S2ST systems are typically composed of a cascade of speech recognition, machine translation, and speech synthesis subsystems. However, such cascade systems may suffer from longer latency, loss of information (especially paralinguistic and non-linguistic information), and compounding errors between subsystems. Performance of multilingual X=>En S2ST on the CoVoST 2 corpus. On this task, Translatotron 2 again outperformed the original Translatotron by a large margin. Although the results are not directly comparable between S2ST and ST, the close numbers suggest that the translation quality from Translatotron 2 is comparable to a baseline speech-to-text translation model, These results indicate that Translatotron 2 is also highly effective on multilingual S2ST. Translatotron 2 To address this, we designed Translatotron 2 to use only a single speech encoder, which is responsible for both linguistic understanding and voice capture. In this way, the trained models cannot be directed to reproduce non-source voices. This approach can also be applied to the original Translatotron. Following are examples of direct speech-to-speech translation from Translatotron 2 in which the source speaker’s voice is retained: Translatotron 2 outperforms the original Translatotron by large margins in every aspect we measured: higher translation quality (measured by BLEU, where higher is better), speech naturalness (measured by MOS, higher is better), and speech robustness (measured by UDR, lower is better).\n\n### 2. Background\nIn 2019, we introduced Translatotron, the first ever model that was able to directly translate speech between two languages. This direct S2ST model was able to be efficiently trained end-to-end and also had the unique capability of retaining the source speaker’s voice (which is non-linguistic information) in the translated speech. However, despite its ability to produce natural sounding translated speech in high fidelity, it still underperformed compared to a strong baseline cascade S2ST system (e.g., composed of a direct speech-to-text translation model [1, 2] followed by a Tacotron 2 TTS model). It particularly excelled on the more difficult Fisher corpus. The performance of Translatotron 2 on translation quality and speech quality approaches that of a strong baseline cascade system, and is better than the cascade baseline on speech robustness.\n\n### 3. Methods\nTranslatotron 2 is composed of four major components: a speech encoder, a target phoneme decoder, a target speech synthesizer, and an attention module that connects them together. The combination of the encoder, the attention module, and the decoder is similar to a typical direct speech-to-text translation (ST) model. The synthesizer is conditioned on the output from both the decoder and the attention. Model architecture of Translatotron 2 (for translating Spanish speech into English speech). There are three novel changes between Translatotron and Translatotron 2 that are key factors in improving the performance: While the output from the target phoneme decoder is used only as an auxiliary loss in the original Translatotron, it is one of the inputs to the spectrogram synthesizer in Translatotron 2. This strong conditioning makes Translatotron 2 easier to train and yields better performance. The spectrogram synthesizer in the original Translatotron is attention-based, similar to the Tacotron 2 TTS model, and as a consequence, it also suffers from the robustness issues exhibited by Tacotron 2. In contrast, the spectrogram synthesizer employed in Translatotron 2 is duration-based, similar to that used by Non-Attentive Tacotron, which drastically improves the robustness of the synthesized speech. Both Translatotron and Translatotron 2 use an attention-based connection to the encoded source speech. However, in Translatotron 2, this attention is driven by the phoneme decoder instead of the spectrogram synthesizer.\n\n### 4. Results\nMore Powerful and Responsible Voice Retention Translation quality (measured by BLEU, where higher is better) evaluated on two Spanish-English corpora. Speech robustness (measured by UDR, where lower is better) evaluated on two Spanish-English corpora. Multilingual Speech-to-Speech Translation This ensures the acoustic information that the spectrogram synthesizer sees is aligned with the translated content that it’s synthesizing, which helps retain each speaker’s voice across speaker turns. Input (Spanish): TTS-synthesized reference (English): Translatotron 2 prediction (English): Translatotron prediction (English): Input (Spanish): TTS-synthesized reference (English): Translatotron 2 (with ConcatAug) prediction (English): Translatotron 2 (without ConcatAug) prediction (English): Besides Spanish-to-English S2ST, we also evaluated the performance of Translatotron 2 on a multilingual set-up in which the model took speech input from four different languages and translated them into English.\n\n### 5. Takeaways\nIn “Translatotron 2: Robust direct speech-to-speech translation”, we describe an improved version of Translatotron that significantly improves performance while also applying a new method for transferring the source speakers’ voices to the translated speech. The revised approach to voice transference is successful even when the input speech contains multiple speakers speaking in turns while also reducing the potential for misuse and better aligning with our AI Principles. Experiments on three different corpora consistently showed that Translatotron 2 outperforms the original Translatotron by a large margin on translation quality, speech naturalness, and speech robustness. The original Translatotron was able to retain the source speaker's voice in the translated speech, by conditioning its decoder on a speaker embedding generated from a separately trained speaker encoder. However, this approach also enabled it to generate the translated speech in a different speaker's voice if a clip of the target speaker's recording were used as the reference audio to the speaker encoder, or if the embedding of the target speaker were directly available. While this capability was powerful, it had the potential to be misused to spoof audio with arbitrary content, which posed a concern for production deployment. To retain speakers' voices across translation, researchers generally prefer to train S2ST models on parallel utterances with the same speaker's voice on both sides. Such a dataset with human recordings on both sides is extremely difficult to collect, because it requires a large number of fluent bilingual speakers. To avoid this difficulty, we use a modified version of PnG NAT, a TTS model that is capable of cross-lingual voice transferring to synthesize such training targets. Our modified PnG NAT model incorporates a separately trained speaker encoder in the same way as in our previous TTS work — the same strategy used for the original Translatotron — so that it is capable of zero-shot voice transference. To enable S2ST models to retain each speaker’s voice in the translated speech when the input speech contains multiple speakers speaking in turns, we propose a simple concatenation-based data augmentation technique, called ConcatAug. This method augments the training data on the fly by randomly sampling pairs of training examples and concatenating the source speech, the target speech, and the target phoneme sequences into new training examples.", "paper_id": "29", "story_id": "29-01"}
{"paper": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data Joel Shor1, Dotan Emanuel1, Oran Lang1, Omry Tuval1, Michael Brenner1, Julie Cattiau1, Fernando Vieira2, Maeve McNally2, Taylor Charbonneau2, Melissa Nollstadt2, Avinatan Hassidim1, Yossi Matias1 {joelshor, dotan, oranl}@google.com\n\nAbstract\nAutomatic speech recognition (ASR) systems have dramatically improved over the last few years. ASR systems are most often trained from ‘typical’ speech, which means that underrepresented groups don’t experience the same level of improvement. In this paper, we present and evaluate ﬁnetuning techniques to improve ASR for users with non-standard speech. We focus on two types of non-standard speech: speech from people with amyotrophic lateral sclerosis (ALS) and accented speech. We train personalized models that achieve 62% and 35% relative WER improvement on these two groups, bringing the absolute WER for ALS speakers, on a test set of message bank phrases, down to 10% for mild dysarthria and 20% for more serious dysarthria. We show that 71% of the improvement comes from only 5 minutes of training data. Finetuning a particular subset of layers (with many fewer parameters) often gives better results than ﬁnetuning the entire model. This is the ﬁrst step towards building state of the art ASR models for dysarthric speech. Index Terms: speech recognition, personalization, accessibility\n\n1. Introduction\nState-of-the-art speaker-independent ASR systems are made possible by large datasets and data-driven algorithms. This setup works well when the dataset contains a large amount of data from all types of voices that one would want the system to recognize, but fails on groups not well-represented in the data: ASR models trained on thousands of hours of ‘typical users’ recognize voices in the typical use-case, but often fail to recognize accented voices or ones impacted by medical conditions. Getting ASR to work on non-standard speech is difﬁcult for a few reasons. First, it can be difﬁcult to ﬁnd enough speakers to train a state-of-the-art model. Second, individuals within a group like ‘ALS’ or a particular accent can have very different ways of speaking. This paper’s approach overcomes data scarcity by beginning with a base model trained on thousands of hours of standard speech. It gets around sub-group heterogeneity by training personalized models. This paper details the technical approach to Project Euphonia, an accessibility project announced at Google I/O 2019 [1, 2]. To demonstrate our approach, we focus on accented speech from non-native English speakers, and speech from individuals living with ALS. ALS is a progressive neurodegenerative disorder that affects speech production, among other things. About 25% of people with ALS experience slurred speech as their ﬁrst symptom [3]. Most people with ALS eventually lose mobility, so they would especially beneﬁt from being able to interact verbally with smart home devices. The severity of dysarthria for people with ALS is measured with an FRS score, ranging from 0 to 4, with 0 being incomprehensible and 4 normal [4]. In this paper, we improve state-of-the-art ASR for individuals with ALS FRS 1-3 and heavy accents. The main contributions of this paper are: 1. A ﬁnetuning technique for personalizing an ASR model\non dysarthric speech that yields word error rates on a test set of message bank phrases [5] of 10.8% for mild dysarthria (FRS of 3) and 20.9% for severe dysarthria (FRS of 1-2). The ﬁnetuning technique yields a word error rate of 8.5% on heavily accented data. The ﬁnetuned models make recognition mistakes that are distributionally more similar to standard ASR mistakes on standard speech.\n\n2. Demonstrating that the ﬁnetuning gives signiﬁcant improvement over the base model in multiple scenarios,\nincluding different non-standard speech (dysarthric and accented), and on different architectures.\n\n3. On the ALS speech, 71% of the relative WER improvement can be achieved with 5 minutes of data, 80% with\n10 minutes of data. Furthermore, for some models, just training the encoder produces better results, and just training the layers closest to the input yields 90% of total relative improvement.\n\n3. Experiments\nWe create personalized ASR models by starting with a base model trained on standard, unaccented speech. This approach is much more resource efﬁcient than retraining the entire model, from scratch, for each speaker. In order to verify that we are learning something other than just the idiosyncrasies of a particular model, we run most of our experiments starting from two different models: a Bidirectional RNN Transducer (RNNT) model [6], and a Listen, Attend, and Spell (LAS) model [9]. Both are end-to-end sequence-to-sequence models. We follow the training procedure in [8] for the RNN-T model and [10] for the LAS model (on the 1000 hour, open source Librispeech dataset [22]). We ﬁnetune different layer combinations on different amounts of data, on both ALS and accents datasets. We ﬁnetune models per speaker. 3.1. Data 3.1.1. ALS We collected 36.7 hours of audio from 67 people with ALS, in recorded themselves on their home computers using custom software. The sentences were collected from three sources: The Cornell Movie-Dialogs Corpus [23], a collection of sentences used by text-to-speech voice actors, and a modiﬁed selection of sentences from the Boston Children’s Hospital [5]. Note that this corpus is a restricted language domain, but is phonetically very similar to other corpora e.g. Librispeech. The FRS scores of participants were measured by ALS-TDI, and we only evaluate on people with speech FRS 3 and below (17 speakers, 22.1 hours). See the attached multimedia ﬁle for audio examples. 3.1.2. Accented Speech To test our ﬁnetuning method on another type of non-standard speech, we use the L2 Arctic dataset of non-native speech [24]. This dataset consists of 20 speakers with approximately 1 hour of speech per speaker. Each speaker recorded a set of 1150 phonetically balanced utterances. For each of the 20 speakers, we split the data into 90/10 train and test. All of the sentences which contains proper nouns are used in the training set, in order to remove the possibility of the model to artiﬁcially achieve better results on the test set by memorizing them. 3.2. Base Models All our base networks and ﬁnetuning are trained on 80-bin logmel spectrograms computed from a 25ms window and a 10ms hop. We use the same technique presented in [25] and used in [8]: we stack frames in groups of 3 and process them as one ’super-frame.’ We trained RNN-T and LAS architectures (Figure 1). All training was performed with the multicondition training (MTR) techniques described in [8]. During training, we distorted the audio using a room simulator derived from YouTube data. The average SNR of the added noise is 12dB. We use the TensorFlow library Lingvo [26]. 3.2.1. Bidirectional RNN-Transducer In this paper, we primarily work with a bidirectional RNNTransducer (RNN-T) architecture that achieves near state-ofthe-art performance. This architecture was ﬁrst introduced in [6]. We use the version presented in [8]. The network maps acoustic frames to word pieces, which are a linguistic representation between graphemes and words [11]. It has a 5 layer bidirectional convolutional LSTM encoder, a 2 layer LSTM decoder, and a joint layer. It has 49.6M parameters in total. 3.2.2. Listen, Attend, and Spell model To verify which results generalize beyond a particular architecture, we also run some of our experiments with a Listen, Attend, and Spell (LAS) model trained on the open source Librispeech dataset [22]. This architecture was ﬁrst introduced in [9]. The model that we use is described in [10]. It is a sequenceto-sequence with attention model that maps acoustic frames to graphemes. We use an encoder with 4 layers of bidrectional convolutional LSTMs and a 2 layer RNN decoder. The model has a total of 132M parameters. Our grapheme targets are the 26 English lower-case letters, punctuation symbols, and a space. There are 33 target grapheme symbols. The base LAS model was trained to 1M steps on all 960 hours of the Librispeech dataset. It achieved a 5.5% WER on the clean test split and 15.5% on the non-clean test split (the two standard test splits given in the dataset). Table 1: Average WER Improvements Cloud RNN-T LAS Base Finetune Base Finetune Arctic1 24.0 13.3 8.5 22.6 11.3 ALS2 42.7 59.7 20.9 86.3 31.3 ALS3 13.1 33.1 10.8 49.6 17.2 repeating, Speech combined with nonvocal communication. Figure 2: Average Relative WER improvement as a function of the amount of training data. 3.3. Finetuning All our ﬁnetuning uses four Tesla V100 GPUs for no more than four hours. 3.3.1. RNN-T We started by ﬁnetuning 1, 2, and 3 layers in ﬁxed combinations (treating the decoder as a single layer), on both datasets, adjusting hyperparameters as necessary. Let Ei denote the ith layer of the encoder, where lower-numbered layers are closer to the input. We uniformly found that training from E0 up with or without the joint layer was always better than the other methods, so we focused our search on training from E0 up, with or without the joint layer. Next, for each ALS and accented individual, we exhaustively searched our ﬁnetuning space, with various amounts of data. For the RNN-T, this meant ﬁnetuning each of E0, E0-E1, E0-E2, etc.. the entire encoder, with or without the joint layer. 3.3.2. LAS For LAS architectures, we ﬁnetuned various layer combinations and consistently found that the best results from this network came from ﬁnetuning all layers. All results reported in this paper for the LAS network are on ﬁnetuning the entire network, unless otherwise speciﬁed.\n\n4. Results\n4.1. Performance We report our absolute word error rate in Table 1. The results show dramatic improvement over Google cloud ASR [27] model for very non-standard speech (heavy accents and ALS Figure 3: The distribution of phoneme mistakes before and after ﬁnetuning. x-axis is SAMPA phoneme. y-axis is number of times that phoneme was deleted or substituted divided by number of phoneme occurances in the ground truth transcripts. speech below 3 on the ALS Functional Rating Scale [4]) and moderate improvements in ALS speech that is similar to healthy speech. The comparison to Google cloud ASR demonstrates that a healthy speech model ﬁnetuned on non-standard speech produces strong results, but we acknowledge that the comparison is not perfect: the LAS and RNN-T models use the entire audio to make predictions while Google cloud ASR model only looks backwards in time. This lets Google cloud ASR support streaming, but it is also less accurate. The relative difference between base model and the ﬁnetuned model demonstrates that the majority of the improvement comes from the ﬁnetuning process, except in the case of the RNN-T on the Arctic dataset (where the RNN-T baseline is already strong). 4.2. Limited Data On the ALS dataset, ﬁnetuning on ﬁve minutes and ten minutes of data yields 75% and 85% of the WER improvement compared to a model trained on 40 min of data, respectively. On the Arctic dataset, fourteen and twenty minutes of training data yields 70% and 81% of the WER improvement, respectively. Figure 2 shows more details. 4.3. Layers The LAS model consistently performed best when the entire network was ﬁnetuned. The RNN-T model achieved 91% of the relative WER improvement by just ﬁnetuning the joint layer and ﬁrst layer of the encoder (compared to ﬁnetuning the joint layer and the entire encoder, the average WER across all participants regardless of FRS score was 18.1% vs 15.1%). On Arctic, ﬁnetuning the joint layer and the ﬁrst encoder layer achieved 86% of the relative improvement compared to ﬁnetuning the entire network (11.0% vs 10.5%). 4.4. Phoneme mistakes To better understand how our models improved, we looked at the pattern of phoneme mistakes. We started by comparing the distribution of phoneme mistakes made by Google cloud ASR model on standard speech (Librispeech) to the mistakes made on ALS speech. We map the ground truth transcripts and the ASR model outputs to sequences of SAMPA phonemes, then compute the edit distance between the two phoneme sequences. We aggregate over speakers and utterances. First, we compute the probability of a mistake for a particular phoneme in the ground truth transcript. Since the standard speech and ALS speech transcripts are different, we normalize by the number of ground truth transcript phonemes. The phonemes with the ﬁve largest differences between the ALS data and standard speech are p, U, f, k, and Z. These ﬁve account for 20% of the likelihood of a deletion mistake. Next, we investigate which phonemes are mistakenly added. We compute the probability that a particular phoneme will mistakenly appear in the recognized transcript. By far the biggest differences are in the n and m phonemes, which together account for 17% of the insertion / substitution mistakes. Finally, we perform the same analysis on some of our ﬁnetuned models. The unrecognized phoneme distribution becomes more similar to that of standard speech (see ﬁgure 3). Also, surprisingly, the phoneme distribution that the model produced when it made a mistake was much more similar to mistakes on standard speech after ﬁnetuning (KL of 0.26 between standard and ALS, 0.10 after ﬁnetuning).\n\n5. Discussion\nIn this paper, we develop well-performing ASR models for dysarthric and heavily accented individuals by carefully ﬁnetuning healthy-speech models. Speciﬁcally, we demonstrated:\n\n1. Good absolute performance on average dysarthric ALS\nspeakers, large improvements in very dysarthric speakers\n\n2. Better performance on ALS and accented speech when\njust training the RNN-T encoder\n\n3. Much of the improvement is from the ﬁrst 5-10 minutes\nof training data, and can be achieved by just training the ﬁrst encoder layer and the joint layer\n\n4. The ﬁve most mistaken phonemes in ALS speech account for 20% of the mistakes\nPrior to this work, it was unclear how much improvement could be achieved by ﬁnetuning on small amounts of nonstandard speech. We show that with on the order of 1 hour of data, we can create a personalized ASR model that is signiﬁcantly better than Cloud-based services. This improvement comes mainly from the ﬁnetuning process: we see a relative WER improvement over our base model of 70% for dysarthric speech (for both groups) and 35.1% for accented speech. The ﬁnetuning process used four GPUs for fewer than four hours, making this technique very accessible. We achieve better performance by just training the encoder for the RNN-T architecture but not the LAS architecture. This is likely due to a peculiarity of the RNN-T: the RNN-T architecture is factorized into a component whose activations are solely a function of the current audio (the encoder) and another component whose activations are a function of the current predicted transcript (the decoder). The LAS model has no such factorization, so information about the acoustics and linguistics are likely more evenly distributed throughout the network. This claim isn’t precise, since the RNN-T weights used at inference time are determined by the statistical properties of both language and acoustics, but even minor effects from this factorization might mean that ﬁnetuning the encoder only helps prevent the network from overﬁtting to the language seen in the small amount of training data. A large fraction of the improvement comes from the ﬁrst 5-10 minutes of audio. One explanation is that the test and train sentences are linguistically similar enough for the model to learn the kinds of things that are said during testing from a small number of examples. Another explanation is that just ﬁnetuning part of the network allows it to retain the general acoustic and linguistic information from the general speech model while needing minimal modiﬁcations to adapt to a single new speaker. Future work includes testing this hypothesis, possibly by exploring its performance on radically different sentences. The model might be adapting to general non-standard speech or to the individual. This could be tested, for instance, by training a single model on the entire ALS or Arctic corpus, and comparing it to a single speaker model. We didn’t include results from this comparison, and we discuss this in the ‘Future Works’ section. We found that ﬁnetuning just the joint layer and the ﬁrst encoder layer achieved 90% of the relative improvement compared to training the joint layer and the entire encoder. This can be explained by a combination of the acoustic vs linguistic properties discussed earlier in this section and by analogy with a popular computer vision ﬁnetuning technique. Many computer vision papers publish good results by ﬁnetuning the last few layers of a classiﬁcation network that was pretrained on Imagenet [28]. In that technique, authors usually assume that their data follows roughly the distribution of images in imagenet, and that their image label distribution is different. Our problem has the reverse assumption: we assume that the distribution of language (labels) is the same in our problem as in standard speech, but that the distribution of ALS / accented speech is different from that of standard speech. Lastly, we ﬁnd that the ﬁve most deleted / substituted phonemes account for roughly 20% of such errors, and that the two most incorrectly inserted phonemes account for just over 20% of such errors. This kind of observation might lead to ALS-detection techniques: one can try to detect ALS degradation by matching the prediction distribution of a production ASR system. We might also be able to improve the ﬁnetuning process by collecting more speech that involves the most-oftenconfused phonemes.\n\n2. Related Work\nNeural networks are the state-of-the-art systems for ASR in the large data-regime. We explore how well two particular architectures can be ﬁnetuned on a small amount of non-standard data. The RNN-Transducer [6, 7] is a neural network architecture that has shown good results on numerous ASR tasks. It consists of an encoder and decoder network and is conﬁgured such that unidirectional models can perform streaming ASR. In this paper, we use a bidirectional encoder without attention that was shown to achieve comparable results [8]. We also explore the Listen, Attend, and Spell architecture. It is an attention-based, sequence-to-sequence model that maps sequences of acoustic properties to sequences of language [9]. It has produced state-of-the-art results on a challenging 12,500 hour voice-search ASR task, achieving a 4.1% WER [10]. This model uses an encoder to convert the sequence of acoustic frames to a sequence of internal representations, and a decoder with attention to convert the sequence of internal representations to linguistic output. The best network in [10] produced word pieces, which are a linguistic representation between graphemes and words [11]. There are a number of methods for adapting large ASR models to small amounts of data [12, 13, 14]. This paper’s ap- Figure 1: Schematic diagrams of the RNN-T architecture (left) and the LAS architecture (right). Diagrams come from [8]. proach is most similar to [15], which involves selectively ﬁnetuning parts of the ASR model. There are also many published techniques on how to improve ASR speciﬁcally for pathological, or dysarthric, speech: [16] appended articulatory features to the usual acoustic ones to achieve a 4-8% relative WER improvement, and [17] adapts ASR models trained on open source datasets to a dataset of dysarthric speech. However, all these approaches are limited either by the quality of the base model or the amount of data available for ﬁnetuning. Numerous studies have explored the acoustic, articulatory, and phonetic differences between standard and modiﬁed speech. Some of the conditions explored are Parkinsons [18], age [19], dyslexia [20], and ALS [21]. We add to this body of work by describing the phonetic mistakes that a production ASR system makes on a large collection of ALS audio. We are also able to describe our improved ASR model by characterizing which phonemes the improved models are better able to recognize.\n\n6. Future Work\nA major challenge is to build state of the art speech recognition models for strongly dysarthric speech. It is an open question whether there are additional techniques that can be helpful in the low data regime (such as Virtual Adversarial Training, data augmentation, etc). We can also use the phoneme mistakes to weight certain examples during training, or to pick training sentences for people with ALS to record that contain the most egregious phoneme mistakes. We would like to explore pooling data from multiple speakers with similar conditions, but did not do so in this paper. We believe that such an experiment raises more questions around training speaker-independent non-standard speech ALS models, which we feel are outside the scope of this work.\n\n7. Acknowledgements\nWe’d like to thank Tara Sainath, Anshuman Tripathi, Hasim Sak, Ding Zhao, Ron Weiss, Chung-Cheng Chiu, Dan Liebling, and Philip Nelson for technical and project guidance.\n\n8. References\n[1] “Project euphonia: Helping everyone be better understood.” [Online]. Available: https://www.youtube.com/watch?v=OAdegPmkK-o&t=1s [2] “How ai can improve products for people with impaired speech.” [Online]. Available: https://blog.google/outreachinitiatives/accessibility/impaired-speech-recognition/ [3] https://www.als.net/what-is-als/. [4] “The Amyotrophic Lateral Sclerosis Functional Rating Scale: Assessment of Activities of Daily Living in Patients With Amyotrophic Lateral Sclerosis,” Archives of Neurology, vol. 53, no. 2, pp. 141–147, 02 1996. [Online]. Available: [5] J. Costello, “Message banking, voice banking and legacy messages,” Boston, MA: Boston Children’s Hospital, 2014. [6] A. Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012. [7] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural networks,” in 2013 IEEE international conference on acoustics, speech and signal processing. IEEE, 2013, pp. 6645–6649. [8] R. Prabhavalkar, K. Rao, T. N. Sainath, B. Li, L. Johnson, and\nN. Jaitly, “A comparison of sequence-to-sequence models for\nspeech recognition.” in Interspeech, 2017, pp. 939–943. [9] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960–4964. [10] C.-C. Chiu, T. N. Sainath, Y. Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, E. Gonina et al., “Stateof-the-art speech recognition with sequence-to-sequence models,”\nin 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4774–4778. [11] K. Rao, H. Sak, and R. Prabhavalkar, “Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,” in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193–199. [12] K. C. Sim, A. Narayanan, A. Misra, A. Tripathi, G. Pundak, T. N. Sainath, P. Haghani, B. Li, and M. Bacchiani, “Domain adaptation using factorized hidden layer for robust automatic speech recognition,” Proc. Interspeech 2018, pp. 892–896, 2018. [13] C. Wu and M. J. Gales, “Multi-basis adaptive neural network for rapid adaptation in speech recognition,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 4315–4319. [14] K. Mengistu and F. Rudzicz, “Adapting acoustic and lexical models to dysarthric speech,” Proc. ICASSP 2011, 2011. [15] K. L. A. L. S. G. Sameer Bansal, Herman Kamper, “Pretraining on high-resource speech recognition improves lowresource speech-to-text translation,” in Interspeech, 2018. [16] E. Yılmaz, V. Mitra, C. Bartels, and H. Franco, “Articulatory features for asr of pathological speech,” arXiv preprint arXiv:1807.10948, 2018. [17] M. B. Mustafa, S. S. Salim, N. Mohamed, B. Al-Qatab, and C. E. Siong, “Severity-based adaptation with limited data for asr to aid dysarthric speakers,” PloS one, vol. 9, no. 1, p. e86285, 2014. [18] V. Delvaux, K. Huet, M. Piccaluga, S. Van Malderen, and\nB. Harmegnies, “Towards a better characterization of parkinsonian speech: a multidimensional acoustic study.”\n[19] A. Hermes, J. Mertens, and D. M¨ucke, “Age-related effects on sensorimotor control of speech production,” Proc. Interspeech 2018, pp. 1526–1530, 2018. [21] N. Nonavinakere Prabhakera, P. Alku et al., “Dysarthric speech classiﬁcation using glottal features computed from non-words, words and sentences,” in Interspeech, 2018. [22] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: an asr corpus based on public domain audio books,” in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2015, pp. 5206–5210. [23] C. Danescu-Niculescu-Mizil and L. Lee, “Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs,” in Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics. Association for Computational Linguistics, 2011, pp. 76–87. [24] G. Zhao, S. Sonsaat, A. O. Silpachai, I. Lucic, E. ChukharevKhudilaynen, J. Levis, and R. Gutierrez-Osuna, “L2-arctic: A non-native english speech corpus,” Perception Sensing Instrumentation Lab, 2018. [25] H. Sak, A. Senior, K. Rao, and F. Beaufays, “Fast and accurate recurrent neural network acoustic models for speech recognition,” arXiv preprint arXiv:1507.06947, 2015. [26] J. Shen, P. Nguyen, Y. Wu, Z. Chen et al., “Lingvo: a modular and scalable framework for sequence-to-sequence modeling,” 2019. [27] “Google cloud speech api,” https://cloud.google.com/speech-totext/. [28] Y. B. H. L. Jason Yosinski, Jeff Clune, “How transferable are features in deep neural networks?” arXiv preprint arXiv:1411.1792, 2014.\n--- FOOTNOTES 1Google, United States 2ALS Therapy Development Institute, United States partnership with the ALS Therapy Development Institute (ALSTDI). The participants were given sentences to read, and they 1 Non-native English speech from the L2-Arctic dataset. [24] 2 Low FRS (ALS Functional Rating Scale) intelligible with 3 FRS-3 detectable speech disturbance. [4]", "persona": "Student", "response": "### 1. Overview\nThe utility of technology is dependent on its accessibility. One key component of accessibility is automatic speech recognition (ASR), which can greatly improve the ability of those with speech impairments to interact with every-day smart devices. However, ASR systems are most often trained from 'typical' speech, which means that underrepresented groups, such as those with speech impairments or heavy accents, don't experience the same degree of utility. For example, amyotrophic lateral sclerosis (ALS) is a disease that can adversely affect a person’s speech—about 25% of people with ALS experiencing slurred speech as their first symptom. In addition, most people with ALS eventually lose the ability to walk, so being able to interact with automated devices from a distance can be very important. Yet current state-of-the-art ASR models can yield high word error rates (WER) for speakers with only a moderate speech impairment from ALS, effectively barring access to ASR reliant technologies. In “Personalizing ASR for Dysarthric and Accented Speech with Limited Data,” to be presented at Interspeech 2019, we describe some of the research behind Project Euphonia, an ASR platform that performs speech-to-text transcription. This work presents an approach to improve ASR for people with ALS that may also be applicable to many other types of non-standard speech. Using a two-step training approach that starts with a baseline “standard” corpus and then fine-tunes the training with a personalized speech dataset, we have demonstrated significant improvements for speakers with atypical speech over current state-of-the-art models. Turn it down a little bit please Turning down a little bit please In order to create ASR models that work on non-standard speech, one needs to overcome two challenges. The first is that within a particular class of atypical speech, be it a regional accent or a speech impairment, for example, individuals can exhibit very different ways of speaking.\n\n### 2. Background\nThe Neural Network Architecture When developing the models used for training data on atypical speech, we explored two different neural architectures. The first is the RNN-Transducer (RNN-T), a neural network architecture consisting of encoder and decoder networks that has shown good results on numerous ASR tasks. The encoder is bidirectional (i.e., it looks at the entire sentence at once in order to provide context), and thus it requires the entire audio sample to perform speech recognition. The other architecture we explored was Listen, Attend, and Spell (LAS), which is an attention-based, sequence-to-sequence model that maps sequences of acoustic properties to sequences of languages. This model uses an encoder to convert the sequence of acoustic frames to a sequence of internal representations, and a decoder to convert the sequence of internal representations to linguistic output.\n\n### 3. Methods\nThe network produces “word pieces”, which are a linguistic representation between graphemes and words. Comparison of the RNN-Transducer (left) and Listen, Attend, Spell (right) architectures. From Prabhavalkar et al. 2017. A Two-Phased Approach to Training We experimented with fine-tuning the state-of-the-art RNN-T and LAS base models on two types of non-standard speech.\n\n### 4. Results\nIn partnership with the ALS Therapy Development Institute, we first collected about 36 hours of audio from 67 speakers who have ALS. The participants recorded themselves on their home computers using custom software while they read sentences from a very restricted language domain. Many phrases were single sentences with simple grammatical structure (e.g., “What time is the basketball game on tonight?”). This is in contrast with unrestricted language domains, which include domain-specific vocabulary (e.g., science talks) and complex language structure (e.g., a debate). The recordings did not include many of the filler words common in normal speech, such as “um” and “uh”. The audio (left) are recordings of a speaker with ALS. The text transcriptions are output from the Euphonia model (center) and the Standard Speech model (right). Incorrectly transcribed text is underlined. The RNN-T model achieved 91% of the improvement by fine-tuning just two layers, most of which are close to the input. On the accented dataset, fine-tuning the same two layers achieved 86% of the relative improvement compared to fine-tuning the entire network. This is consistent with previous speech work.\n\n### 5. Takeaways\nLet’s try that again It extracts To better understand how our models improved after fine-tuning, we looked at the pattern of phoneme mistakes. We started by comparing the distribution of phoneme mistakes made by the base ASR model on standard speech to the mistakes made on ALS speech. The SAMPA phonemes with the five largest differences between the ALS data and standard speech are p, U, f, k, and Z, which account for 20% of the deletion mistakes. Similarly, the n and m phonemes together account for 17% of the insertion / substitution mistakes. The same analysis on our fine-tuned models verifies that the unrecognized phoneme distribution is more similar to that of standard speech. Our analysis shows that there are two aspects to every mistake: which phoneme the system doesn’t understand, and which phoneme the system thinks was said. Imagine having two systems with identical accuracy: one system always thinks that the f phoneme is actually the g phoneme, while another doesn't know what the f phoneme is and randomly guesses. These two systems will have identical performance and identical distributions of phoneme mistakes, but very different distributions of the predicted phoneme when a mistake is made. Surprisingly, ASR mistakes on ALS speech are far more similar to regular speech mistakes after Euphonia fine-tuning. Deletion / substitution mistakes per SAMPA phoneme on ALS speech before fine-tuning, ALS speech after fine-tuning, and on typical speech (Librispeech dataset). In the future, we intend to explore additional techniques that can be helpful in the low data regime. We also hope to use phoneme mistakes to weight certain examples during training, or to pick training sentences for people with ALS to record that contain the most common phoneme mistakes.", "paper_id": "32", "story_id": "32-01"}
{"paper": "Abstract\nSpeech-to-text capabilities on mobile devices have proven helpful for hearing and speech accessibility, language translation, notetaking, and meeting transcripts. However, our foundational largescale survey (n=263) shows that the inability to distinguish and indicate speaker direction makes them challenging in group conversations. SpeechCompass addresses this limitation through realtime, multi-microphone speech localization, where the direction of speech allows visual separation and guidance (e.g., arrows) in the user interface. We introduce efficient real-time audio localization algorithms and custom sound perception hardware, running on a low-power microcontroller with four integrated microphones, which we characterize in technical evaluations. Informed by a largescale survey (n=494), we conducted an in-person study of group conversations with eight frequent users of mobile speech-to-text, who provided feedback on five visualization styles. The value of diarization and visualizing localization was consistent across participants, with everyone agreeing on the value and potential of directional guidance for group conversations.\n\nIntroduction\nRecent advances have enabled real-time automatic speech recognition (ASR) in mobile and embedded hardware, supporting a range of conversational scenarios [2, 46]. Real-time captioning can enhance human communication in various ways, e.g., for real-time translation between languages, transcriptions during an interview, automatic subtitle generation, hearing accessibility [38, 55], and note-taking. However, there are still unsolved limitations with realtime speech-to-text — specifically, the ability to distinguish multiple speakers (speaker diarization) and tracking the direction of speech (localization). Consider the following scenario: Throughout the day, a person relies on a mobile phone with real-time captioning to understand speech. However, at a dinner table with multiple people, the conversation is difficult to follow since the app cannot distinguish between speakers and concatenates all speech into a single paragraph. Additionally, since the person needs to look at their phone regularly, they struggle with following the turn-taking across speakers and don’t know where to look if there is a speaker change. Also, irrelevant nearby conversations get transcribed as well, which can cause confusion and privacy implications. Many of the challenges in the scenario stem from the spatial complexities of audio, which are challenging to capture with a single-microphone ASR system. The benefits of multi-microphone topologies for localization have been demonstrated in numerous applications, such as public safety [61], virtual reality [39], robot navigation [43], mobile computing [36, 49] and audio accessibility [27, 31]. In this work, we leverage arrays of multiple microphones and apply techniques for microphone array signal processing to demonstrate how this technology could improve ASR performance and usability in such scenarios. Specifically, we identified opportunities for improvements in three areas: (1) Speaker diarization. The transcript can visually separate different speakers based on the direction of the speech. (2) Localization. For spatial sound visualization, the screen can display the direction of the incoming sound. (3) Selective attention. The interface can allow the user to select speech of interest and filter out self-speech. In this work, we developed SpeechCompass, a solution to add diarization and speech localization to mobile captioning. It includes three main parts. First, low-latency localization algorithms that can run on generic microcontrollers or mobile phones. Second, a compact 4-microphone phone case that allows 360-degree localization on a low-power microcontroller. Third, a mobile captioning app that shows how sound localization can be visualized in different ways and used to support multi-speaker conversations through diarized transcripts. We also run our algorithm on an off-the-shelf mobile phone with only two microphones to demonstrate that limited 180-degree localization is possible in the app without additional hardware. While machine learning approaches to single-source speaker diarization have been improving [52], our multi-mic approach has the advantage of lower computational cost, latency, and privacy, and thus is inherently suitable for real-time ASR applications on lowpower, low-cost microcontrollers. It is also language agnostic and can work for sounds other than speech. Our approach is tied to the position of the phone and the speaker, which can be advantageous as diarization can be immediately reconfigured by moving the phone. This paper shows how traditional microphone array processing can significantly benefit diarization and localization for mobile ASR. While diarization with microphone arrays has been well studied [4], it has yet to be applied to mobile ASR and related interfaces. 1.1 Contributions The contributions of this work are: • Real-time sound localization algorithms and optimized, embedded multi-microphone hardware, implemented both as phone-case prototypes with a low-power microcontroller (<20 ms processing time) and on a mobile phone with constrained microphone hardware. The code and design files are available via Github.1 • Mobile captioning UIs for group conversations, enabled through speech localization and diarization, and implemented as mobile speech-to-text applications with different visualization and interaction techniques. • Technical evaluation of localization and diarization using the optimized algorithms and hardware. We characterize localization accuracy and estimation time under various signal-to-noise conditions and speaker configurations, and evaluate diarization accuracy. • Foundational large-scale survey (n=263) with frequent users of captioning technology, showing that noise and speaker separation are important and frequent challenges with existing solutions. • Lab study (n=8) and large-scale survey (n=494) of mobile interfaces and visualizations with frequent users of captioning technology. Informed by a large-scale survey, we designed a lab study with eight frequent users of captioning technology. We contribute insights into interface preferences, customization, and benefits since diarization and localization have not been previously studied for mobile ASR. 2\n\n3 Challenges with mobile captioning: Large-Scale survey with 263 frequent users As we were interested in exploring the potential for more advanced mobile speech perception, we conducted a brief large-scale survey to learn about the challenges of using captioning for speech understanding in in-person meetings and conversations. CHI ’25, April 26-May 1, 2025, Yokohama, Japan Google Opinion Rewards app”), screening for individuals that use technology to understand speech in meetings and conversations, and are frequent users of captioning technology. Our goal was to recruit deaf or hard-of-hearing participants, as we believed that they would have the most relevant experience and insights around mobile captioning technology and interfaces. To mitigate spam, the survey system analyzes question response times. By considering the distribution of response times across questions, it adapts to different question types and response patterns, rejecting sessions with unusually fast responses. We acknowledge that our survey’s focus on frequent users of captioning technology and the growing user base for mobile captioning limits its relevance for other populations, such as individuals who identify with Deaf culture and might be less likely to rely on ASR technology [10]. Unfortunately, we cannot quantify the representation in the survey since restrictions from our institution do not allow us to collect participant hearing levels or use of sign language. Of the 1502 respondents that met our criteria, we focus on the 263 participants (18%), who reported that they used captioning technology to understand people (not TV/video) multiple times per week or more frequently, and for 2 hours or more on the days that they used it. For these 263 participants, the platform reported that 49.8% were women, 48.7% men, and 1.5% unknown, across all age ranges (27%: 18–24, 33%: 25–34, 15%: 35–44, 12%: 45–54, 6%: 55–64, 7%: 65+). The participants were prompted to select challenges among the choices from the list in Figure 3. The choices were synthesized by user feedback from our previous experience with mobile captioning and informed by previous work in mobile captioning [31, 50] 3.2 Survey results: The use of captioning to understand people in conversations 64% of the participants reported daily use of captions in meetings or conversations to understand people, whereas 36% used it multiple times per week. Half of the participants (49%) use technology to understand people face-to-face for 2-3 hours on the day of use. Almost a quarter (23%) of participants use captions for 4-5 hours on the days of use and another quarter (28%) for 6 or more hours. Realtime captions, such as CART (69%), and the Android Live Transcribe and Sound Notifications app (55%) were the top two technologies that were used daily to understand people. The top two issues with current transcription technology, as reported by our participants, were background noise (60%) and the combining of text from different speakers (46%), without the ability to separate them. Participants selected all that apply from the choices shown in Figure 3. Finally, we asked participants about scenarios that are known to be challenging with today’s transcription technology but have the potential to be addressed with more advanced microphone arrays and speech perception algorithms. Scenarios of interest included conversations where ignoring music, noise, or adjacent speech would be critical. We were also interested in group conversations and situations where separating speech from two people is critical. 68-70% of participants experienced these scenarios multiple times per week or more frequently, whereas only 11-12% rarely or never experienced them, as shown in Figure 4. Biggest challenges with my current transcription technology (n=263) Mistakes with background noise Combines/mashes text from different speakers Have to look away from person I'm talking to Can't easily review transcript history Doesn't work with lectures/presentations Don't want to see my speech 0% 5% 10% 15% 20% 25% 30% 35% 40% 45% 50% 55%60% Figure 3: Participant responses to the question What are the biggest challenges with your current captioning or transcription device/technology? (select all that apply)? 31 (12%) 52 (20%) 68 (26%) 43 (16%) 69 (26%) Conversations where it is critical to ignore music / noise 1:1 where it is critical to ignore adjacent speech 61 (23%) 56 (21%) 65 (25%) 53 (20%) 28 (11%) 70 (27%) 34 (13%) 76 (30%) 46 (18%) 31 (12%) Group conversations where it is critical to separate speech from 2 people 0%5% 10%15%20%25%30%35%40%45%50%55%60%65%70%75%80%85%90%95% 100% Figure 4: Survey results of how often participants encountered challenging scenarios with today’s transcription technology. The number of participants and percentage is shown for each choice.\n\n4 SpeechCompass system This section details the SpeechCompass hardware, algorithms, and phone application. The SpeechCompass system diagram is shown in Figure 5. 4.1 Design considerations Based on the foundational large-scale survey with 263 frequent users of mobile captioning (Section 3), previous work, and envisioned user journey, we outline the following design goals for the SpeechCompass prototype: • All-day use. The prototype should be low-power, ensuring that the battery could last a day or more, as 28% of survey participants use captioning technology for six or more hours daily. Current DSP solutions [26, 47] draw about 500 mAh, which would require impractically large batteries, or significant impact on the phone battery. • Accurate 360° localization. The survey participants indicated that a major challenge is how transcription apps combine text from different speakers, which makes it difficult to use in group conversations. To address this challenge, localization accuracy should be under 20° [45] to match human localization abilities and accurately distinguish different speakers. Also, 360° azimuth localization is needed as it avoids front-back confusion, as illustrated in Figure 6A. 157 (60%) 120 (46%) 92 (35%) 88 (33%) 59 (22%) 45 (17%) Rarely / never A few times / month Multiple times / week Daily Multiple times / day Rarely/never A few times/month Multiple times/week Daily Multiple times/day Also, since group conversations are dynamic, speakers and the phone may be located at any angle around the user. • Integration with existing phones and applications. 55% of the survey participants use mobile assistive apps, which is a strong motivator for us to target mobile phones, as they are already always with the user. The device should have multiple input and output options for audio and data, be designed to interface with external applications, and physically integrate with mobile phones. • Low-latency, real-time processing. To integrate with existing applications, the localization needs to be faster than real-time transcription latency, which is expected to be in the 30–300 ms range [66]. Minimizing latency requires hardwarespecific algorithm development and bypassing typical operating system latency. For someone who uses mobile captions daily, adding SpeechCompass for group conversations should be a low effort. The person would use the SpeechCompass phone case instead of a regular one. Their favorite captioning app would automatically use sound localization data to diarize the captions and separate the speakers. Considering the scenario where the person struggles to use mobile captioning around a dinner table, the text from different speakers would be uniquely presented (e.g., using colors or arrows) based on their position around the table. As phones add more microphones, SpeechCompass could become a pure software solution, utilizing multiple microphones on the phone. 4.2 Multi-mic system for embedded perception We chose a 110 MHz M33 ARM Core (STM32L55, STMicroelectronics) as the main microcontroller. This processor provides the low-power and high computing capabilities needed for localization. Four digital microphones (MP34DT01-M, STMicroelectronics), which use the pulse density modulation (PDM) protocol, were arranged to have the largest distance between the microphones for finer resolution and support of 360° of azimuth angles. Although SpeechCompass hardware (Phone case) Microphone Microcontroller Microphone PDM Calculate angle of arrival DFSDM GCC-PHAT Microphone Coordinate transform Microphone Figure 5: SpeechCompass system diagram. The phone case contains four microphones connected to a microcontroller. The audio localization algorithms run on the microcontroller, and the angle estimation is sent over USB to the phone. The SpeechCompass app combines ASR input and angle estimations to provide diarization and directional guidance for the mobile captioning UI. three microphones could resolve similar angles, an additional microphone improves localization accuracy. Furthermore, when handheld, the four-microphone design is more robust to occlusions from the hand or fingers. The microphone data was collected at 16-bit resolution and 44.1kHz. The PDM to PCM (pulse-code modulation) conversion was done by the Digital Filter for Sigma-Delta Modulator (DFSDM) peripheral on the microcontroller. The same clock signal drove the four microphones, so they remained synchronized. We used flexible PCB-mounted microphones and a small main PCB, as shown in Figure 2B, C. 4.3 Localization The localization is computationally intensive, as it requires estimating the delay between all unique pairs of microphones. The delay estimation is usually done using cross-correlation. We implemented a variant of Generalized Cross-Correlation with Phase Transform (GCC-PHAT) [37]. This approach is more robust to noise than standard cross-correlation and takes advantage of ARM’s CMSIS libraries [5], which enable more efficient computations. Then, the time delay between the two microphones is extracted from the cross-correlation. Since the microphone geometry is known, the time delay is converted into the angle of sound arrival. The localization equations are provided in Appendix A.1. With just two microphones, there is front-back confusion. Different sources positioned at the inverse angle about the microphone pair axis will have exactly the same TDOA. (See Figure 6A). To accurately determine the 360° azimuth angle, we need at least three microphones, positioned such that they span the largest plane possible (they cannot all be in-line). Considering the uncertainties of sound propagation, a statistical approach has to be used to determine the actual source location. We determine the TDOA for each microphone pair (six unique pairs for four microphones) and add a second potential TDOA. Then, we perform a coordinate transform so that each angle of arrival is aligned with global azimuth angles. Finally, we do Kernel Density Estimation (KDE) with a Gaussian kernel (bandwidth = 25) with the 600 latest samples. The highest peak will correspond to the angle of arrival, as demonstrated in Figure 6. The KDE was wrapped around CHI ’25, April 26-May 1, 2025, Yokohama, Japan Phone USB-C Phone microphone Power ASR Angle Kernel Density Estimation SpeechCompass app Display diarized ASR UI between 0 and 360° to address the discontinuity around 0 and 360°. The localization is evaluated in Section 5. 4.4 SpeechCompass application: Mobile captioning with speaker separation To investigate the SpeechCompass technique’s potential, we implemented a mobile ASR application (Android) with localization features, as shown in Figure 7. We used the USB Serial for Android library [48] to establish a data connection between the phone and the SpeechCompass microcontroller, which transmits localization data over USB-C. The application improves over existing singlesource ASR applications in three ways, which we describe below. Real-time sound source visualization. We use a semicircle overlay around the edges of the screen to indicate the current sound direction. The semicircle moves according to the azimuth angle of the audio, as shown in Figure 7B. (Alternatively, a vertical line could be used if only 180° localization is available.) The radius of the semicircle is scaled according to localization confidence, corresponding to the KDE peak, with a larger semicircle corresponding to higher confidence. Speaker diarization in transcription. APIs for mobile and web-based ASR have advanced significantly in recent years, but those APIs do not distinguish between multiple speakers or provide direction of the incoming speech. By combining SpeechCompass with mobile ASR, we enable legible transcripts by visually separating speech from different directions. We use the Android Speech Recognition API [3] to obtain real-time transcription and display the results on the mobile phone screen as vertically scrolling captions. The transcript is colored by directly mapping the sound arrival angle to the 360°color wheel, as shown in Figure 7C. In this mapping, the text is colored green if sound arrives from the top of the phone (around 90°) and red for bottom arrival (270°). Other direction indication options include showing a colored arrow next to the text or a directional glyph, as shown in Figure 7A, B. To accurately determine text color, we synchronize speech recognition and azimuth measurements by buffering azimuth angles after the onBeginningOfSpeech() callback and stopping after the onEndOfSpeech() callback. This way, only the angles detected during speech CHI ’25, April 26-May 1, 2025, Yokohama, Japan Artem Dementyev*, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, and Alex Olwal* A Source actual source planar wave θ1 -θ1 Δd mic 1 mic 2 potential source Density Density Azimuth angle θ1-2 -θ1-2 θ1-2+θ2-3+θ1-3 -θ1-2 -θ2-3 -θ1-3 Figure 6: Visualization of localization methods with 2 and 3 microphone configurations. A) Localization with two microphones. The sound will arrive at microphone two before microphone one. This time difference could be used to estimate the angle of arrival. However, with two microphones, ambiguity exists, as the source could be at the inverse angle, shown as a \"potential source.\" The graph on the bottom shows the kernel density estimation (KDE) with actual and potential sources. B) With three or more microphones, this angle ambiguity can be avoided. In our implementation, we use four microphones. The KDE from multiple microphone pairs will have the highest peak at the correct source. Figure 7: The mobile phone application with different direction visualization options. A) Directional glyphs are arrows next to the transcript, indicating the direction of speech. B) Minimap and directional glyph as radius inside the circle. C) Directional cues embedded in the color of the text and boxes around the text. D) Enabling speech suppression for right and bottom speech directions, as shown on the minimap. moments are analyzed, ignoring changes during silence or background noise. We determine the resulting angle by computing the mode of the angles in the azimuth buffer. Speaker suppression. As 60% of the participants found mistakes with background noise to be an issue, we added unwanted speech suppression. With the speaker diarization, we can simulate speaker suppression, by letting the user hide speech from certain B mic 1 mic 2 θ1-2 Δd1 θ1-3 θ2-3 Δd2 Δd3 mic 3 Azimuth angle directions, or their own speech. Our implementation allows tapping on different edges of the screen. For example, as shown in Figure 7C, tapping on the right side of the screen will hide/show speech coming from the right. 5 Technical Evaluation In this section, we report on our technical evaluations of the main features of SpeechCompass. First, we characterize the underlying localization technology. We evaluate localization using an off-theshelf mobile phone (Pixel 6, Google) and 360° localization with the SpeechCompass hardware. Then, we determine the compute time for reliable localization, as it is critical for real-time diarization and visualization. Finally, for the real-world applicability of SpeechCompass, we characterize diarization accuracy in a realistic scenario where we also measure power consumption. 5.1 Localization experiments setup By its nature, localization can not be done precisely due to time resolution. For example, with 80 mm microphone spacing and 44.1 kHz audio sampling rate, the maximum delay is ±10 samples (0.23 ms). This configuration provides a resolution of 9◦. However, the actual accuracy is more nuanced, as it is obtained from multiple microphone pairs with different spacing and depends on many factors, such as the sound’s frequency, environment, and loudness. To better characterize localization accuracy, we conducted a series of experiments with the phone and SpeechCompass hardware. In the setup, the device under test was mounted on a rotating platform. A servo motor (XM430-W350-T, Dynamixel) was used to rotate the device around its axis (azimuth angle) and was synchronized with the data collection. A stationary speaker (S120, Logitech) was mounted 1.5 meters away to provide a direct sound path. Using the servo mount, the device was rotated by 10 degrees from 0–350◦to obtain azimuth angle error. At each position, data was collected for 15 seconds. The loudness was calibrated with a reference sound meter (VL6708, VLIKE). The speech audio came from the audiobook “Alice’s Adventure in Wonderland” [42]. Rain [56] was used as a realistic environmental sound with white noise properties. Each experimental condition was measured three times, and we report the mean value. 5.2 Localization: Off-the-shelf smartphone Many smartphones today have two or three microphones, which has the potential to enable some localization. We investigate the feasibility and accuracy of localization with an unmodified, off-theshelf phone (Pixel 6, Google). This phone has three microphones placed at the bottom, top and near the rear camera, as shown in Figure 8. The top and bottom microphones are on the side edge of the phone, while the rear camera microphone is on the backside. To characterize the opportunity and limitations, we ported our localization algorithm to Java Native Interface (JNI), so it could run on the Android platform. The two-mic-localization runs on the phone in real time. With only a 15 mm separation between microphones 1 and 2, the delay between those two microphones was too small for useful localization. However, the other two pairs — {mic 0, mic 2} and {mic 0, mic 1} — provided useful localization data. Given this geometry, only 180-degree azimuth localization is possible for this device. If the camera microphone (mic 1) had been placed on the left side of the phone (with a larger baseline distance from mic 2), 360-degree localization would have been feasible. The mean error of environmental sound was 23.0 degrees, and speech was 16.0 degrees. In comparison, humans also have up to 20-degree azimuth error when localizing sound without visual cues [45]. This demonstrates that localization on the phone can CHI ’25, April 26-May 1, 2025, Yokohama, Japan still be useful in limited use cases, for example, if the user does not expect any sounds from behind. We also observed that with such a limited amount of data, a simple histogram performed better than the Gaussian distribution approach, as there are not enough statistics for a reliable distribution. 5.3 Localization: SpeechCompass 4-mic system We varied two variables; type of sound (speech and environmental), and loudness in sound pressure level (SPL). The loudness varied from 50–65 dB SPL in 5 dB increments, where 0 dB corresponds to the human hearing threshold. This range covered the span of loudness typically experienced in daily conditions [29], from a quiet office (50 dB) to a normal conversation (65 dB). The ambient loudness in the experiment’s room was 45 dB, so experimenting at this loudness or below was not practical. The experimental results show that sound could be localized reliably for both speech and environmental sound, with different loudness, and with 360◦coverage. The overall error is reported in Figure 10A. The error was 11.1–22.1° for normal conversational loudness (60–65 dB). In comparison, humans also have up to 20degree errors when localizing sound [45]. The error increased significantly as the sound got quieter. The GCC-PHAT picks up the loudest sound, thus making it difficult to estimate TDOA if the sound source is under or at the environmental noise level. Directional effect. The localization error has some dependence on the source angle, with an overall sinusoidal trend, as can be seen in Figure 10B. The lowest error was around 0, 90, 180, and 270°, whereas the highest error was observed at 45 and 135°. This sensitivity can be explained by the rectangular microphone geometry. With the current coordinate system, an angle such as 90° results in 4 out of 6 microphone pairs in the end-fire and/or broadside in relation to the sound source. In such a configuration, the maximum and minimum delays are obtained, thus easier to measure. Type of sound. The error was lower for the environmental sound (11.3◦at 65 dB) when compared to speech (18.3◦at 65 dB). There are a few reasons why speech appears to be harder to localize reliably. First, speech has a very complex acoustic pattern in comparison to noise. There are many frequencies, sudden stops, and loudness variations [63]. This results in more reverberations and reflections, which can increase the error. Also, speech contains cyclic signals, which have ambiguity when subjected to cross-correlation, especially for high frequency, and speech contains signals up to 4 kHz. Effects of elevation angle. In the above experiments, we test the azimuth angle error and keep the elevation angle at zero, so the audio source and microphone array are leveled. In practical use cases, the audio source might be located below or above the microphone array. For example, with the microphone array on the table, the speaker’s head is usually above the array. To characterize the effect of elevation, we move the source elevation from -40 (source below mics) to +40° (source above mics) in 20-degree increments, while keeping the azimuth angle at 90°. The distance is kept at 1.5 meters as in previous experiments and white noise is played at 65 dB. The results in Figure 9 show minor azimuth error (under 2 degrees) at -20 and +20° elevation. The error becomes more pronounced (up to 9.2°) at -40 to +40° elevation, as the source moves outside of the microphone 2-D plane. The 20° elevation would result in a height of CHI ’25, April 26-May 1, 2025, Yokohama, Japan Artem Dementyev*, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, and Alex Olwal* Figure 8: Localization on a mobile phone. A) Microphone positioning and distance. B) Examples of the raw microphone data from three microphones. The top graph shows the end-fire, with the bottom microphone facing the sound source, thus providing maximum delay between the microphones. The delay between mic 0 and mic 2 and 1 can be easily seen. The bottom graph shows broadside, where the microphones are the same distance from the sound, and thus have the same delay. 10 Azimuth angle error (degrees) 8 6 4 2 0 -40 -20 0 +20 +40 Source elevation angle (degrees) Figure 9: Effect of source elevation angle on the azimuth angle accuracy. 0.55m, which would correspond to speakers sitting around a table with the phone 1.5 meter away. The 40° elevation would result in a height of 1.26m, approximately corresponding to speakers standing around a table. The results therefore suggest that SpeechCompass could work well for seated conversation partners, but would need additional calibration to account for situations where speakers are further above the microphone array. 5.4 Compute time to estimate direction It takes 2.9 ms to compute the delay between two microphones on the 110 MHz microcontroller. Therefore the time to collect one data frame for six unique microphone pairs is 17.4 ms. This provides a lower limit on time to estimate direction. However, in practice, the estimation time could be much higher due to the uncertain nature of sounds like speech, as multiple measurements are needed for estimation. Therefore, we experimented to understand how quickly the device can react to the onset of sound in making an accurate direction estimate. In a similar setup as the localization accuracy experiment, we positioned the device 1.5 meters from the sound source. As in the localization accuracy experiment, we used speech and noise, and measured them at different angles. We collected data for 10 seconds without sound, then turned on the sound and collected the data for 50 seconds at 44.1 kHz. This measurement was conducted at angles from 0–360 in 60° increments. 5.4.1 Results. We evaluated the number of samples it took for the sound source angle to converge around the actual angle by examining the KDE maximum after each additional sample. With the noise audio, localization required a mean of 12.7 samples (max: 19, min: 7). This number of samples was typically collected within one frame. So the latency was just the computational latency of 17.4 ms. The speech audio needed on average 5.7X more samples to stabilize, with a mean of 72.7 samples (max: 149, min: 24). The speech contains more sparse localization information, and most measurements (80%) did not have measurable microphone delays. As a result, a mean of 15 frames is required; thus, the mean estimation time is about 263 ms. However, estimation time can range approximately from 70–500 ms, depending on the speech characteristics. 5.5 Power consumption As the device is designed to be portable and powered by its own power source or the phone’s battery, it is essential to understand its power consumption. The power consumption of the whole system was 28 mAh. The four microphones consumed 2.4 mA, while the microcontroller consumed 11.7 mA. The audio codec and other peripherals consumed the rest. Therefore, the device could run speaker localization continuously for 18 hours with a 500 mAh battery. When powered by a smartphone, considering a representative smartphone’s power consumption of 154 mA (3700 mAh battery for 24 hours), this represents an additional 18 percent of the power consumption. 5.6 Diarization accuracy with localization In this section, we evaluate diarization with the SpeechCompass hardware. A schematic view of the setup is shown in Figure 12A. We set up four speech sources at 0, 90, 180 and 270 degrees to mimic a turn-taking conversation with four talkers. The speech material is from Librispeech [51]. A diffuse noise field is simulated using four loudspeakers simultaneously playing uncorrelated noise content. The noise types are babble noise (pub and cafeteria) as well as traffic noise. One synthesized conversation lasts an average of 40 seconds with each speaker speaking in turn (i.e., no overlap in the speech Noise Speech Speech (65 dB) A B 40 Error (degrees) 30 20 10 0 50 55 60 65 100 200 300 0 Loudness (dB SPL) Sound source azimuth angle (degrees) Figure 10: A) SpeechCompass azimuth angle measurement errors for speech and noise sound at different loudness levels. A mean of 0–350° azimuth measurements is reported. B) Azimuth localization error at different angles. Data from one noise and speech trail are shown. The multi-mic device was rotated around its axis from 0–350° while the source was stationary. A 15-second window was used for each estimate. content). The noise level is set to create three signal-to-noise ratio (SNR) levels of 6, 12, and 18 dB respectively. A fourth scenario is generated in clean condition (i.e., no noise). For each scenario, 100 conversations are created, corresponding to about 1.2 hours of active speech. An example diarized segment overlaying TDOA values is shown in Figure 12B, indicating the TDOA follows the speaker but still has some noise. In addition to the SNR sweep, we compared two microphone configurations using respectively three and four microphones, resulting in a total of eight experiments. We computed the diarization error rate (DER) for each scenario using the PyAnnotate toolkit [12]. The DER is computed by summing the durations of false alarms, missed detections, and speaker confusions, and then dividing it by the total ground truth speech duration. The DER for the fourmicrophone configuration performed consistently better than the three-microphone one, across all four SNR conditions, with a relative DER improvement varying from 23 to 35% for an average of 32% as can be seen in Table 12. A four-microphone configuration allows for more unique microphone pairs (6) than a three-microphone configuration (3). This provides extra TDOA information that can be used by the diarization algorithm to segment and tag the speech content between the four talkers more accurately.\n\n6 User evaluation with frequent users of mobile ASR: Lab study and online survey To evaluate the usability of our approach, we decided to conduct an in-person lab evaluation of the SpeechCompass phone case and the speech-to-text application (described in Section 4.4), with frequent users of mobile transcription technology. We first conducted a largescale online pilot study to inform the design of the in-person lab evaluation, which we conducted with eight deaf or hard-of-hearing participants, set up to mimic a realistic conversation scenario. 6.1 Large-scale, online survey (n=494) In this survey, we use screenshots of our interactive UI prototypes to solicit initial user feedback on the potential for our proposed approach, to guide the design of a more realistic in-person lab study. The study was conducted using the same Google Surveys deployment and screening methodology as for the foundational study, CHI ’25, April 26-May 1, 2025, Yokohama, Japan Noise (65 dB) 50 40 Error (degrees) 30 20 10 0 detailed in Section 3. The participants were shown different UI renderings and were asked to rate them. The large-scale online survey could only show static images of the interfaces, due to limitations of the survey tool. Out of 985 respondents we focus our analysis on the 494 participants who use captioning technology multiple times per week or more frequently. As shown in Figure 13A, the colored text was found to be valuable by 60% of participants. Glyph indicators for speech direction, which included arrow and circle+line indicators, were found valuable by 70%. The Edge indicator and the mini map had a less positive reception. To better understand which glyph indicators were favored, we also asked targeted questions about them, as shown in Figure 13B. Circle + line was preferred by 13.1% more respondents than the highlight box (45.1% vs 32.0%), and the arrow was preferred by 21.9% more respondents than the circle + line (51.2% vs 29.3%). 6.2 Lab study (n=8) We recruited 8 participants from our institution who were frequent users of captioning technology. Five were female, three were male, and all were deaf or hard of hearing. One participant was 25–34 years old, four were 34–44, one was 45–53, and two were 65+ (we are only allowed to collect age ranges at our institution). The study took place in a quiet lab over approximately 60 minutes and used the phone-case prototype (Figure 2) with our mobile ASR application (Figure 7). First, the participant was introduced to the technology, prototype, and the purpose of the study. Then, the participant was asked to fill out a background survey, which included demographic questions and their current use and experienced challenges with transcription technology. Afterward, the participant was introduced to different visualization scenarios with the SpeechCompass application. The participant used the SpeechCompass transcription while sitting between the two experimenters, as they all sat around a small table with the SpeechCompass phone case in the center. In each of the seven conditions, which ran for 5 minutes, the experimenters sat across from each other and had short conversations about different topics. The participants were instructed to turn off hearing aid devices if they used any, and were asked to use the SoundCompass UI and transcript to follow the conversation. The experimenters’ casual conversations included topics like weekend plans, hobbies, and the weather. The seven conditions, which CHI ’25, April 26-May 1, 2025, Yokohama, Japan Artem Dementyev*, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, and Alex Olwal* Figure 11: Diarization experiment. A) Data collection setup for diarization testing. The SpeechCompass phone case is placed on a small table in the center of the room. Head and torso simulators are used to play back speech material. Four loudspeakers are placed at the corners of the room to simulate a diffuse noise field. B) Example of TDOA data overlayed with diarization from two speakers for a conversation snippet. The extracted speaker labels from TDOA precisely follow the conversation turns. For TDOA, a frame size of 512 (11.6 ms) with no overlap at 44.1 KHz was used. Then, a running histogram of 522 ms was applied to get speaker labels. 3 mics 4 mics 0.6 0.4 DER (%) 0.46 0.3 0.48 0.37 0.49 0.32 0.48 0.32 0.2 0 no noise 6 dB 12 dB 18 dB SNR (dB) Figure 12: Diarization error rate (DER) for three and fourmicrophone configurations, across a sweep of SNR levels. used the ASR, diarization, and localization functionality for different visualization techniques, are shown in Figure 14 and presented with more UI context in Figure 7. The conditions were: (1) Transcription only. The transcribed text is shown in white on a black background. (2) Edge indicator. A circle (“dot”) that moves around the edge of the screen to point to the currently active speaker. The color of the dot changes based on the direction. (3) Arrow indicator. A glyph using a colored arrow next to a white text block. The glyph points in the direction of the associated speech. (4) Circle + line indicator. A glyph using a circle with a directional line next to a white text block. The glyph points in the direction of the speech associated with the text. (5) Mini map. A colored circle with a smaller circle (“dot”) moves around its edge to point to the currently active speaker. The color of the dot changes based on the direction. (6) Colored text. The text is colored based on the direction that the associated speech was coming from. (7) Everything on. All indicators are turned on (except the Circle + line, as it couldn’t be used simultaneously with the arrow). After participants had completed all conditions, they filled out a form that asked them to rate how desirable each of the five visual indicator styles (Edge indicator, Arrow, Circle + line, Colored map, and Colored text) were on a 7-point Likert scale, from -3: Strongly dislike to +3: Strongly like. Finally, they were asked to rate the overall value of directional feedback to the transcription experience, how strongly they would recommend these features to users of mobile captioning, and whether they had any general free-form feedback about SpeechCompass. Results\nMobile transcription apps (e.g., Android Live Transcribe) were the most used communication technology for the participants. Specifically, three used them multiple times per day, one used them daily, three used them multiple times per week, and one used them rarely. 75% of participants frequently experienced the scenario where multiple people would get mixed up in the transcript (two multiple times per day, two daily, two multiple times per week). All participants agreed that it was challenging to participate in conversations when speech was combined from multiple people. A Kruskal-Wallis (KW) test found a significant effect on participant preferences for visualization techniques (P=.014). The post-hoc pair-wise analyses using the Wilcoxon test with Bonferroni correction did, however, not show statistical significance between any pairs. Of the five visual indicator styles that participants experienced, Colored text was the most well-received (mean (¯𝑥) = 2.625), as it was rated positively by all the participants. The Arrow indicator was also well-received (¯𝑥= 1.125), with six positive, one negative, and one neutral participant. Several participants noted that Arrow and Colored text worked well together: \"Arrows + color seem to be most easier way to indicate the direction.\" (P2) and \"The combination of the colored text with the arrow was the most effective for me.\" (P7). The other indicator styles received more mixed feedback. The feedback for both Edge indicator (¯𝑥= 0.25) and Circle + line (¯𝑥= −0.125) was split between four negative and four positive participants. Some participants were concerned that Edge indicator was distracting and not sufficiently discreet: \"I do prefer the tool be as (n=494) A B Figure 13: Participants’ preferences for different visualization techniques in the online survey. A) Results indicating how valuable the specific indicator would be for the user. B) Preferences for the specific indicators for speech direction. Figure 14: Examples of seven visualization scenarios that participants experienced in the in-person study. Figure 15: Boxplots of results of the in-person study. A) Participants’ preferences for different visualization techniques. B) Overall opinions about augmented mobile ASR application. discrete as possible and would perhaps choose to avoid bright colored things moving around since this would be eye-catching and this kind of attention is often undesired\" (P3) and \"Indicator moving around the edge was distracting and causing a bit of eye strain\" (P2). On the other hand, another participant found this style particularly useful: \"the color dot moving to the speaker direction worked REALLY well\" CHI ’25, April 26-May 1, 2025, Yokohama, Japan Strongly like Like Weakly like Neutral Slightly dislike Dislike Strongly dislike Circle + line Strongly prefer Prefer Slightly prefer Neutral Slightly prefer Prefer Strongly prefer Highlight box Circle + line Strongly prefer Prefer Slightly prefer Neutral Slightly prefer Prefer Strongly prefer Arrow (P1). For Circle + line, some participants struggled with its legibility: \"If the analog direction indicators were larger (and translucent, or set behind)\" (P8) and \"The lines in a circle were a bit slower and not as accurate (buggy)\" (P5). The Mini map was rated positively by five participants and negatively by three. The most favorable participant stated: \"this is also great for environmental awareness for those with CHI ’25, April 26-May 1, 2025, Yokohama, Japan Artem Dementyev*, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, and Alex Olwal* single-sided hearing or no hearing at all.\" (P3) and a participant who disliked the Edge indicator commented: \"steady map in the corner worked a bit better (P5)\". Overall, all participants agreed with the value of directional feedback (¯𝑥= 2.88, seven Strongly agree:+3 and one Agree:+2) and would recommend these features to other users of captioning technology (¯𝑥= 2.63, five Strongly agree:+3 and three Agree:+2): \"I really liked that almost immediately I could tell that there was a speaker change, so that as soon as the text started to show up, I could better contextualize that text as attributed to a new speaker.\" (P1), \"I’m very happy to see this tool being developed, it’s a great addition to other speech recognition tools!\" (P3), and \"This prototype is definitely a life changer and I strongly believe that it will improve the quality of access to communication with speakers for many users\" (P6). 6.4\n\nDiscussion\nOur large-scale survey enabled us to identify essential challenges with current transcription technology for frequent users of captioning and shows that 68-70% of the participants are frequently in situations that today’s technology cannot adequately support. The findings suggest that more advanced speech technology for suppressing noise from adjacent speakers, music, or noise could help address those issues and that speech separation technology can potentially improve group conversations through more readable transcripts. Discussion\nConsistent with the large-scale survey, the value of the diarization and localization features was immediate to all users. The participants were asked if directional guidance would be valuable in their mobile transcription experience. All eight users agreed. Also, all eight users would recommend this feature to mobile captioning users. While the large-scale survey helped inform our testing and exclude conditions (e.g., Highlight box), the lab study allowed us to more rigorously evaluate the techniques in a realistic scenario. This difference became significant for the Edge indicator and Mini map, where issues, such as discreetness and distracting aspects, became evident during live usage. The results suggest that the combination of Colored text and Arrow would meet the preferences of most users, thanks to the balance of directional encoding and clarity. The arrow has redundant benefits too, since colored text might not always be reliably visible depending on lighting and screen conditions (e.g., strong sunlight, or dim display) and might also not be usable for colorblind users. The mixed feedback for other techniques indicates that the interface may also benefit from mechanisms that would allow users to customize the visualization style. Such customization could also apply to rendering properties, such as color, transparency, and line thickness, as some participants found Circle + line particularly difficult to interpret. In both the large-scale survey and the in-person lab study, the Arrow was preferred over Circle + line. Through more customization options and extended usage in their daily lives, participants will be able to provide more nuanced feedback about these techniques.\n\nRelated work\nThis section first discusses previous works in relevant audio applications like diarization, real-time audio transcription, and visualization interfaces. Then, we discuss the relevant literature on audio multi-microphone processing. Figure 2: Overview of the SpeechCompass phone case prototype. A) A mobile phone application interface with a mounted multi-microphone phone case. B) Inside and outside view of the prototype with a flexible PCB microphone mount and a compact main printed circuit board (PCB). C) Pictures of the main PCB with a top and bottom view. 2.1 Real-time mobile speech recognition CART (Communication Access Realtime Translation) is a wellestablished method for providing real-time captions. A trained professional, typically using specialized software and a stenography machine, transcribes speech into text as it is spoken. CART is frequently used in broadcasting scenarios such as lectures and presentations. Recent advances in machine learning enabled real-time automatic mobile speech captioning, such as Live Transcribe [2] and Microsoft Translator [46], and also on head-worn displays [50]. One of the main motivating uses for audio-to-text translation was audio accessibility, and research has shown the benefits of real-time ASR for hearing accessibility in scenarios like classrooms [13, 38]. Research has shown various improvements to real-time ASR, such as ways to display transcription confidence [9] and customizing fonts appearance to improve readability [8]. 2.2 Speaker separation and diarization: Transcripts that distinguish speakers Studies suggest that there is still room for improvements in, e.g., accuracy and usability [22]. We particularly observe that existing approaches have yet to leverage microphone arrays in mobile devices to augment ASR. We outline the existing techniques in Table 1. Acoustic beamforming relies on classical signal processing techniques such as beamforming and localization from multiple microphones [4] to separate and diarize speakers. The main challenge for speaker separation is estimating a beamformer for each speaker using localization and other cues. Recently, neural networks have been employed to successfully aid in beamforming [28, 65]. However, acoustic beamforming has been mainly applied to meeting CHI ’25, April 26-May 1, 2025, Yokohama, Japan room scenarios with a static microphone array, while SpeechCompass uses a similar multimicrophone technology for localization, which is applicable to mobile ASR throughout a user’s everyday life. Blind source separation. This approach separates the speakers using a single microphone without additional cues. The technique is challenging for classical signal processing, but various machinelearning techniques have been successful [19, 30]. Blind speaker separation is effective when done offline on the entire audio file, providing the model with access to both future and past content. This is, however, not possible in a real-time causal system, as in this paper, since the future is not accessible, and the model only processes a limited amount of past information. These constraints for real-time casual systems make blind separation inapplicable for real-time transcription of conversations. Recently, multi-microphone approaches have been combined with speaker separation and diarization. In [59] and [60] Taherian et al. tackle the challenge of speaker separation in multi-speaker scenarios, focusing on conversational or meeting environments. They leverage multi-channel audio and deep learning models to improve separation accuracy to enhance the performance of downstream speech applications like ASR. In one project [59], the authors leverage an end-to-end diarization system to identify individual speaker activity and then use this information to guide a multi-talker separation model. In another approach [60] a multi-input multi-output (MIMO) complex spectral mapping model allows for robust speaker localization, and is used to reduce speaker splitting errors. The complexity of the processing and non-causal components make these solutions unsuitable for real-time processing. CHI ’25, April 26-May 1, 2025, Yokohama, Japan Artem Dementyev*, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, and Alex Olwal* Table 1: Comparison of speaker diarization and separation technologies. The comparison shows that only SpeechCompass can support diarization and visualize sound direction on mobile devices. Acoustic beamforming [5, 25] Audio-visual [15, 18, 31] SpeechCompass [this work] Sound direction (360°) Yes Yes No Yes No No Mobile / on-the-go Yes No No No Yes Yes Computation cost Low Low High High High Medium Real-time Yes Yes No No No Yes Calibration free Yes Yes Yes Yes Yes No Privacy preserving Yes Yes No Yes Yes No Number of mics 3-4 3+ 1 2+ 1 1 Voice fingerprint audio embeddings. This approach extracts unique speaker embeddings from a single microphone [57, 62] and uses them for diarization. Principal Component Analysis (PCA) or other unsupervised methods are typically done on the embeddings. The speaker embedding approach has been the primary go-to for realtime diarization since it can run causally. Adding multi-microphone data to speaker embeddings has improved diarization accuracy [57]. A key disadvantage of speaker embeddings is its reliance on implicit or explicit speaker enrollment, as the initial number of speakers is unknown. Requiring every conversation partner or nearby speaker to explicitly register through a voice sample is particularly impractical in dynamic mobile scenarios. Furthermore, there are privacy concerns as speaker embeddings can be considered biometric information, and asking people to enroll would be in conflict with the discreetness that is often desired for accessibility aids. Audio-visual. Another approach has been to process audio and video using a multimodal model to separate speakers [18, 21]. The camera feed can help infer the active speaker from facial and lip motion when correlated with audio. Researchers have proposed deep learning models for audiovisual speaker separation that operate in the time-frequency domain and use cross-attention for audiovisual fusion [34]. The audio-visual model, while outperforming an audioonly model, however does not leverage spatial information from multiple microphones. The non-causal nature of the separation model (bi-LSTM) makes it ill-suited for real-time applications. This approach works best for meeting rooms and post-processing of recordings, since for mobile ASR, users typically do not point a camera at their conversation partners. There is potential for such applications for head-worn cameras (e.g., in smart glasses), although such approaches would still be dependent on a suitable field of view, sufficient bandwidth, and computing to process video streams. Such approaches will also have power and thermal implications for embedded devices with limited battery size. Several commercial off-the-shelf solutions exist for mobile speaker diarization. The Ava mobile application [6], for example, allows diarization by connecting each speaker’s smartphone to a network. However, this solution requires every speaker to set up and use their phone, which adds setup overhead. Speaksee [58] is another solution that utilizes clip-on microphones for each speaker, with each microphone exclusively picking up speech from its designated wearer. These microphones connect to a central hub that provides Audio embeddings as voice fingerprint [54, 58] Multiple mic blind source separation [55] Single mic blind source separation [16, 27] diarized transcripts. This solution, while effective, requires dedicated hardware, making it more appropriate for formal meeting room scenarios. While our approach may be less accurate than these solutions, because we are not using a dedicated microphone for each speaker, it does not depend on instrumentation or preparation by conversation partners, which is a crucial advantage in real-life situations whether with friends or strangers. It may even be prohibitive in certain scenarios where conversations are very brief (e.g., watercooler conversation) or the person wants to be discreet with their hearing accessibility needs. 2.3 Visualizing non-speech sounds for accessibility use cases Speech is not the only aspect of sound that has been transcribed. For example, sound event recognition and visual alerts are helpful in hearing accessibility [11, 20, 23, 33]. Expressive captions [15], for example, visually convey the speech’s emotion by altering the text’s rendering based on the detected emotion. Closest to this work, research has looked into displaying sound localization cues from a microphone array using head-worm displays, such as HoloLens [27] and Head-Mounted displays (HMD) [32]. Another work integrates localization into a watch [35] and displays sound location with LEDs. These proof-of-concept research projects use off-the-shelf devices, which do not meet power consumption, ergonomics, or form factor requirements needed for all-day use in everyday lives. In contrast, we leverage mobile phones that users already carry and use daily. While we demonstrate a prototype in a phone case form factor, the embedded hardware could also be implemented in wearable devices, such as head-worn displays. Due to captioning’s focus on speech, fine audio qualities and structures, such as rhythmicity, are lost. To communicate such qualities, researchers have proposed real-time visualizations such as audio spectrograms [25] or Stabilized Auditory Images (SAI) [44], a visualization grounded in models of the brain, or tactile cues [16]. However, these approaches not only require extensive user training but also do not convey sound direction or leverage spatial information. Our approach can, however, be combined with those existing methods to extend their expressiveness. 2.4 Sound localization strategies Multiple research projects have investigated sound localization, which can be divided into active and passive localization. Active localization sends out a signal and listens to the response. For example, Cricket [53] showed an indoor localization system using ultrasound beacons and detectors around the room. Active localization does not apply to speech, which is the main target of this paper. As used in this work, a passive localization system listens with an array of microphones. Example of passive localization includes gunshot localization [61], rendering of a sound source in virtual reality [39], robot navigation [43], and people tracking [14]. The Time Difference of Arrival (TDOA) is a popular technique for localization, where measuring the arrival delay between synchronized microphone pairs, allows the use of simple geometry to estimate the angle of arrival. Multiple localization techniques have been proposed in the literature based on the popular and effective TDOA estimation method. Most are variations [17, 40] on generalized cross-correlation [37], performed in the frequency domain. Other approaches have been demonstrated, such as localization with one microphone [54] by adding structures to the microphone that change the sound based on incoming angle, or using two thin wires that change resistance based on the direction of sound [1]. Recent high-end phones use two or three microphones, which allows limited localization on some phones [36, 49]. However, the microphone placement on these phones is typically optimized for phone calls and video capture, with placements that are less well suited for localization, as microphones on the front and back are blocked when the phone is held or placed on a table. Several development platforms offer localization. For example, MiniDSP [47] makes eight and sixteen microphone devices for localization and beamforming. ODAS [26] is an open-source platform for microphone array localization. Those platforms are, however, not designed for portable, battery-powered devices, given that their digital signal processing (DSP) chips have high power consumption (∼500 mA). Although mobile phone ASR has many users, previous work shows that diarization and sound localization still need to be addressed, especially to make it practical for conversations with multiple speakers. While machine learning has improved diarization, the approaches are still challenging to apply to mobile ASR due to their numerous limitations (See Table 1). Traditional multi-microphone processing techniques provide promising strategies for mobile ASR, which are practical today as shown through our technical implementation and evaluations.\n\nConclusion\nMotivated by a foundational large-scale survey with 263 users frequent users of captioning technology, this work demonstrates how microphone array signal processing can supplement ASR by measuring and visualizing the spatial dimension of audio. Although ASR technology has greatly improved, diarization and localization features are not commonly available. To investigate the potential of enabling mobile captioning for group conversations, we implemented a low-latency 360° localization algorithm that can run on general-purpose low-power microcontrollers, and a custom sound perception hardware solution with four microphones. Our technical evaluation of localization and diarization with the SpeechCompass microphone array demonstrates benefits over the traditional single-microphone configuration and pure machinelearning-based approaches. We introduced a mobile captioning app that uses sound localization to diarize and visualize speech directions for group conversations. The integration with our embedded software and hardware brought new capabilities to mobile ASR, including sound source location, speaker diarization, and user control of the diarization, allowing the suppression of specific speech directions. Using the developed mobile phone application, we conducted an in-person study with frequent users of mobile ASR technology to gather feedback on the techniques and different visualization styles, and their potential for improving the captioning of group conversations. All the participants found the diarization, localization, and visualization features to be useful, and particularly appreciated the combination of a directional arrow and colored text. This work demonstrates that low-power microphone array processing can be integrated into new and existing mobile devices, thereby leveraging audio’s natural spatial properties to enhance audio experiences and the understanding of speech. In the future, we hope that our approach will inspire the widespread adoption of advanced microphone arrays that natively unlock the potential of spatial sound processing and perception in mobile and wearable devices.\n\nAcknowledgments\nWe thank Sagar Salva and Dmitrii Votintcev for their ideas on prototypes and interaction designs. We are grateful to Pascal Getreuer, Richard Lyon, Alex Huang, Shao-Fu Shih, and Chet Gnegy for their help with algorithms. We also thank Shaun Kane, James Landay, Malcolm Slaney, and Meredith Morris for their feedback on this paper. We appreciate the contributions of Carson Lau for the phone case mechanical design and Ngan Nguyen for electronics assembly. Finally, we thank Mei Lu, Don Barnett, Ryan Geraghty, and Sanjay Batra for UX research and design.\n\nReferences\n[1] Chandykunju Alex, Kevin Martin Jose, and Arun Joseph. 2013. Sound localization and Visualization device. In 2013 IEEE Global Humanitarian Technology Conference (GHTC). 262–264. doi:10.1109/GHTC.2013.6713692 [2] Android. 2022. Introducing Live Transcribe. https://www.android.com/ accessibility/live-transcribe/. Accessed 2022-03-26. [3] Android. 2022. SpeechRecognizer API Documentation). https://developer.android. com/reference/android/speech/SpeechRecognizer. Accessed 2022-10-25. [4] Xavier Anguera, Chuck Wooters, and Javier Hernando. 2007. Acoustic beamforming for speaker diarization of meetings. IEEE Transactions on Audio, Speech, and Language Processing 15, 7 (2007), 2011–2022. [5] ARM. 2022. CMSIS DSP Software Library. https://www.keil.com/pack/doc/ CMSIS/DSP/html/index.html. Accessed 2022-05-12. [6] Ava. 2022. Ava Captioning Solution. https://www.ava.me/. Accessed 2024-12-10. [7] Jacob Benesty, Jingdong Chen, and Yiteng Huang. 2008. Conventional beamforming techniques. Microphone array signal processing (2008), 39–65. [8] Larwan Berke, Khaled Albusays, Matthew Seita, and Matt Huenerfauth. 2019. Preferred Appearance of Captions Generated by Automatic Speech Recognition for Deaf and Hard-of-Hearing Viewers. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI EA ’19). Association for Computing Machinery, New York, NY, USA, 1–6. doi:10.1145/3290607.3312921 [9] Larwan Berke, Christopher Caulfield, and Matt Huenerfauth. 2017. Deaf and Hard-of-Hearing Perspectives on Imperfect Automatic Speech Recognition for Captioning One-on-One Meetings. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (Baltimore, Maryland, USA) (ASSETS ’17). Association for Computing Machinery, New York, NY, USA, 155–164. doi:10.1145/3132525.3132541 [10] Rachel Boll, Shruti Mahajan, Jeanne Reis, and Erin T. Solovey. 2020. Creating Questionnaires That Align with ASL Linguistic Principles and Cultural Practices within the Deaf Community. , Article 61 (2020), 4 pages. doi:10.1145/3373625. 3418071 [11] Danielle Bragg, Nicholas Huynh, and Richard E. Ladner. 2016. A Personalizable Mobile Sound Detector App Design for Deaf and Hard-of-Hearing Users. In Proceedings of the 18th International ACM SIGACCESS Conference on Computers and Accessibility (Reno, Nevada, USA) (ASSETS ’16). Association for Computing Machinery, New York, NY, USA, 3–13. doi:10.1145/2982142.2982171 [12] Hervé Bredin, Ruiqing Yin, Juan Manuel Coria, Gregory Gelly, Pavel Korshunov, Marvin Lavechin, Diego Fustes, Hadrien Titeux, Wassim Bouaziz, and MariePhilippe Gill. 2020. pyannote.audio: neural building blocks for speaker diarization. In ICASSP 2020, IEEE International Conference on Acoustics, Speech, and Signal Processing. [13] Janine Butler, Brian Trager, and Byron Behm. 2019. Exploration of Automatic Speech Recognition for Deaf and Hard of Hearing Students in Higher Education Classes. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility (Pittsburgh, PA, USA) (ASSETS ’19). Association for Computing Machinery, New York, NY, USA, 32–42. doi:10.1145/3308561.3353772 [14] Chao Cai, Henglin Pu, Peng Wang, Zhe Chen, and Jun Luo. 2021. We Hear Your PACE: Passive Acoustic Localization of Multiple Walking Persons. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 5, 2, Article 55 (jun 2021), 24 pages. doi:10.1145/3463510 [15] Caluã de Lacerda Pataca, Saad Hassan, Nathan Tinker, Roshan Lalintha Peiris, and Matt Huenerfauth. 2024. Caption Royale: Exploring the Design Space of Affective Captions from the Perspective of Deaf and Hard-of-Hearing Individuals. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI ’24). Association for Computing Machinery, New York, NY, USA, Article 899, 17 pages. doi:10.1145/3613904.3642258 [16] Artem Dementyev, Pascal Getreuer, Dimitri Kanevsky, Malcolm Slaney, and Richard F Lyon. 2021. VHP: Vibrotactile Haptics Platform for On-Body Applications. In The 34th Annual ACM Symposium on User Interface Software and Technology (Virtual Event, USA) (UIST ’21). Association for Computing Machinery, New York, NY, USA, 598–612. doi:10.1145/3472749.3474772 CHI ’25, April 26-May 1, 2025, Yokohama, Japan [17] Hoang Do, Harvey F Silverman, and Ying Yu. 2007. A real-time SRP-PHAT source location implementation using stochastic region contraction (SRC) on a largeaperture microphone array. In 2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP’07, Vol. 1. IEEE, I–121. [18] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T Freeman, and Michael Rubinstein. 2018. Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation. arXiv preprint arXiv:1804.03619 (2018). [19] Hakan Erdogan, John R Hershey, Shinji Watanabe, and Jonathan Le Roux. 2015. Phase-sensitive and recognition-boosted speech separation using deep recurrent neural networks. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 708–712. [20] Leah Findlater, Bonnie Chinh, Dhruv Jain, Jon Froehlich, Raja Kushalnagar, and Angela Carey Lin. 2019. Deaf and Hard-of-Hearing Individuals’ Preferences for Wearable and Mobile Sound Awareness Technologies. In Proceedings of the Uk) (CHI ’19). Association for Computing Machinery, New York, NY, USA, 1–13. doi:10.1145/3290605.3300276 [21] Israel D Gebru, Sileye Ba, Xiaofei Li, and Radu Horaud. 2017. Audio-visual speaker diarization based on spatiotemporal bayesian fusion. IEEE transactions on pattern analysis and machine intelligence 40, 5 (2017), 1086–1099. [22] Abraham Glasser, Kesavan Kushalnagar, and Raja Kushalnagar. 2017. Deaf, Hard of Hearing, and Hearing Perspectives on Using Automatic Speech Recognition in Conversation. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility (Baltimore, Maryland, USA) (ASSETS ’17). Association for Computing Machinery, New York, NY, USA, 427–432. doi:10.1145/3132525.3134781 [23] Steven M. Goodman, Ping Liu, Dhruv Jain, Emma J. McDonnell, Jon E. Froehlich, and Leah Findlater. 2021. Toward User-Driven Sound Recognizer Personalization with People Who Are d/Deaf or Hard of Hearing. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 5, 2, Article 63 (jun 2021), 23 pages. doi:10.1145/ 3463501 [24] Google. 2018. Google Surveys Methodology. http://services.google.com/fh/files/ misc/white_paper_how_google_surveys_works.pdf. Accessed 2022-03-22. [25] Beth G Greene, David B Pisoni, and Thomas D Carrell. 1984. Recognition of speech spectrograms. The Journal of the Acoustical Society of America 76, 1 (1984), 32–43. [26] François Grondin and François Michaud. 2019. Lightweight and optimized sound source localization and tracking methods for open and closed microphone array configurations. Robotics and Autonomous Systems 113 (2019), 63–80. [27] Ru Guo, Yiru Yang, Johnson Kuang, Xue Bin, Dhruv Jain, Steven Goodman, Leah Findlater, and Jon Froehlich. 2020. HoloSound: Combining Speech and Sound Identification for Deaf or Hard of Hearing Users on a Head-Mounted Display. In Proceedings of the 22nd International ACM SIGACCESS Conference on Computers and Accessibility (Virtual Event, Greece) (ASSETS ’20). Association for Computing Machinery, New York, NY, USA, Article 71, 4 pages. doi:10.1145/3373625.3418031 [28] Jahn Heymann, Lukas Drude, and Reinhold Haeb-Umbach. 2016. Neural network based spectral mask estimation for acoustic beamforming. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 196–200. [29] Pulsar Instruments. 2022. Decibel chart – decibel levels of common sounds. https://pulsarinstruments.com/news/decibel-chart-noise-level. Accessed 202207-27. [30] Yusuf Isik, Jonathan Le Roux, Zhuo Chen, Shinji Watanabe, and John R Hershey.\n2016. Single-channel multi-speaker separation using deep clustering. arXiv\npreprint arXiv:1607.02173 (2016). [31] Dhruv Jain, Leah Findlater, Jamie Gilkeson, Benjamin Holland, Ramani Duraiswami, Dmitry Zotkin, Christian Vogler, and Jon E. Froehlich. 2015. HeadMounted Display Visualizations to Support Sound Awareness for the Deaf and Hard of Hearing. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (Seoul, Republic of Korea) (CHI ’15). Association for Computing Machinery, New York, NY, USA, 241–250. doi:10.1145/2702123. 2702393 [32] Dhruv Jain, Leah Findlater, Jamie Gilkeson, Benjamin Holland, Ramani Duraiswami, Dmitry Zotkin, Christian Vogler, and Jon E. Froehlich. 2015. HeadMounted Display Visualizations to Support Sound Awareness for the Deaf and Hard of Hearing. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (Seoul, Republic of Korea) (CHI ’15). Association for Computing Machinery, New York, NY, USA, 241–250. doi:10.1145/2702123. 2702393 [33] Dhruv Jain, Kelly Mack, Akli Amrous, Matt Wright, Steven Goodman, Leah Findlater, and Jon E. Froehlich. 2020. HomeSound: An Iterative Field Deployment of an In-Home Sound Awareness System for Deaf or Hard of Hearing Users. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI ’20). Association for Computing Machinery, New York, NY, USA, 1–12. doi:10.1145/3313831.3376758 [34] Vahid Ahmadi Kalkhorani, Anurag Kumar, Ke Tan, Buye Xu, and DeLiang Wang.\n2024. Audiovisual Speaker Separation with Full- and Sub-Band Modeling in the\nCHI ’25, April 26-May 1, 2025, Yokohama, Japan Artem Dementyev*, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, and Alex Olwal* Time-Frequency Domain. In ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 12001–12005. doi:10.1109/ ICASSP48485.2024.10446297 [35] Yoshihiro Kaneko, Inho Chung, and Kenji Suzuki. 2013. Light-emitting device for supporting auditory awareness of hearing-impaired people during group conversations. In 2013 IEEE International Conference on Systems, Man, and Cybernetics. IEEE, 3567–3572. [36] Ellington Kirby, Seoyoon Park, Yan Wang, and Yingying Chen. 2016. HearHere: Smartphone Based Audio Localization Using Time Difference of Arrival: Demo. In Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking (New York City, New York) (MobiCom ’16). Association for Computing Machinery, New York, NY, USA, 509–510. doi:10.1145/2973750.2985625 [37] Charles Knapp and Glifford Carter. 1976. The generalized correlation method for estimation of time delay. IEEE transactions on acoustics, speech, and signal processing 24, 4 (1976), 320–327. [38] Raja S. Kushalnagar, Gary W. Behm, Aaron W. Kelstone, and Shareef Ali.\n2015. Tracked Speech-To-Text Display: Enhancing Accessibility and Readability of Real-Time Speech-To-Text. In Proceedings of the 17th International ACM\nSIGACCESS Conference on Computers & Accessibility (Lisbon, Portugal) (ASSETS ’15). Association for Computing Machinery, New York, NY, USA, 223–230. doi:10.1145/2700648.2809843 [39] Ahmet Köse, Aleksei Tepljakov, and Sergei Astapov. 2017. Real-time localization and visualization of a sound source for virtual reality applications. In 2017 25th International Conference on Software, Telecommunications and Computer Networks (SoftCOM). 1–6. doi:10.23919/SOFTCOM.2017.8115577 [40] Bowon Lee, Amir Said, Ton Kalker, and Ronald W Schafer. 2008. Maximum likelihood time delay estimation with phase domain analysis in the generalized cross correlation framework. In 2008 Hands-Free Speech Communication and Microphone Arrays. IEEE, 89–92. [41] Bo Li, Tara N Sainath, Ron J Weiss, Kevin W Wilson, and Michiel Bacchiani.\n2016. Neural network adaptive beamforming for robust multichannel speech\nrecognition. (2016). [42] LibriVox. 2022. Alice’s Adventures in Wonderland by Lewis Carroll (Version 2). https://librivox.org/alices-adventures-in-wonderland-by-lewis-carroll-4/. Accessed 2022-07-12. [43] Hong Liu and Miao Shen. 2010. Continuous sound source localization based on microphone array for mobile robots. In 2010 IEEE/RSJ International Conference on Intelligent Robots and Systems. IEEE, 4332–4339. [44] Richard F Lyon. 2017. Human and machine hearing: extracting meaning from sound. Cambridge University Press. [45] James C Makous and John C Middlebrooks. 1990. Two-dimensional sound localization by human listeners. The journal of the Acoustical Society of America 87, 5 (1990), 2188–2200. [46] Microsoft. 2022. Translator. https://translator.microsoft.com/. Accessed 2022-0326. [47] MiniDSP. 2022. USB Mic array. https://www.minidsp.com/products/usb-audiointerface/uma-8-16-usb-mic-array. Accessed 2022-05-15. [48] Kai Morich. 2022. usb-serial-for-android). https://github.com/mik3y/usb-serialfor-android. Accessed 2022-11-09. [49] Pius Kavuma Basajjabaka Mugagga and Simon Winberg. 2015. Sound source localisation on Android smartphones: A first step to using smartphones as auditory sensors for training A.I systems with Big Data. In AFRICON 2015. 1–5. doi:10.1109/AFRCON.2015.7331970 [50] Alex Olwal, Kevin Balke, Dmitrii Votintcev, Thad Starner, Paula Conn, Bonnie Chinh, and Benoit Corda. 2020. Wearable Subtitles: Augmenting Spoken Communication with Lightweight Eyewear for All-Day Captioning. Association for 3379337.3415817 [51] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 5206–5210. [52] Tae Jin Park, Naoyuki Kanda, Dimitrios Dimitriadis, Kyu J Han, Shinji Watanabe, and Shrikanth Narayanan. 2022. A review of speaker diarization: Recent advances with deep learning. Computer Speech & Language 72 (2022), 101317. [53] Nissanka B. Priyantha, Anit Chakraborty, and Hari Balakrishnan. 2000. The Cricket Location-Support System. In Proceedings of the 6th Annual International Conference on Mobile Computing and Networking (Boston, Massachusetts, USA) (MobiCom ’00). Association for Computing Machinery, New York, NY, USA, 32–43. doi:10.1145/345910.345917 [54] Ashutosh Saxena and Andrew Y Ng. 2009. Learning sound location from a single microphone. In 2009 IEEE International Conference on Robotics and Automation. IEEE, 1737–1742. [55] Matthew Seita. 2020. Designing Automatic Speech Recognition Technologies to Improve Accessibility for Deaf and Hard-of-Hearing People in Small Group Meetings. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI, USA) (CHI EA ’20). Association for Computing Machinery, New York, NY, USA, 1–8. doi:10.1145/3334480.3375039 [56] Deep Sleep. 2022. Rain Sound. https://www.youtube.com/watch?v= [57] David Snyder, Daniel Garcia-Romero, Gregory Sell, Alan McCree, Daniel Povey, and Sanjeev Khudanpur. 2019. Speaker recognition for multi-speaker conversations using x-vectors. In ICASSP 2019-2019 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, 5796–5800. [58] Speaksee. 2022. Speaksee microphone kit. https://speak-see.com/. Accessed 2022-10-25. [59] Hassan Taherian, Ashutosh Pandey, Daniel Wong, Buye Xu, and DeLiang Wang.\n2024. Leveraging Sound Localization to Improve Continuous Speaker Separation.\nIn ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 621–625. doi:10.1109/ICASSP48485.2024.10446934 [60] Hassan Taherian and DeLiang Wang. 2024. Multi-Channel Conversational Speaker Separation via Neural Diarization. IEEE/ACM Transactions on Audio, Speech, and Language Processing 32 (2024), 2467–2476. doi:10.1109/TASLP.2024. 3393726 [61] Giuseppe Valenzise, Luigi Gerosa, Marco Tagliasacchi, Fabio Antonacci, and Augusto Sarti. 2007. Scream and gunshot detection and localization for audiosurveillance systems. In 2007 IEEE Conference on Advanced Video and Signal Based Surveillance. IEEE, 21–26. [62] Quan Wang, Carlton Downey, Li Wan, Philip Andrew Mansfield, and Ignacio Lopz Moreno. 2018. Speaker diarization with LSTM. In 2018 IEEE International conference on acoustics, speech and signal processing (ICASSP). IEEE, 5239–5243. [63] Lucinda Wilder. 1975. Articulatory and acoustic characteristics of speech sounds. In Understanding Language. Elsevier, 31–76. [64] Yuteng Xiao, Jihang Yin, Honggang Qi, Hongsheng Yin, and Gang Hua. 2017. MVDR algorithm based on estimated diagonal loading for beamforming. Mathematical Problems in Engineering 2017 (2017). [65] Yang Yang, George Sung, Shao-Fu Shih, Hakan Erdogan, Chehung Lee, and Matthias Grundmann. 2024. Binaural Angular Separation Network. In ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 1201–1205. [66] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo-yiin Chang, Tara N Sainath, Yanzhang He, Arun Narayanan, Wei Han, Anmol Gulati, Yonghui Wu, et al. 2021. Fastemit: Low-latency streaming asr with sequence-level emission regularization. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 6004–6008. A\n\nAppendix\nA.1 Localization algorithm The following GCC-PHAT formulation was used: 𝐺(𝑓) = F −1 \u0012 𝑋1(𝑓)[𝑋2(𝑓)]∗ \u0013 , (1) (|𝑋1(𝑓)[𝑋2(𝑓)]∗|)−0.3 where 𝑋1(𝑓) and 𝑋2(𝑓) are the Fourier transforms of the two microphone signals, []∗denotes complex conjugate, and F −1 is the inverse Fourier transform. 𝐺(𝑓) is the resulting cross-correlation. We use partial normalization to the power of -0.3 since it provides more robustness to noise by giving less weight to delays around end-fires, which are more likely due to noise. Partial normalization deviates from the original GCC-PHAT as it uses full normalization to weigh all delays equally. The time delay between the two microphones is extracted from the cross-correlation in the following way: Δ𝑡= 𝑎𝑟𝑔𝑚𝑎𝑥(𝐺(𝑓)) 𝑓𝑠 , (2) Where 𝑓𝑠is the audio sampling frequency (44.1 kHz), and𝑎𝑟𝑔𝑚𝑎𝑥 is the index of the maximum peak in the cross-correlation, corresponding to delay in samples. To convert the time delay into azimuth angle (in the microphone plane), the microphone spacing needs to be known to calculate the maximum delay: where 𝑐= 343𝑚/𝑠𝑒𝑐is the speed of sound, and Δ𝑑is the distance between the microphones. Assuming far-field sound waves, we can use a simple formula to calculate the azimuth angle. The far-field approximation assumes CHI ’25, April 26-May 1, 2025, Yokohama, Japan planar sound waves and is valid approximately if the microphone is a meter or more away from the source. 𝜃𝑎𝑧𝑖𝑚𝑢𝑡ℎ= 𝑐𝑜𝑠−1( Δ𝑡 Δ𝑡𝑚𝑎𝑥 ) (4)\n--- FOOTNOTES © 2025 Copyright held by the owner/author(s). 6X (for each unique mic pair) 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland 13EL6Mgeocc&t=3448s. Accessed 2022-07-12.", "persona": "Student", "response": "### 1. Overview\nSpeech-to-text capabilities on mobile devices, such as Live Transcribe, have become invaluable for hearing and speech accessibility, language translation, note-taking, and meeting transcripts. However, when multiple people participate in a conversation, existing mobile automatic speech recognition (ASR) apps typically concatenate all transcribed speech together, making it difficult to follow who is saying what. This limitation creates cognitive overload for users who need to simultaneously process the transcript, try to identify speakers, and participate in the conversation. The current solutions that rely on machine learning (ML) are difficult to set up in mobile scenarios. For example, audio-visual speech separation requires speakers to be visible to a camera and speaker embedding approaches require a model to determine the unique voiceprint of each speaker. Reduced latency: Extracting speaker embeddings requires sufficient information to cluster the speakers, which may cause lag. We imagine that multi-microphone localization for mobile transcription could have numerous practical applications. One example could be in the classroom setting, where students could more easily follow discussions between instructors and classmates. Similarly in business meetings, interviews or social gatherings, users could track speaker changes in multi-person conversations. User interface for visualizing speaker direction Edge indicators: Visual cues around the screen edges highlight speaker direction. Second, we demonstrated the prototype to eight frequent users of mobile speech-to-text and gathered feedback. The prototype was used to diarize and visualize a conversation between the researchers.\n\n### 2. Background\nLower computational and memory costs: Since there is no model nor weights, the algorithm can run on small microcontrollers with limited memory and compute. To understand the limitations of current mobile captioning technology, we conducted an online survey with 263 frequent users of captioning technology. The results show that current solutions struggle with a significant limitation — the inability to distinguish between speakers makes them challenging to use in group conversations. What's next? SpeechCompass demonstrates significant improvements for mobile captioning in group conversations, and there are numerous possible directions for additional development: We hope that this research inspires continued innovation in making communication more accessible and inclusive for everyone. Integration with additional wearable form factors like smart glasses and smartwatches Enhanced noise robustness through machine learning approaches\n\n### 3. Methods\nWe implement SpeechCompass in two different forms: as a phone case prototype with four microphones connected to a low-power microcontroller and as software for existing phones with two microphones. The phone case design provides optimal microphone placement to enable 360-degree sound localization. The software implementation offers only 180-degree localization on devices with two or more microphones, such as the Pixel phone. In both implementations, the phone is used for speech recognition and transcripts are visualized using a mobile application. Greater privacy preservation: SpeechCompass assumes that different speakers are physically in separate places and does not require video or any unique personally identifying information, like speaker embeddings (unique identity of individual’s voice). Language-agnostic operation: SpeechCompass looks at differences between the audio waveforms, without prior assumptions about the content and works for sounds beyond speech. Instant reconfiguration: SpeechCompass can be reconfigured instantly by moving the phone. To evaluate the SpeechCompass software, we placed a phone case on a rotating platform with a stationary speaker playing speech or noise.\n\n### 4. Results\nTechnical evaluation User evaluation and feedback Longitudinal studies to understand adoption and behavior in everyday scenarios The platform was rotated at 10 degree increments and the angle of arrival was measured for each angle. Our evaluation shows that SpeechCompass can accurately localize sound direction with an average error of 11°–22° for normal conversational loudness (60–65 dB). The accuracy is roughly comparable to human localization abilities. For example, if a person were asked where sound was heard behind them, their answer would typically have up to 20 degrees error.\n\n### 5. Takeaways\nIn “SpeechCompass: Enhancing Mobile Captioning with Diarization and Directional Guidance via Multi-Microphone Localization”, recipient of a Best url Award at CHI 2025, we explore an approach that enhances mobile captioning with speaker diarization (separating speakers in an ASR transcript) and real-time localization of incoming sound. SpeechCompass creates user-friendly transcripts for group conversations by providing color-coded visual separation for each speaker and directional indicators (arrows) to help users determine the direction from which speech is coming. This multi-microphone approach lowers computational costs, reduces latency, and enhances privacy preservation. Efficient real-time audio localization Unlike ML approaches to single-source speaker diarization, the SpeechCompass multi-microphone approach offers several advantages: For diarization, we used diarization error rate (DER), a standard metric for diarization that corresponds to correctness of color coded speaker diarization in the interface. Our tests showed the four-microphone configuration consistently outperformed the three-microphone setup, with relative DER improvements of 23%–35% across different signal-to-noise (SNR) conditions. Colored text: Speakers are separated using different colored text. Directional glyphs: Arrows, dials in a circle and color highlights on the boxes around the text point to the location of each speaker. Unwanted speech suppression: The user can click on the sides of the screen to suppress speech coming from those directions. This can be used to remove their own speech. Irrelevant nearby conversation can be removed from the transcript, which enhances the privacy of nearby speakers.", "paper_id": "34", "story_id": "34-01"}
{"paper": "Point·E: A System for Generating 3D Point Clouds from Complex Prompts Alex Nichol * 1 Heewoo Jun * 1 Prafulla Dhariwal 1 Pamela Mishkin 1 Mark Chen 1\n\nAbstract\nWhile recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method ﬁrst generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https: //github.com/openai/point-e. arXiv:2212.08751v1  [cs.CV]  16 Dec 2022\n\n1. Introduction\nWith the recent explosion of text-to-image generative models, it is now possible to generate and modify high-quality images from natural language descriptions in a number of seconds (Ramesh et al., 2021; Ding et al., 2021; Nichol et al., 2021; Ramesh et al., 2022; Gafni et al., 2022; Yu et al., 2022; Saharia et al., 2022; Feng et al., 2022; Balaji et al., 2022). Inspired by these results, recent works have explored text-conditional generation in other modalities, such as video (Hong et al., 2022; Singer et al., 2022; Ho et al., 2022b;a) and 3D objects (Jain et al., 2021; Poole et al., 2022; Lin et al., 2022a; Sanghi et al., 2021; 2022). In this work, we focus speciﬁcally on the problem of text-to-3D generation, which has signiﬁcant potential to democratize <heewoo@openai.com>. virtual reality, gaming, and industrial design. Recent methods for text-to-3D synthesis typically fall into one of two categories: 1. Methods which train generative models directly on\npaired (text, 3D) data (Chen et al., 2018; Mittal et al., 2022; Fu et al., 2022; Zeng et al., 2022) or unlabeled While these methods can leverage existing generative modeling approaches to produce samples efﬁciently, they are difﬁcult to scale to diverse and complex text prompts due to the lack of large-scale 3D datasets (Sanghi et al., 2022).\n\n2. Methods which leverage pre-trained text-image models to optimize differentiable 3D representations (Jain\net al., 2021; Poole et al., 2022; Lin et al., 2022a). These methods are often able to handle complex and diverse text prompts, but require expensive optimization processes to produce each sample. Furthermore, due to the lack of a strong 3D prior, these methods can fall into local minima which don’t correspond to meaningful or coherent 3D objects (Poole et al., 2022). We aim to combine the beneﬁts of both categories by pairing a text-to-image model with an image-to-3D model. Our textto-image model leverages a large corpus of (text, image) pairs, allowing it to follow diverse and complex prompts, while our image-to-3D model is trained on a smaller dataset of (image, 3D) pairs. To produce a 3D object from a text prompt, we ﬁrst sample an image using the text-to-image model, and then sample a 3D object conditioned on the sampled image. Both of these steps can be performed in a number of seconds, and do not require expensive optimization procedures. Figure 1 depicts this two-stage generation process. We base our generative stack on diffusion (Sohl-Dickstein et al., 2015; Song & Ermon, 2020b; Ho et al., 2020), a recently proposed generative framework which has become a popular choice for text-conditional image generation. For our text-to-image model, we use a version of GLIDE (Nichol et al., 2021) ﬁne-tuned on 3D renderings (Section 4.2). For our image-to-3D model, we use a stack of diffusion models which generate RGB point clouds conditioned on images (Section 4.3 and 4.4 detail our novel Transformerbased architecture for this task). For rendering-based evaluations, we go one step further and produce meshes from generated point clouds using a regression-based approach (Section 4.5). We ﬁnd that our system can often produce colored 3D point clouds that match both simple and complex text prompts (See Figure 2). We refer to our system as Point·E, since it generates point clouds efﬁciently. We release our point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e. Figure 1. A high-level overview of our pipeline. First, a text prompt is fed into a GLIDE model to produce a synthetic rendered view. Next, a point cloud diffusion stack conditions on this image to produce a 3D RGB point cloud. “a corgi wearing a red santa hat” “a multicolored rainbow pumpkin” “an elaborate fountain” “a trafﬁc cone” “a vase of purple ﬂowers” “a small red cube is sitting on top of a large blue cube. red on top, blue on bottom” “a pair of purple headphones” “a yellow rubber duck” “a red mug ﬁlled with coffee” “a humanoid robot with a round head” “a pair of 3d glasses, left lens is red right is blue” “an avocado chair, a chair imitating an avocado”\n\n2. Background\nOur method builds off of a growing body of work on diffusion-based models, which were ﬁrst proposed by SohlDickstein et al. (2015) and popularized more recently (Song & Ermon, 2020b;a; Ho et al., 2020). We follow the Gaussian diffusion setup of Ho et al. (2020), which we brieﬂy describe here. We aim to sample from some distribution q(x0) using a neural network approximation pθ(x0). Under Gaussian diffusion, we deﬁne a noising process q(xt|xt−1) := N(xt; p 1 −βtxt−1, βtI) for integer timesteps t ∈[0, T]. Intuitively, this process gradually adds Gaussian noise to a signal, with the amount of noise added at each timestep determined by some noise schedule βt. We employ a noise schedule such that, by the ﬁnal timestep t = T, the sample xT contains almost no information (i.e. it looks like Gaussian noise). Ho et al. (2020) note that it is possible to directly jump to a given timestep of the noising process without running the whole chain: xt = √¯αtx0 + √ 1 −¯αtϵ where ϵ ∼N(0, I) and ¯αt := Qt s=0 1 −βt. To train a diffusion model, we approximate q(xt−1|xt) as a neural network pθ(xt−1|xt). We can then produce a sample by starting at random Gaussian noise xT and gradually reversing the noising process until arriving at a noiseless sample x0. With enough small steps, pθ(xt−1|xt) can be parameterized as a diagonal Gaussian distribution, and Ho et al. (2020) propose to parameterize the mean of this distribution by predicting ϵ, the effective noise added to a sample xt. While Ho et al. (2020) ﬁx the variance Σ of pθ(xt−1|xt) to a reasonable per-timestep heuristic, Nichol & Dhariwal (2021) achieve better results by predicting the variance as well as the mean. Diffusion sampling can be cast through the lens of differential equations (Song et al., 2020), allowing one to use various SDE and ODE solvers to sample from these models. Karras et al. (2022) ﬁnd that a carefully-designed secondorder ODE solver provides a good trade-off between quality and sampling efﬁciency, and we employ this sampler for our point cloud diffusion models. To trade off sample diversity for ﬁdelity in diffusion models, several guidance strategies may be used. Dhariwal & Nichol (2021) introduce classiﬁer guidance, where gradients from a noise-aware classiﬁer ∇xtpθ(y|xt) are used to perturb every sampling step. They ﬁnd that increasing the scale of the perturbation increases generation ﬁdelity while reducing sample diversity. Ho & Salimans (2021) introduce classiﬁer-free guidance, wherein a conditional diffusion model p(xt−1|xt, y) is trained with the class label stochastically dropped and replaced with an additional ∅ class. During sampling, the model’s output ϵ is linearly extrapolated away from the unconditional prediction towards the conditional prediction: ϵguided := ϵθ(xt, ∅) + s · (ϵθ(xt, y) −ϵθ(xt, ∅)) for some guidance scale s ≥1. This approach is straightforward to implement, requiring only that conditioning information is randomly dropped during training time. We employ this technique throughout our models, using the drop probability 0.1.\n\n4. Method\nRather than training a single generative model to directly produce point clouds conditioned on text, we instead break the generation process into three steps. First, we generate a synthetic view conditioned on a text caption. Next, we produce a coarse point cloud (1,024 points) conditioned on the synthetic view. And ﬁnally, we produce a ﬁne point cloud (4,096 points) conditioned on the low-resolution point cloud and the synthetic view. In practice, we assume that the image contains the relevant information from the text, and do not explicitly condition the point clouds on the text. To generate text-conditional synthetic views, we use a 3billion parameter GLIDE model (Nichol et al., 2021) ﬁnetuned on rendered 3D models from our dataset (Section 4.2). To generate low-resolution point clouds, we use a conditional, permutation invariant diffusion model (Section 4.3). To upsample these low-resolution point clouds, we use a similar (but smaller) diffusion model which is additionally conditioned on the low-resolution point cloud (Section 4.4). with associated RGB colors for each point. We describe our data processing pipeline in more detail in Section 4.1. 4.1. Dataset We train our models on several million 3D models. We found that data formats and quality varied wildly across our dataset, prompting us to develop various post-processing steps to ensure higher data quality. To convert all of our data into one generic format, we rendered every 3D model from 20 random camera angles as RGBAD images using Blender (Community, 2018), which supports a variety of 3D formats and comes with an optimized rendering engine. For each model, our Blender script normalizes the model to a bounding cube, conﬁgures a standard lighting setup, and ﬁnally exports RGBAD images using Blender’s built-in realtime rendering engine. We then converted each object into a colored point cloud using its renderings. In particular, we ﬁrst constructed a dense point cloud for each object by computing points for each pixel in each RGBAD image. These point clouds typically contain hundreds of thousands of unevenly spaced points, so we additionally used farthest point sampling to create uniform clouds of 4K points. By constructing point clouds directly from renders, we were able to sidestep various issues that might arise from attempting to sample points directly from 3D meshes, such as sampling points which are contained within the model or dealing with 3D models that are stored in unusual ﬁle formats. Finally, we employed various heuristics to reduce the frequency of low-quality models in our dataset. First, we eliminated ﬂat objects by computing the SVD of each point cloud and only retaining those where the smallest singular value was above a certain threshold. Next, we clustered the dataset by CLIP features (for each object, we averaged features over all renders). We found that some clusters contained many low-quality categories of models, while other clusters appeared more diverse or interpretable. We binned these clusters into several buckets of varying quality, and used a weighted mixture of the resulting buckets as our ﬁnal dataset. 4.2. View Synthesis GLIDE Model Our point cloud models are conditioned on rendered views from our dataset, which were all produced using the same renderer and lighting settings. Therefore, to ensure that these models correctly handle generated synthetic views, we aim to explicitly generate 3D renders that match the distribution of our dataset. Figure 3. Our point cloud diffusion model architecture. Images are fed through a frozen, pre-trained CLIP model, and the output grid is fed as tokens into the transformer. Both the timestep t and noised input xt are also fed in as tokens. The output tokens corresponding to xt are used to predict ϵ and Σ. set, we only sample images from the 3D dataset 5% of the time, using the original dataset for the remaining 95%. We ﬁne-tune for 100K iterations, meaning that the model has made several epochs over the 3D dataset (but has never seen the same exact rendered viewpoint twice). To ensure that we always sample in-distribution renders (rather than only sampling them 5% of the time), we add a special token to every 3D render’s text prompt indicating that it is a 3D render; we then sample with this token at test time. 4.3. Point Cloud Diffusion To generate point clouds with diffusion, we extend the framework used by Zhou et al. (2021a) to include RGB colors for each point in a point cloud. In particular, we represent a point cloud as a tensor of shape K × 6, where K is the number of points, and the inner dimension contains (x, y, z) coordinates as well as (R, G, B) colors. All coordinates and colors are normalized to the range [−1, 1]. We then generate these tensors directly with diffusion, starting from random noise of shape K × 6, and gradually denoising it. Unlike prior work which leverages 3D-speciﬁc architectures to process point clouds, we use a simple Transformer-based model (Vaswani et al., 2017) to predict both ϵ and Σ conditioned on the image, timestep t, and noised point cloud xt. An overview of our architecture can be seen in Figure\n\n3. As input context to this model, we run each point in\nthe point cloud through a linear layer with output dimension D, obtaining a K × D input tensor. Additionally, we run the timestep t through a small MLP, obtaining another D-dimensional vector to prepend to the context. To condition on the image, we feed it through a pre-trained ViT-L/14 CLIP model, take the last layer embeddings from this CLIP model (of shape 256 × D′), and linearly project it into another tensor of shape 256 × D before prepending it to the Transformer context. In Section 5.1, we ﬁnd that this is superior to using a single CLIP image or text embedding, as done by Sanghi et al. (2021); Zeng et al. (2022); Sanghi et al. (2022). The final input context to our model is of shape (K +257)× D. To obtain a final output sequence of length K, we take the final K tokens of output and project it to obtain epsilon and Σ predictions for the K input points. Notably, we do not employ positional encodings for this model. As a result, the model itself is permutation-invariant to the input point clouds (although the output order is tied to the input order).4.4. Point Cloud Upsampler For image diffusion models, the best quality is typically achieved by using some form of hierarchy, where a lowresolution base model produces output which is then upsampled by another model (Nichol & Dhariwal, 2021; Saharia et al., 2021; Ho et al., 2021; Rombach et al., 2021). We employ this approach to point cloud generation by ﬁrst generating 1K points with a large base model, and then upsampling to 4K points using a smaller upsampling model. Notably, our models’ compute requirements scale with the number of points, so it is four times more expensive to generate 4K points than 1K points for a ﬁxed model size. Our upsampler uses the same architecture as our base model, with extra conditioning tokens for the low-resolution point cloud. To arrive at 4K points, the upsampler conditions on added to the low-resolution pointcloud. We pass the conditioning points through a separate linear embedding layer than the one used for xt, allowing the model to distinguish conditioning information from new points without requiring the use of positional embeddings. 4.5. Producing Meshes For rendering-based evaluations, we do not render generated point clouds directly. Rather, we convert the point clouds into textured meshes and render these meshes using Blender. Producing meshes from point clouds is a well-studied, sometimes difﬁcult problem. Point clouds produced by our models often have cracks, outliers, or other types of noise that make the problem particularly challenging. We brieﬂy tried using pre-trained SAP models (Peng et al., 2021) for this purpose, but found that the resulting meshes sometimes lost large portions or important details of the shape that were present in the point clouds. Rather than training new SAP models, we opted to take a simpler approach. To convert point clouds into meshes, we use a regressionbased model to predict the signed distance ﬁeld of an object given its point cloud, and then apply marching cubes (Lorensen & Cline, 1987) to the resulting SDF to extract a mesh. We then assign colors to each vertex of the mesh using the color of the nearest point from the original point cloud. For details, see Appendix C.\n\n5. Results\nIn the following sections, we conduct a number of ablations and comparisons to evaluate how our method performs and scales. We adopt the CLIP R-Precision (Park et al., 2021) metric for evaluating text-to-3D methods end-to-end, using the same object-centric evaluation prompts as Jain et al. (2021). Additionally, we introduce a new pair of metrics which we refer to as P-IS and P-FID, which are point cloud analogs for Inception Score (Salimans et al., 2016) and FID (Heusel et al., 2017), respectively. To construct our P-IS and P-FID metrics, we employ a modiﬁed PointNet++ model (Qi et al., 2017) to extract features and predict class probabilities for point clouds. For details, see Appendix B. 5.1. Model Scaling and Ablations In this section, we train a variety of base diffusion models to study the effect of scaling and to ablate the importance of image conditioning. We train the following base models and evaluate them throughout training: • 40M (uncond.): a small model without any conditioning information. • 40M (text vec.): a small model which only conditions on text captions, not rendered images. The text caption is embedded with CLIP, and the CLIP embedding is appended as a single extra token of context. This model depends on the text captions present in our 3D dataset, and does not leverage the ﬁne-tuned GLIDE model. • 40M (image vec.): a small model which conditions on CLIP image embeddings of rendered images, similar to Sanghi et al. (2021). This differs from the other image-conditional models in that the image is encoded into a single token of context, rather than as a sequence of latents corresponding to the CLIP latent grid. • 40M: a small model with full image conditioning through a grid of CLIP latents. • 300M: a medium model with full image conditioning through a grid of CLIP latents. 20 15 P-FID 10 5 0.25 0.50 0.75 1.00 1.25 training iterations 1e6 (a) P-FID 13 12 P-IS 11 10 9 0.25 0.50 0.75 1.00 1.25 training iterations 1e6 (b) P-IS 0.4 CLIP R-Precision 0.3 0.2 0.1 0.0 0.25 0.50 0.75 1.00 1.25 training iterations 1e6 (c) CLIP R-Precision Figure 4. Sample-based evaluations computed throughout training across different base model runs. The same upsampler and conditioning images are used for all runs. In order to isolate changes to the base model, we use the same (image conditional) upsampler model for all evaluations, and use the same 306 pre-generated synthetic views for the CLIP R-Precision evaluation prompts. Here we use the ViT-L/14 CLIP model to compute CLIP R-Precision, but we report results with an alternative CLIP model in Section 5.3. In Figure 4, we present the results of our ablations. We ﬁnd that using only text conditioning with no text-to-image step results in much worse CLIP R-Precision (see Appendix E for more details). Furthermore, we ﬁnd that using a single CLIP embedding to condition on images is worse than using a grid of embeddings, suggesting that the point cloud model beneﬁts from seeing more (spatial) information about the conditioning image. Finally, we ﬁnd that scaling our model improves the speed of P-FID convergence, and increases ﬁnal CLIP R-Precision. (a) Image to point cloud sample for the prompt “a very realistic 3D rendering of a corgi”. (b) Image to point cloud sample for the prompt “a trafﬁc cone”. Figure 5. Two common failure modes of our model. In the top example, the model incorrectly interprets the relative proportions of different parts of the depicted object, producing a tall dog instead of a short, long dog. In the bottom example, the model cannot see underneath the trafﬁc cone, and incorrectly infers a second mirrored cone. 5.2. Qualitative Results We ﬁnd that Point·E can often produce consistent and highquality 3D shapes for complex prompts. In Figure 2, we show various point cloud samples which demonstrate our model’s ability to infer a variety of shapes while correctly binding colors to the relevant parts of the shapes. Sometimes the point cloud diffusion model fails to understand or extrapolate the conditioning image, resulting in a shape that does not match the original prompt. We ﬁnd that this is usually due to one of two issues: 1) the model incorrectly interprets the shape of the object depicted in the image, or 2) the model incorrectly infers some part of the shape that is occluded in the image. In Figure 5, we present an example of each of these two failure modes. 5.3. Comparison to Other Methods As text-conditional 3D synthesis is a fairly new area of research, there is not yet a standard set of benchmarks for this task. However, several other works evaluate 3D generation using CLIP R-Precision, and we compare to these methods in Table 1. In addition to CLIP R-Precision, we also note the reported sampling compute requirements for each method. Table 1. Comparison of Point·E to other 3D generative techniques as measured by CLIP R-Precision (with two different CLIP base models) on COCO evaluation prompts. ∗50 P100-minutes converted to V100-minutes using conversion rate 1\n\n3. †Assuming 2\nV100 minutes = 1 A100 minute and 1 TPUv4-minute = 1 A100minute. We report DreamFields results from Poole et al. (2022).\n\nMethod\nViT-B/32 ViT-L/14 Latency DreamFields 78.6% 82.9% ∼200 V100-hr† CLIP-Mesh 67.8% 74.5% ∼17 V100-min∗ DreamFusion 75.1% 79.7% ∼12 V100-hr† Point·E (40M, text-only) 15.4% 16.2% Point·E (40M) 36.5% 38.8% 1.0 V100-min Point·E (300M) 40.3% 45.6% 1.2 V100-min Point·E (1B) 41.1% 46.8% 1.5 V100-min Conditioning images 69.6% 86.6% - • Unlike multi-view optimization-based methods like DreamFusion, Point·E does not explicitly optimize every view to match the text prompt. This could result in lower CLIP R-Precision simply because certain objects are not easy to identify from all angles. • Our method produces point clouds which must be preprocessed before rendering. Converting point clouds into meshes is a difﬁcult problem, and the approach we use can sometimes lose information present in the point clouds themselves. While our method performs worse on this evaluation than state-of-the-art techniques, it produces samples in a small fraction of the time. This could make it more practical for certain applications, or could allow for the discovery of higher-quality 3D objects by sampling many objects and selecting the best one according to some heuristic.\n\n3. Related Work\nSeveral prior works have explored generative models over point clouds. Achlioptas et al. (2017) train point cloud autoencoders, and ﬁt generative priors (either GANs (Goodfellow et al., 2014) or GMMs) on the resulting latent representations. Mo et al. (2019) generate point clouds using a VAE (Kingma & Welling, 2013) on hierarchical graph representations of 3D objects. Yang et al. (2019) train a two-stage ﬂow model for point cloud generation: ﬁrst, a prior ﬂow model produces a latent vector, and then a second ﬂow model samples points conditioned on the latent vector. Along the same lines, Luo & Hu (2021); Cai et al. (2020) both train two-stage models where the second stage is a diffusion model over individual points in a point cloud, and the ﬁrst stage is a latent ﬂow model or a latent GAN, respectively. Zeng et al. (2022) train a two-stage hierarchical VAE on point clouds with diffusion priors at both stages. Most similar to our work, Zhou et al. (2021a) introduce PVD, a single diffusion model that generates point clouds directly. Compared to previous point cloud diffusion methods such as PVD, our Transformer-based model architecture is simpler and incorporates less 3D-speciﬁc structure. Unlike prior works, our models also produce RGB channels alongside point cloud coordinates. A growing body of work explores the problem of 3D model generation in representations other than point clouds. Several works aim to train 3D-aware GANs from datasets of et al., 2021; Or-El et al., 2021; Gu et al., 2021; Zhou et al., 2021b). These GANs are typically applied to the problem of novel view synthesis in forward-facing scenes, and do not attempt to reconstruct full 360-degree views of objects. More recently, Gao et al. (2022) train a GAN that directly produces full 3D meshes, paired with a discriminator that inputs differentiably-rendered (Laine et al., 2020) views of the generated meshes. Bautista et al. (2022) generates complete 3D scenes by ﬁrst learning a representation space that decodes into NeRFs (Mildenhall et al., 2020), and then training a diffusion prior on this representation space. However, none of these works have demonstrated the ability to generate arbitrary 3D models conditioned on open-ended, complex text-prompts. Several recent works have explored the problem of textconditional 3D generation by optimizing 3D representations according to a text-image matching objective. Jain et al. (2021) introduce DreamFields, a method which optimizes the parameters of a NeRF using an objective based on CLIP (Radford et al., 2021). Notably, this method requires no (2022) optimizes a mesh using a CLIP-guided objective, ﬁnding that the mesh representation is more efﬁcient to optimize than a NeRF. More recently, Poole et al. (2022) extend DreamFields to leverage a pre-trained text-to-image diffusion model instead of CLIP, producing more coherent and complex objects. Lin et al. (2022a) build off of this technique, but convert the NeRF representation into a mesh and then reﬁne the mesh representation in a second optimization stage. While these approaches are able to produce diverse and complex objects or scenes, the optimization procedures typically require multiple GPU hours to converge, making them difﬁcult to apply in practical settings. While the above approaches are all based on optimization against a text-image model and do not leverage 3D data, other methods for text-conditional 3D synthesis make use of 3D data, possibly paired with text labels. Chen et al. (2018) employ a dataset of text-3D pairs to train a GAN to generate 3D representations conditioned on text. Liu et al. (2022) also leverage paired text-3D data to generate models in a joint representation space. Sanghi et al. (2021) employ a ﬂow-based model to generate 3D latent representations, and ﬁnd some text-to-3D capabilities when conditioning their model on CLIP embeddings. More recently, Zeng et al. (2022) achieve similar results when conditioning on CLIP embeddings, but employ a hierarchical VAE on point clouds for their generative stack. Mittal et al. (2022) and Fu et al. (2022) employ a VQ-VAE (van den Oord et al., 2017) with an autoregressive prior to sample 3D shapes conditioned on text labels. More recently, Sanghi et al. (2022) also employ a VQ-VAE approach, but leverage CLIP embeddings to avoid the need for explicit text labels in the dataset. While many of these works demonstrate promising early results, they tend to be limited to simple prompts or a narrow set of object categories due to the limited availability of 3D training data. Our method sidesteps this issue by leveraging a pre-trained text-to-image model to condition our 3D generation procedure. A large body of research focuses on reconstructing 3D models from single or few images. Notably, this is an underspeciﬁed problem, since the model must impute some details not present in the conditioning image(s). Nevertheless, some regression-based methods have shown promising results on this task (Choy et al., 2016; Wang et al., 2018; Gkioxari et al., 2019; Groueix et al., 2018; Yu et al., 2020; Lin et al., 2022b). A separate body of literature studies generative approaches for single- or multi-view reconstruction. Fan et al. (2016) predict point clouds of objects from single views using a VAE. Sun et al. (2018) use a hybrid of a ﬂow predictor and a GAN to generate novel views from few images. Kosiorek et al. (2021) use a view-conditional VAE to generate latent vectors for a NeRF decoder. Watson et al. (2022) employ an image-to-image diffusion model to synthesize novel views of an object conditioned on a single view, allowing many consistent views to be synthesized autoregressively.\n\n7. Conclusion\nWe have presented Point·E, a system for text-conditional synthesis of 3D point clouds that ﬁrst generates synthetic views and then generates colored point clouds conditioned on these views. We ﬁnd that Point·E is capable of efﬁciently producing diverse and complex 3D shapes conditioned on text prompts. We hope that our approach can serve as a starting point for further work in the ﬁeld of text-to-3D synthesis.\n\n8. Acknowledgements\nWe would like to thank everyone behind ChatGPT for creating a tool that helped provide useful writing feedback.\n\nReferences\nAchlioptas, P., Diamanti, O., Mitliagkas, I., and Guibas, L. Learning representations and generative models for 3d point clouds. arXiv:1707.02392, 2017. Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and Liu, M.-Y. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers, 2022. Dehghan, A., and Susskind, J. Gaudi: A neural architect for immersive 3d scene generation. arXiv:2207.13751, 2022. Cai, R., Yang, G., Averbuch-Elor, H., Hao, Z., Belongie, S., Snavely, N., and Hariharan, B. Learning gradient ﬁelds for shape generation. arXiv:2008.06520, 2020. Chan, E. R., Monteiro, M., Kellnhofer, P., Wu, J., and Wetzstein, G. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. arXiv:2012.00926, 2020. Chan, E. R., Lin, C. Z., Chan, M. A., Nagano, K., Pan, B., Mello, S. D., Gallo, O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., and Wetzstein, G. Efﬁcient geometry-aware 3d generative adversarial networks. arXiv:2112.07945, 2021. Chen, K., Choy, C. B., Savva, M., Chang, A. X., Funkhouser, T., and Savarese, S. Text2shape: Generating shapes from natural language by learning joint embeddings. arXiv:1803.08495, 2018. Choy, C. B., Xu, D., Gwak, J., Chen, K., and Savarese, S. 3d-r2n2: A uniﬁed approach for single and multi-view 3d object reconstruction. arXiv:1604.00449, 2016. Community, B. O. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www. blender.org. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. arXiv:2105.05233, 2021. Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., and Tang, J. Cogview: Mastering text-to-image generation via transformers. arXiv:2105.13290, 2021. Fan, H., Su, H., and Guibas, L. A point set generation network for 3d object reconstruction from a single image. arXiv:1612.00603, 2016. Feng, Z., Zhang, Z., Yu, X., Fang, Y., Li, L., Chen, X., Lu, Y., Liu, J., Yin, W., Feng, S., Sun, Y., Tian, H., Wu, H., and Wang, H. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-ofdenoising-experts. arXiv:2210.15257, 2022. Fu, R., Zhan, X., Chen, Y., Ritchie, D., and Sridhar, S. Shapecrafter: A recursive text-conditioned 3d shape generation model. arXiv:2207.09446, 2022. Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., and Taigman, Y. Make-a-scene: Scene-based text-toimage generation with human priors. arXiv:2203.13131, 2022. Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., and Fidler, S. Get3d: A generative model of high quality 3d textured shapes learned from images. arXiv:2209.11163, 2022. Gkioxari, G., Malik, J., and Johnson, J. Mesh r-cnn. arXiv:1906.02739, 2019. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. arXiv:1406.2661, 2014. Groueix, T., Fisher, M., Kim, V. G., Russell, B. C., and Aubry, M. Atlasnet: A papier-mˆach´e approach to learning 3d surface generation. arXiv:1802.05384, 2018. Gu, J., Liu, L., Wang, P., and Theobalt, C. Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. arXiv:2110.08985, 2021. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017. Ho, J. and Salimans, T. Classiﬁer-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https:// openreview.net/forum?id=qw8AKxfYbI. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. arXiv:2006.11239, 2020. Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and Salimans, T. Cascaded diffusion models for high ﬁdelity image generation. arXiv:2106.15282, 2021. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., and Salimans, T. Imagen video: High deﬁnition video generation with diffusion models. arXiv:2210.02303, 2022a. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet,\nD. J.\nVideo diffusion models. arXiv:2204.03458, 2022b. Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022. Jain, A., Mildenhall, B., Barron, J. T., Abbeel, P., and Poole,\nB. Zero-shot text-guided object generation with dream\nﬁelds. arXiv:2112.01455, 2021. Khalid, N. M., Xie, T., Belilovsky, E., and Popa, T. Clipmesh: Generating textured meshes from text using pretrained image-text models. arXiv:2203.13333, 2022. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv:1312.6114, 2013. Kosiorek, A. R., Strathmann, H., Zoran, D., Moreno, P., Schneider, R., Mokr´a, S., and Rezende, D. J. NeRFVAE: A geometry aware 3D scene generative model. arXiv:2104.00587, April 2021. Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., and Aila, T. Modular primitives for high-performance differentiable rendering. arXiv:2011.03277, 2020. Lin, C.-H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.-Y., and Lin, T.-Y. Magic3d: High-resolution text-to-3d content creation. arXiv:2211.10440, 2022a. Lin, K.-E., Yen-Chen, L., Lai, W.-S., Lin, T.-Y., Shih, Y.-C., and Ramamoorthi, R. Vision transformer for nerf-based view synthesis from a single input image. arXiv:2207.05736, 2022b. Liu, Z., Wang, Y., Qi, X., and Fu, C.-W. Towards implicit text-guided 3d shape generation. arXiv:2203.14622, 2022. Lorensen, W. E. and Cline, H. E. Marching cubes: A high resolution 3d surface construction algorithm. In Stone, M. C. (ed.), SIGGRAPH, pp. 163–169. ACM, 1987. ISBN 0-89791-227-6. URL http: //dblp.uni-trier.de/db/conf/siggraph/ siggraph1987.html#LorensenC87. Luo, S. and Hu, W. Diffusion probabilistic models for 3d point cloud generation. arXiv:2103.01458, 2021. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron,\nJ. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis.\narXiv:2003.08934, 2020. Mishkin, P., Ahmad, L., Brundage, M., Krueger, G., and Sastry, G. Dall·e 2 preview - risks and limitations. 2022. URL https://github.com/ openai/dalle-2-preview/blob/main/ system-card.md. Mittal, P., Cheng, Y.-C., Singh, M., and Tulsiani, S. Autosdf: Shape priors for 3d completion, reconstruction and generation. arXiv:2203.09516, 2022. Mo, K., Guerrero, P., Yi, L., Su, H., Wonka, P., Mitra, N., and Guibas, L. J. Structurenet: Hierarchical graph networks for 3d shape generation. arXiv:1908.00575, 2019. Neely, E. L. The risks of revolution: Ethical dilemmas in 3d printing from a us perspective. Science and Engineering Ethics, 22(5):1285–1297, Oct 2016. ISSN 14715546. doi: 10.1007/s11948-015-9707-4. URL https: Nichol, A. and Dhariwal, P. Improved denoising diffusion probabilistic models. arXiv:2102.09672, 2021. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv:2112.10741, 2021. Or-El, R., Luo, X., Shan, M., Shechtman, E., Park,\nJ. J., and Kemelmacher-Shlizerman, I. Stylesdf: Highresolution 3d-consistent image and geometry generation.\narXiv:2112.11427, 2021. Park, D. H., Azadi, S., Liu, X., Darrell, T., and Rohrbach, A. Benchmark for compositional text-to-image synthesis. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1),\n2021. URL https://openreview.net/forum?\nid=bKBhQhPeKaF. Peng, S., Jiang, C. M., Liao, Y., Niemeyer, M., Pollefeys, M., and Geiger, A. Shape as points: A differentiable poisson solver. arXiv:2106.03452, 2021. Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. arXiv:2209.14988, 2022. Qi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv:1706.02413, 2017. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. arXiv:2103.00020, 2021. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-toimage generation. arXiv:2102.12092, 2021. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents. arXiv:2204.06125, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. arXiv:2112.10752, 2021. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi,\nS. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., and\nNorouzi, M. Photorealistic text-to-image diffusion models with deep language understanding. arXiv:2205.11487, 2022. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. arXiv:1606.03498, 2016. Sanghi, A., Chu, H., Lambourne, J. G., Wang, Y., Cheng, C.-Y., Fumero, M., and Malekshan, K. R. Clip-forge: Towards zero-shot text-to-shape generation. arXiv:2110.02624, 2021. Sanghi, A., Fu, R., Liu, V., Willis, K., Shayani, H., Khasahmadi, A. H., Sridhar, S., and Ritchie, D. Textcraft: Zeroshot generation of high-ﬁdelity and diverse shapes from text. arXiv:2211.01427, 2022. Schwarz, K., Liao, Y., Niemeyer, M., and Geiger, A. Graf: Generative radiance ﬁelds for 3d-aware image synthesis. 2020. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., and Taigman, Y. Make-a-video: Text-to-video generation without text-video data. arXiv:2209.14792, 2022. Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv:1503.03585, 2015. Song, Y. and Ermon, S. Improved techniques for training score-based generative models. arXiv:2006.09011, 2020a. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. arXiv:arXiv:1907.05600, 2020b. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv:2011.13456, 2020. Straub, J. and Kerlin, S. Evaluation of the use of 3D printing and imaging to create working replica keys. In Javidi, B. and Son, J.-Y. (eds.), Three-Dimensional Imaging, Visualization, and Display 2016, volume 9867, pp. 98670E. International Society for Optics and Photonics, SPIE, 2016. doi: 10.1117/12.2223858. URL self-learned conﬁdence. In Ferrari, V., Hebert, M., Sminchisescu, C., and Weiss, Y. (eds.), Computer Vision – ECCV 2018, pp. 162–178, Cham, 2018. Springer International Publishing. ISBN 978-3-030-01219-9. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. arXiv:1711.00937, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. arXiv:1706.03762, 2017. Walther, G. Printing insecurity? the security implications of 3d-printing of weapons. Science and engineering ethics, 21, 12 2014. doi: 10.1007/s11948-014-9617-x. Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., and Jiang, Y.-G. Pixel2mesh: Generating 3d mesh models from single rgb images. arXiv:1804.01654, 2018. Watson, D., Chan, W., Martin-Brualla, R., Ho, J., Tagliasacchi, A., and Norouzi, M. Novel view synthesis with diffusion models. arXiv:2210.04628, 2022. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. Yang, G., Huang, X., Hao, Z., Liu, M.-Y., Belongie, S., and Hariharan, B. Pointﬂow: 3d point cloud generation with continuous normalizing ﬂows. arXiv:1906.12320, 2019. Yu, A., Ye, V., Tancik, M., and Kanazawa, A. pixelnerf: Neural radiance ﬁelds from one or few images. arXiv:2012.02190, 2020. Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., and Wu, Y. Scaling autoregressive models for contentrich text-to-image generation. arXiv:2206.10789, 2022. Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., and Kreis, K. Lion: Latent point diffusion models for 3d shape generation. arXiv:2210.06978, 2022. Zhou, L., Du, Y., and Wu, J. 3d shape generation and completion through point-voxel diffusion. arXiv:2104.03670, 2021a. Zhou, P., Xie, L., Ni, B., and Tian, Q. Cips-3d: A 3d-aware generator of gans based on conditionally-independent pixel synthesis. arXiv:2110.09788, 2021b. Table 2. Training hyper-parameters for our point cloud diffusion models. Width and depth refer to the size of the transformer backbone. Model Width Depth LR # Params Base (40M) 512 12 1e-4 40,466,956 Base (300M) 1024 24 7e-5 311,778,316 Base (1B) 2048 24 5e-5 1,244,311,564 Upsampler 512 12 1e-4 40,470,540 Table 3. Sampling hyperparameters for ﬁgures and CLIP RPrecision evaluations. Hyperparameter Base Upsampler Timesteps 64 64 Guidance scale 3.0 3.0 Schurn 3 0 σmin 1e-3 1e-3 σmax 120 160\n\nA. Hyperparameters\nWe train all of our diffusion models with batch size 64 for 1,300,000 iterations. In Table 2, we enumerate the training hyperparameters that were varied across model sizes. We train all of our models with 1024 diffusion timesteps. For our upsampler model, we use the linear noise schedule from Ho et al. (2020), and for our base models, we use the cosine noise schedule proposed by Nichol & Dhariwal (2021). For P-FID and P-IS evaluations, we produce 10K samples using stochastic DDPM with the full noise schedule. For CLIP R-Precision and ﬁgures in the paper, we use 64 steps (128 function evaluations) of the Heun sampler from Karras et al. (2022) for both the base and upsampler models. Table 3 enumerates the hyperparameters used for Heun sampling. When sampling from GLIDE, we use 150 diffusion steps for the base model, and 50 diffusion steps for the upsampling model. We report sampling time for each component of our stack in Table 4.\nB. P-FID and P-IS Metrics\nTo evaluate P-FID and P-IS, we train a PointNet++ model on ModelNet40 (Wu et al., 2015) using an open source implementation.1 We modify the baseline model in several ways. First, we double the width of the model, resulting in roughly 16 million parameters. Next, we apply some additional data augmentations to make the model more robust to out-of-distribution samples. In particular, we apply random Table 4. Sampling performance for various components of our model. We use the Karras sampler for our base and upsampler models, but not for GLIDE. Model V100 seconds GLIDE 46.28 Upsampler (40M) 12.58 Base (40M) 3.35 Base (300M) 12.78 Base (1B) 28.67 rotations to each point cloud, and we add Gaussian noise to the points with standard deviation sampled from U[0, 0.01]. To compute P-FID, we extract features for each point cloud from the layer before the ﬁnal ReLU activation. To compute P-IS, we use the predicted class probabilities for the 40 classes from ModelNet40. We note that our generative models are trained on a dataset which only has P-IS 12.95, so our best reported P-IS score of ∼13 is near the expected upper bound.\nC. Mesh Extraction\nTo convert point clouds into meshes, we train a model which predicts SDFs from point clouds and apply marching cubes to the resulting SDFs. We parametrize our SDF model as an encoder-decoder Transformer. First, an 8-layer encoder processes the input point cloud as an unordered sequence, producing a sequence of hidden representations. Then, a 4-layer cross-attention decoder takes 3D coordinates and the sequence of latent vectors, and predicts an SDF value. Each input query point is processed independently, allowing for efﬁcient batching. Using more layers in the encoder and fewer in the decoder allows us to amortize the encoding cost across many query points. We train our SDF regression model on a subset of 2.4 million manifold meshes from our dataset, and add Gaussian noise with σ = 0.005 to the point clouds as data augmentation. We train the model fθ(x) to predict the SDF y with a weighted L1 objective: ( 1 · ||fθ(x) −y||1 fθ(x) > y 4 · ||fθ(x) −y||1 fθ(x) ≤y Here, we deﬁne the SDF such that points outside of the surface have negative sign. Therefore, in the face of uncertainty, the model is encouraged predict that points are inside the surface. We found this to be helpful in initial experiments, likely because it helps prevent the resulting meshes from effectively ignoring thin or noisy parts of the point cloud. Figure 7. Examples of point clouds (left) and corresponding extracted meshes (right). We ﬁnd that our method often produces smooth meshes and removes outliers (middle row), but can sometimes miss thin/sparse parts of objects (bottom row). of 128 × 128 × 128, resulting in 1283 queries to the SDF model. In Figure 7, we show examples of input point clouds and corresponding output meshes from our model. We observe that our method works well in many cases, but sometimes fails to capture thin or sparse parts of a point cloud.\nD. Conditioning on DALL·E 2 Samples\nIn our main experiments, we use a specialized text-to-image model to produce in-distribution conditioning images for our point cloud models. In this section, we explore what happens if we use renders from a pre-existing text-to-image model, DALL·E 2. In Figure 8, we present three image-to-3D examples where the conditioning images are generated by DALL·E 2. We ﬁnd that DALL·E 2 tends to include shadows under objects, and our point cloud model interprets these as a dark ground plane. We also ﬁnd that our point cloud model can misinterpret shapes from the generated images when the objects take up too much of the image. In these cases, adding a border around the generated images can improve the reconstructed shapes. Figure 8. Examples of point clouds reconstructed from DALL·2 generations. The top image was produced using the prompt “a 3d rendering of an avocado chair, chair imitating an avocado, full view, white background”. The middle image was produced using the prompt “a simple full view of a 3d rendering of a corgi in front of a white background”. The bottom image is the same as the middle image, but with an additional white border.\nE. Pure Text-Conditional Generation\nIn Section 5.1, we train a pure text-conditional point cloud model without an additional image generation step. While we ﬁnd that this model performs worse on evaluations than our full system, it still achieves non-trivial results. In this section, we explore the capabilities and limitations of this model. In Figure 9, we show examples where our text-conditional model is able to produce point clouds matching the provided text prompt. Notably, these examples include simple prompts that describe single objects. In Figure 10, we show examples where this model struggles with prompts that combine multiple concepts. Finally, we expect that this model has inherited biases from our 3D dataset. We present one possible example of this in Figure 11, wherein the model produces longer and narrower objects for the prompt “a woman” than for the prompt “a man” when using a ﬁxed diffusion noise seed. “a motorbike” “a dog” “a desk lamp” “a guitar” “an ambulance” “a laptop computer” Figure 9. Selected point clouds generated by our pure textconditional 40M parameter point cloud diffusion model. (a) Prompt: “a small red cube is sitting on top of a large blue cube. red on top, blue on bottom” (b) Prompt: “a corgi wearing a red santa hat” Figure 10. Sample grids where our small, pure text-conditional model fails to understand complex prompts. (a) Prompt: “a man” (b) Prompt: “a woman” Figure 11. Sample grids from our pure text-conditional 40M parameter model. Samples in the top grid use the same noise seed as the corresponding samples in the bottom grid.\n--- FOOTNOTES 3D content creation for a wide range of applications such as *Equal contribution 1OpenAI, San Francisco, USA. Correspondence to: Alex Nichol <alex@openai.com>, Heewoo Jun 3D data (Sanghi et al., 2021; 2022; Watson et al., 2022). 2D images (Chan et al., 2020; Schwarz et al., 2020; Chan 3D training data. Building on this principle, Khalid et al. 1K points and generates an additional 3K points which are 40M (uncond.) 40M (text vec.) 40M (image vec.) 40M 300M 1B 40M (uncond.) 40M (text vec.) 40M (image vec.) 40M 300M 1B 1B 300M 40M 40M (image vec.) 40M (text vec.) 40M (uncond.) 16 V100-sec", "persona": "General Public", "response": "### 1. Overview\nThe next breakthrough to take the AI world by storm might be 3D model generators. This week, OpenAI open sourced Point-E, a machine learning system that creates a 3D object given a text prompt. According to a url published alongside the code base, Point-E can produce 3D models in one to two minutes on a single Nvidia V100 GPU. “While our method performs worse on this evaluation than state-of-the-art techniques, it produces samples in a small fraction of the time,” they wrote in the url. “This could make it more practical for certain applications, or could allow for the discovery of higher-quality 3D object.” OpenAI might be the latest company to jump into the 3D object generator fray, but — as alluded to earlier — it certainly isn’t the first. Earlier this year, Google released DreamFusion, an expanded version of Dream Fields, a generative 3D system that the company unveiled back in 2021. Unlike Dream Fields, DreamFusion requires no prior training, meaning that it can generate 3D representations of objects without 3D data.\n\n### 2. Background\nTo get around this limitation, the Point-E team trained an additional AI system to convert Point-E’s point clouds to meshes. (Meshes — the collections of vertices, edges and faces that define an object — are commonly used in 3D modeling and design.) But they note in the url that the model can sometimes miss certain parts of objects, resulting in blocky or distorted shapes. While all eyes are on 2D art generators at the present, model-synthesizing AI could be the next big industry disruptor. 3D models are widely used in film and TV, interior design, architecture and various science fields. Architectural firms use them to demo proposed buildings and landscapes, for example, while engineers leverage models as designs of new devices, vehicles and structures. 3D models usually take a while to craft, though — anywhere between several hours to several days.\n\n### 3. Methods\nAI like Point-E could change that if the kinks are someday worked out, and make OpenAI a respectable profit doing so. The question is what sort of intellectual property disputes might arise in time. There’s a large market for 3D models, with several online marketplaces including CGStudio and CreativeMarket allowing artists to sell content they’ve created. If Point-E catches on and its models make their way onto the marketplaces, model artists might protest, pointing to evidence that modern generative AI borrows heavily from its training data — existing 3D models, in Point-E’s case. Like DALL-E 2, Point-E doesn’t credit or cite any of the artists that might’ve influenced its generations.\n\n### 4. Results\nPoint-E doesn’t create 3D objects in the traditional sense. Rather, it generates point clouds, or discrete sets of data points in space that represent a 3D shape — hence the cheeky abbreviation. (The “E” in Point-E is short for “efficiency,” because it’s ostensibly faster than previous 3D object generation approaches.) Point clouds are easier to synthesize from a computational standpoint, but they don’t capture an object’s fine-grained shape or texture — a key limitation of Point-E currently. When given a text prompt — for example, “a 3D printable gear, a single gear 3 inches in diameter and half inch thick” — Point-E’s text-to-image model generates a synthetic rendered object that’s fed to the image-to-3D model, which then generates a point cloud. After training the models on a dataset of “several million” 3D objects and associated metadata, Point-E could produce colored point clouds that frequently matched text prompts, the OpenAI researchers say.\n\n### 5. Takeaways\nIt’s not perfect — Point-E’s image-to-3D model sometimes fails to understand the image from the text-to-image model, resulting in a shape that doesn’t match the text prompt. Still, it’s orders of magnitude faster than the previous state-of-the-art — at least according to the OpenAI team. What are the applications, exactly? Well, the OpenAI researchers point out that Point-E’s point clouds could be used to fabricate real-world objects, for example through 3D printing. With the additional mesh-converting model, the system could — once it’s a little more polished — also find its way into game and animation development workflows. To their credit, the researchers do mention that they expect Point-E to suffer from other problems, like biases inherited from the training data and a lack of safeguards around models that might be used to create “dangerous objects.” That’s perhaps why they’re careful to characterize Point-E as a “starting point” that they hope will inspire “further work” in the field of text-to-3D synthesis.", "paper_id": "20", "story_id": "20-02"}
{"paper": "Point·E: A System for Generating 3D Point Clouds from Complex Prompts Alex Nichol * 1 Heewoo Jun * 1 Prafulla Dhariwal 1 Pamela Mishkin 1 Mark Chen 1\n\nAbstract\nWhile recent work on text-conditional 3D object generation has shown promising results, the state-of-the-art methods typically require multiple GPU-hours to produce a single sample. This is in stark contrast to state-of-the-art generative image models, which produce samples in a number of seconds or minutes. In this paper, we explore an alternative method for 3D object generation which produces 3D models in only 1-2 minutes on a single GPU. Our method ﬁrst generates a single synthetic view using a text-to-image diffusion model, and then produces a 3D point cloud using a second diffusion model which conditions on the generated image. While our method still falls short of the state-of-the-art in terms of sample quality, it is one to two orders of magnitude faster to sample from, offering a practical trade-off for some use cases. We release our pre-trained point cloud diffusion models, as well as evaluation code and models, at https: //github.com/openai/point-e. arXiv:2212.08751v1  [cs.CV]  16 Dec 2022\n\n1. Introduction\nWith the recent explosion of text-to-image generative models, it is now possible to generate and modify high-quality images from natural language descriptions in a number of seconds (Ramesh et al., 2021; Ding et al., 2021; Nichol et al., 2021; Ramesh et al., 2022; Gafni et al., 2022; Yu et al., 2022; Saharia et al., 2022; Feng et al., 2022; Balaji et al., 2022). Inspired by these results, recent works have explored text-conditional generation in other modalities, such as video (Hong et al., 2022; Singer et al., 2022; Ho et al., 2022b;a) and 3D objects (Jain et al., 2021; Poole et al., 2022; Lin et al., 2022a; Sanghi et al., 2021; 2022). In this work, we focus speciﬁcally on the problem of text-to-3D generation, which has signiﬁcant potential to democratize <heewoo@openai.com>. virtual reality, gaming, and industrial design. Recent methods for text-to-3D synthesis typically fall into one of two categories: 1. Methods which train generative models directly on\npaired (text, 3D) data (Chen et al., 2018; Mittal et al., 2022; Fu et al., 2022; Zeng et al., 2022) or unlabeled While these methods can leverage existing generative modeling approaches to produce samples efﬁciently, they are difﬁcult to scale to diverse and complex text prompts due to the lack of large-scale 3D datasets (Sanghi et al., 2022).\n\n2. Methods which leverage pre-trained text-image models to optimize differentiable 3D representations (Jain\net al., 2021; Poole et al., 2022; Lin et al., 2022a). These methods are often able to handle complex and diverse text prompts, but require expensive optimization processes to produce each sample. Furthermore, due to the lack of a strong 3D prior, these methods can fall into local minima which don’t correspond to meaningful or coherent 3D objects (Poole et al., 2022). We aim to combine the beneﬁts of both categories by pairing a text-to-image model with an image-to-3D model. Our textto-image model leverages a large corpus of (text, image) pairs, allowing it to follow diverse and complex prompts, while our image-to-3D model is trained on a smaller dataset of (image, 3D) pairs. To produce a 3D object from a text prompt, we ﬁrst sample an image using the text-to-image model, and then sample a 3D object conditioned on the sampled image. Both of these steps can be performed in a number of seconds, and do not require expensive optimization procedures. Figure 1 depicts this two-stage generation process. We base our generative stack on diffusion (Sohl-Dickstein et al., 2015; Song & Ermon, 2020b; Ho et al., 2020), a recently proposed generative framework which has become a popular choice for text-conditional image generation. For our text-to-image model, we use a version of GLIDE (Nichol et al., 2021) ﬁne-tuned on 3D renderings (Section 4.2). For our image-to-3D model, we use a stack of diffusion models which generate RGB point clouds conditioned on images (Section 4.3 and 4.4 detail our novel Transformerbased architecture for this task). For rendering-based evaluations, we go one step further and produce meshes from generated point clouds using a regression-based approach (Section 4.5). We ﬁnd that our system can often produce colored 3D point clouds that match both simple and complex text prompts (See Figure 2). We refer to our system as Point·E, since it generates point clouds efﬁciently. We release our point cloud diffusion models, as well as evaluation code and models, at https://github.com/openai/point-e. Figure 1. A high-level overview of our pipeline. First, a text prompt is fed into a GLIDE model to produce a synthetic rendered view. Next, a point cloud diffusion stack conditions on this image to produce a 3D RGB point cloud. “a corgi wearing a red santa hat” “a multicolored rainbow pumpkin” “an elaborate fountain” “a trafﬁc cone” “a vase of purple ﬂowers” “a small red cube is sitting on top of a large blue cube. red on top, blue on bottom” “a pair of purple headphones” “a yellow rubber duck” “a red mug ﬁlled with coffee” “a humanoid robot with a round head” “a pair of 3d glasses, left lens is red right is blue” “an avocado chair, a chair imitating an avocado”\n\n2. Background\nOur method builds off of a growing body of work on diffusion-based models, which were ﬁrst proposed by SohlDickstein et al. (2015) and popularized more recently (Song & Ermon, 2020b;a; Ho et al., 2020). We follow the Gaussian diffusion setup of Ho et al. (2020), which we brieﬂy describe here. We aim to sample from some distribution q(x0) using a neural network approximation pθ(x0). Under Gaussian diffusion, we deﬁne a noising process q(xt|xt−1) := N(xt; p 1 −βtxt−1, βtI) for integer timesteps t ∈[0, T]. Intuitively, this process gradually adds Gaussian noise to a signal, with the amount of noise added at each timestep determined by some noise schedule βt. We employ a noise schedule such that, by the ﬁnal timestep t = T, the sample xT contains almost no information (i.e. it looks like Gaussian noise). Ho et al. (2020) note that it is possible to directly jump to a given timestep of the noising process without running the whole chain: xt = √¯αtx0 + √ 1 −¯αtϵ where ϵ ∼N(0, I) and ¯αt := Qt s=0 1 −βt. To train a diffusion model, we approximate q(xt−1|xt) as a neural network pθ(xt−1|xt). We can then produce a sample by starting at random Gaussian noise xT and gradually reversing the noising process until arriving at a noiseless sample x0. With enough small steps, pθ(xt−1|xt) can be parameterized as a diagonal Gaussian distribution, and Ho et al. (2020) propose to parameterize the mean of this distribution by predicting ϵ, the effective noise added to a sample xt. While Ho et al. (2020) ﬁx the variance Σ of pθ(xt−1|xt) to a reasonable per-timestep heuristic, Nichol & Dhariwal (2021) achieve better results by predicting the variance as well as the mean. Diffusion sampling can be cast through the lens of differential equations (Song et al., 2020), allowing one to use various SDE and ODE solvers to sample from these models. Karras et al. (2022) ﬁnd that a carefully-designed secondorder ODE solver provides a good trade-off between quality and sampling efﬁciency, and we employ this sampler for our point cloud diffusion models. To trade off sample diversity for ﬁdelity in diffusion models, several guidance strategies may be used. Dhariwal & Nichol (2021) introduce classiﬁer guidance, where gradients from a noise-aware classiﬁer ∇xtpθ(y|xt) are used to perturb every sampling step. They ﬁnd that increasing the scale of the perturbation increases generation ﬁdelity while reducing sample diversity. Ho & Salimans (2021) introduce classiﬁer-free guidance, wherein a conditional diffusion model p(xt−1|xt, y) is trained with the class label stochastically dropped and replaced with an additional ∅ class. During sampling, the model’s output ϵ is linearly extrapolated away from the unconditional prediction towards the conditional prediction: ϵguided := ϵθ(xt, ∅) + s · (ϵθ(xt, y) −ϵθ(xt, ∅)) for some guidance scale s ≥1. This approach is straightforward to implement, requiring only that conditioning information is randomly dropped during training time. We employ this technique throughout our models, using the drop probability 0.1.\n\n4. Method\nRather than training a single generative model to directly produce point clouds conditioned on text, we instead break the generation process into three steps. First, we generate a synthetic view conditioned on a text caption. Next, we produce a coarse point cloud (1,024 points) conditioned on the synthetic view. And ﬁnally, we produce a ﬁne point cloud (4,096 points) conditioned on the low-resolution point cloud and the synthetic view. In practice, we assume that the image contains the relevant information from the text, and do not explicitly condition the point clouds on the text. To generate text-conditional synthetic views, we use a 3billion parameter GLIDE model (Nichol et al., 2021) ﬁnetuned on rendered 3D models from our dataset (Section 4.2). To generate low-resolution point clouds, we use a conditional, permutation invariant diffusion model (Section 4.3). To upsample these low-resolution point clouds, we use a similar (but smaller) diffusion model which is additionally conditioned on the low-resolution point cloud (Section 4.4). with associated RGB colors for each point. We describe our data processing pipeline in more detail in Section 4.1. 4.1. Dataset We train our models on several million 3D models. We found that data formats and quality varied wildly across our dataset, prompting us to develop various post-processing steps to ensure higher data quality. To convert all of our data into one generic format, we rendered every 3D model from 20 random camera angles as RGBAD images using Blender (Community, 2018), which supports a variety of 3D formats and comes with an optimized rendering engine. For each model, our Blender script normalizes the model to a bounding cube, conﬁgures a standard lighting setup, and ﬁnally exports RGBAD images using Blender’s built-in realtime rendering engine. We then converted each object into a colored point cloud using its renderings. In particular, we ﬁrst constructed a dense point cloud for each object by computing points for each pixel in each RGBAD image. These point clouds typically contain hundreds of thousands of unevenly spaced points, so we additionally used farthest point sampling to create uniform clouds of 4K points. By constructing point clouds directly from renders, we were able to sidestep various issues that might arise from attempting to sample points directly from 3D meshes, such as sampling points which are contained within the model or dealing with 3D models that are stored in unusual ﬁle formats. Finally, we employed various heuristics to reduce the frequency of low-quality models in our dataset. First, we eliminated ﬂat objects by computing the SVD of each point cloud and only retaining those where the smallest singular value was above a certain threshold. Next, we clustered the dataset by CLIP features (for each object, we averaged features over all renders). We found that some clusters contained many low-quality categories of models, while other clusters appeared more diverse or interpretable. We binned these clusters into several buckets of varying quality, and used a weighted mixture of the resulting buckets as our ﬁnal dataset. 4.2. View Synthesis GLIDE Model Our point cloud models are conditioned on rendered views from our dataset, which were all produced using the same renderer and lighting settings. Therefore, to ensure that these models correctly handle generated synthetic views, we aim to explicitly generate 3D renders that match the distribution of our dataset. Figure 3. Our point cloud diffusion model architecture. Images are fed through a frozen, pre-trained CLIP model, and the output grid is fed as tokens into the transformer. Both the timestep t and noised input xt are also fed in as tokens. The output tokens corresponding to xt are used to predict ϵ and Σ. set, we only sample images from the 3D dataset 5% of the time, using the original dataset for the remaining 95%. We ﬁne-tune for 100K iterations, meaning that the model has made several epochs over the 3D dataset (but has never seen the same exact rendered viewpoint twice). To ensure that we always sample in-distribution renders (rather than only sampling them 5% of the time), we add a special token to every 3D render’s text prompt indicating that it is a 3D render; we then sample with this token at test time. 4.3. Point Cloud Diffusion To generate point clouds with diffusion, we extend the framework used by Zhou et al. (2021a) to include RGB colors for each point in a point cloud. In particular, we represent a point cloud as a tensor of shape K × 6, where K is the number of points, and the inner dimension contains (x, y, z) coordinates as well as (R, G, B) colors. All coordinates and colors are normalized to the range [−1, 1]. We then generate these tensors directly with diffusion, starting from random noise of shape K × 6, and gradually denoising it. Unlike prior work which leverages 3D-speciﬁc architectures to process point clouds, we use a simple Transformer-based model (Vaswani et al., 2017) to predict both ϵ and Σ conditioned on the image, timestep t, and noised point cloud xt. An overview of our architecture can be seen in Figure\n\n3. As input context to this model, we run each point in\nthe point cloud through a linear layer with output dimension D, obtaining a K × D input tensor. Additionally, we run the timestep t through a small MLP, obtaining another D-dimensional vector to prepend to the context. To condition on the image, we feed it through a pre-trained ViT-L/14 CLIP model, take the last layer embeddings from this CLIP model (of shape 256 × D′), and linearly project it into another tensor of shape 256 × D before prepending it to the Transformer context. In Section 5.1, we ﬁnd that this is superior to using a single CLIP image or text embedding, as done by Sanghi et al. (2021); Zeng et al. (2022); Sanghi et al. (2022). The final input context to our model is of shape (K +257)× D. To obtain a final output sequence of length K, we take the final K tokens of output and project it to obtain epsilon and Σ predictions for the K input points. Notably, we do not employ positional encodings for this model. As a result, the model itself is permutation-invariant to the input point clouds (although the output order is tied to the input order).4.4. Point Cloud Upsampler For image diffusion models, the best quality is typically achieved by using some form of hierarchy, where a lowresolution base model produces output which is then upsampled by another model (Nichol & Dhariwal, 2021; Saharia et al., 2021; Ho et al., 2021; Rombach et al., 2021). We employ this approach to point cloud generation by ﬁrst generating 1K points with a large base model, and then upsampling to 4K points using a smaller upsampling model. Notably, our models’ compute requirements scale with the number of points, so it is four times more expensive to generate 4K points than 1K points for a ﬁxed model size. Our upsampler uses the same architecture as our base model, with extra conditioning tokens for the low-resolution point cloud. To arrive at 4K points, the upsampler conditions on added to the low-resolution pointcloud. We pass the conditioning points through a separate linear embedding layer than the one used for xt, allowing the model to distinguish conditioning information from new points without requiring the use of positional embeddings. 4.5. Producing Meshes For rendering-based evaluations, we do not render generated point clouds directly. Rather, we convert the point clouds into textured meshes and render these meshes using Blender. Producing meshes from point clouds is a well-studied, sometimes difﬁcult problem. Point clouds produced by our models often have cracks, outliers, or other types of noise that make the problem particularly challenging. We brieﬂy tried using pre-trained SAP models (Peng et al., 2021) for this purpose, but found that the resulting meshes sometimes lost large portions or important details of the shape that were present in the point clouds. Rather than training new SAP models, we opted to take a simpler approach. To convert point clouds into meshes, we use a regressionbased model to predict the signed distance ﬁeld of an object given its point cloud, and then apply marching cubes (Lorensen & Cline, 1987) to the resulting SDF to extract a mesh. We then assign colors to each vertex of the mesh using the color of the nearest point from the original point cloud. For details, see Appendix C.\n\n5. Results\nIn the following sections, we conduct a number of ablations and comparisons to evaluate how our method performs and scales. We adopt the CLIP R-Precision (Park et al., 2021) metric for evaluating text-to-3D methods end-to-end, using the same object-centric evaluation prompts as Jain et al. (2021). Additionally, we introduce a new pair of metrics which we refer to as P-IS and P-FID, which are point cloud analogs for Inception Score (Salimans et al., 2016) and FID (Heusel et al., 2017), respectively. To construct our P-IS and P-FID metrics, we employ a modiﬁed PointNet++ model (Qi et al., 2017) to extract features and predict class probabilities for point clouds. For details, see Appendix B. 5.1. Model Scaling and Ablations In this section, we train a variety of base diffusion models to study the effect of scaling and to ablate the importance of image conditioning. We train the following base models and evaluate them throughout training: • 40M (uncond.): a small model without any conditioning information. • 40M (text vec.): a small model which only conditions on text captions, not rendered images. The text caption is embedded with CLIP, and the CLIP embedding is appended as a single extra token of context. This model depends on the text captions present in our 3D dataset, and does not leverage the ﬁne-tuned GLIDE model. • 40M (image vec.): a small model which conditions on CLIP image embeddings of rendered images, similar to Sanghi et al. (2021). This differs from the other image-conditional models in that the image is encoded into a single token of context, rather than as a sequence of latents corresponding to the CLIP latent grid. • 40M: a small model with full image conditioning through a grid of CLIP latents. • 300M: a medium model with full image conditioning through a grid of CLIP latents. 20 15 P-FID 10 5 0.25 0.50 0.75 1.00 1.25 training iterations 1e6 (a) P-FID 13 12 P-IS 11 10 9 0.25 0.50 0.75 1.00 1.25 training iterations 1e6 (b) P-IS 0.4 CLIP R-Precision 0.3 0.2 0.1 0.0 0.25 0.50 0.75 1.00 1.25 training iterations 1e6 (c) CLIP R-Precision Figure 4. Sample-based evaluations computed throughout training across different base model runs. The same upsampler and conditioning images are used for all runs. In order to isolate changes to the base model, we use the same (image conditional) upsampler model for all evaluations, and use the same 306 pre-generated synthetic views for the CLIP R-Precision evaluation prompts. Here we use the ViT-L/14 CLIP model to compute CLIP R-Precision, but we report results with an alternative CLIP model in Section 5.3. In Figure 4, we present the results of our ablations. We ﬁnd that using only text conditioning with no text-to-image step results in much worse CLIP R-Precision (see Appendix E for more details). Furthermore, we ﬁnd that using a single CLIP embedding to condition on images is worse than using a grid of embeddings, suggesting that the point cloud model beneﬁts from seeing more (spatial) information about the conditioning image. Finally, we ﬁnd that scaling our model improves the speed of P-FID convergence, and increases ﬁnal CLIP R-Precision. (a) Image to point cloud sample for the prompt “a very realistic 3D rendering of a corgi”. (b) Image to point cloud sample for the prompt “a trafﬁc cone”. Figure 5. Two common failure modes of our model. In the top example, the model incorrectly interprets the relative proportions of different parts of the depicted object, producing a tall dog instead of a short, long dog. In the bottom example, the model cannot see underneath the trafﬁc cone, and incorrectly infers a second mirrored cone. 5.2. Qualitative Results We ﬁnd that Point·E can often produce consistent and highquality 3D shapes for complex prompts. In Figure 2, we show various point cloud samples which demonstrate our model’s ability to infer a variety of shapes while correctly binding colors to the relevant parts of the shapes. Sometimes the point cloud diffusion model fails to understand or extrapolate the conditioning image, resulting in a shape that does not match the original prompt. We ﬁnd that this is usually due to one of two issues: 1) the model incorrectly interprets the shape of the object depicted in the image, or 2) the model incorrectly infers some part of the shape that is occluded in the image. In Figure 5, we present an example of each of these two failure modes. 5.3. Comparison to Other Methods As text-conditional 3D synthesis is a fairly new area of research, there is not yet a standard set of benchmarks for this task. However, several other works evaluate 3D generation using CLIP R-Precision, and we compare to these methods in Table 1. In addition to CLIP R-Precision, we also note the reported sampling compute requirements for each method. Table 1. Comparison of Point·E to other 3D generative techniques as measured by CLIP R-Precision (with two different CLIP base models) on COCO evaluation prompts. ∗50 P100-minutes converted to V100-minutes using conversion rate 1\n\n3. †Assuming 2\nV100 minutes = 1 A100 minute and 1 TPUv4-minute = 1 A100minute. We report DreamFields results from Poole et al. (2022).\n\nMethod\nViT-B/32 ViT-L/14 Latency DreamFields 78.6% 82.9% ∼200 V100-hr† CLIP-Mesh 67.8% 74.5% ∼17 V100-min∗ DreamFusion 75.1% 79.7% ∼12 V100-hr† Point·E (40M, text-only) 15.4% 16.2% Point·E (40M) 36.5% 38.8% 1.0 V100-min Point·E (300M) 40.3% 45.6% 1.2 V100-min Point·E (1B) 41.1% 46.8% 1.5 V100-min Conditioning images 69.6% 86.6% - • Unlike multi-view optimization-based methods like DreamFusion, Point·E does not explicitly optimize every view to match the text prompt. This could result in lower CLIP R-Precision simply because certain objects are not easy to identify from all angles. • Our method produces point clouds which must be preprocessed before rendering. Converting point clouds into meshes is a difﬁcult problem, and the approach we use can sometimes lose information present in the point clouds themselves. While our method performs worse on this evaluation than state-of-the-art techniques, it produces samples in a small fraction of the time. This could make it more practical for certain applications, or could allow for the discovery of higher-quality 3D objects by sampling many objects and selecting the best one according to some heuristic.\n\n3. Related Work\nSeveral prior works have explored generative models over point clouds. Achlioptas et al. (2017) train point cloud autoencoders, and ﬁt generative priors (either GANs (Goodfellow et al., 2014) or GMMs) on the resulting latent representations. Mo et al. (2019) generate point clouds using a VAE (Kingma & Welling, 2013) on hierarchical graph representations of 3D objects. Yang et al. (2019) train a two-stage ﬂow model for point cloud generation: ﬁrst, a prior ﬂow model produces a latent vector, and then a second ﬂow model samples points conditioned on the latent vector. Along the same lines, Luo & Hu (2021); Cai et al. (2020) both train two-stage models where the second stage is a diffusion model over individual points in a point cloud, and the ﬁrst stage is a latent ﬂow model or a latent GAN, respectively. Zeng et al. (2022) train a two-stage hierarchical VAE on point clouds with diffusion priors at both stages. Most similar to our work, Zhou et al. (2021a) introduce PVD, a single diffusion model that generates point clouds directly. Compared to previous point cloud diffusion methods such as PVD, our Transformer-based model architecture is simpler and incorporates less 3D-speciﬁc structure. Unlike prior works, our models also produce RGB channels alongside point cloud coordinates. A growing body of work explores the problem of 3D model generation in representations other than point clouds. Several works aim to train 3D-aware GANs from datasets of et al., 2021; Or-El et al., 2021; Gu et al., 2021; Zhou et al., 2021b). These GANs are typically applied to the problem of novel view synthesis in forward-facing scenes, and do not attempt to reconstruct full 360-degree views of objects. More recently, Gao et al. (2022) train a GAN that directly produces full 3D meshes, paired with a discriminator that inputs differentiably-rendered (Laine et al., 2020) views of the generated meshes. Bautista et al. (2022) generates complete 3D scenes by ﬁrst learning a representation space that decodes into NeRFs (Mildenhall et al., 2020), and then training a diffusion prior on this representation space. However, none of these works have demonstrated the ability to generate arbitrary 3D models conditioned on open-ended, complex text-prompts. Several recent works have explored the problem of textconditional 3D generation by optimizing 3D representations according to a text-image matching objective. Jain et al. (2021) introduce DreamFields, a method which optimizes the parameters of a NeRF using an objective based on CLIP (Radford et al., 2021). Notably, this method requires no (2022) optimizes a mesh using a CLIP-guided objective, ﬁnding that the mesh representation is more efﬁcient to optimize than a NeRF. More recently, Poole et al. (2022) extend DreamFields to leverage a pre-trained text-to-image diffusion model instead of CLIP, producing more coherent and complex objects. Lin et al. (2022a) build off of this technique, but convert the NeRF representation into a mesh and then reﬁne the mesh representation in a second optimization stage. While these approaches are able to produce diverse and complex objects or scenes, the optimization procedures typically require multiple GPU hours to converge, making them difﬁcult to apply in practical settings. While the above approaches are all based on optimization against a text-image model and do not leverage 3D data, other methods for text-conditional 3D synthesis make use of 3D data, possibly paired with text labels. Chen et al. (2018) employ a dataset of text-3D pairs to train a GAN to generate 3D representations conditioned on text. Liu et al. (2022) also leverage paired text-3D data to generate models in a joint representation space. Sanghi et al. (2021) employ a ﬂow-based model to generate 3D latent representations, and ﬁnd some text-to-3D capabilities when conditioning their model on CLIP embeddings. More recently, Zeng et al. (2022) achieve similar results when conditioning on CLIP embeddings, but employ a hierarchical VAE on point clouds for their generative stack. Mittal et al. (2022) and Fu et al. (2022) employ a VQ-VAE (van den Oord et al., 2017) with an autoregressive prior to sample 3D shapes conditioned on text labels. More recently, Sanghi et al. (2022) also employ a VQ-VAE approach, but leverage CLIP embeddings to avoid the need for explicit text labels in the dataset. While many of these works demonstrate promising early results, they tend to be limited to simple prompts or a narrow set of object categories due to the limited availability of 3D training data. Our method sidesteps this issue by leveraging a pre-trained text-to-image model to condition our 3D generation procedure. A large body of research focuses on reconstructing 3D models from single or few images. Notably, this is an underspeciﬁed problem, since the model must impute some details not present in the conditioning image(s). Nevertheless, some regression-based methods have shown promising results on this task (Choy et al., 2016; Wang et al., 2018; Gkioxari et al., 2019; Groueix et al., 2018; Yu et al., 2020; Lin et al., 2022b). A separate body of literature studies generative approaches for single- or multi-view reconstruction. Fan et al. (2016) predict point clouds of objects from single views using a VAE. Sun et al. (2018) use a hybrid of a ﬂow predictor and a GAN to generate novel views from few images. Kosiorek et al. (2021) use a view-conditional VAE to generate latent vectors for a NeRF decoder. Watson et al. (2022) employ an image-to-image diffusion model to synthesize novel views of an object conditioned on a single view, allowing many consistent views to be synthesized autoregressively.\n\n7. Conclusion\nWe have presented Point·E, a system for text-conditional synthesis of 3D point clouds that ﬁrst generates synthetic views and then generates colored point clouds conditioned on these views. We ﬁnd that Point·E is capable of efﬁciently producing diverse and complex 3D shapes conditioned on text prompts. We hope that our approach can serve as a starting point for further work in the ﬁeld of text-to-3D synthesis.\n\n8. Acknowledgements\nWe would like to thank everyone behind ChatGPT for creating a tool that helped provide useful writing feedback.\n\nReferences\nAchlioptas, P., Diamanti, O., Mitliagkas, I., and Guibas, L. Learning representations and generative models for 3d point clouds. arXiv:1707.02392, 2017. Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis, K., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Karras, T., and Liu, M.-Y. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers, 2022. Dehghan, A., and Susskind, J. Gaudi: A neural architect for immersive 3d scene generation. arXiv:2207.13751, 2022. Cai, R., Yang, G., Averbuch-Elor, H., Hao, Z., Belongie, S., Snavely, N., and Hariharan, B. Learning gradient ﬁelds for shape generation. arXiv:2008.06520, 2020. Chan, E. R., Monteiro, M., Kellnhofer, P., Wu, J., and Wetzstein, G. pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. arXiv:2012.00926, 2020. Chan, E. R., Lin, C. Z., Chan, M. A., Nagano, K., Pan, B., Mello, S. D., Gallo, O., Guibas, L., Tremblay, J., Khamis, S., Karras, T., and Wetzstein, G. Efﬁcient geometry-aware 3d generative adversarial networks. arXiv:2112.07945, 2021. Chen, K., Choy, C. B., Savva, M., Chang, A. X., Funkhouser, T., and Savarese, S. Text2shape: Generating shapes from natural language by learning joint embeddings. arXiv:1803.08495, 2018. Choy, C. B., Xu, D., Gwak, J., Chen, K., and Savarese, S. 3d-r2n2: A uniﬁed approach for single and multi-view 3d object reconstruction. arXiv:1604.00449, 2016. Community, B. O. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018. URL http://www. blender.org. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. arXiv:2105.05233, 2021. Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao, Z., Yang, H., and Tang, J. Cogview: Mastering text-to-image generation via transformers. arXiv:2105.13290, 2021. Fan, H., Su, H., and Guibas, L. A point set generation network for 3d object reconstruction from a single image. arXiv:1612.00603, 2016. Feng, Z., Zhang, Z., Yu, X., Fang, Y., Li, L., Chen, X., Lu, Y., Liu, J., Yin, W., Feng, S., Sun, Y., Tian, H., Wu, H., and Wang, H. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-ofdenoising-experts. arXiv:2210.15257, 2022. Fu, R., Zhan, X., Chen, Y., Ritchie, D., and Sridhar, S. Shapecrafter: A recursive text-conditioned 3d shape generation model. arXiv:2207.09446, 2022. Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., and Taigman, Y. Make-a-scene: Scene-based text-toimage generation with human priors. arXiv:2203.13131, 2022. Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z., and Fidler, S. Get3d: A generative model of high quality 3d textured shapes learned from images. arXiv:2209.11163, 2022. Gkioxari, G., Malik, J., and Johnson, J. Mesh r-cnn. arXiv:1906.02739, 2019. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio, Y. Generative adversarial networks. arXiv:1406.2661, 2014. Groueix, T., Fisher, M., Kim, V. G., Russell, B. C., and Aubry, M. Atlasnet: A papier-mˆach´e approach to learning 3d surface generation. arXiv:1802.05384, 2018. Gu, J., Liu, L., Wang, P., and Theobalt, C. Stylenerf: A style-based 3d-aware generator for high-resolution image synthesis. arXiv:2110.08985, 2021. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in Neural Information Processing Systems 30 (NIPS 2017), 2017. Ho, J. and Salimans, T. Classiﬁer-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. URL https:// openreview.net/forum?id=qw8AKxfYbI. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. arXiv:2006.11239, 2020. Ho, J., Saharia, C., Chan, W., Fleet, D. J., Norouzi, M., and Salimans, T. Cascaded diffusion models for high ﬁdelity image generation. arXiv:2106.15282, 2021. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., and Salimans, T. Imagen video: High deﬁnition video generation with diffusion models. arXiv:2210.02303, 2022a. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., and Fleet,\nD. J.\nVideo diffusion models. arXiv:2204.03458, 2022b. Hong, W., Ding, M., Zheng, W., Liu, X., and Tang, J. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv:2205.15868, 2022. Jain, A., Mildenhall, B., Barron, J. T., Abbeel, P., and Poole,\nB. Zero-shot text-guided object generation with dream\nﬁelds. arXiv:2112.01455, 2021. Khalid, N. M., Xie, T., Belilovsky, E., and Popa, T. Clipmesh: Generating textured meshes from text using pretrained image-text models. arXiv:2203.13333, 2022. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. arXiv:1312.6114, 2013. Kosiorek, A. R., Strathmann, H., Zoran, D., Moreno, P., Schneider, R., Mokr´a, S., and Rezende, D. J. NeRFVAE: A geometry aware 3D scene generative model. arXiv:2104.00587, April 2021. Laine, S., Hellsten, J., Karras, T., Seol, Y., Lehtinen, J., and Aila, T. Modular primitives for high-performance differentiable rendering. arXiv:2011.03277, 2020. Lin, C.-H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler, S., Liu, M.-Y., and Lin, T.-Y. Magic3d: High-resolution text-to-3d content creation. arXiv:2211.10440, 2022a. Lin, K.-E., Yen-Chen, L., Lai, W.-S., Lin, T.-Y., Shih, Y.-C., and Ramamoorthi, R. Vision transformer for nerf-based view synthesis from a single input image. arXiv:2207.05736, 2022b. Liu, Z., Wang, Y., Qi, X., and Fu, C.-W. Towards implicit text-guided 3d shape generation. arXiv:2203.14622, 2022. Lorensen, W. E. and Cline, H. E. Marching cubes: A high resolution 3d surface construction algorithm. In Stone, M. C. (ed.), SIGGRAPH, pp. 163–169. ACM, 1987. ISBN 0-89791-227-6. URL http: //dblp.uni-trier.de/db/conf/siggraph/ siggraph1987.html#LorensenC87. Luo, S. and Hu, W. Diffusion probabilistic models for 3d point cloud generation. arXiv:2103.01458, 2021. Mildenhall, B., Srinivasan, P. P., Tancik, M., Barron,\nJ. T., Ramamoorthi, R., and Ng, R. Nerf: Representing scenes as neural radiance ﬁelds for view synthesis.\narXiv:2003.08934, 2020. Mishkin, P., Ahmad, L., Brundage, M., Krueger, G., and Sastry, G. Dall·e 2 preview - risks and limitations. 2022. URL https://github.com/ openai/dalle-2-preview/blob/main/ system-card.md. Mittal, P., Cheng, Y.-C., Singh, M., and Tulsiani, S. Autosdf: Shape priors for 3d completion, reconstruction and generation. arXiv:2203.09516, 2022. Mo, K., Guerrero, P., Yi, L., Su, H., Wonka, P., Mitra, N., and Guibas, L. J. Structurenet: Hierarchical graph networks for 3d shape generation. arXiv:1908.00575, 2019. Neely, E. L. The risks of revolution: Ethical dilemmas in 3d printing from a us perspective. Science and Engineering Ethics, 22(5):1285–1297, Oct 2016. ISSN 14715546. doi: 10.1007/s11948-015-9707-4. URL https: Nichol, A. and Dhariwal, P. Improved denoising diffusion probabilistic models. arXiv:2102.09672, 2021. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B., Sutskever, I., and Chen, M. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv:2112.10741, 2021. Or-El, R., Luo, X., Shan, M., Shechtman, E., Park,\nJ. J., and Kemelmacher-Shlizerman, I. Stylesdf: Highresolution 3d-consistent image and geometry generation.\narXiv:2112.11427, 2021. Park, D. H., Azadi, S., Liu, X., Darrell, T., and Rohrbach, A. Benchmark for compositional text-to-image synthesis. In Thirty-ﬁfth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1),\n2021. URL https://openreview.net/forum?\nid=bKBhQhPeKaF. Peng, S., Jiang, C. M., Liao, Y., Niemeyer, M., Pollefeys, M., and Geiger, A. Shape as points: A differentiable poisson solver. arXiv:2106.03452, 2021. Poole, B., Jain, A., Barron, J. T., and Mildenhall, B. Dreamfusion: Text-to-3d using 2d diffusion. arXiv:2209.14988, 2022. Qi, C. R., Yi, L., Su, H., and Guibas, L. J. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. arXiv:1706.02413, 2017. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever, I. Learning transferable visual models from natural language supervision. arXiv:2103.00020, 2021. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-toimage generation. arXiv:2102.12092, 2021. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents. arXiv:2204.06125, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. arXiv:2112.10752, 2021. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi,\nS. S., Lopes, R. G., Salimans, T., Ho, J., Fleet, D. J., and\nNorouzi, M. Photorealistic text-to-image diffusion models with deep language understanding. arXiv:2205.11487, 2022. Salimans, T., Goodfellow, I., Zaremba, W., Cheung, V., Radford, A., and Chen, X. Improved techniques for training gans. arXiv:1606.03498, 2016. Sanghi, A., Chu, H., Lambourne, J. G., Wang, Y., Cheng, C.-Y., Fumero, M., and Malekshan, K. R. Clip-forge: Towards zero-shot text-to-shape generation. arXiv:2110.02624, 2021. Sanghi, A., Fu, R., Liu, V., Willis, K., Shayani, H., Khasahmadi, A. H., Sridhar, S., and Ritchie, D. Textcraft: Zeroshot generation of high-ﬁdelity and diverse shapes from text. arXiv:2211.01427, 2022. Schwarz, K., Liao, Y., Niemeyer, M., and Geiger, A. Graf: Generative radiance ﬁelds for 3d-aware image synthesis. 2020. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H., Ashual, O., Gafni, O., Parikh, D., Gupta, S., and Taigman, Y. Make-a-video: Text-to-video generation without text-video data. arXiv:2209.14792, 2022. Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised learning using nonequilibrium thermodynamics. arXiv:1503.03585, 2015. Song, Y. and Ermon, S. Improved techniques for training score-based generative models. arXiv:2006.09011, 2020a. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. arXiv:arXiv:1907.05600, 2020b. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. arXiv:2011.13456, 2020. Straub, J. and Kerlin, S. Evaluation of the use of 3D printing and imaging to create working replica keys. In Javidi, B. and Son, J.-Y. (eds.), Three-Dimensional Imaging, Visualization, and Display 2016, volume 9867, pp. 98670E. International Society for Optics and Photonics, SPIE, 2016. doi: 10.1117/12.2223858. URL self-learned conﬁdence. In Ferrari, V., Hebert, M., Sminchisescu, C., and Weiss, Y. (eds.), Computer Vision – ECCV 2018, pp. 162–178, Cham, 2018. Springer International Publishing. ISBN 978-3-030-01219-9. van den Oord, A., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. arXiv:1711.00937, 2017. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. arXiv:1706.03762, 2017. Walther, G. Printing insecurity? the security implications of 3d-printing of weapons. Science and engineering ethics, 21, 12 2014. doi: 10.1007/s11948-014-9617-x. Wang, N., Zhang, Y., Li, Z., Fu, Y., Liu, W., and Jiang, Y.-G. Pixel2mesh: Generating 3d mesh models from single rgb images. arXiv:1804.01654, 2018. Watson, D., Chan, W., Martin-Brualla, R., Ho, J., Tagliasacchi, A., and Norouzi, M. Novel view synthesis with diffusion models. arXiv:2210.04628, 2022. Wu, Z., Song, S., Khosla, A., Yu, F., Zhang, L., Tang, X., and Xiao, J. 3d shapenets: A deep representation for volumetric shapes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. Yang, G., Huang, X., Hao, Z., Liu, M.-Y., Belongie, S., and Hariharan, B. Pointﬂow: 3d point cloud generation with continuous normalizing ﬂows. arXiv:1906.12320, 2019. Yu, A., Ye, V., Tancik, M., and Kanazawa, A. pixelnerf: Neural radiance ﬁelds from one or few images. arXiv:2012.02190, 2020. Yu, J., Xu, Y., Koh, J. Y., Luong, T., Baid, G., Wang, Z., Vasudevan, V., Ku, A., Yang, Y., Ayan, B. K., Hutchinson, B., Han, W., Parekh, Z., Li, X., Zhang, H., Baldridge, J., and Wu, Y. Scaling autoregressive models for contentrich text-to-image generation. arXiv:2206.10789, 2022. Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O., Fidler, S., and Kreis, K. Lion: Latent point diffusion models for 3d shape generation. arXiv:2210.06978, 2022. Zhou, L., Du, Y., and Wu, J. 3d shape generation and completion through point-voxel diffusion. arXiv:2104.03670, 2021a. Zhou, P., Xie, L., Ni, B., and Tian, Q. Cips-3d: A 3d-aware generator of gans based on conditionally-independent pixel synthesis. arXiv:2110.09788, 2021b. Table 2. Training hyper-parameters for our point cloud diffusion models. Width and depth refer to the size of the transformer backbone. Model Width Depth LR # Params Base (40M) 512 12 1e-4 40,466,956 Base (300M) 1024 24 7e-5 311,778,316 Base (1B) 2048 24 5e-5 1,244,311,564 Upsampler 512 12 1e-4 40,470,540 Table 3. Sampling hyperparameters for ﬁgures and CLIP RPrecision evaluations. Hyperparameter Base Upsampler Timesteps 64 64 Guidance scale 3.0 3.0 Schurn 3 0 σmin 1e-3 1e-3 σmax 120 160\n\nA. Hyperparameters\nWe train all of our diffusion models with batch size 64 for 1,300,000 iterations. In Table 2, we enumerate the training hyperparameters that were varied across model sizes. We train all of our models with 1024 diffusion timesteps. For our upsampler model, we use the linear noise schedule from Ho et al. (2020), and for our base models, we use the cosine noise schedule proposed by Nichol & Dhariwal (2021). For P-FID and P-IS evaluations, we produce 10K samples using stochastic DDPM with the full noise schedule. For CLIP R-Precision and ﬁgures in the paper, we use 64 steps (128 function evaluations) of the Heun sampler from Karras et al. (2022) for both the base and upsampler models. Table 3 enumerates the hyperparameters used for Heun sampling. When sampling from GLIDE, we use 150 diffusion steps for the base model, and 50 diffusion steps for the upsampling model. We report sampling time for each component of our stack in Table 4.\nB. P-FID and P-IS Metrics\nTo evaluate P-FID and P-IS, we train a PointNet++ model on ModelNet40 (Wu et al., 2015) using an open source implementation.1 We modify the baseline model in several ways. First, we double the width of the model, resulting in roughly 16 million parameters. Next, we apply some additional data augmentations to make the model more robust to out-of-distribution samples. In particular, we apply random Table 4. Sampling performance for various components of our model. We use the Karras sampler for our base and upsampler models, but not for GLIDE. Model V100 seconds GLIDE 46.28 Upsampler (40M) 12.58 Base (40M) 3.35 Base (300M) 12.78 Base (1B) 28.67 rotations to each point cloud, and we add Gaussian noise to the points with standard deviation sampled from U[0, 0.01]. To compute P-FID, we extract features for each point cloud from the layer before the ﬁnal ReLU activation. To compute P-IS, we use the predicted class probabilities for the 40 classes from ModelNet40. We note that our generative models are trained on a dataset which only has P-IS 12.95, so our best reported P-IS score of ∼13 is near the expected upper bound.\nC. Mesh Extraction\nTo convert point clouds into meshes, we train a model which predicts SDFs from point clouds and apply marching cubes to the resulting SDFs. We parametrize our SDF model as an encoder-decoder Transformer. First, an 8-layer encoder processes the input point cloud as an unordered sequence, producing a sequence of hidden representations. Then, a 4-layer cross-attention decoder takes 3D coordinates and the sequence of latent vectors, and predicts an SDF value. Each input query point is processed independently, allowing for efﬁcient batching. Using more layers in the encoder and fewer in the decoder allows us to amortize the encoding cost across many query points. We train our SDF regression model on a subset of 2.4 million manifold meshes from our dataset, and add Gaussian noise with σ = 0.005 to the point clouds as data augmentation. We train the model fθ(x) to predict the SDF y with a weighted L1 objective: ( 1 · ||fθ(x) −y||1 fθ(x) > y 4 · ||fθ(x) −y||1 fθ(x) ≤y Here, we deﬁne the SDF such that points outside of the surface have negative sign. Therefore, in the face of uncertainty, the model is encouraged predict that points are inside the surface. We found this to be helpful in initial experiments, likely because it helps prevent the resulting meshes from effectively ignoring thin or noisy parts of the point cloud. Figure 7. Examples of point clouds (left) and corresponding extracted meshes (right). We ﬁnd that our method often produces smooth meshes and removes outliers (middle row), but can sometimes miss thin/sparse parts of objects (bottom row). of 128 × 128 × 128, resulting in 1283 queries to the SDF model. In Figure 7, we show examples of input point clouds and corresponding output meshes from our model. We observe that our method works well in many cases, but sometimes fails to capture thin or sparse parts of a point cloud.\nD. Conditioning on DALL·E 2 Samples\nIn our main experiments, we use a specialized text-to-image model to produce in-distribution conditioning images for our point cloud models. In this section, we explore what happens if we use renders from a pre-existing text-to-image model, DALL·E 2. In Figure 8, we present three image-to-3D examples where the conditioning images are generated by DALL·E 2. We ﬁnd that DALL·E 2 tends to include shadows under objects, and our point cloud model interprets these as a dark ground plane. We also ﬁnd that our point cloud model can misinterpret shapes from the generated images when the objects take up too much of the image. In these cases, adding a border around the generated images can improve the reconstructed shapes. Figure 8. Examples of point clouds reconstructed from DALL·2 generations. The top image was produced using the prompt “a 3d rendering of an avocado chair, chair imitating an avocado, full view, white background”. The middle image was produced using the prompt “a simple full view of a 3d rendering of a corgi in front of a white background”. The bottom image is the same as the middle image, but with an additional white border.\nE. Pure Text-Conditional Generation\nIn Section 5.1, we train a pure text-conditional point cloud model without an additional image generation step. While we ﬁnd that this model performs worse on evaluations than our full system, it still achieves non-trivial results. In this section, we explore the capabilities and limitations of this model. In Figure 9, we show examples where our text-conditional model is able to produce point clouds matching the provided text prompt. Notably, these examples include simple prompts that describe single objects. In Figure 10, we show examples where this model struggles with prompts that combine multiple concepts. Finally, we expect that this model has inherited biases from our 3D dataset. We present one possible example of this in Figure 11, wherein the model produces longer and narrower objects for the prompt “a woman” than for the prompt “a man” when using a ﬁxed diffusion noise seed. “a motorbike” “a dog” “a desk lamp” “a guitar” “an ambulance” “a laptop computer” Figure 9. Selected point clouds generated by our pure textconditional 40M parameter point cloud diffusion model. (a) Prompt: “a small red cube is sitting on top of a large blue cube. red on top, blue on bottom” (b) Prompt: “a corgi wearing a red santa hat” Figure 10. Sample grids where our small, pure text-conditional model fails to understand complex prompts. (a) Prompt: “a man” (b) Prompt: “a woman” Figure 11. Sample grids from our pure text-conditional 40M parameter model. Samples in the top grid use the same noise seed as the corresponding samples in the bottom grid.\n--- FOOTNOTES 3D content creation for a wide range of applications such as *Equal contribution 1OpenAI, San Francisco, USA. Correspondence to: Alex Nichol <alex@openai.com>, Heewoo Jun 3D data (Sanghi et al., 2021; 2022; Watson et al., 2022). 2D images (Chan et al., 2020; Schwarz et al., 2020; Chan 3D training data. Building on this principle, Khalid et al. 1K points and generates an additional 3K points which are 40M (uncond.) 40M (text vec.) 40M (image vec.) 40M 300M 1B 40M (uncond.) 40M (text vec.) 40M (image vec.) 40M 300M 1B 1B 300M 40M 40M (image vec.) 40M (text vec.) 40M (uncond.) 16 V100-sec", "persona": "Student", "response": "### 1. Overview\nAt the heart of 3D imaging applications, point clouds are used to efficiently represent three-dimensional spatial data. That’s why recent years have seen a flood of algorithms for processing, understanding, and making predictions using point clouds. These point clouds can be generated via either laser scanning techniques, such as lidar, or photogrammetry, or even via generative techniques, such as OpenAI’s recently released Point-E Read on and learn how to harness FiftyOne to inspect, explore, and interact with your 3D data! As an example, here is some sample code to generate a point cloud only dataset with 50 samples, and 10 ground truth bounding boxes per sample. To generate predictions, we randomly perturb some of the ground truth bounding boxes, and omit others from our predicted detections. The code can be found in this gist We use the sampler from the Point-E text2pointcloud example notebook, and convert the resulting point clouds using Open3d\n\n### 2. Background\n3D perception in computer vision enables computers and machines to understand the depth and structure of the 3D world around us, just as we do. Work in this field is exciting, with limitless potential for applications to revolutionize the way we live and work across all industries, from automotive to virtual reality. Support for evaluating 3D object detection predictions If you’d like, you can also pass in a normal vector to specify the plane with respect to which the routine should perform the projection, for instance, try projection_normal=(0.5, 0.5, 0.) ! Join the thousands of engineers and data scientists already using FiftyOne to solve some of the most challenging problems in computer vision today! Create point cloud-only datasets Then we generate an example dataset in FiftyOne, assigning each point cloud a random filename:\n\n### 3. Methods\nMultiple point cloud slices in grouped datasets In FiftyOne, grouped datasets allow you to combine samples - potentially with varied media types (image, video, and point cloud) - in groups, with samples occupying different slices. New in this release, FiftyOne has revamped grouped datasets so that groups can have multiple point cloud samples. All that is left to do is compute the orthographic projections. Here we will use a non-default projection_normal so that our preview image is not a bird’s eye view: How to visualize point clouds, create orthographic projections, and evaluate detections with the latest release of FiftyOne The latest release of the FiftyOne computer vision toolset, 0.20, includes enhanced point cloud support to deliver unprecedented access to and control over your 3D data.\n\n### 4. Results\nSupport for point cloud-only datasets in the FiftyOne App Group point cloud slices Multiple point clouds for the same scene, coming from different sensors Examining the effect of subsampling point clouds with millions of points Transforming point clouds by rotation or scaling operations pcd_cluster group slice. See Let’s see this in action, clustering our point clouds with DBSCAN . We’ll create a new group slice, and add a new sample to each group in thegroup slice. See this gist for the corresponding code. Then we compute the orthographic projection images for these new point clouds - but this time, we pass in shading_mode=rgb , because we’ve used the point cloud’s RGB channels to encode cluster numbers. We’ll also use slightly different bounds, so it is easier to see the clusters. Evaluate 3D object detections evaluate_detections() already supported the 2D object detection bounding boxes in image and video datasets. Today’s release extends these capabilities to 3D bounding boxes, with arbitrary rotation angles. If you’re familiar with FiftyOne’s Evaluation API , you’ll know that thealready supported the 2D object detection bounding boxes in image and video datasets. Today’s release extends these capabilities to 3D bounding boxes, with arbitrary rotation angles. FiftyOne automatically recognizes when the bounding box is three dimensional, and applies the appropriate method to compute 3D intersection over union (IoU) scores, which are used to determine whether a prediction agrees with a ground truth object.\n\n### 5. Takeaways\nOrthographic projections, including bird’s eye view (BEV) Enhanced rendering and customization These features add to FiftyOne’s existing 3D capabilities for working with and visualizing point clouds. Preview 3D data with orthographic projections compute_orthographic_projection_images() method. Our new 3D utils integrate this functionality into the FiftyOne library and the FiftyOne App via themethod. By default, this method generates bird’s eye view projections of your point clouds, which then show up in the FiftyOne App as previews of point cloud samples (with filterable projections of polylines and bounding boxes as well). Also note that we’ve passed in bounds , telling FiftyOne where to crop the generated images, as well as a shading_mode , specifying that the point cloud’s intensity should be used to color the projection (as opposed to the height values, or colors of the individual points). Inside of the 3D visualizer , you can also control a variety of characteristics of the look and feel, including setting point size and turning grid lines on or off. Coloring points by cluster index or semantic segmentation label One situation in which this might be useful, for instance, is if you’re generating point clouds from scratch. Let’s see this with an example, using OpenAI’s Point-E to turn text prompts into three dimensional point cloud models. (3D point cloud) synthesis If you aren’t working with your 3D point clouds in FiftyOne, you’re missing out. Visualize your point clouds in the same place that you visualize your images, videos, geo data, and more.", "paper_id": "20", "story_id": "20-03"}
{"paper": "Abstract\nWe propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-speciﬁc control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.\n\n1. Introduction\nThe resolution and quality of images produced by generative methods — especially generative adversarial networks (GAN) [22] — have seen rapid improvement recently [30, 45, 5]. Yet the generators continue to operate as black boxes, and despite recent efforts [3], the understanding of various aspects of the image synthesis process, e.g., the origin of stochastic features, is still lacking. The properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations [13, 52, 37] provide no quantitative way to compare different generators against each other. Motivated by style transfer literature [27], we re-design the generator architecture in a way that exposes novel ways to control the image synthesis process. Our generator starts from a learned constant input and adjusts the “style” of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales. Combined with noise injected directly into the network, this architectural change leads to automatic, unsupervised separation of high-level attributes Timo Aila NVIDIA taila@nvidia.com (e.g., pose, identity) from stochastic variation (e.g., freckles, hair) in the generated images, and enables intuitive scale-speciﬁc mixing and interpolation operations. We do not modify the discriminator or the loss function in any way, and our work is thus orthogonal to the ongoing discussion about GAN loss functions, regularization, and hyperparameters [24, 45, 5, 40, 44, 36]. Our generator embeds the input latent code into an intermediate latent space, which has a profound effect on how the factors of variation are represented in the network. The input latent space must follow the probability density of the training data, and we argue that this leads to some degree of unavoidable entanglement. Our intermediate latent space is free from that restriction and is therefore allowed to be disentangled. As previous methods for estimating the degree of latent space disentanglement are not directly applicable in our case, we propose two new automated metrics — perceptual path length and linear separability — for quantifying these aspects of the generator. Using these metrics, we show that compared to a traditional generator architecture, our generator admits a more linear, less entangled representation of different factors of variation. Finally, we present a new dataset of human faces (Flickr-Faces-HQ, FFHQ) that offers much higher quality and covers considerably wider variation than existing high-resolution datasets (Appendix A). We have made this dataset publicly available, along with our source code and pre-trained networks.1 The accompanying video can be found under the same link.\n\n2. Style-based generator\nTraditionally the latent code is provided to the generator through an input layer, i.e., the ﬁrst layer of a feedforward network (Figure 1a). We depart from this design by omitting the input layer altogether and starting from a learned constant instead (Figure 1b, right). Given a latent code z in the input latent space Z, a non-linear mapping network f : Z →W ﬁrst produces w ∈W (Figure 1b, left). For simplicity, we set the dimensionality of both Noise Latent Latent Synthesis network Normalize Normalize B Const 4×4×512 Mapping network style Fully-connected A AdaIN PixelNorm Conv 3×3 FC Conv 3×3 B FC style A AdaIN FC PixelNorm 4×4 FC FC 4×4 Upsample Upsample FC FC Conv 3×3 Conv 3×3 B style FC PixelNorm A AdaIN Conv 3×3 Conv 3×3 A B PixelNorm style AdaIN 8×8 8×8 (a) Traditional (b) Style-based generator Figure 1. While a traditional generator [30] feeds the latent code though the input layer only, we ﬁrst map the input to an intermediate latent space W, which then controls the generator through adaptive instance normalization (AdaIN) at each convolution layer. Gaussian noise is added after each convolution, before evaluating the nonlinearity. Here “A” stands for a learned afﬁne transform, and “B” applies learned per-channel scaling factors to the noise input. The mapping network f consists of 8 layers and the synthesis network g consists of 18 layers — two for each resolution (42 −10242). The output of the last layer is converted to RGB using a separate 1 × 1 convolution, similar to Karras et al. [30]. Our generator has a total of 26.2M trainable parameters, compared to 23.1M in the traditional generator. spaces to 512, and the mapping f is implemented using an 8-layer MLP, a decision we will analyze in Section 4.1. Learned afﬁne transformations then specialize w to styles y = (ys, yb) that control adaptive instance normalization (AdaIN) [27, 17, 21, 16] operations after each convolution layer of the synthesis network g. The AdaIN operation is deﬁned as AdaIN(xi, y) = ys,i xi −µ(xi) σ(xi) + yb,i, (1) where each feature map xi is normalized separately, and then scaled and biased using the corresponding scalar components from style y. Thus the dimensionality of y is twice the number of feature maps on that layer. Comparing our approach to style transfer, we compute the spatially invariant style y from vector w instead of an example image. We choose to reuse the word “style” for y because similar network architectures are already used for feedforward style transfer [27], unsupervised image-toimage translation [28], and domain mixtures [23]. Compared to more general feature transforms [38, 57], AdaIN is particularly well suited for our purposes due to its efﬁciency and compact representation.\n\nMethod\nCelebA-HQ FFHQ A Baseline Progressive GAN [30] 7.79 8.04 B + Tuning (incl. bilinear up/down) 6.11 5.25 C + Add mapping and styles 5.34 4.85 D + Remove traditional input 5.07 4.88 E + Add noise inputs 5.06 4.42 F + Mixing regularization 5.17 4.40 Table 1. Fr´echet inception distance (FID) for various generator designs (lower is better). In this paper we calculate the FIDs using 50,000 images drawn randomly from the training set, and report the lowest distance encountered over the course of training. Finally, we provide our generator with a direct means to generate stochastic detail by introducing explicit noise inputs. These are single-channel images consisting of uncorrelated Gaussian noise, and we feed a dedicated noise image to each layer of the synthesis network. The noise image is broadcasted to all feature maps using learned perfeature scaling factors and then added to the output of the corresponding convolution, as illustrated in Figure 1b. The implications of adding the noise inputs are discussed in Sections 3.2 and 3.3. 2.1. Quality of generated images Before studying the properties of our generator, we demonstrate experimentally that the redesign does not compromise image quality but, in fact, improves it considerably. Table 1 gives Fr´echet inception distances (FID) [25] for various generator architectures in CELEBA-HQ [30] and our new FFHQ dataset (Appendix A). Results for other datasets are given in Appendix E. Our baseline conﬁguration (A) is the Progressive GAN setup of Karras et al. [30], from which we inherit the networks and all hyperparameters except where stated otherwise. We ﬁrst switch to an improved baseline (B) by using bilinear up/downsampling operations [64], longer training, and tuned hyperparameters. A detailed description of training setups and hyperparameters is included in Appendix C. We then improve this new baseline further by adding the mapping network and AdaIN operations (C), and make a surprising observation that the network no longer beneﬁts from feeding the latent code into the ﬁrst convolution layer. We therefore simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 × 4 × 512 constant tensor (D). We ﬁnd it quite remarkable that the synthesis network is able to produce meaningful results even though it receives input only through the styles that control the AdaIN operations. Finally, we introduce the noise inputs (E) that improve the results further, as well as novel mixing regularization (F) that decorrelates neighboring styles and enables more ﬁnegrained control over the generated imagery (Section 3.1). We evaluate our methods using two different loss functions: for CELEBA-HQ we rely on WGAN-GP [24], Figure 2. Uncurated set of images produced by our style-based generator (conﬁg F) with the FFHQ dataset. Here we used a variation of the truncation trick [42, 5, 34] with ψ = 0.7 for resolutions 42 −322. Please see the accompanying video for more results. while FFHQ uses WGAN-GP for conﬁguration A and nonsaturating loss [22] with R1 regularization [44, 51, 14] for conﬁgurations B–F. We found these choices to give the best results. Our contributions do not modify the loss function. We observe that the style-based generator (E) improves FIDs quite signiﬁcantly over the traditional generator (B), almost 20%, corroborating the large-scale ImageNet measurements made in parallel work [6, 5]. Figure 2 shows an uncurated set of novel images generated from the FFHQ dataset using our generator. As conﬁrmed by the FIDs, the average quality is high, and even accessories such as eyeglasses and hats get successfully synthesized. For this ﬁgure, we avoided sampling from the extreme regions of W using the so-called truncation trick [42, 5, 34] — Appendix B details how the trick can be performed in W instead of Z. Note that our generator allows applying the truncation selectively to low resolutions only, so that highresolution details are not affected. All FIDs in this paper are computed without the truncation trick, and we only use it for illustrative purposes in Figure 2 and the video. All images are generated in 10242 2.2. Prior art Much of the work on GAN architectures has focused on improving the discriminator by, e.g., using multiple discriminators [18, 47, 11], multiresolution discrimination [60, 55], or self-attention [63]. The work on generator side has mostly focused on the exact distribution in the input latent space [5] or shaping the input latent space via Gaussian mixture models [4], clustering [48], or encouraging convexity [52]. Recent conditional generators feed the class identiﬁer through a separate embedding network to a large number of layers in the generator [46], while the latent is still provided though the input layer. A few authors have considered feeding parts of the latent code to multiple generator layers [9, 5]. In parallel work, Chen et al. [6] “self modulate” the generator using AdaINs, similarly to our work, but do not consider an intermediate latent space or noise inputs.\n\n3. Properties of the style-based generator\nOur generator architecture makes it possible to control the image synthesis via scale-speciﬁc modiﬁcations to the styles. We can view the mapping network and afﬁne transformations as a way to draw samples for each style from a learned distribution, and the synthesis network as a way to generate a novel image based on a collection of styles. The effects of each style are localized in the network, i.e., modifying a speciﬁc subset of the styles can be expected to affect only certain aspects of the image. To see the reason for this localization, let us consider how the AdaIN operation (Eq. 1) ﬁrst normalizes each channel to zero mean and unit variance, and only then applies scales and biases based on the style. The new per-channel statistics, as dictated by the style, modify the relative importance of features for the subsequent convolution operation, but they do not depend on the original statistics because of the normalization. Thus each style controls only one convolution before being overridden by the next AdaIN operation. 3.1. Style mixing To further encourage the styles to localize, we employ mixing regularization, where a given percentage of images are generated using two random latent codes instead of one during training. When generating such an image, we simply switch from one latent code to another — an operation we refer to as style mixing — at a randomly selected point in the synthesis network. To be speciﬁc, we run two latent codes z1, z2 through the mapping network, and have the corresponding w1, w2 control the styles so that w1 applies before the crossover point and w2 after it. This regularization technique prevents the network from assuming that adjacent styles are correlated. Table 2 shows how enabling mixing regularization dur- Source B Source A Coarse styles from source B Middle styles from source B Fine from B Figure 3. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated by copying a speciﬁed subset of styles from source B and taking the rest from source A. Copying the styles corresponding to coarse spatial resolutions (42 – 82) brings high-level aspects such as pose, general hair style, face shape, and eyeglasses from source B, while all colors (eyes, hair, lighting) and ﬁner facial features resemble A. If we instead copy the styles of middle resolutions (162 – 322) from B, we inherit smaller scale facial features, hair style, eyes open/closed from B, while the pose, general face shape, and eyeglasses from A are preserved. Finally, copying the ﬁne styles (642 – 10242) from B brings mainly the color scheme and microstructure. Mixing Number of latents during testing regularization 1 2 3 4 E 0% 4.42 8.22 12.88 17.41 50% 4.41 6.10 8.71 11.61 F 90% 4.40 5.11 6.88 9.03 100% 4.83 5.17 6.63 8.40 Table 2. FIDs in FFHQ for networks trained by enabling the mixing regularization for different percentage of training examples. Here we stress test the trained networks by randomizing 1 . . . 4 latents and the crossover points between them. Mixing regularization improves the tolerance to these adverse operations signiﬁcantly. Labels E and F refer to the conﬁgurations in Table 1. (a) Generated image (b) Stochastic variation (c) Standard deviation Figure 4. Examples of stochastic variation. (a) Two generated images. (b) Zoom-in with different realizations of input noise. While the overall appearance is almost identical, individual hairs are placed very differently. (c) Standard deviation of each pixel over 100 different realizations, highlighting which parts of the images are affected by the noise. The main areas are the hair, silhouettes, and parts of background, but there is also interesting stochastic variation in the eye reﬂections. Global aspects such as identity and pose are unaffected by stochastic variation. ing training improves the localization considerably, indicated by improved FIDs in scenarios where multiple latents are mixed at test time. Figure 3 presents examples of images synthesized by mixing two latent codes at various scales. We can see that each subset of styles controls meaningful high-level attributes of the image. 3.2. Stochastic variation There are many aspects in human portraits that can be regarded as stochastic, such as the exact placement of hairs, stubble, freckles, or skin pores. Any of these can be randomized without affecting our perception of the image as long as they follow the correct distribution. Let us consider how a traditional generator implements stochastic variation. Given that the only input to the network is through the input layer, the network needs to invent a way to generate spatially-varying pseudorandom numbers (a) (b) (c) (d) Figure 5. Effect of noise inputs at different layers of our generator. (a) Noise is applied to all layers. (b) No noise. (c) Noise in ﬁne layers only (642 – 10242). (d) Noise in coarse layers only (42 – 322). We can see that the artiﬁcial omission of noise leads to featureless “painterly” look. Coarse noise causes large-scale curling of hair and appearance of larger background features, while the ﬁne noise brings out the ﬁner curls of hair, ﬁner background detail, and skin pores. from earlier activations whenever they are needed. This consumes network capacity and hiding the periodicity of generated signal is difﬁcult — and not always successful, as evidenced by commonly seen repetitive patterns in generated images. Our architecture sidesteps these issues altogether by adding per-pixel noise after each convolution. Figure 4 shows stochastic realizations of the same underlying image, produced using our generator with different noise realizations. We can see that the noise affects only the stochastic aspects, leaving the overall composition and high-level aspects such as identity intact. Figure 5 further illustrates the effect of applying stochastic variation to different subsets of layers. Since these effects are best seen in animation, please consult the accompanying video for a demonstration of how changing the noise input of one layer leads to stochastic variation at a matching scale. We ﬁnd it interesting that the effect of noise appears tightly localized in the network. We hypothesize that at any point in the generator, there is pressure to introduce new content as soon as possible, and the easiest way for our network to create stochastic variation is to rely on the noise provided. A fresh set of noise is available for every layer, and thus there is no incentive to generate the stochastic effects from earlier activations, leading to a localized effect. (a) Distribution of (b) Mapping from (c) Mapping from features in training set Z to features W to features Figure 6. Illustrative example with two factors of variation (image features, e.g., masculinity and hair length). (a) An example training set where some combination (e.g., long haired males) is missing. (b) This forces the mapping from Z to image features to become curved so that the forbidden combination disappears in Z to prevent the sampling of invalid combinations. (c) The learned mapping from Z to W is able to “undo” much of the warping. 3.3. Separation of global effects from stochasticity The previous sections as well as the accompanying video demonstrate that while changes to the style have global effects (changing pose, identity, etc.), the noise affects only inconsequential stochastic variation (differently combed hair, beard, etc.). This observation is in line with style transfer literature, where it has been established that spatially invariant statistics (Gram matrix, channel-wise mean, variance, etc.) reliably encode the style of an image [20, 39] while spatially varying features encode a speciﬁc instance. In our style-based generator, the style affects the entire image because complete feature maps are scaled and biased with the same values. Therefore, global effects such as pose, lighting, or background style can be controlled coherently. Meanwhile, the noise is added independently to each pixel and is thus ideally suited for controlling stochastic variation. If the network tried to control, e.g., pose using the noise, that would lead to spatially inconsistent decisions that would then be penalized by the discriminator. Thus the network learns to use the global and local channels appropriately, without explicit guidance.\n\n4. Disentanglement studies\nThere are various deﬁnitions for disentanglement [54, 50, 2, 7, 19], but a common goal is a latent space that consists of linear subspaces, each of which controls one factor of variation. However, the sampling probability of each combination of factors in Z needs to match the corresponding density in the training data. As illustrated in Figure 6, this precludes the factors from being fully disentangled with typical datasets and input latent distributions.2 A major beneﬁt of our generator architecture is that the intermediate latent space W does not have to support sam- pling according to any ﬁxed distribution; its sampling density is induced by the learned piecewise continuous mapping f(z). This mapping can be adapted to “unwarp” W so that the factors of variation become more linear. We posit that there is pressure for the generator to do so, as it should be easier to generate realistic images based on a disentangled representation than based on an entangled representation. As such, we expect the training to yield a less entangled W in an unsupervised setting, i.e., when the factors of variation are not known in advance [10, 35, 49, 8, 26, 32, 7]. Unfortunately the metrics recently proposed for quantifying disentanglement [26, 32, 7, 19] require an encoder network that maps input images to latent codes. These metrics are ill-suited for our purposes since our baseline GAN lacks such an encoder. While it is possible to add an extra network for this purpose [8, 12, 15], we want to avoid investing effort into a component that is not a part of the actual solution. To this end, we describe two new ways of quantifying disentanglement, neither of which requires an encoder or known factors of variation, and are therefore computable for any image dataset and generator. 4.1. Perceptual path length As noted by Laine [37], interpolation of latent-space vectors may yield surprisingly non-linear changes in the image. For example, features that are absent in either endpoint may appear in the middle of a linear interpolation path. This is a sign that the latent space is entangled and the factors of variation are not properly separated. To quantify this effect, we can measure how drastic changes the image undergoes as we perform interpolation in the latent space. Intuitively, a less curved latent space should result in perceptually smoother transition than a highly curved latent space. As a basis for our metric, we use a perceptually-based pairwise image distance [65] that is calculated as a weighted difference between two VGG16 [58] embeddings, where the weights are ﬁt so that the metric agrees with human perceptual similarity judgments. If we subdivide a latent space interpolation path into linear segments, we can deﬁne the total perceptual length of this segmented path as the sum of perceptual differences over each segment, as reported by the image distance metric. A natural deﬁnition for the perceptual path length would be the limit of this sum under inﬁnitely ﬁne subdivision, but in practice we approximate it using a small subdivision epsilon ϵ = 10−4. The average perceptual path length in latent space Z, over all possible endpoints, is therefore lZ = E h 1 ϵ2 d \u0000 G(slerp(z1, z2; t)), G(slerp(z1, z2; t + ϵ)) \u0001i , (2)\n\nMethod\nPath length Separafull end bility B Traditional generator Z 412.0 415.3 10.78 D Style-based generator W 446.2 376.6 3.61 E + Add noise inputs W 200.5 160.6 3.54 + Mixing 50% W 231.5 182.1 3.51 F + Mixing 90% W 234.0 195.9 3.79 Table 3. Perceptual path lengths and separability scores for various generator architectures in FFHQ (lower is better). We perform the measurements in Z for the traditional network, and in W for stylebased ones. Making the network resistant to style mixing appears to distort the intermediate latent space W somewhat. We hypothesize that mixing makes it more difﬁcult for W to efﬁciently encode factors of variation that span multiple scales. ceptual distance between the resulting images. Here slerp denotes spherical interpolation [56], which is the most appropriate way of interpolating in our normalized input latent space [61]. To concentrate on the facial features instead of background, we crop the generated images to contain only the face prior to evaluating the pairwise image metric. As the metric d is quadratic [65], we divide by ϵ2. We compute the expectation by taking 100,000 samples. Computing the average perceptual path length in W is carried out in a similar fashion: lW = E h 1 ϵ2 d \u0000 g(lerp(f(z1), f(z2); t)), g(lerp(f(z1), f(z2); t + ϵ)) \u0001i , (3) where the only difference is that interpolation happens in W space. Because vectors in W are not normalized in any fashion, we use linear interpolation (lerp). Table 3 shows that this full-path length is substantially shorter for our style-based generator with noise inputs, indicating that W is perceptually more linear than Z. Yet, this measurement is in fact slightly biased in favor of the input latent space Z. If W is indeed a disentangled and “ﬂattened” mapping of Z, it may contain regions that are not on the input manifold — and are thus badly reconstructed by the generator — even between points that are mapped from the input manifold, whereas the input latent space Z has no such regions by deﬁnition. It is therefore to be expected that if we restrict our measure to path endpoints, i.e., t ∈{0, 1}, we should obtain a smaller lW while lZ is not affected. This is indeed what we observe in Table 3. Table 4 shows how path lengths are affected by the mapping network. We see that both traditional and style-based generators beneﬁt from having a mapping network, and additional depth generally improves the perceptual path length as well as FIDs. It is interesting that while lW improves in the traditional generator, lZ becomes considerably worse, illustrating our claim that the input latent space can indeed be arbitrarily entangled in GANs.\n\nMethod\nFID Path length Separafull end bility B Traditional 0 Z 5.25 412.0 415.3 10.78 Traditional 8 Z 4.87 896.2 902.0 170.29 Traditional 8 W 4.87 324.5 212.2 6.52 Style-based 0 Z 5.06 283.5 285.5 9.88 Style-based 1 W 4.60 219.9 209.4 6.81 Style-based 2 W 4.43 217.8 199.9 6.25 F Style-based 8 W 4.40 234.0 195.9 3.79 Table 4. The effect of a mapping network in FFHQ. The number in method name indicates the depth of the mapping network. We see that FID, separability, and path length all beneﬁt from having a mapping network, and this holds for both style-based and traditional generator architectures. Furthermore, a deeper mapping network generally performs better than a shallow one. 4.2. Linear separability If a latent space is sufﬁciently disentangled, it should be possible to ﬁnd direction vectors that consistently correspond to individual factors of variation. We propose another metric that quantiﬁes this effect by measuring how well the latent-space points can be separated into two distinct sets via a linear hyperplane, so that each set corresponds to a speciﬁc binary attribute of the image. In order to label the generated images, we train auxiliary classiﬁcation networks for a number of binary attributes, e.g., to distinguish male and female faces. In our tests, the classiﬁers had the same architecture as the discriminator we use (i.e., same as in [30]), and were trained using the CELEBA-HQ dataset that retains the 40 attributes available in the original CelebA dataset. To measure the separability of one attribute, we generate 200,000 images with z ∼P(z) and classify them using the auxiliary classiﬁcation network. We then sort the samples according to classiﬁer conﬁdence and remove the least conﬁdent half, yielding 100,000 labeled latent-space vectors. For each attribute, we ﬁt a linear SVM to predict the label based on the latent-space point — z for traditional and w for style-based — and classify the points by this plane. We then compute the conditional entropy H(Y |X) where X are the classes predicted by the SVM and Y are the classes determined by the pre-trained classiﬁer. This tells how much additional information is required to determine the true class of a sample, given that we know on which side of the hyperplane it lies. A low value suggests consistent latent space directions for the corresponding factor(s) of variation. We calculate the ﬁnal separability score as exp(P i H(Yi|Xi)), where i enumerates the 40 attributes. Similar to the inception score [53], the exponentiation brings the values from logarithmic to linear domain so that they are easier to compare. Tables 3 and 4 show that W is consistently better separable than Z, suggesting a less entangled representation. Figure 7. The FFHQ dataset offers a lot of variety in terms of age, ethnicity, viewpoint, lighting, and image background. Furthermore, increasing the depth of the mapping network improves both image quality and separability in W, which is in line with the hypothesis that the synthesis network inherently favors a disentangled input representation. Interestingly, adding a mapping network in front of a traditional generator results in severe loss of separability in Z but improves the situation in the intermediate latent space W, and the FID improves as well. This shows that even the traditional generator architecture performs better when we introduce an intermediate latent space that does not have to follow the distribution of the training data.\n\n5. Conclusion\nBased on both our results and parallel work by Chen et al. [6], it is becoming clear that the traditional GAN generator architecture is in every way inferior to a style-based design. This is true in terms of established quality metrics, and we further believe that our investigations to the separation of high-level attributes and stochastic effects, as well as the linearity of the intermediate latent space will prove fruitful in improving the understanding and controllability of GAN synthesis. We note that our average path length metric could easily be used as a regularizer during training, and perhaps some variant of the linear separability metric could act as one, too. In general, we expect that methods for directly shaping the intermediate latent space during training will provide interesting avenues for future work.\n\n6. Acknowledgements\nWe thank Jaakko Lehtinen, David Luebke, and Tuomas Kynk¨a¨anniemi for in-depth discussions and helpful comments; Janne Hellsten, Tero Kuosmanen, and Pekka J¨anis for compute infrastructure and help with the code release.\n\nReferences\n[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur,\nJ. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner,\nP. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and\nX. Zheng. TensorFlow: a system for large-scale machine\nlearning. In Proc. 12th USENIX Conference on Operating Systems Design and Implementation, OSDI’16, pages 265– 283, 2016. 9 [2] A. Achille and S. Soatto. On the emergence of invariance and disentangling in deep representations. CoRR, abs/1706.01350, 2017. 6 [3] D. Bau, J. Zhu, H. Strobelt, B. Zhou, J. B. Tenenbaum, W. T. Freeman, and A. Torralba. GAN dissection: Visualizing and understanding generative adversarial networks. In Proc. ICLR, 2019. 1 [4] M. Ben-Yosef and D. Weinshall. Gaussian mixture generative adversarial networks for diverse datasets, and the unsupervised clustering of images. CoRR, abs/1808.10356, 2018. 3 [5] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural image synthesis. CoRR, abs/1809.11096, 2018. 1, 3, 8 Figure 12. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN CAT dataset at 2562. FID computed for 50K images was 8.53. [6] T. Chen, M. Lucic, N. Houlsby, and S. Gelly. On self modulation for generative adversarial networks. CoRR, abs/1810.01365, 2018. 3, 8 [7] T. Q. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud. Isolating sources of disentanglement in variational autoencoders. CoRR, abs/1802.04942, 2018. 6 [8] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: interpretable representation learning by information maximizing generative adversarial nets. CoRR, abs/1606.03657, 2016. 6 [9] E. L. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a Laplacian pyramid of adversarial networks. CoRR, abs/1506.05751, 2015. 3 [10] G. Desjardins, A. Courville, and Y. Bengio. Disentangling factors of variation via generative entangling. CoRR, abs/1210.5474, 2012. 6 [11] T. Doan, J. Monteiro, I. Albuquerque, B. Mazoure, A. Durand, J. Pineau, and R. D. Hjelm. Online adaptative curriculum learning for GANs. CoRR, abs/1808.00020, 2018. 3 [12] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature learning. CoRR, abs/1605.09782, 2016. 6 [13] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. CoRR, abs/1411.5928, 2014. 1 [14] H. Drucker and Y. L. Cun. Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 3(6):991–997, 1992. 3 [15] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky,\nO. Mastropietro, and A. Courville. Adversarially learned inference. In Proc. ICLR, 2017. 6\n[16] V. Dumoulin, E. Perez, N. Schucher, F. Strub, H. d. Vries,\nA. Courville, and Y. Bengio.\nFeature-wise transformations. Distill, 2018. https://distill.pub/2018/feature-wisetransformations. 2 [17] V. Dumoulin, J. Shlens, and M. Kudlur. A learned representation for artistic style. CoRR, abs/1610.07629, 2016. 2 [18] I. P. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-adversarial networks. CoRR, abs/1611.01673, 2016. 3 [19] C. Eastwood and C. K. I. Williams. A framework for the quantitative evaluation of disentangled representations. In Proc. ICLR, 2018. 6 [20] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Proc. CVPR, 2016. 6 [21] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens. Exploring the structure of a real-time, arbitrary neural artistic stylization network. CoRR, abs/1705.06830, 2017. 2 [22] I. Goodfellow,\nJ. Pouget-Abadie,\nM. Mirza,\nB. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Networks. In NIPS, 2014. 1, 3, 9\n[23] W.-S. Z. Guang-Yuan Hao, Hong-Xing Yu. MIXGAN: learning concepts from different domains for mixture generation. CoRR, abs/1807.01659, 2018. 2 [24] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of Wasserstein GANs. CoRR, abs/1704.00028, 2017. 1, 2 [25] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and\nS. Hochreiter. GANs trained by a two time-scale update rule\nconverge to a local Nash equilibrium. In Proc. NIPS, pages 6626–6637, 2017. 2 [26] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,\nM. Botvinick, S. Mohamed, and A. Lerchner.\nbeta-vae: Learning basic visual concepts with a constrained variational framework. In Proc. ICLR, 2017. 6 [27] X. Huang and S. J. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. CoRR, abs/1703.06868, 2017. 1, 2 [28] X. Huang, M. Liu, S. J. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. CoRR, abs/1804.04732, 2018. 2 [29] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. 9 [30] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. 1, 2, 7, 8, 9 [31] V. Kazemi and J. Sullivan. One millisecond face alignment with an ensemble of regression trees. In Proc. CVPR, 2014. 8 [32] H. Kim and A. Mnih. Disentangling by factorising. In Proc. ICML, 2018. 6 [33] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 9 [34] D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. CoRR, abs/1807.03039, 2018. 3, 8 [35] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014. 6 [36] K. Kurach, M. Lucic, X. Zhai, M. Michalski, and S. Gelly. The gan landscape: Losses, architectures, regularization, and normalization. CoRR, abs/1807.04720, 2018. 1 [37] S. Laine. Feature-based metrics for exploring the latent space of generative models. ICLR workshop poster, 2018. 1, 6 [38] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang. Universal style transfer via feature transforms. In Proc. NIPS, 2017. 2 [39] Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural style transfer. CoRR, abs/1701.01036, 2017. 6 [40] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are GANs created equal? a large-scale study. CoRR, abs/1711.10337, 2017. 1 [41] A. L. Maas, A. Y. Hannun, and A. Ng. Rectiﬁer nonlinearities improve neural network acoustic models. In Proc. International Conference on Machine Learning (ICML), volume 30, 2013. 9 [42] M. Marchesi. Megapixel size image creation using generative adversarial networks. CoRR, abs/1706.00082, 2017. 3, 8 [43] L. Matthey,\nI. Higgins,\nD. Hassabis,\nand A. Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. 6 [44] L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge? CoRR, abs/1801.04406, 2018. 1, 3, 9 [45] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. CoRR, abs/1802.05957, 2018. 1, 9 [46] T. Miyato and M. Koyama. cGANs with projection discriminator. CoRR, abs/1802.05637, 2018. 3 [47] G. Mordido, H. Yang, and C. Meinel. Dropout-gan: Learning from a dynamic ensemble of discriminators. CoRR, abs/1807.11346, 2018. 3 [48] S. Mukherjee, H. Asnani, E. Lin, and S. Kannan. ClusterGAN : Latent space clustering in generative adversarial networks. CoRR, abs/1809.03627, 2018. 3 [49] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proc. ICML, 2014. 6 [50] K. Ridgeway. A survey of inductive biases for factorial representation-learning. CoRR, abs/1612.05299, 2016. 6 [51] A. S. Ross and F. Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. CoRR, abs/1711.09404,\n2017. 3\n[52] T. Sainburg, M. Thielk, B. Theilman, B. Migliori, and\nT. Gentner. Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourage convex latent distributions. CoRR, abs/1807.06650,\n2018. 1, 3\n[53] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung,\nA. Radford, and X. Chen. Improved techniques for training\nGANs. In NIPS, 2016. 7 [54] J. Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863–879, 1992. 6 [55] R. Sharma, S. Barratt, S. Ermon, and V. Pande. Improved training with curriculum gans. CoRR, abs/1807.09295, 2018. 3 [56] K. Shoemake. Animating rotation with quaternion curves. In Proc. SIGGRAPH ’85, 1985. 7 [57] A. Siarohin, E. Sangineto, and N. Sebe. Whitening and coloring transform for GANs. CoRR, abs/1806.00420, 2018. 2 [58] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 6 [59] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning\nResearch, 15:1929–1958, 2014. 9 [60] T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. CoRR, abs/1711.11585, 2017. 3 [61] T. White. Sampling generative networks: Notes on a few effective techniques. CoRR, abs/1609.04468, 2016. 7 [62] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015. 9 [63] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. CoRR, abs/1805.08318, 2018. 3, 9 [64] R. Zhang. Making convolutional networks shift-invariant again, 2019. 2, 9 [65] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. CVPR, 2018. 6, 7\n--- FOOTNOTES 12M to 25M images. We use the same learning rates as 5M 10M 15M 20M 25M 5M 10M 15M 20M 25M\n\nA. The FFHQ dataset\nWe have collected a new dataset of human faces, FlickrFaces-HQ (FFHQ), consisting of 70,000 high-quality images at 10242 resolution (Figure 7). The dataset includes vastly more variation than CELEBA-HQ [30] in terms of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr ψ = 1 ψ = 0.7 ψ = 0.5 ψ = 0 ψ = −0.5 ψ = −1 Figure 8. The effect of truncation trick as a function of style scale ψ. When we fade ψ →0, all faces converge to the “mean” face of FFHQ. This face is similar for all trained networks, and the interpolation towards it never seems to cause artifacts. By applying negative scaling to styles, we get the corresponding opposite or “anti-face”. It is interesting that various high-level attributes often ﬂip between the opposites, including viewpoint, glasses, age, coloring, hair length, and often gender. (thus inheriting all the biases of that website) and automatically aligned [31] and cropped. Only images under permissive licenses were collected. Various automatic ﬁlters were used to prune the set, and ﬁnally Mechanical Turk allowed us to remove the occasional statues, paintings, or photos of photos. We have made the dataset publicly available at https://github.com/NVlabs/ffhq-dataset\n\nB. Truncation trick in W\nIf we consider the distribution of training data, it is clear that areas of low density are poorly represented and thus likely to be difﬁcult for the generator to learn. This is a signiﬁcant open problem in all generative modeling techniques. However, it is known that drawing latent vectors from a truncated [42, 5] or otherwise shrunk [34] sampling space tends to improve average image quality, although some amount of variation is lost. We can follow a similar strategy. To begin, we compute the center of mass of W as ¯w = Ez∼P (z)[f(z)]. In case of FFHQ this point represents a sort of an average face (Figure 8, ψ = 0). We can then scale the deviation of a given w from the center as w′ = ¯w + ψ(w −¯w), where ψ < 1. While Brock et al. [5] observe that only a subset of networks is amenable to such truncation even when orthogonal regularization is used, truncation in W space seems to work reliably even without changes to the loss function.\n\nC. Hyperparameters and training details\nWe build upon the ofﬁcial TensorFlow [1] implementation of Progressive GANs by Karras et al. [30], from which we inherit most of the training details.3 This original setup corresponds to conﬁguration A in Table 1. In particular, we use the same discriminator architecture, resolutiondependent minibatch sizes, Adam [33] hyperparameters, and exponential moving average of the generator. We enable mirror augmentation for CelebA-HQ and FFHQ, but disable it for LSUN. Our training time is approximately one week on an NVIDIA DGX-1 with 8 Tesla V100 GPUs. For our improved baseline (B in Table 1), we make several modiﬁcations to improve the overall result quality. We replace the nearest-neighbor up/downsampling in both networks with bilinear sampling, which we implement by lowpass ﬁltering the activations with a separable 2nd order binomial ﬁlter after each upsampling layer and before each downsampling layer [64]. We implement progressive growing the same way as Karras et al. [30], but we start from 82 images instead of 42. For the FFHQ dataset, we switch from WGAN-GP to the non-saturating loss [22] with R1 regularization [44] using γ = 10. With R1 we found that the FID scores keep decreasing for considerably longer than with WGAN-GP, and we thus increase the training time from Karras et al. [30] for FFHQ, but we found that setting the learning rate to 0.002 instead of 0.003 for 5122 and 10242 leads to better stability with CelebA-HQ. For our style-based generator (F in Table 1), we use leaky ReLU [41] with α = 0.2 and equalized learning rate [30] for all layers. We use the same feature map counts in our convolution layers as Karras et al. [30]. Our mapping network consists of 8 fully-connected layers, and the dimensionality of all input and output activations — including z and w — is 512. We found that increasing the depth of the mapping network tends to make the training unstable with high learning rates. We thus reduce the learning rate by two orders of magnitude for the mapping network, i.e., λ′ = 0.01·λ. We initialize all weights of the convolutional, fully-connected, and afﬁne transform layers using N(0, 1). The constant input in synthesis network is initialized to one. The biases and noise scaling factors are initialized to zero, except for the biases associated with ys that we initialize to one. The classiﬁers used by our separability metric (Section 4.2) have the same architecture as our discriminator except that minibatch standard deviation [30] is disabled. We use the learning rate of 10−3, minibatch size of 8, Adam optimizer, and training length of 150,000 images. The classiﬁers are trained independently of generators, and the same 40 classiﬁers, one for each CelebA attribute, are used FID Path length 10 500 9 400 8 300 7 200 6 100 5 0 resolution Full resolution Full 40 0 Traditional (B) Traditional (B) Style-based (F) Style-based (F), full Style-based (F), end Figure 9. FID and perceptual path length metrics over the course of training in our conﬁgurations B and F using the FFHQ dataset. Horizontal axis denotes the number of training images seen by the discriminator. The dashed vertical line at 8.4M images marks the point when training has progressed to full 10242 resolution. On the right, we show only one curve for the traditional generator’s path length measurements, because there is no discernible difference between full-path and endpoint sampling in Z. for measuring the separability metric for all generators. We will release the pre-trained classiﬁer networks so that our measurements can be reproduced. We do not use batch normalization [29], spectral normalization [45], attention mechanisms [63], dropout [59], or pixelwise feature vector normalization [30] in our networks.\n\nD. Training convergence\nFigure 9 shows how the FID and perceptual path length metrics evolve during the training of our conﬁgurations B and F with the FFHQ dataset. With R1 regularization active in both conﬁgurations, FID continues to slowly decrease as the training progresses, motivating our choice to increase the training time from 12M images to 25M images. Even when the training has reached the full 10242 resolution, the slowly rising path lengths indicate that the improvements in FID come at the cost of a more entangled representation. Considering future work, it is an interesting question whether this is unavoidable, or if it were possible to encourage shorter path lengths without compromising the convergence of FID.\n\nE. Other datasets\nFigures 10, 11, and 12 show an uncurated set of results for LSUN [62] BEDROOM, CARS, and CATS, respectively. In these images we used the truncation trick from Appendix Bwith ψ = 0.7 for resolutions 42 −322. The accompanying video provides results for style mixing and stochastic variation tests. As can be seen therein, in case of Figure 10. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN BEDROOM dataset at 2562. FID computed for 50K images was 2.65. BEDROOM the coarse styles basically control the viewpoint of the camera, middle styles select the particular furniture, and ﬁne styles deal with colors and smaller details of materials. In CARS the effects are roughly similar. Stochastic variation affects primarily the fabrics in BEDROOM, backgrounds and headlamps in CARS, and fur, background, and interestingly, the positioning of paws in CATS. Somewhat surprisingly the wheels of a car never seem to rotate based on stochastic inputs. These datasets were trained using the same setup as FFHQ for the duration of 70M images for BEDROOM and CATS, and 46M for CARS. We suspect that the results for BEDROOM are starting to approach the limits of the training data, as in many images the most objectionable issues are the severe compression artifacts that have been inherited from the low-quality training data. CARS has much higher quality training data that also allows higher spatial resolution (512 × 384 instead of 2562), and CATS continues to be a difﬁcult dataset due to the high intrinsic variation in poses, zoom levels, and backgrounds. Figure 11. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN CAR dataset at 512 × 384. FID computed for 50K images was 3.27.", "persona": "Student", "response": "### 1. Overview\nExplained: A Style-Based Generator Architecture for GANs - Generating and Tuning Realistic Artificial Faces Generative Adversarial Networks (GAN) are a relatively new concept in Machine Learning, introduced for the first time in 2014. Their goal is to synthesize artificial samples, such as images, that are indistinguishable from authentic images. A common example of a GAN application is to generate artificial face images by learning from a dataset of celebrity faces. While GAN images became more realistic over time, one of their main challenges is controlling their output, i.e. changing specific features such pose, face shape and hair style in an image of a face. The Mapping Network’s goal is to encode the input vector into an intermediate vector whose different elements control different visual features. This is a non-trivial process since the ability to control visual features with the input vector is limited, as it must follow the probability density of the training data.\n\n### 2. Background\nFor example, if images of people with black hair are more common in the dataset, then more input values will be mapped to that feature. As a result, the model isn’t capable of mapping parts of the input (elements in the vector) to features, a phenomenon called features entanglement. However, by using another neural network the model can generate a vector that doesn’t have to follow the training data distribution and can reduce the correlation between features. Stochastic variation There are many aspects in people’s faces that are small and can be seen as stochastic, such as freckles, exact placement of hairs, wrinkles, features which make the image more realistic and increase the variety of outputs. The common method to insert these small features into GAN images is adding random noise to the input vector.\n\n### 3. Methods\nMapping Network The Mapping Network consists of 8 fully connected layers and its output ⱳ is of the same size as the input layer (512×1). The generator with the Mapping Network (in addition to the ProGAN synthesis network) Style Modules (AdaIN) The AdaIN (Adaptive Instance Normalization) module transfers the encoded information ⱳ, created by the Mapping Network, into the generated image. The module is added to each resolution level of the Synthesis Network and defines the visual expression of the features in that level: Each channel of the convolution layer output is first normalized to make sure the scaling and shifting of step 3 have the expected effect. The intermediate vector ⱳ is transformed using another fully-connected layer (marked as A) into a scale and bias for each channel. The scale and bias vectors shift each channel of the convolution output, thereby defining the importance of each filter in the convolution. This tuning translates the information from ⱳ to a visual representation. The generator’s Adaptive Instance Normalization (AdaIN) Removing traditional input The Synthesis Network input is replaced with a constant input Adding scaled noise to each resolution level of the synthesis network After training the model, an “average” ⱳavg is produced by selecting many random inputs; generating their intermediate vectors with the mapping network; and calculating the mean of these vectors.\n\n### 4. Results\nWhen generating new images, instead of using Mapping Network output directly, ⱳ is transformed into ⱳ_new=ⱳ_avg+𝞧(ⱳ -ⱳ_avg), where the value of 𝞧 defines how far the image can be from the “average” image (and how diverse the output can be). Interestingly, by using a different 𝞧 for each level, before the affine transformation block, the model can control how far from average each set of features is, as shown in the video below. By comparing these metrics for the input vector z and the intermediate vector ⱳ, the authors show that features in ⱳ are significantly more separable. These metrics also show the benefit of selecting 8 layers in the Mapping Network in comparison to 1 or 2 layers. Implementation Details\n\n### 5. Takeaways\nThe basic components of every GAN are two neural networks - a generator that synthesizes new samples from scratch, and a discriminator that takes samples from both the training data and the generator’s output and predicts if they are “real” or “fake”. How StyleGAN works The new generator includes several additions to the ProGAN’s generators: Fine-tuning Additional improvement of StyleGAN upon ProGAN was updating several network hyperparameters, such as training duration and loss function, and replacing the up/downscaling from nearest neighbors to bilinear sampling. Though this step is significant for the model performance, it’s less innovative and therefore won’t be described here in detail (Appendix C in the url). An overview of StyleGAN The performance (FID score) of the model in different configurations compared to ProGAN. The lower score the better the model (Source: StyleGAN) Perceptual path length — measure the difference between consecutive images (their VGG16 embeddings) when interpolating between two random inputs. Drastic changes mean that multiple features have changed together and that they might be entangled. Linear separability — the ability to classify inputs into binary classes, such as male and female. The better the classification the more separable the features.", "paper_id": "04", "story_id": "04-03"}
{"paper": "Abstract\nWe propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-speciﬁc control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.\n\n1. Introduction\nThe resolution and quality of images produced by generative methods — especially generative adversarial networks (GAN) [22] — have seen rapid improvement recently [30, 45, 5]. Yet the generators continue to operate as black boxes, and despite recent efforts [3], the understanding of various aspects of the image synthesis process, e.g., the origin of stochastic features, is still lacking. The properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations [13, 52, 37] provide no quantitative way to compare different generators against each other. Motivated by style transfer literature [27], we re-design the generator architecture in a way that exposes novel ways to control the image synthesis process. Our generator starts from a learned constant input and adjusts the “style” of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales. Combined with noise injected directly into the network, this architectural change leads to automatic, unsupervised separation of high-level attributes Timo Aila NVIDIA taila@nvidia.com (e.g., pose, identity) from stochastic variation (e.g., freckles, hair) in the generated images, and enables intuitive scale-speciﬁc mixing and interpolation operations. We do not modify the discriminator or the loss function in any way, and our work is thus orthogonal to the ongoing discussion about GAN loss functions, regularization, and hyperparameters [24, 45, 5, 40, 44, 36]. Our generator embeds the input latent code into an intermediate latent space, which has a profound effect on how the factors of variation are represented in the network. The input latent space must follow the probability density of the training data, and we argue that this leads to some degree of unavoidable entanglement. Our intermediate latent space is free from that restriction and is therefore allowed to be disentangled. As previous methods for estimating the degree of latent space disentanglement are not directly applicable in our case, we propose two new automated metrics — perceptual path length and linear separability — for quantifying these aspects of the generator. Using these metrics, we show that compared to a traditional generator architecture, our generator admits a more linear, less entangled representation of different factors of variation. Finally, we present a new dataset of human faces (Flickr-Faces-HQ, FFHQ) that offers much higher quality and covers considerably wider variation than existing high-resolution datasets (Appendix A). We have made this dataset publicly available, along with our source code and pre-trained networks.1 The accompanying video can be found under the same link.\n\n2. Style-based generator\nTraditionally the latent code is provided to the generator through an input layer, i.e., the ﬁrst layer of a feedforward network (Figure 1a). We depart from this design by omitting the input layer altogether and starting from a learned constant instead (Figure 1b, right). Given a latent code z in the input latent space Z, a non-linear mapping network f : Z →W ﬁrst produces w ∈W (Figure 1b, left). For simplicity, we set the dimensionality of both Noise Latent Latent Synthesis network Normalize Normalize B Const 4×4×512 Mapping network style Fully-connected A AdaIN PixelNorm Conv 3×3 FC Conv 3×3 B FC style A AdaIN FC PixelNorm 4×4 FC FC 4×4 Upsample Upsample FC FC Conv 3×3 Conv 3×3 B style FC PixelNorm A AdaIN Conv 3×3 Conv 3×3 A B PixelNorm style AdaIN 8×8 8×8 (a) Traditional (b) Style-based generator Figure 1. While a traditional generator [30] feeds the latent code though the input layer only, we ﬁrst map the input to an intermediate latent space W, which then controls the generator through adaptive instance normalization (AdaIN) at each convolution layer. Gaussian noise is added after each convolution, before evaluating the nonlinearity. Here “A” stands for a learned afﬁne transform, and “B” applies learned per-channel scaling factors to the noise input. The mapping network f consists of 8 layers and the synthesis network g consists of 18 layers — two for each resolution (42 −10242). The output of the last layer is converted to RGB using a separate 1 × 1 convolution, similar to Karras et al. [30]. Our generator has a total of 26.2M trainable parameters, compared to 23.1M in the traditional generator. spaces to 512, and the mapping f is implemented using an 8-layer MLP, a decision we will analyze in Section 4.1. Learned afﬁne transformations then specialize w to styles y = (ys, yb) that control adaptive instance normalization (AdaIN) [27, 17, 21, 16] operations after each convolution layer of the synthesis network g. The AdaIN operation is deﬁned as AdaIN(xi, y) = ys,i xi −µ(xi) σ(xi) + yb,i, (1) where each feature map xi is normalized separately, and then scaled and biased using the corresponding scalar components from style y. Thus the dimensionality of y is twice the number of feature maps on that layer. Comparing our approach to style transfer, we compute the spatially invariant style y from vector w instead of an example image. We choose to reuse the word “style” for y because similar network architectures are already used for feedforward style transfer [27], unsupervised image-toimage translation [28], and domain mixtures [23]. Compared to more general feature transforms [38, 57], AdaIN is particularly well suited for our purposes due to its efﬁciency and compact representation.\n\nMethod\nCelebA-HQ FFHQ A Baseline Progressive GAN [30] 7.79 8.04 B + Tuning (incl. bilinear up/down) 6.11 5.25 C + Add mapping and styles 5.34 4.85 D + Remove traditional input 5.07 4.88 E + Add noise inputs 5.06 4.42 F + Mixing regularization 5.17 4.40 Table 1. Fr´echet inception distance (FID) for various generator designs (lower is better). In this paper we calculate the FIDs using 50,000 images drawn randomly from the training set, and report the lowest distance encountered over the course of training. Finally, we provide our generator with a direct means to generate stochastic detail by introducing explicit noise inputs. These are single-channel images consisting of uncorrelated Gaussian noise, and we feed a dedicated noise image to each layer of the synthesis network. The noise image is broadcasted to all feature maps using learned perfeature scaling factors and then added to the output of the corresponding convolution, as illustrated in Figure 1b. The implications of adding the noise inputs are discussed in Sections 3.2 and 3.3. 2.1. Quality of generated images Before studying the properties of our generator, we demonstrate experimentally that the redesign does not compromise image quality but, in fact, improves it considerably. Table 1 gives Fr´echet inception distances (FID) [25] for various generator architectures in CELEBA-HQ [30] and our new FFHQ dataset (Appendix A). Results for other datasets are given in Appendix E. Our baseline conﬁguration (A) is the Progressive GAN setup of Karras et al. [30], from which we inherit the networks and all hyperparameters except where stated otherwise. We ﬁrst switch to an improved baseline (B) by using bilinear up/downsampling operations [64], longer training, and tuned hyperparameters. A detailed description of training setups and hyperparameters is included in Appendix C. We then improve this new baseline further by adding the mapping network and AdaIN operations (C), and make a surprising observation that the network no longer beneﬁts from feeding the latent code into the ﬁrst convolution layer. We therefore simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 × 4 × 512 constant tensor (D). We ﬁnd it quite remarkable that the synthesis network is able to produce meaningful results even though it receives input only through the styles that control the AdaIN operations. Finally, we introduce the noise inputs (E) that improve the results further, as well as novel mixing regularization (F) that decorrelates neighboring styles and enables more ﬁnegrained control over the generated imagery (Section 3.1). We evaluate our methods using two different loss functions: for CELEBA-HQ we rely on WGAN-GP [24], Figure 2. Uncurated set of images produced by our style-based generator (conﬁg F) with the FFHQ dataset. Here we used a variation of the truncation trick [42, 5, 34] with ψ = 0.7 for resolutions 42 −322. Please see the accompanying video for more results. while FFHQ uses WGAN-GP for conﬁguration A and nonsaturating loss [22] with R1 regularization [44, 51, 14] for conﬁgurations B–F. We found these choices to give the best results. Our contributions do not modify the loss function. We observe that the style-based generator (E) improves FIDs quite signiﬁcantly over the traditional generator (B), almost 20%, corroborating the large-scale ImageNet measurements made in parallel work [6, 5]. Figure 2 shows an uncurated set of novel images generated from the FFHQ dataset using our generator. As conﬁrmed by the FIDs, the average quality is high, and even accessories such as eyeglasses and hats get successfully synthesized. For this ﬁgure, we avoided sampling from the extreme regions of W using the so-called truncation trick [42, 5, 34] — Appendix B details how the trick can be performed in W instead of Z. Note that our generator allows applying the truncation selectively to low resolutions only, so that highresolution details are not affected. All FIDs in this paper are computed without the truncation trick, and we only use it for illustrative purposes in Figure 2 and the video. All images are generated in 10242 2.2. Prior art Much of the work on GAN architectures has focused on improving the discriminator by, e.g., using multiple discriminators [18, 47, 11], multiresolution discrimination [60, 55], or self-attention [63]. The work on generator side has mostly focused on the exact distribution in the input latent space [5] or shaping the input latent space via Gaussian mixture models [4], clustering [48], or encouraging convexity [52]. Recent conditional generators feed the class identiﬁer through a separate embedding network to a large number of layers in the generator [46], while the latent is still provided though the input layer. A few authors have considered feeding parts of the latent code to multiple generator layers [9, 5]. In parallel work, Chen et al. [6] “self modulate” the generator using AdaINs, similarly to our work, but do not consider an intermediate latent space or noise inputs.\n\n3. Properties of the style-based generator\nOur generator architecture makes it possible to control the image synthesis via scale-speciﬁc modiﬁcations to the styles. We can view the mapping network and afﬁne transformations as a way to draw samples for each style from a learned distribution, and the synthesis network as a way to generate a novel image based on a collection of styles. The effects of each style are localized in the network, i.e., modifying a speciﬁc subset of the styles can be expected to affect only certain aspects of the image. To see the reason for this localization, let us consider how the AdaIN operation (Eq. 1) ﬁrst normalizes each channel to zero mean and unit variance, and only then applies scales and biases based on the style. The new per-channel statistics, as dictated by the style, modify the relative importance of features for the subsequent convolution operation, but they do not depend on the original statistics because of the normalization. Thus each style controls only one convolution before being overridden by the next AdaIN operation. 3.1. Style mixing To further encourage the styles to localize, we employ mixing regularization, where a given percentage of images are generated using two random latent codes instead of one during training. When generating such an image, we simply switch from one latent code to another — an operation we refer to as style mixing — at a randomly selected point in the synthesis network. To be speciﬁc, we run two latent codes z1, z2 through the mapping network, and have the corresponding w1, w2 control the styles so that w1 applies before the crossover point and w2 after it. This regularization technique prevents the network from assuming that adjacent styles are correlated. Table 2 shows how enabling mixing regularization dur- Source B Source A Coarse styles from source B Middle styles from source B Fine from B Figure 3. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated by copying a speciﬁed subset of styles from source B and taking the rest from source A. Copying the styles corresponding to coarse spatial resolutions (42 – 82) brings high-level aspects such as pose, general hair style, face shape, and eyeglasses from source B, while all colors (eyes, hair, lighting) and ﬁner facial features resemble A. If we instead copy the styles of middle resolutions (162 – 322) from B, we inherit smaller scale facial features, hair style, eyes open/closed from B, while the pose, general face shape, and eyeglasses from A are preserved. Finally, copying the ﬁne styles (642 – 10242) from B brings mainly the color scheme and microstructure. Mixing Number of latents during testing regularization 1 2 3 4 E 0% 4.42 8.22 12.88 17.41 50% 4.41 6.10 8.71 11.61 F 90% 4.40 5.11 6.88 9.03 100% 4.83 5.17 6.63 8.40 Table 2. FIDs in FFHQ for networks trained by enabling the mixing regularization for different percentage of training examples. Here we stress test the trained networks by randomizing 1 . . . 4 latents and the crossover points between them. Mixing regularization improves the tolerance to these adverse operations signiﬁcantly. Labels E and F refer to the conﬁgurations in Table 1. (a) Generated image (b) Stochastic variation (c) Standard deviation Figure 4. Examples of stochastic variation. (a) Two generated images. (b) Zoom-in with different realizations of input noise. While the overall appearance is almost identical, individual hairs are placed very differently. (c) Standard deviation of each pixel over 100 different realizations, highlighting which parts of the images are affected by the noise. The main areas are the hair, silhouettes, and parts of background, but there is also interesting stochastic variation in the eye reﬂections. Global aspects such as identity and pose are unaffected by stochastic variation. ing training improves the localization considerably, indicated by improved FIDs in scenarios where multiple latents are mixed at test time. Figure 3 presents examples of images synthesized by mixing two latent codes at various scales. We can see that each subset of styles controls meaningful high-level attributes of the image. 3.2. Stochastic variation There are many aspects in human portraits that can be regarded as stochastic, such as the exact placement of hairs, stubble, freckles, or skin pores. Any of these can be randomized without affecting our perception of the image as long as they follow the correct distribution. Let us consider how a traditional generator implements stochastic variation. Given that the only input to the network is through the input layer, the network needs to invent a way to generate spatially-varying pseudorandom numbers (a) (b) (c) (d) Figure 5. Effect of noise inputs at different layers of our generator. (a) Noise is applied to all layers. (b) No noise. (c) Noise in ﬁne layers only (642 – 10242). (d) Noise in coarse layers only (42 – 322). We can see that the artiﬁcial omission of noise leads to featureless “painterly” look. Coarse noise causes large-scale curling of hair and appearance of larger background features, while the ﬁne noise brings out the ﬁner curls of hair, ﬁner background detail, and skin pores. from earlier activations whenever they are needed. This consumes network capacity and hiding the periodicity of generated signal is difﬁcult — and not always successful, as evidenced by commonly seen repetitive patterns in generated images. Our architecture sidesteps these issues altogether by adding per-pixel noise after each convolution. Figure 4 shows stochastic realizations of the same underlying image, produced using our generator with different noise realizations. We can see that the noise affects only the stochastic aspects, leaving the overall composition and high-level aspects such as identity intact. Figure 5 further illustrates the effect of applying stochastic variation to different subsets of layers. Since these effects are best seen in animation, please consult the accompanying video for a demonstration of how changing the noise input of one layer leads to stochastic variation at a matching scale. We ﬁnd it interesting that the effect of noise appears tightly localized in the network. We hypothesize that at any point in the generator, there is pressure to introduce new content as soon as possible, and the easiest way for our network to create stochastic variation is to rely on the noise provided. A fresh set of noise is available for every layer, and thus there is no incentive to generate the stochastic effects from earlier activations, leading to a localized effect. (a) Distribution of (b) Mapping from (c) Mapping from features in training set Z to features W to features Figure 6. Illustrative example with two factors of variation (image features, e.g., masculinity and hair length). (a) An example training set where some combination (e.g., long haired males) is missing. (b) This forces the mapping from Z to image features to become curved so that the forbidden combination disappears in Z to prevent the sampling of invalid combinations. (c) The learned mapping from Z to W is able to “undo” much of the warping. 3.3. Separation of global effects from stochasticity The previous sections as well as the accompanying video demonstrate that while changes to the style have global effects (changing pose, identity, etc.), the noise affects only inconsequential stochastic variation (differently combed hair, beard, etc.). This observation is in line with style transfer literature, where it has been established that spatially invariant statistics (Gram matrix, channel-wise mean, variance, etc.) reliably encode the style of an image [20, 39] while spatially varying features encode a speciﬁc instance. In our style-based generator, the style affects the entire image because complete feature maps are scaled and biased with the same values. Therefore, global effects such as pose, lighting, or background style can be controlled coherently. Meanwhile, the noise is added independently to each pixel and is thus ideally suited for controlling stochastic variation. If the network tried to control, e.g., pose using the noise, that would lead to spatially inconsistent decisions that would then be penalized by the discriminator. Thus the network learns to use the global and local channels appropriately, without explicit guidance.\n\n4. Disentanglement studies\nThere are various deﬁnitions for disentanglement [54, 50, 2, 7, 19], but a common goal is a latent space that consists of linear subspaces, each of which controls one factor of variation. However, the sampling probability of each combination of factors in Z needs to match the corresponding density in the training data. As illustrated in Figure 6, this precludes the factors from being fully disentangled with typical datasets and input latent distributions.2 A major beneﬁt of our generator architecture is that the intermediate latent space W does not have to support sam- pling according to any ﬁxed distribution; its sampling density is induced by the learned piecewise continuous mapping f(z). This mapping can be adapted to “unwarp” W so that the factors of variation become more linear. We posit that there is pressure for the generator to do so, as it should be easier to generate realistic images based on a disentangled representation than based on an entangled representation. As such, we expect the training to yield a less entangled W in an unsupervised setting, i.e., when the factors of variation are not known in advance [10, 35, 49, 8, 26, 32, 7]. Unfortunately the metrics recently proposed for quantifying disentanglement [26, 32, 7, 19] require an encoder network that maps input images to latent codes. These metrics are ill-suited for our purposes since our baseline GAN lacks such an encoder. While it is possible to add an extra network for this purpose [8, 12, 15], we want to avoid investing effort into a component that is not a part of the actual solution. To this end, we describe two new ways of quantifying disentanglement, neither of which requires an encoder or known factors of variation, and are therefore computable for any image dataset and generator. 4.1. Perceptual path length As noted by Laine [37], interpolation of latent-space vectors may yield surprisingly non-linear changes in the image. For example, features that are absent in either endpoint may appear in the middle of a linear interpolation path. This is a sign that the latent space is entangled and the factors of variation are not properly separated. To quantify this effect, we can measure how drastic changes the image undergoes as we perform interpolation in the latent space. Intuitively, a less curved latent space should result in perceptually smoother transition than a highly curved latent space. As a basis for our metric, we use a perceptually-based pairwise image distance [65] that is calculated as a weighted difference between two VGG16 [58] embeddings, where the weights are ﬁt so that the metric agrees with human perceptual similarity judgments. If we subdivide a latent space interpolation path into linear segments, we can deﬁne the total perceptual length of this segmented path as the sum of perceptual differences over each segment, as reported by the image distance metric. A natural deﬁnition for the perceptual path length would be the limit of this sum under inﬁnitely ﬁne subdivision, but in practice we approximate it using a small subdivision epsilon ϵ = 10−4. The average perceptual path length in latent space Z, over all possible endpoints, is therefore lZ = E h 1 ϵ2 d \u0000 G(slerp(z1, z2; t)), G(slerp(z1, z2; t + ϵ)) \u0001i , (2)\n\nMethod\nPath length Separafull end bility B Traditional generator Z 412.0 415.3 10.78 D Style-based generator W 446.2 376.6 3.61 E + Add noise inputs W 200.5 160.6 3.54 + Mixing 50% W 231.5 182.1 3.51 F + Mixing 90% W 234.0 195.9 3.79 Table 3. Perceptual path lengths and separability scores for various generator architectures in FFHQ (lower is better). We perform the measurements in Z for the traditional network, and in W for stylebased ones. Making the network resistant to style mixing appears to distort the intermediate latent space W somewhat. We hypothesize that mixing makes it more difﬁcult for W to efﬁciently encode factors of variation that span multiple scales. ceptual distance between the resulting images. Here slerp denotes spherical interpolation [56], which is the most appropriate way of interpolating in our normalized input latent space [61]. To concentrate on the facial features instead of background, we crop the generated images to contain only the face prior to evaluating the pairwise image metric. As the metric d is quadratic [65], we divide by ϵ2. We compute the expectation by taking 100,000 samples. Computing the average perceptual path length in W is carried out in a similar fashion: lW = E h 1 ϵ2 d \u0000 g(lerp(f(z1), f(z2); t)), g(lerp(f(z1), f(z2); t + ϵ)) \u0001i , (3) where the only difference is that interpolation happens in W space. Because vectors in W are not normalized in any fashion, we use linear interpolation (lerp). Table 3 shows that this full-path length is substantially shorter for our style-based generator with noise inputs, indicating that W is perceptually more linear than Z. Yet, this measurement is in fact slightly biased in favor of the input latent space Z. If W is indeed a disentangled and “ﬂattened” mapping of Z, it may contain regions that are not on the input manifold — and are thus badly reconstructed by the generator — even between points that are mapped from the input manifold, whereas the input latent space Z has no such regions by deﬁnition. It is therefore to be expected that if we restrict our measure to path endpoints, i.e., t ∈{0, 1}, we should obtain a smaller lW while lZ is not affected. This is indeed what we observe in Table 3. Table 4 shows how path lengths are affected by the mapping network. We see that both traditional and style-based generators beneﬁt from having a mapping network, and additional depth generally improves the perceptual path length as well as FIDs. It is interesting that while lW improves in the traditional generator, lZ becomes considerably worse, illustrating our claim that the input latent space can indeed be arbitrarily entangled in GANs.\n\nMethod\nFID Path length Separafull end bility B Traditional 0 Z 5.25 412.0 415.3 10.78 Traditional 8 Z 4.87 896.2 902.0 170.29 Traditional 8 W 4.87 324.5 212.2 6.52 Style-based 0 Z 5.06 283.5 285.5 9.88 Style-based 1 W 4.60 219.9 209.4 6.81 Style-based 2 W 4.43 217.8 199.9 6.25 F Style-based 8 W 4.40 234.0 195.9 3.79 Table 4. The effect of a mapping network in FFHQ. The number in method name indicates the depth of the mapping network. We see that FID, separability, and path length all beneﬁt from having a mapping network, and this holds for both style-based and traditional generator architectures. Furthermore, a deeper mapping network generally performs better than a shallow one. 4.2. Linear separability If a latent space is sufﬁciently disentangled, it should be possible to ﬁnd direction vectors that consistently correspond to individual factors of variation. We propose another metric that quantiﬁes this effect by measuring how well the latent-space points can be separated into two distinct sets via a linear hyperplane, so that each set corresponds to a speciﬁc binary attribute of the image. In order to label the generated images, we train auxiliary classiﬁcation networks for a number of binary attributes, e.g., to distinguish male and female faces. In our tests, the classiﬁers had the same architecture as the discriminator we use (i.e., same as in [30]), and were trained using the CELEBA-HQ dataset that retains the 40 attributes available in the original CelebA dataset. To measure the separability of one attribute, we generate 200,000 images with z ∼P(z) and classify them using the auxiliary classiﬁcation network. We then sort the samples according to classiﬁer conﬁdence and remove the least conﬁdent half, yielding 100,000 labeled latent-space vectors. For each attribute, we ﬁt a linear SVM to predict the label based on the latent-space point — z for traditional and w for style-based — and classify the points by this plane. We then compute the conditional entropy H(Y |X) where X are the classes predicted by the SVM and Y are the classes determined by the pre-trained classiﬁer. This tells how much additional information is required to determine the true class of a sample, given that we know on which side of the hyperplane it lies. A low value suggests consistent latent space directions for the corresponding factor(s) of variation. We calculate the ﬁnal separability score as exp(P i H(Yi|Xi)), where i enumerates the 40 attributes. Similar to the inception score [53], the exponentiation brings the values from logarithmic to linear domain so that they are easier to compare. Tables 3 and 4 show that W is consistently better separable than Z, suggesting a less entangled representation. Figure 7. The FFHQ dataset offers a lot of variety in terms of age, ethnicity, viewpoint, lighting, and image background. Furthermore, increasing the depth of the mapping network improves both image quality and separability in W, which is in line with the hypothesis that the synthesis network inherently favors a disentangled input representation. Interestingly, adding a mapping network in front of a traditional generator results in severe loss of separability in Z but improves the situation in the intermediate latent space W, and the FID improves as well. This shows that even the traditional generator architecture performs better when we introduce an intermediate latent space that does not have to follow the distribution of the training data.\n\n5. Conclusion\nBased on both our results and parallel work by Chen et al. [6], it is becoming clear that the traditional GAN generator architecture is in every way inferior to a style-based design. This is true in terms of established quality metrics, and we further believe that our investigations to the separation of high-level attributes and stochastic effects, as well as the linearity of the intermediate latent space will prove fruitful in improving the understanding and controllability of GAN synthesis. We note that our average path length metric could easily be used as a regularizer during training, and perhaps some variant of the linear separability metric could act as one, too. In general, we expect that methods for directly shaping the intermediate latent space during training will provide interesting avenues for future work.\n\n6. Acknowledgements\nWe thank Jaakko Lehtinen, David Luebke, and Tuomas Kynk¨a¨anniemi for in-depth discussions and helpful comments; Janne Hellsten, Tero Kuosmanen, and Pekka J¨anis for compute infrastructure and help with the code release.\n\nReferences\n[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur,\nJ. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner,\nP. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and\nX. Zheng. TensorFlow: a system for large-scale machine\nlearning. In Proc. 12th USENIX Conference on Operating Systems Design and Implementation, OSDI’16, pages 265– 283, 2016. 9 [2] A. Achille and S. Soatto. On the emergence of invariance and disentangling in deep representations. CoRR, abs/1706.01350, 2017. 6 [3] D. Bau, J. Zhu, H. Strobelt, B. Zhou, J. B. Tenenbaum, W. T. Freeman, and A. Torralba. GAN dissection: Visualizing and understanding generative adversarial networks. In Proc. ICLR, 2019. 1 [4] M. Ben-Yosef and D. Weinshall. Gaussian mixture generative adversarial networks for diverse datasets, and the unsupervised clustering of images. CoRR, abs/1808.10356, 2018. 3 [5] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural image synthesis. CoRR, abs/1809.11096, 2018. 1, 3, 8 Figure 12. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN CAT dataset at 2562. FID computed for 50K images was 8.53. [6] T. Chen, M. Lucic, N. Houlsby, and S. Gelly. On self modulation for generative adversarial networks. CoRR, abs/1810.01365, 2018. 3, 8 [7] T. Q. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud. Isolating sources of disentanglement in variational autoencoders. CoRR, abs/1802.04942, 2018. 6 [8] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: interpretable representation learning by information maximizing generative adversarial nets. CoRR, abs/1606.03657, 2016. 6 [9] E. L. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a Laplacian pyramid of adversarial networks. CoRR, abs/1506.05751, 2015. 3 [10] G. Desjardins, A. Courville, and Y. Bengio. Disentangling factors of variation via generative entangling. CoRR, abs/1210.5474, 2012. 6 [11] T. Doan, J. Monteiro, I. Albuquerque, B. Mazoure, A. Durand, J. Pineau, and R. D. Hjelm. Online adaptative curriculum learning for GANs. CoRR, abs/1808.00020, 2018. 3 [12] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature learning. CoRR, abs/1605.09782, 2016. 6 [13] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. CoRR, abs/1411.5928, 2014. 1 [14] H. Drucker and Y. L. Cun. Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 3(6):991–997, 1992. 3 [15] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky,\nO. Mastropietro, and A. Courville. Adversarially learned inference. In Proc. ICLR, 2017. 6\n[16] V. Dumoulin, E. Perez, N. Schucher, F. Strub, H. d. Vries,\nA. Courville, and Y. Bengio.\nFeature-wise transformations. Distill, 2018. https://distill.pub/2018/feature-wisetransformations. 2 [17] V. Dumoulin, J. Shlens, and M. Kudlur. A learned representation for artistic style. CoRR, abs/1610.07629, 2016. 2 [18] I. P. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-adversarial networks. CoRR, abs/1611.01673, 2016. 3 [19] C. Eastwood and C. K. I. Williams. A framework for the quantitative evaluation of disentangled representations. In Proc. ICLR, 2018. 6 [20] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Proc. CVPR, 2016. 6 [21] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens. Exploring the structure of a real-time, arbitrary neural artistic stylization network. CoRR, abs/1705.06830, 2017. 2 [22] I. Goodfellow,\nJ. Pouget-Abadie,\nM. Mirza,\nB. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Networks. In NIPS, 2014. 1, 3, 9\n[23] W.-S. Z. Guang-Yuan Hao, Hong-Xing Yu. MIXGAN: learning concepts from different domains for mixture generation. CoRR, abs/1807.01659, 2018. 2 [24] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of Wasserstein GANs. CoRR, abs/1704.00028, 2017. 1, 2 [25] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and\nS. Hochreiter. GANs trained by a two time-scale update rule\nconverge to a local Nash equilibrium. In Proc. NIPS, pages 6626–6637, 2017. 2 [26] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,\nM. Botvinick, S. Mohamed, and A. Lerchner.\nbeta-vae: Learning basic visual concepts with a constrained variational framework. In Proc. ICLR, 2017. 6 [27] X. Huang and S. J. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. CoRR, abs/1703.06868, 2017. 1, 2 [28] X. Huang, M. Liu, S. J. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. CoRR, abs/1804.04732, 2018. 2 [29] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. 9 [30] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. 1, 2, 7, 8, 9 [31] V. Kazemi and J. Sullivan. One millisecond face alignment with an ensemble of regression trees. In Proc. CVPR, 2014. 8 [32] H. Kim and A. Mnih. Disentangling by factorising. In Proc. ICML, 2018. 6 [33] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 9 [34] D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. CoRR, abs/1807.03039, 2018. 3, 8 [35] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014. 6 [36] K. Kurach, M. Lucic, X. Zhai, M. Michalski, and S. Gelly. The gan landscape: Losses, architectures, regularization, and normalization. CoRR, abs/1807.04720, 2018. 1 [37] S. Laine. Feature-based metrics for exploring the latent space of generative models. ICLR workshop poster, 2018. 1, 6 [38] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang. Universal style transfer via feature transforms. In Proc. NIPS, 2017. 2 [39] Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural style transfer. CoRR, abs/1701.01036, 2017. 6 [40] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are GANs created equal? a large-scale study. CoRR, abs/1711.10337, 2017. 1 [41] A. L. Maas, A. Y. Hannun, and A. Ng. Rectiﬁer nonlinearities improve neural network acoustic models. In Proc. International Conference on Machine Learning (ICML), volume 30, 2013. 9 [42] M. Marchesi. Megapixel size image creation using generative adversarial networks. CoRR, abs/1706.00082, 2017. 3, 8 [43] L. Matthey,\nI. Higgins,\nD. Hassabis,\nand A. Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. 6 [44] L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge? CoRR, abs/1801.04406, 2018. 1, 3, 9 [45] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. CoRR, abs/1802.05957, 2018. 1, 9 [46] T. Miyato and M. Koyama. cGANs with projection discriminator. CoRR, abs/1802.05637, 2018. 3 [47] G. Mordido, H. Yang, and C. Meinel. Dropout-gan: Learning from a dynamic ensemble of discriminators. CoRR, abs/1807.11346, 2018. 3 [48] S. Mukherjee, H. Asnani, E. Lin, and S. Kannan. ClusterGAN : Latent space clustering in generative adversarial networks. CoRR, abs/1809.03627, 2018. 3 [49] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proc. ICML, 2014. 6 [50] K. Ridgeway. A survey of inductive biases for factorial representation-learning. CoRR, abs/1612.05299, 2016. 6 [51] A. S. Ross and F. Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. CoRR, abs/1711.09404,\n2017. 3\n[52] T. Sainburg, M. Thielk, B. Theilman, B. Migliori, and\nT. Gentner. Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourage convex latent distributions. CoRR, abs/1807.06650,\n2018. 1, 3\n[53] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung,\nA. Radford, and X. Chen. Improved techniques for training\nGANs. In NIPS, 2016. 7 [54] J. Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863–879, 1992. 6 [55] R. Sharma, S. Barratt, S. Ermon, and V. Pande. Improved training with curriculum gans. CoRR, abs/1807.09295, 2018. 3 [56] K. Shoemake. Animating rotation with quaternion curves. In Proc. SIGGRAPH ’85, 1985. 7 [57] A. Siarohin, E. Sangineto, and N. Sebe. Whitening and coloring transform for GANs. CoRR, abs/1806.00420, 2018. 2 [58] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 6 [59] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning\nResearch, 15:1929–1958, 2014. 9 [60] T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. CoRR, abs/1711.11585, 2017. 3 [61] T. White. Sampling generative networks: Notes on a few effective techniques. CoRR, abs/1609.04468, 2016. 7 [62] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015. 9 [63] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. CoRR, abs/1805.08318, 2018. 3, 9 [64] R. Zhang. Making convolutional networks shift-invariant again, 2019. 2, 9 [65] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. CVPR, 2018. 6, 7\n--- FOOTNOTES 12M to 25M images. We use the same learning rates as 5M 10M 15M 20M 25M 5M 10M 15M 20M 25M\n\nA. The FFHQ dataset\nWe have collected a new dataset of human faces, FlickrFaces-HQ (FFHQ), consisting of 70,000 high-quality images at 10242 resolution (Figure 7). The dataset includes vastly more variation than CELEBA-HQ [30] in terms of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr ψ = 1 ψ = 0.7 ψ = 0.5 ψ = 0 ψ = −0.5 ψ = −1 Figure 8. The effect of truncation trick as a function of style scale ψ. When we fade ψ →0, all faces converge to the “mean” face of FFHQ. This face is similar for all trained networks, and the interpolation towards it never seems to cause artifacts. By applying negative scaling to styles, we get the corresponding opposite or “anti-face”. It is interesting that various high-level attributes often ﬂip between the opposites, including viewpoint, glasses, age, coloring, hair length, and often gender. (thus inheriting all the biases of that website) and automatically aligned [31] and cropped. Only images under permissive licenses were collected. Various automatic ﬁlters were used to prune the set, and ﬁnally Mechanical Turk allowed us to remove the occasional statues, paintings, or photos of photos. We have made the dataset publicly available at https://github.com/NVlabs/ffhq-dataset\n\nB. Truncation trick in W\nIf we consider the distribution of training data, it is clear that areas of low density are poorly represented and thus likely to be difﬁcult for the generator to learn. This is a signiﬁcant open problem in all generative modeling techniques. However, it is known that drawing latent vectors from a truncated [42, 5] or otherwise shrunk [34] sampling space tends to improve average image quality, although some amount of variation is lost. We can follow a similar strategy. To begin, we compute the center of mass of W as ¯w = Ez∼P (z)[f(z)]. In case of FFHQ this point represents a sort of an average face (Figure 8, ψ = 0). We can then scale the deviation of a given w from the center as w′ = ¯w + ψ(w −¯w), where ψ < 1. While Brock et al. [5] observe that only a subset of networks is amenable to such truncation even when orthogonal regularization is used, truncation in W space seems to work reliably even without changes to the loss function.\n\nC. Hyperparameters and training details\nWe build upon the ofﬁcial TensorFlow [1] implementation of Progressive GANs by Karras et al. [30], from which we inherit most of the training details.3 This original setup corresponds to conﬁguration A in Table 1. In particular, we use the same discriminator architecture, resolutiondependent minibatch sizes, Adam [33] hyperparameters, and exponential moving average of the generator. We enable mirror augmentation for CelebA-HQ and FFHQ, but disable it for LSUN. Our training time is approximately one week on an NVIDIA DGX-1 with 8 Tesla V100 GPUs. For our improved baseline (B in Table 1), we make several modiﬁcations to improve the overall result quality. We replace the nearest-neighbor up/downsampling in both networks with bilinear sampling, which we implement by lowpass ﬁltering the activations with a separable 2nd order binomial ﬁlter after each upsampling layer and before each downsampling layer [64]. We implement progressive growing the same way as Karras et al. [30], but we start from 82 images instead of 42. For the FFHQ dataset, we switch from WGAN-GP to the non-saturating loss [22] with R1 regularization [44] using γ = 10. With R1 we found that the FID scores keep decreasing for considerably longer than with WGAN-GP, and we thus increase the training time from Karras et al. [30] for FFHQ, but we found that setting the learning rate to 0.002 instead of 0.003 for 5122 and 10242 leads to better stability with CelebA-HQ. For our style-based generator (F in Table 1), we use leaky ReLU [41] with α = 0.2 and equalized learning rate [30] for all layers. We use the same feature map counts in our convolution layers as Karras et al. [30]. Our mapping network consists of 8 fully-connected layers, and the dimensionality of all input and output activations — including z and w — is 512. We found that increasing the depth of the mapping network tends to make the training unstable with high learning rates. We thus reduce the learning rate by two orders of magnitude for the mapping network, i.e., λ′ = 0.01·λ. We initialize all weights of the convolutional, fully-connected, and afﬁne transform layers using N(0, 1). The constant input in synthesis network is initialized to one. The biases and noise scaling factors are initialized to zero, except for the biases associated with ys that we initialize to one. The classiﬁers used by our separability metric (Section 4.2) have the same architecture as our discriminator except that minibatch standard deviation [30] is disabled. We use the learning rate of 10−3, minibatch size of 8, Adam optimizer, and training length of 150,000 images. The classiﬁers are trained independently of generators, and the same 40 classiﬁers, one for each CelebA attribute, are used FID Path length 10 500 9 400 8 300 7 200 6 100 5 0 resolution Full resolution Full 40 0 Traditional (B) Traditional (B) Style-based (F) Style-based (F), full Style-based (F), end Figure 9. FID and perceptual path length metrics over the course of training in our conﬁgurations B and F using the FFHQ dataset. Horizontal axis denotes the number of training images seen by the discriminator. The dashed vertical line at 8.4M images marks the point when training has progressed to full 10242 resolution. On the right, we show only one curve for the traditional generator’s path length measurements, because there is no discernible difference between full-path and endpoint sampling in Z. for measuring the separability metric for all generators. We will release the pre-trained classiﬁer networks so that our measurements can be reproduced. We do not use batch normalization [29], spectral normalization [45], attention mechanisms [63], dropout [59], or pixelwise feature vector normalization [30] in our networks.\n\nD. Training convergence\nFigure 9 shows how the FID and perceptual path length metrics evolve during the training of our conﬁgurations B and F with the FFHQ dataset. With R1 regularization active in both conﬁgurations, FID continues to slowly decrease as the training progresses, motivating our choice to increase the training time from 12M images to 25M images. Even when the training has reached the full 10242 resolution, the slowly rising path lengths indicate that the improvements in FID come at the cost of a more entangled representation. Considering future work, it is an interesting question whether this is unavoidable, or if it were possible to encourage shorter path lengths without compromising the convergence of FID.\n\nE. Other datasets\nFigures 10, 11, and 12 show an uncurated set of results for LSUN [62] BEDROOM, CARS, and CATS, respectively. In these images we used the truncation trick from Appendix Bwith ψ = 0.7 for resolutions 42 −322. The accompanying video provides results for style mixing and stochastic variation tests. As can be seen therein, in case of Figure 10. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN BEDROOM dataset at 2562. FID computed for 50K images was 2.65. BEDROOM the coarse styles basically control the viewpoint of the camera, middle styles select the particular furniture, and ﬁne styles deal with colors and smaller details of materials. In CARS the effects are roughly similar. Stochastic variation affects primarily the fabrics in BEDROOM, backgrounds and headlamps in CARS, and fur, background, and interestingly, the positioning of paws in CATS. Somewhat surprisingly the wheels of a car never seem to rotate based on stochastic inputs. These datasets were trained using the same setup as FFHQ for the duration of 70M images for BEDROOM and CATS, and 46M for CARS. We suspect that the results for BEDROOM are starting to approach the limits of the training data, as in many images the most objectionable issues are the severe compression artifacts that have been inherited from the low-quality training data. CARS has much higher quality training data that also allows higher spatial resolution (512 × 384 instead of 2562), and CATS continues to be a difﬁcult dataset due to the high intrinsic variation in poses, zoom levels, and backgrounds. Figure 11. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN CAR dataset at 512 × 384. FID computed for 50K images was 3.27.", "persona": "Student", "response": "### 1. Overview\nGene­rative adversarial networks have­ fundamentally altered dee­p learning, especially for crafting highly re­alistic synthetic images. StyleGANs re­present a category of adve­rsarial networks that integrate an innovative­ style-oriented ge­nerator structure to attain unseen oversight ove­r visual qualities in generated images. In this pie­ce, we will investigate pre­cisely what StyleGANs are, the­ir design and pivotal parts, different variations of Style­GANs, how they develop past conve­ntional adversarial networks, their capabilities, strengths, and limitations. What are Style Generative Adversarial Networks (StyleGANs)? StyleGANs are GANs that use a style-based generator to produce high-quality images with fine-grained control over their visual attributes. Developed by researchers at Nvidia, StyleGANs introduce an intermediate latent space (W space) between the input noise vector (Z space) and the generator. This W space consists of style vectors that are fed to each layer of the generator, allowing granular control over styles at different resolutions.\n\n### 2. Background\nTraditional GANs consist of two neural networks - a generator (G) that creates synthetic images from random noise and a discriminator (D) that tries to distinguish between real and synthetic images. They are trained adversarially where G tries to better fool D over time. This architecture allows granular control over visual attributes at various scales leading to high-quality image generation and editing capabilities. Refines the StyleGAN2 framework further for best results. Among these iterations, StyleGAN2 and StyleGAN2-ADA offer the best trade-off currently between image quality, training stability, and ease of use for most applications.\n\n### 3. Methods\nAn intermediate mapping network between input noise (Z) and the generator (G). This produces the style vector (W) by mapping Z via multilayer perception (MLP). W vectors are then transformed into styles via learned affine transformations and fed to layers of the synthesis network (the generator G). Stochastic variation is introduced within G via per-pixel noise injection after each convolution layer. A technique called style mixing regularises the generator by combining styles from different inputs. The original StyleGAN architecture introduced key innovations like the intermediate mapping network, style-based generator, and AdaIN. New generator regularisation and noise injection techniques reduce artefacts.\n\n### 4. Results\nThe Generator Architecture Mapping Network Changes the input noise vector (Z) to an intermediate latent space (W) via an 8-layer MLP. Synthesis Network The actual generator consists of convolutions. It takes in W vectors transformed into style vectors via affine transformations to generate the image through upsampling. Noise is injected at each layer to introduce stochasticity. Latent Space (W Space) Concept of Style-Based Synthesis This works by first using a mapping network to convert the input noise vector (Z) to the latent W space.\n\n### 5. Takeaways\nFoundation of StyleGANs Types of StyleGANs 1. StyleGAN The training was more reliable across a wider range of datasets compared to StyleGAN. 3. StyleGAN2-ADA 4. StyleGAN3 Drastically improved training speed through executor neural network parallelism. Latent space is more disentangled leading to better editing control. Comparison of StyleGAN models Key Components of StyleGAN Architecture The generator is the most crucial component of any GAN. In StyleGAN, the generator consists of two main subnetworks: Advantages of StyleGANs Every minute attribute can be tuned by manipulating specific style vectors in the W latent space without affecting other attributes. This level of control is unmatched by other GAN models. 2. Reliable and Stable Training Dynamics Improving training stability was a key focus in later StyleGAN editions. StyleGAN2 and beyond models can reliably achieve convergence across datasets.", "paper_id": "04", "story_id": "04-06"}
{"paper": "Abstract\nWe propose an alternative generator architecture for generative adversarial networks, borrowing from style transfer literature. The new architecture leads to an automatically learned, unsupervised separation of high-level attributes (e.g., pose and identity when trained on human faces) and stochastic variation in the generated images (e.g., freckles, hair), and it enables intuitive, scale-speciﬁc control of the synthesis. The new generator improves the state-of-the-art in terms of traditional distribution quality metrics, leads to demonstrably better interpolation properties, and also better disentangles the latent factors of variation. To quantify interpolation quality and disentanglement, we propose two new, automated methods that are applicable to any generator architecture. Finally, we introduce a new, highly varied and high-quality dataset of human faces.\n\n1. Introduction\nThe resolution and quality of images produced by generative methods — especially generative adversarial networks (GAN) [22] — have seen rapid improvement recently [30, 45, 5]. Yet the generators continue to operate as black boxes, and despite recent efforts [3], the understanding of various aspects of the image synthesis process, e.g., the origin of stochastic features, is still lacking. The properties of the latent space are also poorly understood, and the commonly demonstrated latent space interpolations [13, 52, 37] provide no quantitative way to compare different generators against each other. Motivated by style transfer literature [27], we re-design the generator architecture in a way that exposes novel ways to control the image synthesis process. Our generator starts from a learned constant input and adjusts the “style” of the image at each convolution layer based on the latent code, therefore directly controlling the strength of image features at different scales. Combined with noise injected directly into the network, this architectural change leads to automatic, unsupervised separation of high-level attributes Timo Aila NVIDIA taila@nvidia.com (e.g., pose, identity) from stochastic variation (e.g., freckles, hair) in the generated images, and enables intuitive scale-speciﬁc mixing and interpolation operations. We do not modify the discriminator or the loss function in any way, and our work is thus orthogonal to the ongoing discussion about GAN loss functions, regularization, and hyperparameters [24, 45, 5, 40, 44, 36]. Our generator embeds the input latent code into an intermediate latent space, which has a profound effect on how the factors of variation are represented in the network. The input latent space must follow the probability density of the training data, and we argue that this leads to some degree of unavoidable entanglement. Our intermediate latent space is free from that restriction and is therefore allowed to be disentangled. As previous methods for estimating the degree of latent space disentanglement are not directly applicable in our case, we propose two new automated metrics — perceptual path length and linear separability — for quantifying these aspects of the generator. Using these metrics, we show that compared to a traditional generator architecture, our generator admits a more linear, less entangled representation of different factors of variation. Finally, we present a new dataset of human faces (Flickr-Faces-HQ, FFHQ) that offers much higher quality and covers considerably wider variation than existing high-resolution datasets (Appendix A). We have made this dataset publicly available, along with our source code and pre-trained networks.1 The accompanying video can be found under the same link.\n\n2. Style-based generator\nTraditionally the latent code is provided to the generator through an input layer, i.e., the ﬁrst layer of a feedforward network (Figure 1a). We depart from this design by omitting the input layer altogether and starting from a learned constant instead (Figure 1b, right). Given a latent code z in the input latent space Z, a non-linear mapping network f : Z →W ﬁrst produces w ∈W (Figure 1b, left). For simplicity, we set the dimensionality of both Noise Latent Latent Synthesis network Normalize Normalize B Const 4×4×512 Mapping network style Fully-connected A AdaIN PixelNorm Conv 3×3 FC Conv 3×3 B FC style A AdaIN FC PixelNorm 4×4 FC FC 4×4 Upsample Upsample FC FC Conv 3×3 Conv 3×3 B style FC PixelNorm A AdaIN Conv 3×3 Conv 3×3 A B PixelNorm style AdaIN 8×8 8×8 (a) Traditional (b) Style-based generator Figure 1. While a traditional generator [30] feeds the latent code though the input layer only, we ﬁrst map the input to an intermediate latent space W, which then controls the generator through adaptive instance normalization (AdaIN) at each convolution layer. Gaussian noise is added after each convolution, before evaluating the nonlinearity. Here “A” stands for a learned afﬁne transform, and “B” applies learned per-channel scaling factors to the noise input. The mapping network f consists of 8 layers and the synthesis network g consists of 18 layers — two for each resolution (42 −10242). The output of the last layer is converted to RGB using a separate 1 × 1 convolution, similar to Karras et al. [30]. Our generator has a total of 26.2M trainable parameters, compared to 23.1M in the traditional generator. spaces to 512, and the mapping f is implemented using an 8-layer MLP, a decision we will analyze in Section 4.1. Learned afﬁne transformations then specialize w to styles y = (ys, yb) that control adaptive instance normalization (AdaIN) [27, 17, 21, 16] operations after each convolution layer of the synthesis network g. The AdaIN operation is deﬁned as AdaIN(xi, y) = ys,i xi −µ(xi) σ(xi) + yb,i, (1) where each feature map xi is normalized separately, and then scaled and biased using the corresponding scalar components from style y. Thus the dimensionality of y is twice the number of feature maps on that layer. Comparing our approach to style transfer, we compute the spatially invariant style y from vector w instead of an example image. We choose to reuse the word “style” for y because similar network architectures are already used for feedforward style transfer [27], unsupervised image-toimage translation [28], and domain mixtures [23]. Compared to more general feature transforms [38, 57], AdaIN is particularly well suited for our purposes due to its efﬁciency and compact representation.\n\nMethod\nCelebA-HQ FFHQ A Baseline Progressive GAN [30] 7.79 8.04 B + Tuning (incl. bilinear up/down) 6.11 5.25 C + Add mapping and styles 5.34 4.85 D + Remove traditional input 5.07 4.88 E + Add noise inputs 5.06 4.42 F + Mixing regularization 5.17 4.40 Table 1. Fr´echet inception distance (FID) for various generator designs (lower is better). In this paper we calculate the FIDs using 50,000 images drawn randomly from the training set, and report the lowest distance encountered over the course of training. Finally, we provide our generator with a direct means to generate stochastic detail by introducing explicit noise inputs. These are single-channel images consisting of uncorrelated Gaussian noise, and we feed a dedicated noise image to each layer of the synthesis network. The noise image is broadcasted to all feature maps using learned perfeature scaling factors and then added to the output of the corresponding convolution, as illustrated in Figure 1b. The implications of adding the noise inputs are discussed in Sections 3.2 and 3.3. 2.1. Quality of generated images Before studying the properties of our generator, we demonstrate experimentally that the redesign does not compromise image quality but, in fact, improves it considerably. Table 1 gives Fr´echet inception distances (FID) [25] for various generator architectures in CELEBA-HQ [30] and our new FFHQ dataset (Appendix A). Results for other datasets are given in Appendix E. Our baseline conﬁguration (A) is the Progressive GAN setup of Karras et al. [30], from which we inherit the networks and all hyperparameters except where stated otherwise. We ﬁrst switch to an improved baseline (B) by using bilinear up/downsampling operations [64], longer training, and tuned hyperparameters. A detailed description of training setups and hyperparameters is included in Appendix C. We then improve this new baseline further by adding the mapping network and AdaIN operations (C), and make a surprising observation that the network no longer beneﬁts from feeding the latent code into the ﬁrst convolution layer. We therefore simplify the architecture by removing the traditional input layer and starting the image synthesis from a learned 4 × 4 × 512 constant tensor (D). We ﬁnd it quite remarkable that the synthesis network is able to produce meaningful results even though it receives input only through the styles that control the AdaIN operations. Finally, we introduce the noise inputs (E) that improve the results further, as well as novel mixing regularization (F) that decorrelates neighboring styles and enables more ﬁnegrained control over the generated imagery (Section 3.1). We evaluate our methods using two different loss functions: for CELEBA-HQ we rely on WGAN-GP [24], Figure 2. Uncurated set of images produced by our style-based generator (conﬁg F) with the FFHQ dataset. Here we used a variation of the truncation trick [42, 5, 34] with ψ = 0.7 for resolutions 42 −322. Please see the accompanying video for more results. while FFHQ uses WGAN-GP for conﬁguration A and nonsaturating loss [22] with R1 regularization [44, 51, 14] for conﬁgurations B–F. We found these choices to give the best results. Our contributions do not modify the loss function. We observe that the style-based generator (E) improves FIDs quite signiﬁcantly over the traditional generator (B), almost 20%, corroborating the large-scale ImageNet measurements made in parallel work [6, 5]. Figure 2 shows an uncurated set of novel images generated from the FFHQ dataset using our generator. As conﬁrmed by the FIDs, the average quality is high, and even accessories such as eyeglasses and hats get successfully synthesized. For this ﬁgure, we avoided sampling from the extreme regions of W using the so-called truncation trick [42, 5, 34] — Appendix B details how the trick can be performed in W instead of Z. Note that our generator allows applying the truncation selectively to low resolutions only, so that highresolution details are not affected. All FIDs in this paper are computed without the truncation trick, and we only use it for illustrative purposes in Figure 2 and the video. All images are generated in 10242 2.2. Prior art Much of the work on GAN architectures has focused on improving the discriminator by, e.g., using multiple discriminators [18, 47, 11], multiresolution discrimination [60, 55], or self-attention [63]. The work on generator side has mostly focused on the exact distribution in the input latent space [5] or shaping the input latent space via Gaussian mixture models [4], clustering [48], or encouraging convexity [52]. Recent conditional generators feed the class identiﬁer through a separate embedding network to a large number of layers in the generator [46], while the latent is still provided though the input layer. A few authors have considered feeding parts of the latent code to multiple generator layers [9, 5]. In parallel work, Chen et al. [6] “self modulate” the generator using AdaINs, similarly to our work, but do not consider an intermediate latent space or noise inputs.\n\n3. Properties of the style-based generator\nOur generator architecture makes it possible to control the image synthesis via scale-speciﬁc modiﬁcations to the styles. We can view the mapping network and afﬁne transformations as a way to draw samples for each style from a learned distribution, and the synthesis network as a way to generate a novel image based on a collection of styles. The effects of each style are localized in the network, i.e., modifying a speciﬁc subset of the styles can be expected to affect only certain aspects of the image. To see the reason for this localization, let us consider how the AdaIN operation (Eq. 1) ﬁrst normalizes each channel to zero mean and unit variance, and only then applies scales and biases based on the style. The new per-channel statistics, as dictated by the style, modify the relative importance of features for the subsequent convolution operation, but they do not depend on the original statistics because of the normalization. Thus each style controls only one convolution before being overridden by the next AdaIN operation. 3.1. Style mixing To further encourage the styles to localize, we employ mixing regularization, where a given percentage of images are generated using two random latent codes instead of one during training. When generating such an image, we simply switch from one latent code to another — an operation we refer to as style mixing — at a randomly selected point in the synthesis network. To be speciﬁc, we run two latent codes z1, z2 through the mapping network, and have the corresponding w1, w2 control the styles so that w1 applies before the crossover point and w2 after it. This regularization technique prevents the network from assuming that adjacent styles are correlated. Table 2 shows how enabling mixing regularization dur- Source B Source A Coarse styles from source B Middle styles from source B Fine from B Figure 3. Two sets of images were generated from their respective latent codes (sources A and B); the rest of the images were generated by copying a speciﬁed subset of styles from source B and taking the rest from source A. Copying the styles corresponding to coarse spatial resolutions (42 – 82) brings high-level aspects such as pose, general hair style, face shape, and eyeglasses from source B, while all colors (eyes, hair, lighting) and ﬁner facial features resemble A. If we instead copy the styles of middle resolutions (162 – 322) from B, we inherit smaller scale facial features, hair style, eyes open/closed from B, while the pose, general face shape, and eyeglasses from A are preserved. Finally, copying the ﬁne styles (642 – 10242) from B brings mainly the color scheme and microstructure. Mixing Number of latents during testing regularization 1 2 3 4 E 0% 4.42 8.22 12.88 17.41 50% 4.41 6.10 8.71 11.61 F 90% 4.40 5.11 6.88 9.03 100% 4.83 5.17 6.63 8.40 Table 2. FIDs in FFHQ for networks trained by enabling the mixing regularization for different percentage of training examples. Here we stress test the trained networks by randomizing 1 . . . 4 latents and the crossover points between them. Mixing regularization improves the tolerance to these adverse operations signiﬁcantly. Labels E and F refer to the conﬁgurations in Table 1. (a) Generated image (b) Stochastic variation (c) Standard deviation Figure 4. Examples of stochastic variation. (a) Two generated images. (b) Zoom-in with different realizations of input noise. While the overall appearance is almost identical, individual hairs are placed very differently. (c) Standard deviation of each pixel over 100 different realizations, highlighting which parts of the images are affected by the noise. The main areas are the hair, silhouettes, and parts of background, but there is also interesting stochastic variation in the eye reﬂections. Global aspects such as identity and pose are unaffected by stochastic variation. ing training improves the localization considerably, indicated by improved FIDs in scenarios where multiple latents are mixed at test time. Figure 3 presents examples of images synthesized by mixing two latent codes at various scales. We can see that each subset of styles controls meaningful high-level attributes of the image. 3.2. Stochastic variation There are many aspects in human portraits that can be regarded as stochastic, such as the exact placement of hairs, stubble, freckles, or skin pores. Any of these can be randomized without affecting our perception of the image as long as they follow the correct distribution. Let us consider how a traditional generator implements stochastic variation. Given that the only input to the network is through the input layer, the network needs to invent a way to generate spatially-varying pseudorandom numbers (a) (b) (c) (d) Figure 5. Effect of noise inputs at different layers of our generator. (a) Noise is applied to all layers. (b) No noise. (c) Noise in ﬁne layers only (642 – 10242). (d) Noise in coarse layers only (42 – 322). We can see that the artiﬁcial omission of noise leads to featureless “painterly” look. Coarse noise causes large-scale curling of hair and appearance of larger background features, while the ﬁne noise brings out the ﬁner curls of hair, ﬁner background detail, and skin pores. from earlier activations whenever they are needed. This consumes network capacity and hiding the periodicity of generated signal is difﬁcult — and not always successful, as evidenced by commonly seen repetitive patterns in generated images. Our architecture sidesteps these issues altogether by adding per-pixel noise after each convolution. Figure 4 shows stochastic realizations of the same underlying image, produced using our generator with different noise realizations. We can see that the noise affects only the stochastic aspects, leaving the overall composition and high-level aspects such as identity intact. Figure 5 further illustrates the effect of applying stochastic variation to different subsets of layers. Since these effects are best seen in animation, please consult the accompanying video for a demonstration of how changing the noise input of one layer leads to stochastic variation at a matching scale. We ﬁnd it interesting that the effect of noise appears tightly localized in the network. We hypothesize that at any point in the generator, there is pressure to introduce new content as soon as possible, and the easiest way for our network to create stochastic variation is to rely on the noise provided. A fresh set of noise is available for every layer, and thus there is no incentive to generate the stochastic effects from earlier activations, leading to a localized effect. (a) Distribution of (b) Mapping from (c) Mapping from features in training set Z to features W to features Figure 6. Illustrative example with two factors of variation (image features, e.g., masculinity and hair length). (a) An example training set where some combination (e.g., long haired males) is missing. (b) This forces the mapping from Z to image features to become curved so that the forbidden combination disappears in Z to prevent the sampling of invalid combinations. (c) The learned mapping from Z to W is able to “undo” much of the warping. 3.3. Separation of global effects from stochasticity The previous sections as well as the accompanying video demonstrate that while changes to the style have global effects (changing pose, identity, etc.), the noise affects only inconsequential stochastic variation (differently combed hair, beard, etc.). This observation is in line with style transfer literature, where it has been established that spatially invariant statistics (Gram matrix, channel-wise mean, variance, etc.) reliably encode the style of an image [20, 39] while spatially varying features encode a speciﬁc instance. In our style-based generator, the style affects the entire image because complete feature maps are scaled and biased with the same values. Therefore, global effects such as pose, lighting, or background style can be controlled coherently. Meanwhile, the noise is added independently to each pixel and is thus ideally suited for controlling stochastic variation. If the network tried to control, e.g., pose using the noise, that would lead to spatially inconsistent decisions that would then be penalized by the discriminator. Thus the network learns to use the global and local channels appropriately, without explicit guidance.\n\n4. Disentanglement studies\nThere are various deﬁnitions for disentanglement [54, 50, 2, 7, 19], but a common goal is a latent space that consists of linear subspaces, each of which controls one factor of variation. However, the sampling probability of each combination of factors in Z needs to match the corresponding density in the training data. As illustrated in Figure 6, this precludes the factors from being fully disentangled with typical datasets and input latent distributions.2 A major beneﬁt of our generator architecture is that the intermediate latent space W does not have to support sam- pling according to any ﬁxed distribution; its sampling density is induced by the learned piecewise continuous mapping f(z). This mapping can be adapted to “unwarp” W so that the factors of variation become more linear. We posit that there is pressure for the generator to do so, as it should be easier to generate realistic images based on a disentangled representation than based on an entangled representation. As such, we expect the training to yield a less entangled W in an unsupervised setting, i.e., when the factors of variation are not known in advance [10, 35, 49, 8, 26, 32, 7]. Unfortunately the metrics recently proposed for quantifying disentanglement [26, 32, 7, 19] require an encoder network that maps input images to latent codes. These metrics are ill-suited for our purposes since our baseline GAN lacks such an encoder. While it is possible to add an extra network for this purpose [8, 12, 15], we want to avoid investing effort into a component that is not a part of the actual solution. To this end, we describe two new ways of quantifying disentanglement, neither of which requires an encoder or known factors of variation, and are therefore computable for any image dataset and generator. 4.1. Perceptual path length As noted by Laine [37], interpolation of latent-space vectors may yield surprisingly non-linear changes in the image. For example, features that are absent in either endpoint may appear in the middle of a linear interpolation path. This is a sign that the latent space is entangled and the factors of variation are not properly separated. To quantify this effect, we can measure how drastic changes the image undergoes as we perform interpolation in the latent space. Intuitively, a less curved latent space should result in perceptually smoother transition than a highly curved latent space. As a basis for our metric, we use a perceptually-based pairwise image distance [65] that is calculated as a weighted difference between two VGG16 [58] embeddings, where the weights are ﬁt so that the metric agrees with human perceptual similarity judgments. If we subdivide a latent space interpolation path into linear segments, we can deﬁne the total perceptual length of this segmented path as the sum of perceptual differences over each segment, as reported by the image distance metric. A natural deﬁnition for the perceptual path length would be the limit of this sum under inﬁnitely ﬁne subdivision, but in practice we approximate it using a small subdivision epsilon ϵ = 10−4. The average perceptual path length in latent space Z, over all possible endpoints, is therefore lZ = E h 1 ϵ2 d \u0000 G(slerp(z1, z2; t)), G(slerp(z1, z2; t + ϵ)) \u0001i , (2)\n\nMethod\nPath length Separafull end bility B Traditional generator Z 412.0 415.3 10.78 D Style-based generator W 446.2 376.6 3.61 E + Add noise inputs W 200.5 160.6 3.54 + Mixing 50% W 231.5 182.1 3.51 F + Mixing 90% W 234.0 195.9 3.79 Table 3. Perceptual path lengths and separability scores for various generator architectures in FFHQ (lower is better). We perform the measurements in Z for the traditional network, and in W for stylebased ones. Making the network resistant to style mixing appears to distort the intermediate latent space W somewhat. We hypothesize that mixing makes it more difﬁcult for W to efﬁciently encode factors of variation that span multiple scales. ceptual distance between the resulting images. Here slerp denotes spherical interpolation [56], which is the most appropriate way of interpolating in our normalized input latent space [61]. To concentrate on the facial features instead of background, we crop the generated images to contain only the face prior to evaluating the pairwise image metric. As the metric d is quadratic [65], we divide by ϵ2. We compute the expectation by taking 100,000 samples. Computing the average perceptual path length in W is carried out in a similar fashion: lW = E h 1 ϵ2 d \u0000 g(lerp(f(z1), f(z2); t)), g(lerp(f(z1), f(z2); t + ϵ)) \u0001i , (3) where the only difference is that interpolation happens in W space. Because vectors in W are not normalized in any fashion, we use linear interpolation (lerp). Table 3 shows that this full-path length is substantially shorter for our style-based generator with noise inputs, indicating that W is perceptually more linear than Z. Yet, this measurement is in fact slightly biased in favor of the input latent space Z. If W is indeed a disentangled and “ﬂattened” mapping of Z, it may contain regions that are not on the input manifold — and are thus badly reconstructed by the generator — even between points that are mapped from the input manifold, whereas the input latent space Z has no such regions by deﬁnition. It is therefore to be expected that if we restrict our measure to path endpoints, i.e., t ∈{0, 1}, we should obtain a smaller lW while lZ is not affected. This is indeed what we observe in Table 3. Table 4 shows how path lengths are affected by the mapping network. We see that both traditional and style-based generators beneﬁt from having a mapping network, and additional depth generally improves the perceptual path length as well as FIDs. It is interesting that while lW improves in the traditional generator, lZ becomes considerably worse, illustrating our claim that the input latent space can indeed be arbitrarily entangled in GANs.\n\nMethod\nFID Path length Separafull end bility B Traditional 0 Z 5.25 412.0 415.3 10.78 Traditional 8 Z 4.87 896.2 902.0 170.29 Traditional 8 W 4.87 324.5 212.2 6.52 Style-based 0 Z 5.06 283.5 285.5 9.88 Style-based 1 W 4.60 219.9 209.4 6.81 Style-based 2 W 4.43 217.8 199.9 6.25 F Style-based 8 W 4.40 234.0 195.9 3.79 Table 4. The effect of a mapping network in FFHQ. The number in method name indicates the depth of the mapping network. We see that FID, separability, and path length all beneﬁt from having a mapping network, and this holds for both style-based and traditional generator architectures. Furthermore, a deeper mapping network generally performs better than a shallow one. 4.2. Linear separability If a latent space is sufﬁciently disentangled, it should be possible to ﬁnd direction vectors that consistently correspond to individual factors of variation. We propose another metric that quantiﬁes this effect by measuring how well the latent-space points can be separated into two distinct sets via a linear hyperplane, so that each set corresponds to a speciﬁc binary attribute of the image. In order to label the generated images, we train auxiliary classiﬁcation networks for a number of binary attributes, e.g., to distinguish male and female faces. In our tests, the classiﬁers had the same architecture as the discriminator we use (i.e., same as in [30]), and were trained using the CELEBA-HQ dataset that retains the 40 attributes available in the original CelebA dataset. To measure the separability of one attribute, we generate 200,000 images with z ∼P(z) and classify them using the auxiliary classiﬁcation network. We then sort the samples according to classiﬁer conﬁdence and remove the least conﬁdent half, yielding 100,000 labeled latent-space vectors. For each attribute, we ﬁt a linear SVM to predict the label based on the latent-space point — z for traditional and w for style-based — and classify the points by this plane. We then compute the conditional entropy H(Y |X) where X are the classes predicted by the SVM and Y are the classes determined by the pre-trained classiﬁer. This tells how much additional information is required to determine the true class of a sample, given that we know on which side of the hyperplane it lies. A low value suggests consistent latent space directions for the corresponding factor(s) of variation. We calculate the ﬁnal separability score as exp(P i H(Yi|Xi)), where i enumerates the 40 attributes. Similar to the inception score [53], the exponentiation brings the values from logarithmic to linear domain so that they are easier to compare. Tables 3 and 4 show that W is consistently better separable than Z, suggesting a less entangled representation. Figure 7. The FFHQ dataset offers a lot of variety in terms of age, ethnicity, viewpoint, lighting, and image background. Furthermore, increasing the depth of the mapping network improves both image quality and separability in W, which is in line with the hypothesis that the synthesis network inherently favors a disentangled input representation. Interestingly, adding a mapping network in front of a traditional generator results in severe loss of separability in Z but improves the situation in the intermediate latent space W, and the FID improves as well. This shows that even the traditional generator architecture performs better when we introduce an intermediate latent space that does not have to follow the distribution of the training data.\n\n5. Conclusion\nBased on both our results and parallel work by Chen et al. [6], it is becoming clear that the traditional GAN generator architecture is in every way inferior to a style-based design. This is true in terms of established quality metrics, and we further believe that our investigations to the separation of high-level attributes and stochastic effects, as well as the linearity of the intermediate latent space will prove fruitful in improving the understanding and controllability of GAN synthesis. We note that our average path length metric could easily be used as a regularizer during training, and perhaps some variant of the linear separability metric could act as one, too. In general, we expect that methods for directly shaping the intermediate latent space during training will provide interesting avenues for future work.\n\n6. Acknowledgements\nWe thank Jaakko Lehtinen, David Luebke, and Tuomas Kynk¨a¨anniemi for in-depth discussions and helpful comments; Janne Hellsten, Tero Kuosmanen, and Pekka J¨anis for compute infrastructure and help with the code release.\n\nReferences\n[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean,\nM. Devin, S. Ghemawat, G. Irving, M. Isard, M. Kudlur,\nJ. Levenberg, R. Monga, S. Moore, D. G. Murray, B. Steiner,\nP. Tucker, V. Vasudevan, P. Warden, M. Wicke, Y. Yu, and\nX. Zheng. TensorFlow: a system for large-scale machine\nlearning. In Proc. 12th USENIX Conference on Operating Systems Design and Implementation, OSDI’16, pages 265– 283, 2016. 9 [2] A. Achille and S. Soatto. On the emergence of invariance and disentangling in deep representations. CoRR, abs/1706.01350, 2017. 6 [3] D. Bau, J. Zhu, H. Strobelt, B. Zhou, J. B. Tenenbaum, W. T. Freeman, and A. Torralba. GAN dissection: Visualizing and understanding generative adversarial networks. In Proc. ICLR, 2019. 1 [4] M. Ben-Yosef and D. Weinshall. Gaussian mixture generative adversarial networks for diverse datasets, and the unsupervised clustering of images. CoRR, abs/1808.10356, 2018. 3 [5] A. Brock, J. Donahue, and K. Simonyan. Large scale GAN training for high ﬁdelity natural image synthesis. CoRR, abs/1809.11096, 2018. 1, 3, 8 Figure 12. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN CAT dataset at 2562. FID computed for 50K images was 8.53. [6] T. Chen, M. Lucic, N. Houlsby, and S. Gelly. On self modulation for generative adversarial networks. CoRR, abs/1810.01365, 2018. 3, 8 [7] T. Q. Chen, X. Li, R. B. Grosse, and D. K. Duvenaud. Isolating sources of disentanglement in variational autoencoders. CoRR, abs/1802.04942, 2018. 6 [8] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, and P. Abbeel. InfoGAN: interpretable representation learning by information maximizing generative adversarial nets. CoRR, abs/1606.03657, 2016. 6 [9] E. L. Denton, S. Chintala, A. Szlam, and R. Fergus. Deep generative image models using a Laplacian pyramid of adversarial networks. CoRR, abs/1506.05751, 2015. 3 [10] G. Desjardins, A. Courville, and Y. Bengio. Disentangling factors of variation via generative entangling. CoRR, abs/1210.5474, 2012. 6 [11] T. Doan, J. Monteiro, I. Albuquerque, B. Mazoure, A. Durand, J. Pineau, and R. D. Hjelm. Online adaptative curriculum learning for GANs. CoRR, abs/1808.00020, 2018. 3 [12] J. Donahue, P. Kr¨ahenb¨uhl, and T. Darrell. Adversarial feature learning. CoRR, abs/1605.09782, 2016. 6 [13] A. Dosovitskiy, J. T. Springenberg, and T. Brox. Learning to generate chairs with convolutional neural networks. CoRR, abs/1411.5928, 2014. 1 [14] H. Drucker and Y. L. Cun. Improving generalization performance using double backpropagation. IEEE Transactions on Neural Networks, 3(6):991–997, 1992. 3 [15] V. Dumoulin, I. Belghazi, B. Poole, A. Lamb, M. Arjovsky,\nO. Mastropietro, and A. Courville. Adversarially learned inference. In Proc. ICLR, 2017. 6\n[16] V. Dumoulin, E. Perez, N. Schucher, F. Strub, H. d. Vries,\nA. Courville, and Y. Bengio.\nFeature-wise transformations. Distill, 2018. https://distill.pub/2018/feature-wisetransformations. 2 [17] V. Dumoulin, J. Shlens, and M. Kudlur. A learned representation for artistic style. CoRR, abs/1610.07629, 2016. 2 [18] I. P. Durugkar, I. Gemp, and S. Mahadevan. Generative multi-adversarial networks. CoRR, abs/1611.01673, 2016. 3 [19] C. Eastwood and C. K. I. Williams. A framework for the quantitative evaluation of disentangled representations. In Proc. ICLR, 2018. 6 [20] L. A. Gatys, A. S. Ecker, and M. Bethge. Image style transfer using convolutional neural networks. In Proc. CVPR, 2016. 6 [21] G. Ghiasi, H. Lee, M. Kudlur, V. Dumoulin, and J. Shlens. Exploring the structure of a real-time, arbitrary neural artistic stylization network. CoRR, abs/1705.06830, 2017. 2 [22] I. Goodfellow,\nJ. Pouget-Abadie,\nM. Mirza,\nB. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio. Generative Adversarial Networks. In NIPS, 2014. 1, 3, 9\n[23] W.-S. Z. Guang-Yuan Hao, Hong-Xing Yu. MIXGAN: learning concepts from different domains for mixture generation. CoRR, abs/1807.01659, 2018. 2 [24] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville. Improved training of Wasserstein GANs. CoRR, abs/1704.00028, 2017. 1, 2 [25] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and\nS. Hochreiter. GANs trained by a two time-scale update rule\nconverge to a local Nash equilibrium. In Proc. NIPS, pages 6626–6637, 2017. 2 [26] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glorot,\nM. Botvinick, S. Mohamed, and A. Lerchner.\nbeta-vae: Learning basic visual concepts with a constrained variational framework. In Proc. ICLR, 2017. 6 [27] X. Huang and S. J. Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. CoRR, abs/1703.06868, 2017. 1, 2 [28] X. Huang, M. Liu, S. J. Belongie, and J. Kautz. Multimodal unsupervised image-to-image translation. CoRR, abs/1804.04732, 2018. 2 [29] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015. 9 [30] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive growing of GANs for improved quality, stability, and variation. CoRR, abs/1710.10196, 2017. 1, 2, 7, 8, 9 [31] V. Kazemi and J. Sullivan. One millisecond face alignment with an ensemble of regression trees. In Proc. CVPR, 2014. 8 [32] H. Kim and A. Mnih. Disentangling by factorising. In Proc. ICML, 2018. 6 [33] D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 9 [34] D. P. Kingma and P. Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions. CoRR, abs/1807.03039, 2018. 3, 8 [35] D. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014. 6 [36] K. Kurach, M. Lucic, X. Zhai, M. Michalski, and S. Gelly. The gan landscape: Losses, architectures, regularization, and normalization. CoRR, abs/1807.04720, 2018. 1 [37] S. Laine. Feature-based metrics for exploring the latent space of generative models. ICLR workshop poster, 2018. 1, 6 [38] Y. Li, C. Fang, J. Yang, Z. Wang, X. Lu, and M.-H. Yang. Universal style transfer via feature transforms. In Proc. NIPS, 2017. 2 [39] Y. Li, N. Wang, J. Liu, and X. Hou. Demystifying neural style transfer. CoRR, abs/1701.01036, 2017. 6 [40] M. Lucic, K. Kurach, M. Michalski, S. Gelly, and O. Bousquet. Are GANs created equal? a large-scale study. CoRR, abs/1711.10337, 2017. 1 [41] A. L. Maas, A. Y. Hannun, and A. Ng. Rectiﬁer nonlinearities improve neural network acoustic models. In Proc. International Conference on Machine Learning (ICML), volume 30, 2013. 9 [42] M. Marchesi. Megapixel size image creation using generative adversarial networks. CoRR, abs/1706.00082, 2017. 3, 8 [43] L. Matthey,\nI. Higgins,\nD. Hassabis,\nand A. Lerchner. dsprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset/, 2017. 6 [44] L. Mescheder, A. Geiger, and S. Nowozin. Which training methods for GANs do actually converge? CoRR, abs/1801.04406, 2018. 1, 3, 9 [45] T. Miyato, T. Kataoka, M. Koyama, and Y. Yoshida. Spectral normalization for generative adversarial networks. CoRR, abs/1802.05957, 2018. 1, 9 [46] T. Miyato and M. Koyama. cGANs with projection discriminator. CoRR, abs/1802.05637, 2018. 3 [47] G. Mordido, H. Yang, and C. Meinel. Dropout-gan: Learning from a dynamic ensemble of discriminators. CoRR, abs/1807.11346, 2018. 3 [48] S. Mukherjee, H. Asnani, E. Lin, and S. Kannan. ClusterGAN : Latent space clustering in generative adversarial networks. CoRR, abs/1809.03627, 2018. 3 [49] D. J. Rezende, S. Mohamed, and D. Wierstra. Stochastic backpropagation and approximate inference in deep generative models. In Proc. ICML, 2014. 6 [50] K. Ridgeway. A survey of inductive biases for factorial representation-learning. CoRR, abs/1612.05299, 2016. 6 [51] A. S. Ross and F. Doshi-Velez. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. CoRR, abs/1711.09404,\n2017. 3\n[52] T. Sainburg, M. Thielk, B. Theilman, B. Migliori, and\nT. Gentner. Generative adversarial interpolative autoencoding: adversarial training on latent space interpolations encourage convex latent distributions. CoRR, abs/1807.06650,\n2018. 1, 3\n[53] T. Salimans, I. J. Goodfellow, W. Zaremba, V. Cheung,\nA. Radford, and X. Chen. Improved techniques for training\nGANs. In NIPS, 2016. 7 [54] J. Schmidhuber. Learning factorial codes by predictability minimization. Neural Computation, 4(6):863–879, 1992. 6 [55] R. Sharma, S. Barratt, S. Ermon, and V. Pande. Improved training with curriculum gans. CoRR, abs/1807.09295, 2018. 3 [56] K. Shoemake. Animating rotation with quaternion curves. In Proc. SIGGRAPH ’85, 1985. 7 [57] A. Siarohin, E. Sangineto, and N. Sebe. Whitening and coloring transform for GANs. CoRR, abs/1806.00420, 2018. 2 [58] K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556, 2014. 6 [59] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and\nR. Salakhutdinov. Dropout: A simple way to prevent neural networks from overﬁtting. Journal of Machine Learning\nResearch, 15:1929–1958, 2014. 9 [60] T. Wang, M. Liu, J. Zhu, A. Tao, J. Kautz, and B. Catanzaro. High-resolution image synthesis and semantic manipulation with conditional GANs. CoRR, abs/1711.11585, 2017. 3 [61] T. White. Sampling generative networks: Notes on a few effective techniques. CoRR, abs/1609.04468, 2016. 7 [62] F. Yu, Y. Zhang, S. Song, A. Seff, and J. Xiao. LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop. CoRR, abs/1506.03365, 2015. 9 [63] H. Zhang, I. Goodfellow, D. Metaxas, and A. Odena. Self-attention generative adversarial networks. CoRR, abs/1805.08318, 2018. 3, 9 [64] R. Zhang. Making convolutional networks shift-invariant again, 2019. 2, 9 [65] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. CVPR, 2018. 6, 7\n--- FOOTNOTES 12M to 25M images. We use the same learning rates as 5M 10M 15M 20M 25M 5M 10M 15M 20M 25M\n\nA. The FFHQ dataset\nWe have collected a new dataset of human faces, FlickrFaces-HQ (FFHQ), consisting of 70,000 high-quality images at 10242 resolution (Figure 7). The dataset includes vastly more variation than CELEBA-HQ [30] in terms of age, ethnicity and image background, and also has much better coverage of accessories such as eyeglasses, sunglasses, hats, etc. The images were crawled from Flickr ψ = 1 ψ = 0.7 ψ = 0.5 ψ = 0 ψ = −0.5 ψ = −1 Figure 8. The effect of truncation trick as a function of style scale ψ. When we fade ψ →0, all faces converge to the “mean” face of FFHQ. This face is similar for all trained networks, and the interpolation towards it never seems to cause artifacts. By applying negative scaling to styles, we get the corresponding opposite or “anti-face”. It is interesting that various high-level attributes often ﬂip between the opposites, including viewpoint, glasses, age, coloring, hair length, and often gender. (thus inheriting all the biases of that website) and automatically aligned [31] and cropped. Only images under permissive licenses were collected. Various automatic ﬁlters were used to prune the set, and ﬁnally Mechanical Turk allowed us to remove the occasional statues, paintings, or photos of photos. We have made the dataset publicly available at https://github.com/NVlabs/ffhq-dataset\n\nB. Truncation trick in W\nIf we consider the distribution of training data, it is clear that areas of low density are poorly represented and thus likely to be difﬁcult for the generator to learn. This is a signiﬁcant open problem in all generative modeling techniques. However, it is known that drawing latent vectors from a truncated [42, 5] or otherwise shrunk [34] sampling space tends to improve average image quality, although some amount of variation is lost. We can follow a similar strategy. To begin, we compute the center of mass of W as ¯w = Ez∼P (z)[f(z)]. In case of FFHQ this point represents a sort of an average face (Figure 8, ψ = 0). We can then scale the deviation of a given w from the center as w′ = ¯w + ψ(w −¯w), where ψ < 1. While Brock et al. [5] observe that only a subset of networks is amenable to such truncation even when orthogonal regularization is used, truncation in W space seems to work reliably even without changes to the loss function.\n\nC. Hyperparameters and training details\nWe build upon the ofﬁcial TensorFlow [1] implementation of Progressive GANs by Karras et al. [30], from which we inherit most of the training details.3 This original setup corresponds to conﬁguration A in Table 1. In particular, we use the same discriminator architecture, resolutiondependent minibatch sizes, Adam [33] hyperparameters, and exponential moving average of the generator. We enable mirror augmentation for CelebA-HQ and FFHQ, but disable it for LSUN. Our training time is approximately one week on an NVIDIA DGX-1 with 8 Tesla V100 GPUs. For our improved baseline (B in Table 1), we make several modiﬁcations to improve the overall result quality. We replace the nearest-neighbor up/downsampling in both networks with bilinear sampling, which we implement by lowpass ﬁltering the activations with a separable 2nd order binomial ﬁlter after each upsampling layer and before each downsampling layer [64]. We implement progressive growing the same way as Karras et al. [30], but we start from 82 images instead of 42. For the FFHQ dataset, we switch from WGAN-GP to the non-saturating loss [22] with R1 regularization [44] using γ = 10. With R1 we found that the FID scores keep decreasing for considerably longer than with WGAN-GP, and we thus increase the training time from Karras et al. [30] for FFHQ, but we found that setting the learning rate to 0.002 instead of 0.003 for 5122 and 10242 leads to better stability with CelebA-HQ. For our style-based generator (F in Table 1), we use leaky ReLU [41] with α = 0.2 and equalized learning rate [30] for all layers. We use the same feature map counts in our convolution layers as Karras et al. [30]. Our mapping network consists of 8 fully-connected layers, and the dimensionality of all input and output activations — including z and w — is 512. We found that increasing the depth of the mapping network tends to make the training unstable with high learning rates. We thus reduce the learning rate by two orders of magnitude for the mapping network, i.e., λ′ = 0.01·λ. We initialize all weights of the convolutional, fully-connected, and afﬁne transform layers using N(0, 1). The constant input in synthesis network is initialized to one. The biases and noise scaling factors are initialized to zero, except for the biases associated with ys that we initialize to one. The classiﬁers used by our separability metric (Section 4.2) have the same architecture as our discriminator except that minibatch standard deviation [30] is disabled. We use the learning rate of 10−3, minibatch size of 8, Adam optimizer, and training length of 150,000 images. The classiﬁers are trained independently of generators, and the same 40 classiﬁers, one for each CelebA attribute, are used FID Path length 10 500 9 400 8 300 7 200 6 100 5 0 resolution Full resolution Full 40 0 Traditional (B) Traditional (B) Style-based (F) Style-based (F), full Style-based (F), end Figure 9. FID and perceptual path length metrics over the course of training in our conﬁgurations B and F using the FFHQ dataset. Horizontal axis denotes the number of training images seen by the discriminator. The dashed vertical line at 8.4M images marks the point when training has progressed to full 10242 resolution. On the right, we show only one curve for the traditional generator’s path length measurements, because there is no discernible difference between full-path and endpoint sampling in Z. for measuring the separability metric for all generators. We will release the pre-trained classiﬁer networks so that our measurements can be reproduced. We do not use batch normalization [29], spectral normalization [45], attention mechanisms [63], dropout [59], or pixelwise feature vector normalization [30] in our networks.\n\nD. Training convergence\nFigure 9 shows how the FID and perceptual path length metrics evolve during the training of our conﬁgurations B and F with the FFHQ dataset. With R1 regularization active in both conﬁgurations, FID continues to slowly decrease as the training progresses, motivating our choice to increase the training time from 12M images to 25M images. Even when the training has reached the full 10242 resolution, the slowly rising path lengths indicate that the improvements in FID come at the cost of a more entangled representation. Considering future work, it is an interesting question whether this is unavoidable, or if it were possible to encourage shorter path lengths without compromising the convergence of FID.\n\nE. Other datasets\nFigures 10, 11, and 12 show an uncurated set of results for LSUN [62] BEDROOM, CARS, and CATS, respectively. In these images we used the truncation trick from Appendix Bwith ψ = 0.7 for resolutions 42 −322. The accompanying video provides results for style mixing and stochastic variation tests. As can be seen therein, in case of Figure 10. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN BEDROOM dataset at 2562. FID computed for 50K images was 2.65. BEDROOM the coarse styles basically control the viewpoint of the camera, middle styles select the particular furniture, and ﬁne styles deal with colors and smaller details of materials. In CARS the effects are roughly similar. Stochastic variation affects primarily the fabrics in BEDROOM, backgrounds and headlamps in CARS, and fur, background, and interestingly, the positioning of paws in CATS. Somewhat surprisingly the wheels of a car never seem to rotate based on stochastic inputs. These datasets were trained using the same setup as FFHQ for the duration of 70M images for BEDROOM and CATS, and 46M for CARS. We suspect that the results for BEDROOM are starting to approach the limits of the training data, as in many images the most objectionable issues are the severe compression artifacts that have been inherited from the low-quality training data. CARS has much higher quality training data that also allows higher spatial resolution (512 × 384 instead of 2562), and CATS continues to be a difﬁcult dataset due to the high intrinsic variation in poses, zoom levels, and backgrounds. Figure 11. Uncurated set of images produced by our style-based generator (conﬁg F) with the LSUN CAR dataset at 512 × 384. FID computed for 50K images was 3.27.", "persona": "Student", "response": "### 1. Overview\nGenerative Adversarial Networks have been the go-to machine learning technique for generative content in the past few years. Seemingly magically converting random inputs into highly detailed outputs, they have found applications in generating images, generating music, and even generating medication. StyleGAN in more detail Image by author Suppose that the picture below was generated by StyleGAN. It wasn’t, but suppose it is. You can see that it contains a variety of components which all have different granularity: In Euclidean geometry, an affine transformation, or an affinity (from the Latin, affinis, “connected with”), is a geometric transformation that preserves lines and parallelism (but not necessarily distances and angles). https://en.wikipedia.org/wiki/Affine\\_transformation The affine transformations are learned during training and hence are the components that can be used to control the image synthesis process for the lower-granularity components, such as the hair style, skin color, and so forth — whereas, remember, the randomness is used to control the position of, for example, the individual hairs. You should now be able to explain how styles and randomness allow us to generate a unique image.\n\n### 2. Background\nLet’s now take a more precise look at how styles can control image generation. Adaptive Instance Normalization based style addition The end result of StyleGAN when trained on faces is really cool! :) Karras, T., Laine, S., & Aila, T. (2018). A style-based generator architecture for generative adversarial networks.\n\n### 3. Methods\nThere are two vertical blocks involved: The mapping network, called f , is visible on the left. It maps a (normalized) latent vector z ε Z into another vector w from an intermediate latent space, called W . This mapping network is a simple set of fully-connected feedforward layers. , is visible on the left. It maps a (normalized) latent vector ε Z into another vector from an intermediate latent space, called . This mapping network is a simple set of fully-connected feedforward layers. The synthesis network, called g and visible on the right, uses w to generate a “style” that controls the image synthesis process.\n\n### 4. Results\nIt begins with a Constant, 4 x 4 x 512 dimensional vector. Scaled noise samples (B) are generated and added to this Constant tensor. Subsequently, the style (A) is added via Adaptive Instance Normalization (AdaIN) operations, after which a convolution operation is applied. This is followed by another noise addition and AdaIN-based styling operation. We then arrive at an image at a 4x4 pixel resolution.\n\n### 5. Takeaways\nStyleGAN: A Gentle Introduction StyleGAN is a GAN type that really moved the state-of-the-art in GANs forward. When the url introducing StyleGAN, “A style-based generator architecture for generative adversarial networks” by Karras et al. (2018) appeared, GANs required heavy regularization and were not able to produce such stunning results as they are known for today. In this article, we’ll dive deep into the StyleGAN architecture. Firstly, we introduce the high-level architecture of a classic or vanilla GAN, so that we can subsequently introduce StyleGAN’s high-level architecture and compare them. This will shed some light on high-level differences and on how StyleGAN is radically different compared to approaches that were prominent at the time. Then, we’ll take a look at each individual StyleGAN component and discuss it in detail. This way, you’ll also learn about what’s beyond the high level details, and understand the impact of each individual component. Classic GANs, a recap Problems with classic Generative Adversarial Networks Classic GANs were quite a bit of a breakthrough when the first GAN url appeared in 2014. In “Generative adversarial networks”, Goodfellow et al. (2014) introduced the training procedure that was discussed above. Soon, however, it became clear that training a classic GAN poses a few challenges — as becomes clear from Karras et al.", "paper_id": "04", "story_id": "04-04"}
{"paper": "A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo\n\nMuch progress towards artificial intelligence has been made using supervised learning systems that are trained to replicate the decisions of human experts1–4. However, expert data sets are often expensive, unreliable or simply unavailable. Even when reliable data sets are available, they may impose a ceiling on the performance of systems trained in this manner5. By contrast, reinforcement learning systems are trained from their own experience, in principle allowing them to exceed human capabilities, and to operate in domains where human expertise is lacking. Recently, there has been rapid progress towards this goal, using deep neural networks trained by reinforcement learning. These systems have outperformed humans in computer games, such as Atari6,7 and 3D virtual environments8–10. However, the most challenging domains in terms of human intellect—such as the game of Go, widely viewed as a grand challenge for artificial intelligence11—require a precise and sophisticated lookahead in vast search spaces. Fully general methods have not previously achieved human-level performance in these domains. AlphaGo was the first program to achieve superhuman performance in Go. The published version12, which we refer to as AlphaGo Fan, defeated the European champion Fan Hui in October 2015. AlphaGo Fan used two deep neural networks: a policy network that outputs move probabilities and a value network that outputs a position evaluation. The policy network was trained initially by supervised learning to accurately predict human expert moves, and was subsequently refined by policy-gradient reinforcement learning. The value network was trained to predict the winner of games played by the policy network against itself. Once trained, these networks were combined with a Monte Carlo tree search (MCTS)13–15 to provide a lookahead search, using the policy network to narrow down the search to high-probability moves, and using the value network (in conjunction with Monte Carlo rollouts using a fast rollout policy) to evaluate positions in the tree. A subsequent version, which we refer to as AlphaGo Lee, used a similar approach (see Methods), and defeated Lee Sedol, the winner of 18 international titles, in March 2016. Our program, AlphaGo Zero, differs from AlphaGo Fan and AlphaGo Lee12 in several important aspects. First and foremost, it is trained solely by self-play reinforcement learning, starting from random play, without any supervision or use of human data. Second, it uses only the black and white stones from the board as input features. Third, it uses a single neural network, rather than separate policy and value networks. Finally, it uses a simpler tree search that relies upon this single neural network to evaluate positions and sample moves, without performing any Monte Carlo rollouts. To achieve these results, we introduce a new reinforcement learning algorithm that incorporates lookahead search inside the training loop, resulting in rapid improvement and precise and stable learning. Further technical differences in the search algorithm, training procedure and network architecture are described in Methods.\n\nReinforcement learning in AlphaGo Zero Our new method uses a deep neural network fθ with parameters θ. This neural network takes as an input the raw board representation s of the position and its history, and outputs both move probabilities and a value, (p, v) = fθ(s). The vector of move probabilities p represents the probability of selecting each move a (including pass), pa = Pr(a| s). The value v is a scalar evaluation, estimating the probability of the current player winning from position s. This neural network combines the roles of both policy network and value network12 into a single architecture. The neural network consists of many residual blocks4 of convolutional layers16,17 with batch normalization18 and rectifier nonlinearities19 (see Methods). The neural network in AlphaGo Zero is trained from games of self- play by a novel reinforcement learning algorithm. In each position s, an MCTS search is executed, guided by the neural network fθ. The MCTS search outputs probabilities π of playing each move. These search probabilities usually select much stronger moves than the raw move probabilities p of the neural network fθ(s); MCTS may therefore be viewed as a powerful policy improvement operator20,21. Self-play with search—using the improved MCTS-based policy to select each move, then using the game winner z as a sample of the value—may be viewed as a powerful policy evaluation operator. The main idea of our reinforcement learning algorithm is to use these search operators repeatedly in a policy iteration procedure22,23: the neural network’s parameters are updated to make the move probabilities and value (p, v) = fθ(s) more closely match the improved search probabilities and self- play winner (π, z); these new parameters are used in the next iteration of self-play to make the search even stronger. Figure 1 illustrates the self-play training pipeline. The MCTS uses the neural network fθ to guide its simulations (see Fig. 2). Each edge (s, a) in the search tree stores a prior probability P(s, a), a visit count N(s, a), and an action value Q(s, a). Each simulation starts from the root state and iteratively selects moves that maximize an upper confidence bound Q(s, a) + U(s, a), where U(s, a) ∝ P(s, a) / (1 + N(s, a)) (refs 12, 24), until a leaf node s′ is encountered. This leaf position is expanded and evaluated only once by the network to generate both prior probabilities and evaluation, (P(s′ , ·),V(s′ )) = fθ(s′ ). Each edge (s, a) traversed in the simulation is updated to increment its visit count N(s, a), and to update its action value to the mean evaluation over these simulations, = / ∑ ′ Q s a N s a V s ( , ) 1 ( , ) s s a s | →′ ′ ( ) where , s, a→ s′ indicates that a simulation eventually reached s′ after taking move a from position s. MCTS may be viewed as a self-play algorithm that, given neural network parameters θ and a root position s, computes a vector of search probabilities recommending moves to play, π = αθ(s), proportional to the exponentiated visit count for each move, πa ∝ N(s, a)1/τ, where τ is a temperature parameter. The neural network is trained by a self-play reinforcement learning algorithm that uses MCTS to play each move. First, the neural network is initialized to random weights θ0. At each subsequent iteration i ≥ 1, games of self-play are generated (Fig. 1a). At each time-step t, an MCTS s ( ) t search π α = θ− t i 1 is executed using the previous iteration of neural network θ− f i 1 and a move is played by sampling the search probabilities πt. A game terminates at step T when both players pass, when the search value drops below a resignation threshold or when the game exceeds a maximum length; the game is then scored to give a final reward of rT ∈ {− 1,+ 1} (see Methods for details). The data for each time-step t is stored as (st, πt, zt), where zt = ± rT is the game winner from the perspective of the current player at step t. In parallel (Fig. 1b), new network parameters θi are trained from data (s, π, z) sampled uniformly among all time-steps of the last iteration(s) of self-play. The neural network = θ ( , ) ( ) p v f s i is adjusted to minimize the error between the predicted value v and the self-play winner z, and to maximize the similarity of the neural network move probabilities p to the search probabilities π. Specifically, the parameters θ are adjusted by gradient descent on a loss function l that sums over the mean-squared error and cross-entropy losses, respectively:FORMULA where c is a parameter controlling the level of L2 weight regularization (to prevent overfitting).\n\nEmpirical analysis of AlphaGo Zero training We applied our reinforcement learning pipeline to train our program AlphaGo Zero. Training started from completely random behaviour and continued without human intervention for approximately three days. Over the course of training, 4.9 million games of self-play were generated, using 1,600 simulations for each MCTS, which corresponds to approximately 0.4 s thinking time per move. Parameters were updated from 700,000 mini-batches of 2,048 positions. The neural network contained 20 residual blocks (see Methods for further details). Figure 3a shows the performance of AlphaGo Zero during self-play reinforcement learning, as a function of training time, on an Elo scale25 . Learning progressed smoothly throughout training, and did not suffer from the oscillations or catastrophic forgetting that have been suggested in previous literature26–28. Surprisingly, AlphaGo Zero outperformed AlphaGo Lee after just 36 h. In comparison, AlphaGo Lee was trained over several months. After 72 h, we evaluated AlphaGo Zero against the exact version of AlphaGo Lee that defeated Lee Sedol, under the same 2 h time controls and match conditions that were used in the man– machine match in Seoul (see Methods). AlphaGo Zero used a single machine with 4 tensor processing units (TPUs)29, whereas AlphaGo Lee was distributed over many machines and used 48 TPUs. AlphaGo Zero defeated AlphaGo Lee by 100 games to 0 (see Extended Data Fig. 1 and Supplementary Information). To assess the merits of self-play reinforcement learning, compared to learning from human data, we trained a second neural network (using the same architecture) to predict expert moves in the KGS Server dataset; this achieved state-of-the-art prediction accuracy compared to previous work12,30–33 (see Extended Data Tables 1 and 2 for current and previous results, respectively). Supervised learning achieved a better initial performance, and was better at predicting human professional moves (Fig. 3). Notably, although supervised learning achieved higher move prediction accuracy, the self-learned player performed much better overall, defeating the human-trained player within the first 24 h of training. This suggests that AlphaGo Zero may be learning a strategy that is qualitatively different to human play. To separate the contributions of architecture and algorithm, we compared the performance of the neural network architecture in AlphaGo Zero with the previous neural network architecture used in AlphaGo Lee (see Fig. 4). Four neural networks were created, using either separate policy and value networks, as were used in AlphaGo Lee, or combined policy and value networks, as used in AlphaGo Zero; and using either the convolutional network architecture from AlphaGo Lee or the residual network architecture from AlphaGo Zero. Each network was trained to minimize the same loss function (equation (1)), using a fixed dataset of self-play games generated by AlphaGo Zero after 72 h of self-play training. Using a residual network was more accurate, achieved lower error and improved performance in AlphaGo by over 600 Elo. Combining policy and value together into a single network slightly reduced the move prediction accuracy, but reduced the value error and boosted playing performance in AlphaGo by around another 600 Elo. This is partly due to improved computational efficiency, but more importantly the dual objective regularizes the network to a common representation that supports multiple use cases. Knowledge learned by AlphaGo Zero AlphaGo Zero discovered a remarkable level of Go knowledge during its self-play training process. This included not only fundamental elements of human Go knowledge, but also non-standard strategies beyond the scope of traditional Go knowledge. Figure 5 shows a timeline indicating when professional joseki (corner sequences) were discovered (Fig. 5a and Extended Data Fig. 2); ultimately AlphaGo Zero preferred new joseki variants that were previously unknown (Fig. 5b and Extended Data Fig. 3). Figure 5c shows several fast self-play games played at different stages of training (see Supplementary Information). Tournament length games played at regular intervals throughout training are shown in Extended Data Fig. 4 and in the Supplementary Information. AlphaGo Zero rapidly progressed from entirely random moves towards a sophisticated understanding of Go concepts, including fuseki (opening), tesuji (tactics), life-and-death, ko (repeated board situations), yose (endgame), capturing races, sente (initiative), shape, influence and territory, all discovered from first principles. Surprisingly, shicho (‘ladder’ capture sequences that may span the whole board)—one of the first elements of Go knowledge learned by humans—were only understood by AlphaGo Zero much later in training.\n\nFinal performance of AlphaGo Zero We subsequently applied our reinforcement learning pipeline to a second instance of AlphaGo Zero using a larger neural network and over a longer duration. Training again started from completely random behaviour and continued for approximately 40 days. Over the course of training, 29 million games of self-play were generated. Parameters were updated from 3.1 million mini-batches of 2,048 positions each. The neural network contained 40 residual blocks. The learning curve is shown in Fig. 6a. Games played at regular intervals throughout training are shown in Extended Data Fig. 5 and in the Supplementary Information. We evaluated the fully trained AlphaGo Zero using an internal tournament against AlphaGo Fan, AlphaGo Lee and several previous Go programs. We also played games against the strongest existing program, AlphaGo Master—a program based on the algorithm and architecture presented in this paper but using human data and features (see Methods)—which defeated the strongest human professional players 60–0 in online games in January 201734. In our evaluation, all programs were allowed 5 s of thinking time per move; AlphaGo Zero and AlphaGo Master each played on a single machine with 4 TPUs; AlphaGo Fan and AlphaGo Lee were distributed over 176 GPUs and 48 TPUs, respectively. We also included a player based solely on the raw neural network of AlphaGo Zero; this player simply selected the move with maximum probability. Figure 6b shows the performance of each program on an Elo scale. The raw neural network, without using any lookahead, achieved an Elo rating of 3,055. AlphaGo Zero achieved a rating of 5,185, compared to 4,858 for AlphaGo Master, 3,739 for AlphaGo Lee and 3,144 forAlphaGo Fan.Finally, we evaluated AlphaGo Zero head to head against AlphaGoMaster in a 100-game match with 2-h time controls. AlphaGo Zerowon by 89 games to 11 (see Extended Data Fig. 6 and SupplementaryInformation).\n\nConclusion Our results comprehensively demonstrate that a pure reinforcement learning approach is fully feasible, even in the most challenging of domains: it is possible to train to superhuman level, without human examples or guidance, given no knowledge of the domain beyond basic rules. Furthermore, a pure reinforcement learning approach requires just a few more hours to train, and achieves much better asymptotic performance, compared to training on human expert data. Using this approach, AlphaGo Zero defeated the strongest previous versions of AlphaGo, which were trained from human data using handcrafted features, by a large margin. Humankind has accumulated Go knowledge from millions of games played over thousands of years, collectively distilled into patterns, proverbs and books. In the space of a few days, starting tabula rasa, AlphaGo Zero was able to rediscover much of this Go knowledge, as well as novel strategies that provide new insights into the oldest of games.\n\nReferences\n1. Friedman, J., Hastie, T. & Tibshirani, R. The Elements of\nStatistical Learning: Data Mining, Inference, and Prediction (Springer, 2009)\n2. LeCun, Y., Bengio, Y. & Hinton, G. Deep learning. Nature 521,\n436–444 (2015) Article  CAS  ADS  Google Scholar\n3. Krizhevsky, A., Sutskever, I. & Hinton, G. ImageNet\nclassification with deep convolutional neural networks. In Adv. Neural Inf. Process. Syst. Vol. 25 (eds Pereira, F., Burges,\nC. J. C., Bottou, L. & Weinberger, K. Q. ) 1097–1105 (2012)\n5. Hayes-Roth, F., Waterman, D. & Lenat, D. Building Expert\nSystems (Addison-Wesley, 1984)\n6. Mnih, V. et al. Human-level control through deep\nreinforcement learning. Nature 518, 529–533 (2015) Article  CAS  ADS  Google Scholar\n7. Guo, X., Singh, S. P., Lee, H., Lewis, R. L. & Wang, X. Deep\nlearning for real-time Atari game play using offline MonteCarlo tree search planning. In Adv. Neural Inf. Process. Syst. Vol. 27 (eds Ghahramani, Z., Welling, M., Cortes, C., Lawrence, N. D. & Weinberger, K. Q. ) 3338–3346 (2014)\n8. Mnih, V . et al. Asynchronous methods for deep\nreinforcement learning. In Proc. 33rd Int. Conf. Mach. Learn. Vol. 48 (eds Balcan, M. F. & Weinberger, K. Q. ) 1928–1937 (2016)\n9. Jaderberg, M . et al. Reinforcement learning with\nunsupervised auxiliary tasks. In 5th Int. Conf. Learn. Representations (2017)\n10. Dosovitskiy, A. & Koltun, V. Learning to act by predicting the\nfuture. In 5th Int. Conf. Learn. Representations (2017)\n11. Man´dziuk, J. in Challenges for Computational Intelligence (\nDuch, W. & Man´dziuk, J. ) 407–442 (Springer, 2007)\n12. Silver, D. et al. Mastering the game of Go with deep neural\nnetworks and tree search. Nature 529, 484–489 (2016) Article  CAS  ADS  Google Scholar\n13. Coulom, R. Efficient selectivity and backup operators in\nMonte-Carlo tree search. In 5th Int. Conf. Computers and Games (eds Ciancarini, P. & van den Herik, H. J. ) 72–83 (2006)\n15. Browne, C. et al. A survey of Monte Carlo tree search\nmethods. IEEE Trans. Comput. Intell. AI Games 4, 1–49 (2012) Article  Google Scholar\n16. Fukushima, K. Neocognitron: a self organizing neural network\nmodel for a mechanism of pattern recognition unaffected by shift in position. Biol. Cybern. 36, 193–202 (1980) Article  CAS  Google Scholar\n17. LeCun, Y. & Bengio, Y. in The Handbook of Brain Theory and\nNeural Networks Ch. 3 (ed. Arbib, M. ) 276–278 (MIT Press, 1995)\n18. Ioffe, S. & Szegedy, C. Batch normalization: accelerating deep\nnetwork training by reducing internal covariate shift. In Proc. 32nd Int. Conf. Mach. Learn. Vol. 37 448–456 (2015)\n19. Hahnloser, R. H. R., Sarpeshkar, R., Mahowald, M. A.,\nDouglas, R. J. & Seung, H. S. Digital selection and analogue amplification coexist in a cortex-inspired silicon circuit. Nature 405, 947–951 (2000) Article  CAS  ADS  Google Scholar\n20. Howard, R. Dynamic Programming and Markov Processes (MIT\nPress, 1960)\n21. Sutton, R . & Barto, A. Reinforcement Learning: an Introduction\n(MIT Press, 1998)\n22. Bertsekas, D. P. Approximate policy iteration: a survey and\nsome new methods. J. Control Theory Appl. 9, 310–335 (2011) Article  MathSciNet  Google Scholar\n24. Rosin, C. D. Multi-armed bandits with episode context. Ann.\nMath. Artif. Intell. 61, 203–230 (2011) Article  MathSciNet  Google Scholar\n25. Coulom, R. Whole-history rating: a Bayesian rating system for\nplayers of time-varying strength. In Int. Conf. Comput. Games (eds van den Herik, H. J., Xu, X . Ma, Z . & Winands, M. H. M. ) Vol. 5131 113–124 (Springer, 2008)\n26. Laurent, G. J., Matignon, L. & Le Fort-Piat, N. The world of\nindependent learners is not Markovian. Int. J. KnowledgeBased Intelligent Engineering Systems 15, 55–64 (2011) Article  Google Scholar\n27. Foerster, J. N . et al. Stabilising experience replay for deep\nmulti-agent reinforcement learning. In Proc. 34th Int. Conf. Mach. Learn. Vol. 70 1146–1155 (2017)\n28. Heinrich, J . & Silver, D. Deep reinforcement learning from\nself-play in imperfect-information games. In NIPS Deep Reinforcement Learning Workshop (2016)\n29. Jouppi, N. P . et al. In-datacenter performance analysis of a\nTensor Processing Unit. Proc. 44th Annu. Int. Symp. Comp. Architecture Vol. 17 1–12 (2017)\n30. Maddison, C. J., Huang, A., Sutskever, I . & Silver, D. Move\nevaluation in Go using deep convolutional neural networks. In 3rd Int. Conf. Learn. Representations. (2015)\n31. Clark, C . & Storkey, A. J. Training deep convolutional neural\nnetworks to play Go. In Proc. 32nd Int. Conf. Mach. Learn. Vol. 37 1766–1774 (2015)\n32. Tian, Y. & Zhu, Y. Better computer Go player with neural\nnetwork and long-term prediction. In 4th Int. Conf. Learn. Representations (2016)\n34. Huang, A. AlphaGo master online series of games. https://\ndeepmind.com/research/AlphaGo/match-archive/master (2017)\n35. Barto, A. G. & Duff, M. Monte Carlo matrix inversion and\nreinforcement learning. Adv. Neural Inf. Process. Syst. 6, 687– 694 (1994) Google Scholar\n36. Singh, S. P. & Sutton, R. S. Reinforcement learning with\nreplacing eligibility traces. Mach. Learn. 22, 123–158 (1996) MATH  Google Scholar\n37. Lagoudakis, M. G. & Parr, R. Reinforcement learning as\nclassification: leveraging modern classifiers. In Proc. 20th Int. Conf. Mach. Learn. 424–431 (2003)\n38. Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B. &\nGeist, M. Approximate modified policy iteration and its application to the game of Tetris. J. Mach. Learn. Res. 16, 1629–1676 (2015) MathSciNet  MATH  Google Scholar\n39. Littman, M. L. Markov games as a framework for multi-agent\nreinforcement learning. In Proc. 11th Int. Conf. Mach. Learn. 157–163 (1994)\n40. Enzenberger, M. The integration of a priori knowledge into a\nGo playing neural network. http://www.cgl.ucsf.edu/go/ Programs/neurogo-html/neurogo.html (1996)\n41. Enzenberger, M. in Advances in Computer Games (eds Van Den\nHerik, H. J., Iida, H. & Heinz, E. A. ) 97–108 (2003) Google Scholar\n43. Schraudolph, N. N., Dayan, P. & Sejnowski, T. J. Temporal\ndifference learning of position evaluation in the game of Go. Adv. Neural Inf. Process. Syst. 6, 817–824 (1994) Google Scholar\n44. Silver, D., Sutton, R. & Müller, M. Temporal-difference search\nin computer Go. Mach. Learn. 87, 183–219 (2012) Article  MathSciNet  Google Scholar\n45. Silver, D. Reinforcement Learning and Simulation-Based Search\nin Computer Go. PhD thesis, Univ. Alberta, Edmonton, Canada (2009)\n46. Gelly, S. & Silver, D. Monte-Carlo tree search and rapid action\nvalue estimation in computer Go. Artif. Intell. 175, 1856–1875 (2011) Article  MathSciNet  Google Scholar\n47. Coulom, R. Computing Elo ratings of move patterns in the\ngame of Go. Int. Comput. Games Assoc. J. 30, 198–208 (2007) Google Scholar\n48. Gelly, S., Wang, Y., Munos, R. & Teytaud, O. Modification of\nUCT with patterns in Monte-Carlo Go. Report No. 6062 (INRIA, 2006)\n49. Baxter, J., Tridgell, A. & Weaver, L. Learning to play chess\nusing temporal differences. Mach. Learn. 40, 243–263 (2000) Article  Google Scholar\n52. Schaeffer, J., Hlynka, M . & Jussila, V. Temporal difference\nlearning applied to a high-performance game-playing program. In Proc. 17th Int. Jt Conf. Artif. Intell. Vol. 1 529–534 (2001)\n53. Tesauro, G. TD-gammon, a self-teaching backgammon\nprogram, achieves master-level play. Neural Comput. 6, 215– 219 (1994) Article  Google Scholar\n54. Buro, M. From simple features to sophisticated evaluation\nfunctions. In Proc. 1st Int. Conf. Comput. Games 126–145 (1999)\n55. Sheppard, B. World-championship-caliber Scrabble. Artif.\nIntell. 134, 241–275 (2002) Article  Google Scholar\n56. Moravcˇík, M. et al. DeepStack: expert-level artificial\nintelligence in heads-up no-limit poker. Science 356, 508–513 (2017) Article  ADS  MathSciNet  Google Scholar\n57. Tesauro, G & Galperin, G. On-line policy improvement using\nMonte-Carlo search. In Adv. Neural Inf. Process. Syst. 1068– 1074 (1996)\n58. Tesauro, G. Neurogammon: a neural-network backgammon\nprogram. In Proc. Int. Jt Conf. Neural Netw. Vol. 3, 33–39 (1990)\n59. Samuel, A. L. Some studies in machine learning using the\ngame of checkers II - recent progress. IBM J. Res. Develop. 11, 601–617 (1967) Article  Google Scholar\n61. Zhang, W. & Dietterich, T. G. A reinforcement learning\napproach to job-shop scheduling. In Proc. 14th Int. Jt Conf. Artif. Intell. 1114–1120 (1995)\n62. Cazenave, T., Balbo, F. & Pinson, S. Using a Monte-Carlo\napproach for bus regulation. In Int. IEEE Conf. Intell. Transport. Syst. 1–6 (2009)\n63. Evans, R. & Gao, J. Deepmind AI reduces Google data centre\ncooling bill by 40%. https://deepmind.com/blog/deepmindai-reduces-google-data-centre-cooling-bill-40/ (2016)\n64. Abe, N . et al. Empirical comparison of various reinforcement\nlearning strategies for sequential targeted marketing. In IEEE Int. Conf. Data Mining 3–10 (2002)\n65. Silver, D., Newnham, L., Barker, D., Weller, S. & McFall, J.\nConcurrent reinforcement learning from customer interactions. In Proc. 30th Int. Conf. Mach. Learn. Vol. 28 924– 932 (2013)\n66. Tromp, J. Tromp–Taylor rules. http://tromp.github.io/\ngo.html (1995)\n67. Müller, M. Computer Go. Artif. Intell. 134, 145–179 (2002)\nArticle  Google Scholar\n68. Shahriari, B., Swersky, K., Wang, Z., Adams, R. P. & de\nFreitas, N. Taking the human out of the loop: a review of Bayesian optimization. Proc. IEEE 104, 148–175 (2016) Article  Google Scholar Download references Acknowledgements We thank A. Cain for work on the visuals; A. Barreto, G. Ostrovski,\nT. Ewalds, T. Schaul, J. Oh and N. Heess for reviewing the paper;\nand the rest of the DeepMind team for their support. Author information Author notes\n1. David Silver, Julian Schrittwieser and Karen Simonyan: These\nauthors contributed equally to this work.\n1. DeepMind, 5 New Street Square, London, EC4A 3TW, UK\nDavid Silver, Julian Schrittwieser, Karen Simonyan, Ioannis Antonoglou, Aja Huang, Arthur Guez, Thomas Hubert, Lucas Baker, Matthew Lai, Adrian Bolton, Yutian Chen, Timothy Lillicrap, Fan Hui, Laurent Sifre, George van den Driessche, Thore Graepel & Demis Hassabis Authors\n1. David Silver\nView author publications Search author on:PubMed Google Scholar\n2. Julian Schrittwieser\nView author publications Search author on:PubMed Google Scholar\n4. Ioannis Antonoglou\nView author publications Search author on:PubMed Google Scholar\n5. Aja Huang\nView author publications Search author on:PubMed Google Scholar\n6. Arthur Guez\nView author publications Search author on:PubMed Google Scholar\n7. Thomas Hubert\nView author publications Search author on:PubMed Google Scholar\n8. Lucas Baker\nView author publications Search author on:PubMed Google Scholar\n9. Matthew Lai\nView author publications Search author on:PubMed Google Scholar\n10. Adrian Bolton\nView author publications Search author on:PubMed Google Scholar\n12. Timothy Lillicrap\nView author publications Search author on:PubMed Google Scholar\n13. Fan Hui\nView author publications Search author on:PubMed Google Scholar\n14. Laurent Sifre\nView author publications Search author on:PubMed Google Scholar\n15. George van den Driessche\nView author publications Search author on:PubMed Google Scholar\n16. Thore Graepel\nView author publications Search author on:PubMed Google Scholar\n17. Demis Hassabis\nView author publications Search author on:PubMed Google Scholar Contributions D.S., J.S., K.S., I.A., A.G., L.S. and T.H. designed and implemented the reinforcement learning algorithm in AlphaGo Zero. A.H., J.S., M.L. and D.S. designed and implemented the search in AlphaGo Zero. L.B., J.S., A.H., F.H., T.H., Y.C. and D.S. designed and implemented the evaluation framework for AlphaGo Zero. D.S., A.B., F.H., A.G., T.L., T.G., L.S., G.v.d.D. and D.H. managed and advised on the project. D.S., T.G. and A.G. wrote the paper. Ethics declarations Competing interests The authors declare no competing financial interests. Additional information Reviewer Information Nature thanks S. Singh and the other anonymous reviewer(s) for their contribution to the peer review of this work. Publisher's note: Springer Nature remains neutral with regard to Extended data figures and tables Extended Data Figure 1 Tournament games between AlphaGo Zero (20 blocks, 3 days) versus AlphaGo Lee using 2 h time controls. One hundred moves of the first 20 games are shown; full games are provided in the Supplementary Information. Extended Data Figure 2 Frequency of occurence over time during training, for each joseki from Fig. 5a (corner sequences common in professional play that were discovered by AlphaGo Zero). The corresponding joseki are shown on the right. Extended Data Figure 4 AlphaGo Zero (20 blocks) selfplay games. The 3-day training run was subdivided into 20 periods. The best player from each period (as selected by the evaluator) played a single game against itself, with 2 h time controls. One hundred moves are shown for each game; full games are provided in the Supplementary Information. Extended Data Figure 5 AlphaGo Zero (40 blocks) selfplay games. The 40-day training run was subdivided into 20 periods. The best player from each period (as selected by the evaluator) played a single game against itself, with 2 h time controls. One hundred moves are shown for each game; full games are provided in the Supplementary Information. Extended Data Figure 6 AlphaGo Zero (40 blocks, 40 days) versus AlphaGo Master tournament games using 2 h time controls. One hundred moves of the first 20 games are shown; full games are provided in the Supplementary Information. Extended Data Table 1 Move prediction accuracy Full size table Extended Data Table 2 Game outcome prediction error Reporting Summary (PDF 67 kb) Supplementary Data This zipped file contains the game records of self-play and tournament games played by AlphaGo Zero in .sgf format. (ZIP 82 kb) PowerPoint slides PowerPoint slide for Fig. 1 PowerPoint slide for Fig. 2 PowerPoint slide for Fig. 3 PowerPoint slide for Fig. 4 PowerPoint slide for Fig. 5 PowerPoint slide for Fig. 6 Rights and permissions Reprints and permissions About this article [image] Download citation •  Received: 07 April 2017 •  Accepted: 13 September 2017 •  Published: 19 October 2017 •  Issue Date: 19 October 2017 Share this article Anyone you share the following link with will be able to read this content: Get shareable link Sorry, a shareable link is not currently available for this article. Copy to clipboard Provided by the Springer Nature SharedIt content-sharing initiative This article is cited by •  Comparative study of 2D vs. 3D AI-enhanced ultrasound for fetal crown-rump length evaluation in the first trimester ○  Yuanji Zhang ○  Yuhao Huang ○  Dong Ni •  Applications of machine learning and deep learning in musculoskeletal medicine: a narrative review ○  Martina Feierabend ○  Julius Michael Wolfgart ○  Ulf Krister Hofmann European Journal of Medical Research (2025) •  Deep knowledge tracing and cognitive load estimation for personalized learning path generation using neural network architecture ○  Chunyan Tong ○  Changhong Ren Scientific Reports (2025) •  An explainable deep learning platform for molecular discovery ○  Felix Wong ○  Satotaka Omori ○  James J. Collins Nature Protocols (2025) •  Relative importance sampling for off-policy actor-critic in deep reinforcement learning ○  Mahammad Humayoo ○  Gengzhong Zheng ○  Xueqi Cheng Change institution Buy or subscribe show all Editorial Summary AlphaGo Zero goes solo To beat world champions at the game of Go, the computer program AlphaGo has relied largely on supervised learning from millions of human expert moves. David Silver and colleagues have now produced a system called AlphaGo Zero, which is based purely on reinforcement learning and learns solely from self-play. Starting from random moves, it can reach superhuman level in just a couple of days of training and five million games of self-play, and can now beat all previous versions of AlphaGo. Because the machine independently discovers the same fundamental principles of the game that took humans millennia to conceptualize, the work suggests that such principles have some universal character, beyond human bias. Associated content Learning to play Go from scratch •  Satinder Singh •  Andy Okun •  Andrew Jackson Nature News & Views 19 Oct 2017 Advertisement •  Research articles •  News •  Opinion •  Research Analysis •  Careers •  Books & Culture •  Podcasts •  Videos •  Current issue •  Browse issues •  Collections •  Subjects •  Follow us on Facebook •  Follow us on Twitter •  Subscribe •  Sign up for alerts •  RSS feed About the journal •  Journal Staff •  About the Editors •  Journal Information •  Our publishing models •  Editorial Values Statement •  Journal Metrics •  Awards •  Contact •  Editorial policies •  History of Nature •  Send a news tip •  For Authors •  For Referees •  Language editing services •  Open access funding •  Submit manuscript Search Search articles by subject, keyword or author Show results from All journals This journal Search Advanced search Quick links •  Explore articles by subject •  Find a job •  Guide to authors •  Editorial policies Nature (Nature) ISSN 1476-4687 (online) ISSN 0028-0836 (print) nature.com sitemap About Nature Portfolio •  About us •  Press releases •  Press office •  Contact us •  Journals A-Z •  Articles by subject •  protocols.io •  Nature Index Publishing policies •  Nature portfolio policies •  Open access Author & Researcher services •  Reprints & permissions •  Research data •  Language editing •  Scientific editing •  Nature Masterclasses •  Research Solutions Libraries & institutions •  Librarian service & tools •  Librarian portal •  Open research •  Recommend to library Advertising & partnerships •  Advertising •  Partnerships & Services •  Media kits •  Branded content Professional development Regional websites •  Nature Africa •  Nature China •  Nature India •  Nature Japan •  Nature Middle East •  Privacy Policy •  Use of cookies •  Your privacy choices/Manage cookies •  Legal notice •  Accessibility statement •  Terms & Conditions •  Your US state privacy rights [image] © 2025 Springer Nature Limited Close [image] Close Get the most important science stories of the day, free in your inbox. Sign up for Nature Briefing: AI and Robotics [image]\n--- FOOTNOTES Authors and Affiliations Correspondence to David Silver. jurisdictional claims in published maps and institutional affiliations.\n\nMethods Reinforcement learning. Policy iteration20,21 is a classic algorithm that generates a sequence of improving policies, by alternating between policy evaluation— estimating the value function of the current policy—and policy improvement— using the current value function to generate a better policy. A simple approach to policy evaluation is to estimate the value function from the outcomes of sampled trajectories35,36. A simple approach to policy improvement is to select actions greedily with respect to the value function20. In large state spaces, approximations are necessary to evaluate each policy and to represent its improvement22,23 . Classification-based reinforcement learning37 improves the policy using a simple Monte Carlo search. Many rollouts are executed for each action; the action with the maximum mean value provides a positive training example, while all other actions provide negative training examples; a policy is then trained to classify actions as positive or negative, and used in subsequent rollouts. This may be viewed as a precursor to the policy component of AlphaGo Zero’s training algorithm when τ→ 0. A more recent instantiation, classification-based modified policy iteration (CBMPI), also performs policy evaluation by regressing a value function towards truncated rollout values, similar to the value component of AlphaGo Zero; this achieved state-of-the-art results in the game of Tetris38. However, this previous work was limited to simple rollouts and linear function approximation using handcrafted features. The AlphaGo Zero self-play algorithm can similarly be understood as an approximate policy iteration scheme in which MCTS is used for both policy improvement and policy evaluation. Policy improvement starts with a neural network policy, executes an MCTS based on that policy’s recommendations, and then projects the (much stronger) search policy back into the function space of the neural network. Policy evaluation is applied to the (much stronger) search policy: the outcomes of self-play games are also projected back into the function space of the neural network. These projection steps are achieved by training the neural network parameters to match the search probabilities and self-play game outcome respectively. Guo et al.7 also project the output of MCTS into a neural network, either by regressing a value network towards the search value, or by classifying the action selected by MCTS. This approach was used to train a neural network for playing Atari games; however, the MCTS was fixed—there was no policy iteration—and did not make any use of the trained networks. Self-play reinforcement learning in games. Our approach is most directly applicable to Zero-sum games of perfect information. We follow the formalism of alternating Markov games described in previous work12, noting that algorithms based on value or policy iteration extend naturally to this setting39 . Self-play reinforcement learning has previously been applied to the game of Go. NeuroGo40,41 used a neural network to represent a value function, using a sophisticated architecture based on Go knowledge regarding connectivity, territory and eyes. This neural network was trained by temporal-difference learning42 to predict territory in games of self-play, building on previous work43. A related approach, RLGO44, represented the value function instead by a linear combination of features, exhaustively enumerating all 3 × 3 patterns of stones; it was trained by temporal-difference learning to predict the winner in games of self-play. Both NeuroGo and RLGO achieved a weak amateur level of play. MCTS may also be viewed as a form of self-play reinforcement learning45. The nodes of the search tree contain the value function for the positions encountered during search; these values are updated to predict the winner of simulated games of self-play. MCTS programs have previously achieved strong amateur level in Go46,47 , but used substantial domain expertise: a fast rollout policy, based on handcrafted features13,48, that evaluates positions by running simulations until the end of the game; and a tree policy, also based on handcrafted features, that selects moves within the search tree47 . Self-play reinforcement learning approaches have achieved high levels of performance in other games: chess49–51, checkers52, backgammon53, othello54, Scrabble55 and most recently poker56. In all of these examples, a value function was trained by regression54–56 or temporal-difference learning49–53 from training data generated by self-play. The trained value function was used as an evaluation function in an alpha–beta search49–54, a simple Monte Carlo search55,57 or counterfactual regret minimization56. However, these methods used handcrafted input features49–53,56 or handcrafted feature templates54,55. In addition, the learning process used supervised learning to initialize weights58, hand-selected weights for piece values49,51,52 , handcrafted restrictions on the action space56 or used pre-existing computer programs as training opponents49,50, or to generate game records51 . Many of the most successful and widely used reinforcement learning methods were first introduced in the context of Zero-sum games: temporal-difference learning was first introduced for a checkers-playing program59, while MCTS was introduced for the game of Go13. However, very similar algorithms have subsequently proven highly effective in video games6–8,10, robotics60, industrial control61–63 and online recommendation systems64,65 . AlphaGo versions. We compare three distinct versions of AlphaGo: (1) AlphaGo Fan is the previously published program12 that played against Fan Hui in October 2015. This program was distributed over many machines using 176 GPUs. (2) AlphaGo Lee is the program that defeated Lee Sedol 4–1 in March 2016. It was previously unpublished, but is similar in most regards to AlphaGo Fan12 . However, we highlight several key differences to facilitate a fair comparison. First, the value network was trained from the outcomes of fast games of self-play by AlphaGo, rather than games of self-play by the policy network; this procedure was iterated several times—an initial step towards the tabula rasa algorithm presented in this paper. Second, the policy and value networks were larger than those described in the original paper—using 12 convolutional layers of 256 planes— and were trained for more iterations. This player was also distributed over many machines using 48 TPUs, rather than GPUs, enabling it to evaluate neural networks faster during search. (3) AlphaGo Master is the program that defeated top human players by 60–0 in January 201734. It was previously unpublished, but uses the same neural network architecture, reinforcement learning algorithm, and MCTS algorithm as described in this paper. However, it uses the same handcrafted features and rollouts as AlphaGo Lee12 and training was initialized by supervised learning from human data. (4) AlphaGo Zero is the program described in this paper. It learns from self- play reinforcement learning, starting from random initial weights, without using rollouts, with no human supervision and using only the raw board history as input features. It uses just a single machine in the Google Cloud with 4 TPUs (AlphaGo Zero could also be distributed, but we chose to use the simplest possible search algorithm). Domain knowledge. Our primary contribution is to demonstrate that superhuman performance can be achieved without human domain knowledge. To clarify this contribution, we enumerate the domain knowledge that AlphaGo Zero uses, explicitly or implicitly, either in its training procedure or its MCTS; these are the items of knowledge that would need to be replaced for AlphaGo Zero to learn a different (alternating Markov) game. (1) AlphaGo Zero is provided with perfect knowledge of the game rules. These are used during MCTS, to simulate the positions resulting from a sequence of moves, and to score any simulations that reach a terminal state. Games terminate when both players pass or after 19 × 19 × 2 = 722 moves. In addition, the player is provided with the set of legal moves in each position. (2) AlphaGo Zero uses Tromp–Taylor scoring66 during MCTS simulations and self-play training. This is because human scores (Chinese, Japanese or Korean rules) are not well-defined if the game terminates before territorial boundaries are resolved. However, all tournament and evaluation games were scored using Chinese rules. (3) The input features describing the position are structured as a 19 × 19 image; that is, the neural network architecture is matched to the grid-structure of the board. (4) The rules of Go are invariant under rotation and reflection; this knowledge has been used in AlphaGo Zero both by augmenting the dataset during training to include rotations and reflections of each position, and to sample random rotations or reflections of the position during MCTS (see Search algorithm). Aside from komi, the rules of Go are also invariant to colour transposition; this knowledge is exploited by representing the board from the perspective of the current player (see Neural network architecture). AlphaGo Zero does not use any form of domain knowledge beyond the points listed above. It only uses its deep neural network to evaluate leaf nodes and to select moves (see ‘Search algorithm’). It does not use any rollout policy or tree policy, and the MCTS is not augmented by any other heuristics or domain-specific rules. No legal moves are excluded—even those filling in the player’s own eyes (a standard heuristic used in all previous programs67). The algorithm was started with random initial parameters for the neural network. The neural network architecture (see ‘Neural network architecture’) is based on the current state of the art in image recognition4,18, and hyperparameters for training were chosen accordingly (see ‘Self-play training pipeline’). MCTS search parameters were selected by Gaussian process optimization68, so as to optimize self-play performance of AlphaGo Zero using a neural network trained in a preliminary run. For the larger run (40 blocks, 40 days), MCTS search parameters were re-optimized using the neural network trained in the smaller run (20 blocks, 3 days). The training algorithm was executed autonomously without human intervention. Self-play training pipeline. AlphaGo Zero’s self-play training pipeline consists of three main components, all executed asynchronously in parallel. Neural network parameters θi are continually optimized from recent self-play data; AlphaGo Zero © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. players αθi are continually evaluated; and the best performing player so far, αθ∗, is used to generate new self-play data. Optimization. Each neural network θ f i is optimized on the Google Cloud using TensorFlow, with 64 GPU workers and 19 CPU parameter servers. The batch-size is 32 per worker, for a total mini-batch size of 2,048. Each mini-batch of data is sampled uniformly at random from all positions of the most recent 500,000 games of self-play. Neural network parameters are optimized by stochastic gradient descent with momentum and learning rate annealing, using the loss in equation (1). The learning rate is annealed according to the standard schedule in Extended Data Table 3. The momentum parameter is set to 0.9. The cross-entropy and MSE losses are weighted equally (this is reasonable because rewards are unit scaled, r ∈ {− 1, + 1}) and the L2 regularization parameter is set to c = 10−4. The optimization process produces a new checkpoint every 1,000 training steps. This checkpoint is evaluated by the evaluator and it may be used for generating the next batch of self-play games, as we explain next. Evaluator. To ensure we always generate the best quality data, we evaluate each new neural network checkpoint against the current best network f before using θ∗ it for data generation. The neural network θ f i is evaluated by the performance of an MCTS search αθi that uses θ f i to evaluate leaf positions and prior probabilities (see Search algorithm). Each evaluation consists of 400 games, using an MCTS with 1,600 simulations to select each move, using an infinitesimal temperature τ→ 0 (that is, we deterministically select the move with maximum visit count, to give the strongest possible play). If the new player wins by a margin of > 55% (to avoid selecting on noise alone) then it becomes the best player αθ∗, and is subsequently used for self-play generation, and also becomes the baseline for subsequent comparisons. Self-play. The best current player αθ∗, as selected by the evaluator, is used to generate data. In each iteration, αθ∗ plays 25,000 games of self-play, using 1,600 simulations of MCTS to select each move (this requires approximately 0.4 s per search). For the first 30 moves of each game, the temperature is set to τ = 1; this selects moves proportionally to their visit count in MCTS, and ensures a diverse set of positions are encountered. For the remainder of the game, an infinitesimal temperature is used, τ→ 0. Additional exploration is achieved by adding Dirichlet noise to the prior probabilities in the root node s0, specifically P(s, a) = (1 − ε)pa + εηa, where η ∼ Dir(0.03) and ε = 0.25; this noise ensures that all moves may be tried, but the search may still overrule bad moves. In order to save computation, clearly lost games are resigned. The resignation threshold vresign is selected automatically to keep the fraction of false positives (games that could have been won if AlphaGo had not resigned) below 5%. To measure false positives, we disable resignation in 10% of self-play games and play until termination. Supervised learning. For comparison, we also trained neural network parameters θSL by supervised learning. The neural network architecture was identical to AlphaGo Zero. Mini-batches of data (s, π, z) were sampled at random from the KGS dataset, setting πa = 1 for the human expert move a. Parameters were optimized by stochastic gradient descent with momentum and learning rate annealing, using the same loss as in equation (1), but weighting the MSE component by a factor of 0.01. The learning rate was annealed according to the standard schedule in Extended Data Table 3. The momentum parameter was set to 0.9, and the L2 regularization parameter was set to c = 10−4 . By using a combined policy and value network architecture, and by using a low weight on the value component, it was possible to avoid overfitting to the values (a problem described in previous work12). After 72 h the move prediction accuracy exceeded the state of the art reported in previous work12,30–33, reaching 60.4% on the KGS test set; the value prediction error was also substantially better than previously reported12. The validation set was composed of professional games from GoKifu. Accuracies and MSEs are reported in Extended Data Table 1 and Extended Data Table 2, respectively. Search algorithm. AlphaGo Zero uses a much simpler variant of the asynchronous policy and value MCTS algorithm (APV-MCTS) used in AlphaGo Fan and AlphaGo Lee. Each node s in the search tree contains edges (s, a) for all legal actions ∈ A a s ( ). Each edge stores a set of statistics, { ( , ), ( , ), ( , ), ( , )} N s a W s a Q s a P s a where N(s, a) is the visit count, W(s, a) is the total action value, Q(s, a) is the mean action value and P(s, a) is the prior probability of selecting that edge. Multiple simulations are executed in parallel on separate search threads. The algorithm proceeds by iterating over three phases (Fig. 2a–c), and then selects a move to play (Fig. 2d). Select (Fig. 2a). The selection phase is almost identical to AlphaGo Fan12; we recapitulate here for completeness. The first in-tree phase of each simulation begins at the root node of the search tree, s0, and finishes when the simulation reaches a Article reSEArcH leaf node sL at time-step L. At each of these time-steps, t < L, an action is selected according to the statistics in the search tree, = t argmax( ( , ) t t , + a Q s a U s a ( , )) using a variant of the PUCT algorithm24 a , ∑ N s b b U s a c P s a ( , ) ( , ) ( , ) = puct 1 ( , ) + N s a where cpuct is a constant determining the level of exploration; this search control strategy initially prefers actions with high prior probability and low visit count, but asympotically prefers actions with high action value. Expand and evaluate (Fig. 2b). The leaf node sL is added to a queue for neural network evaluation, (di(p), v) = fθ(di(sL)), where di is a dihedral reflection or rotation selected uniformly at random from i in [1..8]. Positions in the queue are evaluated by the neural network using a mini-batch size of 8; the search thread is locked until evaluation completes. The leaf node is expanded and each edge (sL, a) is initialized to {N(sL, a) = 0, W(sL, a) = 0, Q(sL, a) = 0, P(sL, a) = pa}; the value v is then backed up. Backup (Fig. 2c). The edge statistics are updated in a backward pass through each step t ≤ L. The visit counts are incremented, N(st, at) = N(st, at) + 1, and the action value is updated to the mean value, = + = W s a ( , ) W s a W s a v Q s a ( , ) ( , ) , ( , ) t t t t t t t t N s a ( , ) t t We use virtual loss to ensure each thread evaluates different nodes12,69 . Play (Fig. 2d). At the end of the search AlphaGo Zero selects a move a to play in the root position s0, proportional to its exponentiated visit count, 1 a s N s a N s b π | = / ∑τ τ / / ( ) ( , ) ( , ) 0 0 b 0 1 , where τ is a temperature parameter that controls the level of exploration. The search tree is reused at subsequent time-steps: the child node corresponding to the played action becomes the new root node; the subtree below this child is retained along with all its statistics, while the remainder of the tree is discarded. AlphaGo Zero resigns if its root value and best child value are lower than a threshold value vresign. Compared to the MCTS in AlphaGo Fan and AlphaGo Lee, the principal differences are that AlphaGo Zero does not use any rollouts; it uses a single neural network instead of separate policy and value networks; leaf nodes are always expanded, rather than using dynamic expansion; each search thread simply waits for the neural network evaluation, rather than performing evaluation and backup asynchronously; and there is no tree policy. A transposition table was also used in the large (40 blocks, 40 days) instance of AlphaGo Zero. Neural network architecture. The input to the neural network is a 19 × 19 × 17 image stack comprising 17 binary feature planes. Eight feature planes, Xt, consist of binary values indicating the presence of the current player’s stones (= X 1 i if t intersection i contains a stone of the player’s colour at time-step t; 0 if the intersection is empty, contains an opponent stone, or if t < 0). A further 8 feature planes, Yt, represent the corresponding features for the opponent’s stones. The final feature plane, C, represents the colour to play, and has a constant value of either 1 if black is to play or 0 if white is to play. These planes are concatenated together to give input features st = [Xt, Yt, Xt−1, Yt−1,..., Xt−7, Yt−7, C]. History features Xt, Yt are necessary, because Go is not fully observable solely from the current stones, as repetitions are forbidden; similarly, the colour feature C is necessary, because the komi is not observable. The input features st are processed by a residual tower that consists of a single convolutional block followed by either 19 or 39 residual blocks4 . The convolutional block applies the following modules: (1) A convolution of 256 filters of kernel size 3 × 3 with stride 1 (2) Batch normalization18 (3) A rectifier nonlinearity Each residual block applies the following modules sequentially to its input: (1) A convolution of 256 filters of kernel size 3 × 3 with stride 1 (2) Batch normalization (3) A rectifier nonlinearity (4) A convolution of 256 filters of kernel size 3 × 3 with stride 1 (5) Batch normalization (6) A skip connection that adds the input to the block (7) A rectifier nonlinearity The output of the residual tower is passed into two separate ‘heads’ for computing the policy and value. The policy head applies the following modules: (1) A convolution of 2 filters of kernel size 1 × 1 with stride 1 (2) Batch normalization (3) A rectifier nonlinearity (4) A fully connected linear layer that outputs a vector of size 192 + 1 = 362, corresponding to logit probabilities for all intersections and the pass move The value head applies the following modules: (1) A convolution of 1 filter of kernel size 1 × 1 with stride 1 (2) Batch normalization (3) A rectifier nonlinearity © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved. reSEArcH Article (4) A fully connected linear layer to a hidden layer of size 256 (5) A rectifier nonlinearity (6) A fully connected linear layer to a scalar (7) A tanh nonlinearity outputting a scalar in the range [− 1, 1] The overall network depth, in the 20- or 40-block network, is 39 or 79 parameterized layers, respectively, for the residual tower, plus an additional 2 layers for the policy head and 3 layers for the value head. We note that a different variant of residual networks was simultaneously applied to computer Go33 and achieved an amateur dan-level performance; however, this was restricted to a single-headed policy network trained solely by supervised learning. Neural network architecture comparison. Figure 4 shows the results of a comparison between network architectures. Specifically, we compared four different neural networks: (1) dual–res: the network contains a 20-block residual tower, as described above, followed by both a policy head and a value head. This is the architecture used in AlphaGo Zero. (2) sep–res: the network contains two 20-block residual towers. The first tower is followed by a policy head and the second tower is followed by a value head. (3) dual–conv: the network contains a non-residual tower of 12 convolutional blocks, followed by both a policy head and a value head. (4) sep–conv: the network contains two non-residual towers of 12 convolutional blocks. The first tower is followed by a policy head and the second tower is followed by a value head. This is the architecture used in AlphaGo Lee. Each network was trained on a fixed dataset containing the final 2 million games of self-play data generated by a previous run of AlphaGo Zero, using stochastic gradient descent with the annealing rate, momentum and regularization hyperparameters described for the supervised learning experiment; however, cross-entropy and MSE components were weighted equally, since more data was available. Evaluation. We evaluated the relative strength of AlphaGo Zero (Figs 3a, 6) by measuring the Elo rating of each player. We estimate the probability that player a will defeat player b by a logistic function = 1 P a b ( defeats ) , and + − c e b e a 1 exp( ( ( ) ( )) elo estimate the ratings e(·) by Bayesian logistic regression, computed by the BayesElo program25 using the standard constant celo = 1/400. Elo ratings were computed from the results of a 5 s per move tournament between AlphaGo Zero, AlphaGo Master, AlphaGo Lee and AlphaGo Fan. The raw neural network from AlphaGo Zero was also included in the tournament. The Elo ratings of AlphaGo Fan, Crazy Stone, Pachi and GnuGo were anchored to the tournament values from previous work12, and correspond to the players reported in that work. The results of the matches of AlphaGo Fan against Fan Hui and AlphaGo Lee against Lee Sedol were also included to ground the scale to human references, as otherwise the Elo ratings of AlphaGo are unrealistically high due to self-play bias. The Elo ratings in Figs 3a, 4a, 6a were computed from the results of evaluation games between each iteration of player αθi during self-play training. Further evaluations were also performed against baseline players with Elo ratings anchored to the previously published values12 . We measured the head-to-head performance of AlphaGo Zero against AlphaGo Lee, and the 40-block instance of AlphaGo Zero against AlphaGo Master, using the same player and match conditions that were used against Lee Sedol in Seoul, 2016. Each player received 2 h of thinking time plus 3 byoyomi periods of 60 s per move. All games were scored using Chinese rules with a komi of 7.5 points. Data availability. The datasets used for validation and testing are the GoKifu dataset (available from http://gokifu.com/) and the KGS dataset (available from https://u-go.net/gamerecords/). 35. 36. Barto, A. G. & Duff, M. Monte Carlo matrix inversion and reinforcement learning. Adv. Neural Inf. Process. Syst. 6, 687–694 (1994). Singh, S. P. & Sutton, R. S. Reinforcement learning with replacing eligibility traces. Mach. Learn. 22, 123–158 (1996). 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. Lagoudakis, M. G. & Parr, R. Reinforcement learning as classification: leveraging modern classifiers. In Proc. 20th Int. Conf. Mach. Learn. 424–431 (2003). Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B. & Geist, M. Approximate modified policy iteration and its application to the game of Tetris. J. Mach. Learn. Res. 16, 1629–1676 (2015). Littman, M. L. Markov games as a framework for multi-agent reinforcement learning. In Proc. 11th Int. Conf. Mach. Learn. 157–163 (1994). Enzenberger, M. The integration of a priori knowledge into a Go playing neural network. http://www.cgl.ucsf.edu/go/Programs/neurogo-html/neurogo.html (1996). Enzenberger, M. in Advances in Computer Games (eds Van Den Herik, H. J., Iida, H. & Heinz, E. A.) 97–108 (2003). Sutton, R. Learning to predict by the method of temporal differences. Mach. Learn. 3, 9–44 (1988). Schraudolph, N. N., Dayan, P. & Sejnowski, T. J. Temporal difference learning of position evaluation in the game of Go. Adv. Neural Inf. Process. Syst. 6, 817–824 (1994). Silver, D., Sutton, R. & Müller, M. Temporal-difference search in computer Go. Mach. Learn. 87, 183–219 (2012). Silver, D. Reinforcement Learning and Simulation-Based Search in Computer Go. PhD thesis, Univ. Alberta, Edmonton, Canada (2009). Gelly, S. & Silver, D. Monte-Carlo tree search and rapid action value estimation in computer Go. Artif. Intell. 175, 1856–1875 (2011). Coulom, R. Computing Elo ratings of move patterns in the game of Go. Int. Comput. Games Assoc. J. 30, 198–208 (2007). Gelly, S., Wang, Y., Munos, R. & Teytaud, O. Modification of UCT with patterns in Monte-Carlo Go. Report No. 6062 (INRIA, 2006). Baxter, J., Tridgell, A. & Weaver, L. Learning to play chess using temporal differences. Mach. Learn. 40, 243–263 (2000). Veness, J., Silver, D., Blair, A. & Uther, W. Bootstrapping from game tree search. In Adv. Neural Inf. Process. Syst. 1937–1945 (2009). Lai, M. Giraffe: Using Deep Reinforcement Learning to Play Chess. MSc thesis, Imperial College London (2015). Schaeffer, J., Hlynka, M. & Jussila, V. Temporal difference learning applied to a high-performance game-playing program. In Proc. 17th Int. Jt Conf. Artif. Intell. Vol. 1 529–534 (2001). Tesauro, G. TD-gammon, a self-teaching backgammon program, achieves master-level play. Neural Comput. 6, 215–219 (1994). Buro, M. From simple features to sophisticated evaluation functions. In Proc. 1st Int. Conf. Comput. Games 126–145 (1999). Sheppard, B. World-championship-caliber Scrabble. Artif. Intell. 134, 241–275 (2002). Moravc ˇík, M. et al. DeepStack: expert-level artificial intelligence in heads-up no-limit poker. Science 356, 508–513 (2017). Tesauro, G & Galperin, G. On-line policy improvement using Monte-Carlo search. In Adv. Neural Inf. Process. Syst. 1068–1074 (1996). Tesauro, G. Neurogammon: a neural-network backgammon program. In Proc. Int. Jt Conf. Neural Netw. Vol. 3, 33–39 (1990). Samuel, A. L. Some studies in machine learning using the game of checkers II - recent progress. IBM J. Res. Develop. 11, 601–617 (1967). Kober, J., Bagnell, J. A. & Peters, J. Reinforcement learning in robotics: a survey. Int. J. Robot. Res. 32, 1238–1274 (2013). Zhang, W. & Dietterich, T. G. A reinforcement learning approach to job-shop scheduling. In Proc. 14th Int. Jt Conf. Artif. Intell. 1114–1120 (1995). Cazenave, T., Balbo, F. & Pinson, S. Using a Monte-Carlo approach for bus regulation. In Int. IEEE Conf. Intell. Transport. Syst. 1–6 (2009). Evans, R. & Gao, J. Deepmind AI reduces Google data centre cooling bill by 40%. https://deepmind.com/blog/deepmind-ai-reduces-google-data-centre- cooling-bill-40/ (2016). Abe, N. et al. Empirical comparison of various reinforcement learning strategies for sequential targeted marketing. In IEEE Int. Conf. Data Mining 3–10 (2002). Silver, D., Newnham, L., Barker, D., Weller, S. & McFall, J. Concurrent reinforcement learning from customer interactions. In Proc. 30th Int. Conf. Mach. Learn. Vol. 28 924–932 (2013). Tromp, J. Tromp–Taylor rules. http://tromp.github.io/go.html (1995). Müller, M. Computer Go. Artif. Intell. 134, 145–179 (2002). Shahriari, B., Swersky, K., Wang, Z., Adams, R. P. & de Freitas, N. Taking the human out of the loop: a review of Bayesian optimization. Proc. IEEE 104, 148–175 (2016). Segal, R. B. On the scalability of parallel UCT. Comput. Games 6515, 36–47 (2011).", "persona": "Student", "response": "### 1. Overview\nGoogle’s AI subsidiary DeepMind has unveiled the latest version of its Go-playing software, AlphaGo Zero. The new program is a significantly better player than the version that beat the game’s world champion earlier this year, but, more importantly, it’s also entirely self-taught. DeepMind says this means the company is one step closer to creating general purpose algorithms that can intelligently tackle some of the hardest problems in science, from designing new drugs to more accurately modeling the effects of climate change. For experts in the field, these developments are a big part of what makes this new research exciting. That’s is because they offer a rebuttal to a persistent criticism of contemporary AI: that much of its recent gains come mostly from cheap computing power and massive datasets. Skeptics in the field like pioneer Geoffrey Hinton suggest that machine learning is a bit of a one-trick pony. Piling on data and compute is helping deliver new functions, but the current pace of advances is unsustainable.\n\n### 2. Background\nDeepMind’s latest research offers something of a rebuttal by demonstrating that there are major improvements to be made simply by focusing on algorithms. In the case of AlphaGo Zero, what is particularly clever is the removal of any need for human expertise in the system. Satinder Singh, a computer science professor who wrote an accompanying article on DeepMind’s research in Nature, praises the company’s work as “elegant,” and singles out these aspects. “Over the past five, six years, reinforcement learning has emerged from academia to have much more broader impact in the wider world, and DeepMind can take some of the credit for that,” says Singh. “The fact that they were able to build a better Go player here with an order of magnitude less data, computation, and time, using just straight reinforcement learning — it’s a pretty big achievement.\n\n### 3. Methods\nThe original AlphaGo demonstrated superhuman Go-playing ability, but needed the expertise of human players to get there. Namely, it used a dataset of more than 100,000 Go games as a starting point for its own knowledge. AlphaGo Zero, by comparison, has only been programmed with the basic rules of Go. Everything else it learned from scratch. As described in a url published in Nature today, Zero developed its Go skills by competing against itself.\n\n### 4. Results\nIt started with random moves on the board, but every time it won, Zero updated its own system, and played itself again. And again. Millions of times over. After three days of self-play, Zero was strong enough to defeat the version of itself that beat 18-time world champion Lee Se-dol, winning handily — 100 games to nil. After 40 days, it had a 90 percent win rate against the most advanced version of the original AlphaGo software. DeepMind says this makes it arguably the strongest Go player in history. Silver explained that as Zero played itself, it rediscovered Go strategies developed by humans over millennia.\n\n### 5. Takeaways\n“We’ve removed the constraints of human knowledge.” “By not using human data — by not using human expertise in any fashion — we’ve actually removed the constraints of human knowledge,” said AlphaGo Zero’s lead programmer, David Silver, at a press conference. “It’s therefore able to create knowledge itself from first principles; from a blank slate [...] This enables it to be much more powerful than previous versions.” “This work shows that a combination of existing techniques can go somewhat further than most people in the field have thought, even though the techniques themselves are not fundamentally new,” Ilya Sutskever, a research director at the Elon Musk-backed OpenAI institute, told The Verge. “But ultimately, what matters is that researchers keep advancing the field, and it’s less important if this goal is achieved by developing radically new techniques, or by applying existing techniques in clever and unexpected ways.” Hassabis explains that you can think of AlphaGo as essentially a very good machine for searching through complicated data. In the case of Zero, that data is comprised of possible moves in a game of Go. But because Zero was not programmed to understand Go specifically, it could be reprogrammed to discover information in other fields: drug discovery, protein folding, quantum chemistry, particle physics, and material design.", "paper_id": "46", "story_id": "46-03"}
{"paper": "Toward Fairness in AI for People with Disabilities: A Research Roadmap Anhong Guo1,2, Ece Kamar1, Jennifer Wortman Vaughan1, Hanna Wallach1, Meredith Ringel Morris1 anhongg@cs.cmu.edu, {eckamar, jenn, wallach, merrie}@microsoft.com\n\nABSTRACT\nAI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.\n\nINTRODUCTION\nAs AI systems increasingly pervade modern life, ensuring that they work fairly for all is an important challenge. Researchers have identiﬁed unfair gender and racial bias in existing AI systems [2, 7, 9]. To understand how AI systems work across different groups of people, it is necessary to develop inclusive tools and practices for evaluation and to identify cases in which homogeneous, non-inclusive data [9] or data reﬂecting negative historical biases [2, 7] is used for system training. Although improving the lives of people with disabilities (PWD)1 is a motivator for many state-of-the-art AI systems, and although such systems have the potential to mitigate many disabling conditions [6], considerations regarding fairness in AI for PWD have thus far received little attention [73]. Fairness issues for PWD may be more difﬁcult to remedy than fairness issues for other groups, particularly where people with particular classes of disability may represent a relatively small proportion of a population. Even if included in training and evaluation data, they may be overlooked as outliers by current AI techniques [73]. Such issues threaten to lock PWD out of access to key technologies (e.g., if voice-activated smart speakers do not recognize input from people with speech disabilities), inadvertently amplify existing stereotypes against them (e.g., if a chatbot learns to mimic someone with a disability), or even actively endanger their safety (e.g., if self-driving cars are not trained to recognize pedestrians using wheelchairs). We propose the following research agenda to identify and remedy shortcomings of AI systems for PWD: (1) Identify ways in which inclusion issues for PWD may impact AI systems; (2) Test inclusion hypotheses to understand failure scenarios and the extent to which existing bias mitigation techniques (e.g., [18, 33, 37]) work; (3) Create benchmark datasets to support replication and inclusion (and handle the complex ethical issues that creating such datasets for vulnerable groups might involve); and (4) Innovate new modeling, bias mitigation, and error measurement techniques in order to address any shortcomings of status quo methods with respect to PWD. In this position paper, we take a step toward the ﬁrst of these goals by reﬂecting on ways in which current key classes of AI systems may necessitate particular consideration with respect to different classes of disability. Systematically studying the extent to which these interactions exist in practice, or demonstrating that they deﬁnitely do not, is an important next step toward creating AI inclusive of PWD; however, articulating the extent of a problem is a necessary precursor to remediation. Furthermore, we note that the question of whether it is even ethical to build certain categories of AI is an important one (and may be dependent on use context). Our mention of various classes of AI is not an endorsement of whether we think such systems should be built, but is simply describing how they may interact with disability. Indeed, there is a larger ethical discussion to be had on how limiting some types of AI with negative associations (like synthetic voices that could be used for deepfakes [11]) might disenfranchise PWD who could beneﬁt from such tech (i.e., by limiting the opportunity to realistically reproduce the voice of someone who can no longer speak).\n\nRISK ASSESSMENT OF EXISTING AI SYSTEMS FOR PWD Here, we group existing classes of AI systems by related functionalities, and identify disability constituencies for whom these systems may be problematic. This risk assessment is meant as a starting point to spark further research, and may not be exhaustive. For example, as new AI technologies are developed they would require consideration with respect to disability. Additionally, while we strove to anticipate ways in which classes of AI may fail for some disability groups, we may not have exhaustively identiﬁed all such groups; indeed, the “long tail” of disability and potential co-occurrence of multiple disabilities are two of many reasons that ensuring AI inclusion for PWD is particularly challenging [73]. Computer Vision Computer vision systems analyze still or video camera inputs to identify patterns, such as the presence and attributes of faces, bodies, or objects. Disabilities that may impact a person’s physical appearance (facial features, facial expressions, body size or proportions, presence of assistive equipment, atypical motion properties) are important to consider when designing and testing the fairness of computer vision algorithms. Face Recognition Face recognition systems include capabilities for identifying the presence of a face and/or making inferences about its properties, including face detection, identiﬁcation (i.e., to guess the identity of a speciﬁc person), veriﬁcation (i.e., to validate a claimed identity), and analysis (e.g., gender classiﬁcation, emotion analysis). Face recognition systems are already used in a wide variety of scenarios, including biometric authentication [3, 52], security systems [21], criminal justice [61], interview support software [34], and social/entertainment applications [23], many of which are controversial. We hypothesize that such techniques may not work well for people with differences in facial features and expressions if they were not considered when gathering training data and evaluating models. For instance, various aspects of facial analysis software may not work well for people with conditions such as Down syndrome, achondroplasia, cleft lip/palate, or other conditions that result in characteristic facial differences. Such systems may also fail for people who are blind, which may not only result in differences in eye anatomy, but may also result in a person wearing medical or cosmetic aids such as dark glasses, and may produce unanticipated behaviors, such as a person not holding their face toward a camera at the expected angle. Emotion processing algorithms may misinterpret the facial expressions of someone with autism or Williams syndrome, who may not emote in a conventional manner; expression interpretation may also be problematic for people who have experienced stroke, Parkinson’s disease, Bell’s Palsy, or other conditions that restrict facial movements. Body Recognition Body recognition systems include capabilities for identifying the presence of a body and/or making inferences about its properties, such as body detection, identiﬁcation, veriﬁcation, and analysis. Body recognition systems can power applications using gesture recognition (e.g., in VR and AR [4, 49] or gaming [47]), or gait analysis (e.g., for biometric authentication [78], sports biomechanics [54], and path predictions used by self-driving vehicles [74]). Body recognition systems may not work well for PWD characterized by body shape, posture, or mobility differences. For example, gesture recognition systems2 are unlikely to work well for people with differences in morphology (e.g., a person with an amputated arm may be unable to perform bimanual gestures, or may grip a device differently than expected; a person with polydactyly’s style of touching a screen may register an unanticipated pattern). Failure of gesture recognition systems is also likely in cases where disability affects the nature of motion itself, such as for someone who experiences tremor or spastic motion [56, 57]. Fatigue may also impact gesture performance (and therefore recognition accuracy) over time, particularly for groups that may be more susceptible to fatigue such as due to disability or advanced age. The scheduling of medications whose main- or side-effects mitigate or amplify motor symptoms such as tremor may also result in differential gesture performance within or across days. People who are unable to move at all or who have severely restricted motion (e.g., people with ALS or quadriplegia), may be locked out of using certain technologies if body recognition is the only permitted interaction. Further, body recognition systems may not work well for people with mobility or morphology differences; for example, if a self-driving car’s pedestrian-detection algorithm does not include examples of people with posture differences such as due to cerebral palsy, Parkinson’s disease, advanced age, or who use wheelchairs during its training and evaluation, it may not correctly identify such people as objects to avoid, or may incorrectly estimate the speed and trajectory of those who move differently than expected, similar to Uber’s recent self-driving car accident that killed a pedestrian walking a bicycle [14]. Object, Scene, and Text Recognition Object, scene, and optical character recognition (OCR) systems recognize common objects, logos, text, handwriting, etc., and output labels, captions, and/or properties (i.e., location, activity, relationship). Systems taking advantage of these capabilities have been widely adopted by PWD, particularly people other sensors, such as capacitive touchscreens [15], accelerometers within devices [30, 42], etc.; body and mobility differences may create problems regardless of sensor type, though different sensor classes may have pros and cons for particular populations. with visual impairments, such as Microsoft SeeingAI [51], Google Lookout [25], LookTel Money Reader [44], KNFB Reader [64], OrCam MyEye [63], etc. Most systems for recognizing objects from photos are trained using datasets of images taken by people who are sighted, and the images are often of high quality since many are taken from social media sites such as Flickr [12]. These image data are known to be biased with regard to geographic areas and household income [17]. When applying the models to process images a blind user captures, the error rates often increase because images taken by people who are blind differ substantially in quality from those taken by people who are sighted due to poor framing, blur, unusual angles, poor lighting, etc. [31]. Similar problems may be observed from pictures taken by people with tremor or other motor disabilities [55]. Further, OCR models for handwriting recognition may not work well for people with tremor or other motor disabilities that impact writing neatness. Additionally, error metrics used to evaluate many vision systems may not be adequate to capture the enduser experience of such tools, particularly for end users with disabilities that may prevent them from verifying the system’s output (i.e., someone who is blind must rely on the output of an object detection system) [45]. Speech Systems We use the term “speech systems” to refer to AI systems that recognize the content (i.e., words) and/or properties (i.e., prosody, speaker demographics) of speech, or that generate speech from symbolic inputs such as text, Speech Synthesis Markup Language (SSML), or other encodings. Disabilities that may impact the content or clarity of a user’s speech, as well as those impacting the ability to perceive sound, may reduce the accuracy and usability of speech systems. Speech Recognition Automatic Speech Recognition (ASR) systems take in speech and output text. ASR systems have the potential to be important accessibility tools for people who are deaf or hard of hearing (DHH), such as by producing captions that can be overlaid as subtitles on videos [24, 76], or possibly even using augmented reality to live-caption face-to-face speech [35]. Speech input is also also useful for people who have difﬁculty using their hands to control traditional input devices [5]. ASR may not work correctly for people with atypical speech. ASR systems are known to have bias; for instance, many systems perform better for men than women [58, 66, 68]. Today, many ASR systems do not work well for some older adults, due to differences in pitch, pacing, and clarity of speech by people of very advanced ages, since they are not commonly represented in the training and evaluation of the systems [67]. People with accents, including accents due to disability (e.g., “deaf accent”), also face challenges using current ASR tools [20, 27, 68], though it is possible to train personalized models for such groups [16, 75]. Speech disabilities such as disarthrya, as well as the use of speech-generating augmentative and alternative communication (AAC) devices, can also negatively impact ASR functionality [38]. Further, people who are unable to speak at all (i.e., some people who are deaf, people with some forms of aphasia), may be locked out of using ASR technologies. Additionally, error metrics used to evaluate many ASR systems, such as Word Error Rate, may not be adequate to capture the end-user experience of such tools, particularly for users with disabilities that may prevent them from verifying the system’s output (i.e., someone who is profoundly deaf must trust the output of ASR captioning). Speech Generation Speech generation technologies include technologies such as text to speech (TTS) systems that aim to generate realistic audio from symbolic inputs such as text, SSML, or other markup, as well as emerging AI tools such as voice fonts [10, 48], which aim to realistically mimic the sound of a particular speaker. TTS systems have been widely deployed in voice assistants such as Cortana, Alexa, Siri, and the Google Assistant; TTS is also key to many assistive technologies, including screen readers used by people who are blind and AAC devices used by people with speech and motor disabilities. Voice banking to create personalized voice fonts may be particularly valued by people with degenerative conditions that result in progressive loss of speaking abilities (e.g., ALS) [19, 38]. System defaults for what constitutes comprehensible speaking rates may need adjustments for particular disability segments; development of error metrics related to comprehension may need inclusion of such populations in order to account for diverse user needs – for instance, people with cognitive or intellectual disabilities may require slower speech rates, whereas people with visual impairments may ﬁnd rates too slow [77]. Text-based prediction techniques are often deeply intertwined with speech generation in the case of AAC technologies; the choice of training and evaluation corpora for prediction may need to be adapted to be relevant to the topical needs and desired speech attributes of AAC users, supporting expressivity and authentic self-representation [38]. Speaker Analysis Speaker analysis systems include capabilities for speaker identiﬁcation, speaker veriﬁcation, and making inferences about the speaker’s attributes such as age, gender, and emotion. Speaker analysis systems have a wide range of applications including biometric authentication [59], enhancing speech transcription [72], and personalization [26]. Speaker analysis systems also have the potential to be important accessibility tools for people who are DHH, such as by supporting sound awareness through visualizations [36]. Speaker recognition and speech analysis tools that make inferences about a user’s personal characteristics (i.e., gender, age) may not work well for PWD that signiﬁcantly impact the sound of speech (e.g., dysarthria). Analysis tools that attempt to infer emotional state from prosodic features are likely to fail for speakers with atypical prosody, such as people with autism or some types of dementia. have accuracy and fairness challenges for people with cognitive and/or intellectual disabilities; systems for minority languages used by disability subcommunities, such as American Sign Language, are also a concern [8]. Text Analysis Text analysis systems take text as input, and may attempt to detect content properties (e.g., key phrases, named entities, language) and/or author properties (e.g., sentiment, personality, demographics). Text analysis is broadly applied in record management, information retrieval, and pattern mining. Text analysis systems have the potential to be helpful for PWD that impact reading and writing, such as dyslexia, dysgraphia, or other cognitive differences, such as through visual illustration and focused highlighting [50] or through intelligent spelling, grammar correction, and word or phrase suggestions [28]. Cognitive and intellectual disabilities are likely to impact the efﬁcacy and utility of many aspects of text analysis systems. For example, there is some evidence that spelling correction and query rewriting tools may not accurately handle dyslexic spelling [53, 65]. Further, people with autism may express emotion differently in writing than people who are neurotypical, resulting in incorrect classiﬁcations about their emotional state or personality. If these metrics are used as input to an automatic hiring system [71] or automatic essay grading systems used with many standardized aptitude tests, text analysis systems can have accuracy and fairness challenges for people with cognitive and/or intellectual disabilities. Integrative AI In addition to the aforementioned classes of systems for vision, speech, and text processing, which were focused on single models, many complex AI systems are architectures integrating several models together to achieve more complex behavior. Here, we discuss two common examples of integrative AI: Information Retrieval and Conversational Agents. Information Retrieval Information retrieval (IR) tools, such as those that power web search engines, rely on AI for a variety of purposes, including query rewriting, autocompletion suggestions, spelling corrections, search result ranking, content summarization, and question answering. The input and output of IR systems can have many formats, e.g., image, video, sound, or text. It is likely that many IR systems may inadvertently amplify existing biases against PWD, such as through returning stereotyped and/or over- and under-represented content in search results (a problem that has been documented with respect to gender in image search results [39] and word embeddings [7]). AI systems for advertising, both content-based (i.e., related to the current search query) and behavior-based (i.e., related to a user’s personal characteristics), are also a key component of many commercial IR systems, as well as other online ecosystems (e.g., social media). Advertising algorithms and other types of recommender systems may hold particular risk for PWD by actively propagating discriminatory behavior such as through differential pricing for products and services and/or differential exposure to employment or other opportunities (an issue for which Facebook recently encountered legal trouble, by allowing housing ads that may have differentiated among protected demographics, including PWD [70]). IR systems may pose particular challenges for people with cognitive or intellectual disabilities if not trained and tested with these groups; for example, people with dyslexia have reported that status quo query completion and result ranking techniques may not match their abilities [53]. Conversational Agents Conversational agents provide conversational experiences to end users for various practical applications, including customer service [69], education [13], and health support [22]. They are also powered by a variety of models, e.g., ASR, text analysis, TTS, and/or speaker analysis. Conversational agents have the potential to reduce users’ workload when completing unfamiliar tasks [29], and could potentially provide cognitive assistance to people with dementia or intellectual disabilities that impact memory or executive functioning [43]. If not carefully built, conversational agents could amplify existing biases against PWD, such as through returning stereotyped content in conversations (e.g., Microsoft shut down the chatbot Tay because it started generating hate speech learned from coordinated malicious users [46]). Further, conversational agents may not work well for people with cognitive and/or intellectual disabilities, resulting in poor user experience. Training conversational agents on corpora that include data from people with a variety of cognitive and intellectual capabilities, as well as testing with similarly diverse audiences, is particularly important. For example, conversational agents may need to correctly interpret atypical spelling or phrasing from users with dyslexia, or may need to adjust their vocabulary level to be understood by someone with dementia. Further, conversational agents may need to support conversation in a user’s preferred expressive medium, which may not be written language for some disability segments – i.e., it may be important to support communication via sign languages (for people who are deaf) or via pictures and/or icons (for people with aphasia or autism). Other AI Techniques In addition to assessing risk factors for particular classes of AI applications, it is also worth considering that many AI techniques and practices that comprise the building blocks of such systems may lead to biases against PWD, such as techniques for outlier detection, practices of evaluating systems through aggregate metrics, deﬁnition of objective functions, and using training data that do not capture the true use cases or the true complexity of the real world. Outlier detection algorithms ﬂag outlier input, typically for punitive action, such as fraud detection. Lack of or low representation in training and evaluation data may erroneously result in people with a variety of disabilities being inadvertently ﬂagged by anomaly detection tools, even when their actions should constitute legitimate system inputs. For example, many systems use task completion time as a signal for automatically determining input legitimacy, ranging from CAPTCHAs that aim to distinguish humans from bots to online crowd labor markets that aim to distinguish legitimate workers from spammers [79]. However, many types of disability might manifest in atypical task performance timing, including the use of screen reader or magniﬁer tools by people with vision impairments, difﬁculty performing quick and accurate motions by people with a variety of motor-limiting conditions, people accessing devices through switch inputs due to motor limitations, slow reading times due to cognitive disabilities such as dyslexia, etc. A common approach in evaluating AI systems is measuring performance with aggregate metrics such as accuracy, area under the curve (AUC), or mean square error (MSE). Aggregate metrics hide how performance varies across groups, in particular performance drops for small classes such as PWD [60]. Objective functions that aim to maximize aggregate metrics will likely fail to prioritize performance for PWD. Recent work has introduced techniques that expand the objective functions for model training with terms that penalize performance discrepancies between subgroups [1]. Most AI systems are trained with existing datasets (i.e., data scraped from public corpora such as Flickr images [12]). In some cases, existing data sets may fail to capture the complexity of the real world and may lack representation of diverse groups, such as PWD. This may lead to blind spots in AI models [41]. Actively curating inclusive datasets may be particularly important not only for training, but also for testing AI systems against known benchmarks.\n\nDISCUSSION\nOur research roadmap for increasing fairness in AI for PWD included four proposed steps; this position paper mostly focused on the ﬁrst: identifying ways in which (lack of) inclusion in training and evaluation of AI systems may negatively impact such systems’ fairness for PWD. To address this, we discussed ways in which common categories of AI may need to account for various types of disabilities. Regarding the types of potential harm caused by unfair AI, most of our examples are related to quality of service [9], like voice-activated smart speakers that may not recognize input from people with speech disabilities. Others are related to harms of allocation [2], like using an incorrect prediction of the emotional state or personality of someone with autism as input into an automatic hiring system, or denigration [46], like erroneously ﬂagging inputs from PWD as invalid outliers. Additional potential harms include stereotyping [7] and overor under-representation [39]; IR systems may inadvertently amplify existing biases against PWD by returning stereotyped and/or poorly represented content in search results. For issues related to allocation, quality of service, and representation, measuring objective fairness metrics through benchmarking could be sufﬁcient to reveal bias, while issues related to stereotyping and denigration might require additional qualitative investigations. More thorough considerations of all types of harms with regard to PWD is important for future work. analogous error cases for other minority user groups; our use of cautionary language such as “may cause” or “is likely” reﬂects this uncertainty. Per point #2 of our research roadmap, systematic testing of the hypotheses we have presented here is a necessary step for future research. Item #3 on our roadmap is the creation of public datasets for testing and benchmarking (and handling the complex ethical issues that creating such datasets for vulnerable user groups might involve); this is another key area for future work. Questions that must be addressed include: Is it acceptable to create datasets representing disability by scraping existing online data sources? How could this be done in a way that preserves users’ privacy, and that ensures accurate ground-truth labeling of disability status? Are there potential harms that aggregating data about disability might expose people to? Could such data adequately cover rare conditions and/or intersectionality (either of co-occuring disabilities or of the intersection of disability with other demographics such as gender, race, geolocation, or socioeconomic status)? If curating data from scratch, how can we encourage contributions from target constituencies, and how can we ensure data collection mechanisms are sufﬁciently accessible? Is informed consent in data collection possible for people with some types of intellectual disabilities, and, if not, what methods can be used to promote fairness in AI for this class of end-user? Will it ever be possible to know one has complete coverage of all relevant disability communities, or will this always be an “unknown unknown” [41]? If, as we suspect may be the case, status quo modeling, bias mitigation, and/or error measurement techniques are inadequate for many scenarios affecting PWD, further research into new techniques will be warranted (item #4 in our proposed roadmap). One challenge to consider may be the extent to which it is possible (or desirable) to develop general models that are fair across varied demographics versus creating personalized models for particular user groups. For example, success in developing accurate ASR for deaf speech has thus far focused on custom models for particular users [16, 75]. The need for personalization may be high given the “long tail” of disability, though the need to train personalized models may present additional barriers for PWD, as well as creating a two-tiered system of people for whom general AI models work by default and those for whom they do not. Involvement of PWD not only in evaluating AI systems, but also in deﬁning meaningful usage scenarios, error metrics, and policies, is critical for the development of fair AI.\n\nCONCLUSION\nIn this position paper, we have reﬂected on the ways in which current classes of AI systems, as well as several techniques that are the building blocks of AI, may limit the efﬁcacy and fairness of these systems for people with disabilities. Ultimately, our goal is the creation of new design guidelines, datasets, algorithmic techniques, and error metrics that can help AI systems realize their enormous potential to beneﬁt PWD, while avoiding the possible pitfalls we have outlined here. We hope this paper provides a research roadmap that can guide AI researchers and practitioners in creating systems that are fair to and effective for PWD.\n\nREFERENCES\n1. Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík,\nJohn Langford, and Hanna Wallach. 2018. A reductions approach to fair classiﬁcation. arXiv preprint arXiv:1803.02453 (2018).\n2. Julia Angwin, Jeff Larson, Surya Mattu, Lauren Kirchner,\nand ProPublica. 2016. Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks. (2016). Retrieved July 3, 2019 from https://www.propublica.org/article/ machine-bias-risk-assessments-in-criminal-sentencing\n3. Apple Inc. 2019a. About Face ID advanced technology.\n(2019). Retrieved July 3, 2019 from https://support.apple.com/en-us/HT208108\n4. Apple Inc. 2019b. Augmented Reality – ARKit 3 – Apple\nDeveloper. (2019). Retrieved July 3, 2019 from https://developer.apple.com/augmented-reality/arkit/\n5. Apple Inc. 2019c. macOS Catalina – Introducing Voice\nControl. Your all-access to all devices. (2019). Retrieved July 3, 2019 from https: //www.apple.com/macos/catalina-preview/#accessibility\n6. Jeffrey P. Bigham and Patrick Carrington. 2018. Learning\nfrom the Front: People with Disabilities as Early Adopters of AI. In HCIC 2018.\n7. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing systems. 4349–4357.\n8. Danielle Bragg, Oscar Koller, Mary Bellard, Larwan\nBerke, Patrick Boudreault, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorrim, Tessa Verhoef, Christian Vogler, and Meredith Ringel Morris.\n2019. Sign Language Recognition, Generation, and\nTranslation: An Interdisciplinary Perspective. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility. ACM.\n9. Joy Buolamwini and Timnit Gebru. 2018. Gender shades:\nIntersectional accuracy disparities in commercial gender classiﬁcation. In Conference on Fairness, Accountability and Transparency. 77–91.\n10. Min Chu, Yong Zhao, and Sheng Zhao. 2010. Providing\npersonalized voice font for text-to-speech applications. (April 6 2010). US Patent 7,693,719.\n11. CNN Business. 2019. Deepfake videos: Inside the\nPentagon’s race against disinformation. (2019). Retrieved July 3, 2019 from https://www.cnn.com/interactive/2019/ 01/business/pentagons-race-against-deepfakes/\n12. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.\n13. Duolingo. 2019. Say hello to the Bots. The most\nadvanced way to learn a language. (2019). Retrieved July 3, 2019 from http://bots.duolingo.com\n14. The Economist. 2018. Why Uber’s self-driving car killed\na pedestrian. (2018). Retrieved July 3, 2019 from https://www.economist.com/the-economist-explains/2018/ 05/29/why-ubers-self-driving-car-killed-a-pedestrian\n15. John Greer Elias, Wayne Carl Westerman, and\nMyra Mary Haggerty. 2010. Multi-touch gesture dictionary. (Nov. 23 2010). US Patent 7,840,912.\n16. Engadget. 2019. Google trains its AI to accommodate\nspeech impairments. (2019). Retrieved July 3, 2019 from https://www.engadget.com/2019/05/07/ google-ai-impaired-speech-recognition/\n17. Facebook Artiﬁcial Intelligence. 2019. Does object\nrecognition work for everyone? A new method to assess bias in CV systems. (2019). Retrieved July 3, 2019 from https://ai.facebook.com/blog/ new-way-to-assess-ai-bias-in-object-recognition-systems/\n18. Michael Feldman, Sorelle A Friedler, John Moeller,\nCarlos Scheidegger, and Suresh Venkatasubramanian.\n2015. Certifying and removing disparate impact. In\nProceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 259–268.\n19. Alexander J. Fiannaca, Ann Paradiso, Jon Campbell, and\nMeredith Ringel Morris. 2018. Voicesetting: Voice Authoring UIs for Improved Expressivity in Augmentative Communication. In Proceedings of the Systems (CHI ’18). ACM, New York, NY, USA, Article 283, 12 pages. DOI:\n20. Raymond Fok, Harmanpreet Kaur, Skanda Palani,\nMartez E. Mott, and Walter S. Lasecki. 2018. Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users. In Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’18). ACM, New York, NY, USA,\n21. Australian Border Force. 2019. SmartGates. (2019).\nRetrieved July 3, 2019 from https://www.abf.gov.au/ entering-and-leaving-australia/smartgates\n22. Russell Fulmer, Angela Joerin, Breanna Gentile, Lysanne\nLakerink, and Michiel Rauws. 2018. Using Psychological Artiﬁcial Intelligence (Tess) to Relieve Symptoms of Depression and Anxiety: Randomized Controlled Trial. JMIR mental health 5, 4 (2018), e64.\n23. Google. 2019a. Google Photos Help – Search by people,\nthings, & places in your photos. (2019). Retrieved July 3, 2019 from https://support.google.com/photos/answer/ 6128838?co=GENIE.Platform%3DAndroid&hl=en\n24. Google. 2019b. TouTube Help – Use automatic\ncaptioning. (2019). Retrieved July 3, 2019 from https: //support.google.com/youtube/answer/6373554?hl=en\n25. Google. 2019c. With Lookout, discover your\nsurroundings with the help of AI. (2019). Retrieved July 3, 2019 from https: //www.blog.google/outreach-initiatives/accessibility/ lookout-discover-your-surroundings-help-ai/\n26. Google Assistant. 2017. Tomato, tomahto. Google Home\nnow supports multiple users. (2017). Retrieved July 3, 2019 from https://blog.google/products/assistant/ tomato-tomahto-google-home-now-supports-multiple-users/\n27. Linda G Gottermeier and S Kushalnagar Raja. 2016. User\nEvaluation of Automatic Speech Recognition Systems for Deaf-Hearing Interactions at School and Work. Audiology Today 28, 2 (2016), 20–34.\n28. Grammarly Inc. 2019. Great Writing, Simpliﬁed. (2019).\nRetrieved July 3, 2019 from https://www.grammarly.com\n29. Anhong Guo, Junhan Kong, Michael Rivera, Frank F. Xu,\nand Jeffrey P. Bigham. 2019. StateLens: A Reverse Engineering Solution for Making Existing Dynamic Touchscreens Accessible. In Proceedings of the 32th Annual Symposium on User Interface Software and Technology (UIST ’19). ACM, New York, NY, USA. DOI:\n30. Anhong Guo and Tim Paek. 2016. Exploring Tilt for\nNo-touch, Wrist-only Interactions on Smartwatches. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’16). ACM, New York, NY, USA,\n31. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo,\nChi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. 2018. VizWiz Grand Challenge: Answering Visual Questions from Blind People. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3608–3617.\n32. Vicki L. Hanson, Anna Cavender, and Shari Trewin. 2015.\nWriting About Accessibility. Interactions 22, 6 (Oct.\n33. Moritz Hardt, Eric Price, Nati Srebro, and others. 2016.\nEquality of opportunity in supervised learning. In Advances in neural information processing systems. 3315–3323.\n34. HireVue. 2019. HireVue - Hiring Intelligence |\nAssessment & Video Interview Software. (2019). Retrieved July 3, 2019 from https://www.hirevue.com\n35. Dhruv Jain, Bonnie Chinh, Leah Findlater, Raja\nKushalnagar, and Jon Froehlich. 2018. Exploring Augmented Reality Approaches to Real-Time Captioning: A Preliminary Autoethnographic Study. In Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems (DIS ’18 Companion). ACM, New York, NY, USA, 7–11. DOI: Vogler, and Jon E. Froehlich. 2015. Head-Mounted Display Visualizations to Support Sound Awareness for the Deaf and Hard of Hearing. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 241–250. DOI:\n37. Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and\nJun Sakuma. 2012. Fairness-aware classiﬁer with prejudice remover regularizer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 35–50.\n38. Shaun K. Kane, Meredith Ringel Morris, Ann Paradiso,\nand Jon Campbell. 2017. “At Times Avuncular and Cantankerous, with the Reﬂexes of a Mongoose”: Understanding Self-Expression Through Augmentative and Alternative Communication Devices. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW ’17). ACM, New York, NY, USA, 1166–1179. DOI:\n39. Matthew Kay, Cynthia Matuszek, and Sean A. Munson.\n2015. Unequal Representation and Gender Stereotypes in\nImage Search Results for Occupations. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 3819–3828. DOI:\n40. David Kim, Otmar Hilliges, Shahram Izadi, Alex D.\nButler, Jiawen Chen, Iason Oikonomidis, and Patrick Olivier. 2012. Digits: Freehand 3D Interactions Anywhere Using a Wrist-worn Gloveless Sensor. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 167–176. DOI:\n41. Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and\nEric Horvitz. 2017. Identifying unknown unknowns in the open world: Representations and policies for guided exploration. In Thirty-First AAAI Conference on Artiﬁcial Intelligence.\n42. Gierad Laput, Robert Xiao, and Chris Harrison. 2016.\nViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 321–333. DOI:\n43. Clayton Lewis. 2005. HCI for people with cognitive\ndisabilities. ACM SIGACCESS Accessibility and Computing 83 (2005), 12–17.\n44. LookTel. 2019. LookTel Money Reader. (2019).\nRetrieved July 3, 2019 from http://www.looktel.com/moneyreader\n45. Haley MacLeod, Cynthia L. Bennett, Meredith Ringel\nMorris, and Edward Cutrell. 2017. Understanding Blind People’s Experiences with Computer-Generated Captions of Social Media Images. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 5988–5999. DOI:\n46. Microsoft. 2016. Learning from Tay’s introduction.\n(2016). Retrieved July 3, 2019 from https://blogs.microsoft.com/blog/2016/03/25/ learning-tays-introduction/\n47. Microsoft. 2019a. Azure Kinect DK – BODY\nTRACKING SDK. (2019). Retrieved July 3, 2019 from https://azure.microsoft.com/en-us/services/kinect-dk/\n48. Microsoft. 2019b. Custom Voice. (2019). Retrieved July\n3, 2019 from https://speech.microsoft.com/customvoice\n49. Microsoft. 2019c. Gestures – Mixed Reaity. (2019).\nRetrieved July 3, 2019 from https://docs.microsoft.com/ en-us/windows/mixed-reality/gestures\n50. Microsoft. 2019d. Immersive Reader – An AI Service that\nhelps users read and comprehend text. (2019). Retrieved July 3, 2019 from https://azure.microsoft.com/en-us/ services/cognitive-services/immersive-reader/\n51. Microsoft. 2019e. Seeing AI. (2019). Retrieved July 3,\n2019 from https://www.microsoft.com/en-us/ai/seeing-ai\n52. Microsoft. 2019f. Windows Hello: Discover facial\nrecognition on Windows 10. (2019). Retrieved July 3, 2019 from https://www.microsoft.com/en-us/windows/windows-hello\n53. Meredith Ringel Morris, Adam Fourney, Abdullah Ali,\nand Laura Vonessen. 2018. Understanding the Needs of Searchers with Dyslexia. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18). ACM, New York, NY, USA, Article 35, 12\n54. Motion Analysis. 2019. Sports Biomechanics. (2019).\nRetrieved July 3, 2019 from https: //motionanalysis.com/industry/sports-biomechanics/\n55. Martez E. Mott, Jane E., Cynthia L. Bennett, Edward\nCutrell, and Meredith Ringel Morris. 2018. Understanding the Accessibility of Smartphone Photography for People with Motor Impairments. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18). ACM, New York, NY, USA, Article 520, 12 pages. DOI:\n56. Martez E. Mott, Radu-Daniel Vatavu, Shaun K. Kane,\nand Jacob O. Wobbrock. 2016. Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 1934–1946. DOI:\n57. Martez E. Mott and Jacob O. Wobbrock. 2019. Cluster\nTouch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI ’19). ACM, New York, NY, USA, Article 27, 14 pages. DOI:\n58. Antony Nicol, Chris Casey, and Stuart MacFarlane. 2002.\nChildren are ready for speech technology-but is the technology ready for them. Interaction Design and Children, Eindhoven, The Netherlands (2002).\n59. Nuance Communications, Inc. 2019. Every voice matters:\nOur system knows who is talking and why. (2019). Retrieved July 3, 2019 from https: //www.nuance.com/automotive/voice-biometrics.html\n60. Besmira Nushi, Ece Kamar, and Eric Horvitz. 2018.\nTowards accountable AI: Hybrid human-machine analyses for characterizing system failure. In Sixth AAAI Conference on Human Computation and Crowdsourcing.\n61. Federal Bureau of Investigation. 2019. Next Generation\nIdentiﬁcation (NGI). (2019). Retrieved July 3, 2019 from https://www.fbi.gov/services/cjis/ fingerprints-and-other-biometrics/ngi\n62. Mike Oliver. 2013. The social model of disability: Thirty\nyears on. Disability & society 28, 7 (2013), 1024–1026.\n63. OrCam. 2019. OrCam MyEye 2 – For the Blind and\nVisually Impaired. (2019). Retrieved July 3, 2019 from https://www.orcam.com/en/myeye2/\n64. KNFB Reader. 2018. KNFB Reader gives you easy\naccess to print and ﬁles, anytime, anywhere. (2018). Retrieved July 3, 2019 from https://knfbreader.com\n65. Luz Rello, Miguel Ballesteros, and Jeffrey P. Bigham.\n2015. A Spellchecker for Dyslexia. In Proceedings of the\n17th International ACM SIGACCESS Conference on Computers &#38; Accessibility (ASSETS ’15). ACM, New York, NY, USA, 39–47. DOI:\n66. James A Rodger and Parag C Pendharkar. 2004. A ﬁeld\nstudy of the impact of gender and user’s technical experience on the performance of voice-activated medical tracking application. International Journal of Human-Computer Studies 60, 5-6 (2004), 529–544.\n67. S. Schlögl, G. Chollet, M. Garschall, M. Tscheligi, and G.\nLegouverneur. 2013. Exploring Voice User Interfaces for Seniors. In Proceedings of the 6th International Conference on PErvasive Technologies Related to Assistive Environments (PETRA ’13). ACM, New York, NY, USA, Article 52, 2 pages. DOI:\n68. Rachael Tatman. 2017. Gender and dialect bias in\nYouTube’s automatic captions. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing. 53–59.\n69. TechCrunch. 2016. Facebook launches Messenger\nplatform with chatbots. (2016). Retrieved July 3, 2019 from https: //techcrunch.com/2016/04/12/agents-on-messenger/\n70. The New York Times. 2019. Facebook Engages in\nHousing Discrimination With Its Ad Practices, U.S. Says. (2019). Retrieved July 3, 2019 from https://www.nytimes.com/2019/03/28/us/politics/ facebook-housing-discrimination.html\n71. TopResume. 2019. Get to Know the 5 Most Popular\nPre-Employment Personality Tests. (2019). Retrieved July 3, 2019 from https://www.topresume.com/career-advice/ how-to-pass-the-pre-employment-personality-test\n72. Sue E Tranter and Douglas A Reynolds. 2006. An\noverview of automatic speaker diarization systems. IEEE Transactions on audio, speech, and language processing 14, 5 (2006), 1557–1565.\n73. Shari Trewin. 2018. AI Fairness for People with\nDisabilities: Point of View. arXiv preprint arXiv:1811.10670 (2018).\n74. University of Michigan. 2019. Teaching self-driving cars\nto predict pedestrian movement. (2019). Retrieved July 3, 2019 from https://news.umich.edu/ teaching-self-driving-cars-to-predict-pedestrian-movement/\n75. VentureBeat. 2019. How Microsoft is using AI to\nimprove accessibility. (2019). Retrieved July 3, 2019 from https://venturebeat.com/2019/05/06/ how-microsoft-is-using-ai-to-improve-accessibility/\n76. The Verge. 2019. Android Q’s Live Caption feature adds\nreal-time subtitles to any audio or video playing on your phone. (2019). Retrieved July 3, 2019 from https://www.theverge.com/2019/5/7/18528447/ google-android-q-live-caption-video-transcription-io-2019\n77. Alexandra Vtyurina, Adam Fourney, Meredith Ringel\nMorris, Leah Findlater, and Ryen White. 2019. VERSE: Bridging Screen Readers and Voice Assistants for Enhanced Eyes-Free Web Search. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility. ACM.\n78. Liang Wang, Tieniu Tan, Huazhong Ning, and Weiming\nHu. 2003. Silhouette analysis-based gait recognition for human identiﬁcation. IEEE transactions on pattern analysis and machine intelligence 25, 12 (2003), 1505–1518.\n79. Kathryn Zyskowski, Meredith Ringel Morris, Jeffrey P.\nBigham, Mary L. Gray, and Shaun K. Kane. 2015. Accessible Crowdwork?: Understanding the Value in and Challenge of Microtask Employment for People with Disabilities. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &#38; Social Computing (CSCW ’15). ACM, New York, NY, USA, 1682–1693. DOI:\n--- FOOTNOTES 1 Microsoft Research, Redmond, WA & New York, NY, USA 2 Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA, USA 1Throughout this paper, we use people-ﬁrst language as suggested by 2Many gesture systems use computer vision [40, 49], but some use 2018 CHI Conference on Human Factors in Computing", "persona": "Student", "response": "### 1. Overview\nOne billion disabled people in the world Need for more ethical data sets Around the world, many people have a disability — illness or injury — that affects their body or mind functions. The WHO (The World Health Organisation) estimates that 15% of the world’s population lives with a disability (one billion people). One-fifth of these, between 110 and 190 million people, have a severe disability. In 2017, in France: 1.5 million people were visually impaired and 850,000 suffered from reduced mobility. In 2018, Microsoft has allocated a total budget of $25 million to fund projects focused on accessibility, based on artificial intelligence, over a five-year period. Each selected company will receive a grant that includes free use of Microsoft’s Azure artificial intelligence platform.\n\n### 2. Background\nThe diversity of the projects selected by Microsoft demonstrates the potential of Artificial Intelligence in the fight against disabilities. Visually impaired Neuromuscular disabilities ALS (Charcot’s disease) and MS (Multiple Sclerosis) “I like to listen to music, especially old hits from the 60s. I ask my vocal assistant to play a particular song. He doesn’t recognize the name (or doesn’t like my voice) and plays a completely different song. I try to stop him, but he can’t recognize my voice from the music.\n\n### 3. Methods\nBerkeley University’s Image and Video Processing Lab is developing an application that uses sensors and connected eyewear cameras to provide audio descriptions of the urban environment to visually impaired users. The research project combines Augmented Reality and Artificial Intelligence (Text recognition). Let’s take a concrete example: algorithms for computer vision systems are trained with images that take into account the attributes of faces, body posture or objects. However, disability can have an impact on a person’s body and physical appearance: on facial features, facial expressions, body, size or proportions, the presence of assistive equipment, movement properties. All these elements must be taken into account when designing datasets used to train computer vision models.\n\n### 4. Results\nBuilt with ethics, these models and applications will also be used by people with disabilities. The socio-economic situation of people with disabilities is marked by inequalities compared to the rest of the population. The challenge is double: develop accessibility but also increase the access to education, culture, work and training. From the development of the autonomous car to autonomous domestic assistants such as Amazon Alexa or Google Home, Artificial Intelligence is already helping to meet this challenge. AI used to fight against all handicaps The SuperVision Search application of Harvard Medical School University Hospital guides visually impaired users to the exact locations of desired items as shown in the following video.\n\n### 5. Takeaways\nEpilepsy Seizure Prevention Scientists at the University of Sydney are developing a real-time brain signal processing system that can predict when the next epileptic seizure will hit a patient : 30 minutes maximum before the seizure with more than 80% accuracy. The portable neuromuscular sensing device from Boston-based Pison enables users with neuromuscular disabilities such as ALS to communicate more easily through web micro-gesture-based control and platforms. Disabled people on the fringe of AI applications? AI can really change the daily life of people with disabilities. However, applications already marketed do not always work properly and can isolate them more. Artificial Intelligence researchers and manufacturers should involve people with disabilities in the design of hardware and software that embed AI algorithms. Even more crucial than the design, the datasets that drive the algorithms must integrate data that take into account the disability.", "paper_id": "36", "story_id": "36-04"}
{"paper": "Toward Fairness in AI for People with Disabilities: A Research Roadmap Anhong Guo1,2, Ece Kamar1, Jennifer Wortman Vaughan1, Hanna Wallach1, Meredith Ringel Morris1 anhongg@cs.cmu.edu, {eckamar, jenn, wallach, merrie}@microsoft.com\n\nABSTRACT\nAI technologies have the potential to dramatically impact the lives of people with disabilities (PWD). Indeed, improving the lives of PWD is a motivator for many state-of-the-art AI systems, such as automated speech recognition tools that can caption videos for people who are deaf and hard of hearing, or language prediction algorithms that can augment communication for people with speech or cognitive disabilities. However, widely deployed AI systems may not work properly for PWD, or worse, may actively discriminate against them. These considerations regarding fairness in AI for PWD have thus far received little attention. In this position paper, we identify potential areas of concern regarding how several AI technology categories may impact particular disability constituencies if care is not taken in their design, development, and testing. We intend for this risk assessment of how various classes of AI might interact with various classes of disability to provide a roadmap for future research that is needed to gather data, test these hypotheses, and build more inclusive algorithms.\n\nINTRODUCTION\nAs AI systems increasingly pervade modern life, ensuring that they work fairly for all is an important challenge. Researchers have identiﬁed unfair gender and racial bias in existing AI systems [2, 7, 9]. To understand how AI systems work across different groups of people, it is necessary to develop inclusive tools and practices for evaluation and to identify cases in which homogeneous, non-inclusive data [9] or data reﬂecting negative historical biases [2, 7] is used for system training. Although improving the lives of people with disabilities (PWD)1 is a motivator for many state-of-the-art AI systems, and although such systems have the potential to mitigate many disabling conditions [6], considerations regarding fairness in AI for PWD have thus far received little attention [73]. Fairness issues for PWD may be more difﬁcult to remedy than fairness issues for other groups, particularly where people with particular classes of disability may represent a relatively small proportion of a population. Even if included in training and evaluation data, they may be overlooked as outliers by current AI techniques [73]. Such issues threaten to lock PWD out of access to key technologies (e.g., if voice-activated smart speakers do not recognize input from people with speech disabilities), inadvertently amplify existing stereotypes against them (e.g., if a chatbot learns to mimic someone with a disability), or even actively endanger their safety (e.g., if self-driving cars are not trained to recognize pedestrians using wheelchairs). We propose the following research agenda to identify and remedy shortcomings of AI systems for PWD: (1) Identify ways in which inclusion issues for PWD may impact AI systems; (2) Test inclusion hypotheses to understand failure scenarios and the extent to which existing bias mitigation techniques (e.g., [18, 33, 37]) work; (3) Create benchmark datasets to support replication and inclusion (and handle the complex ethical issues that creating such datasets for vulnerable groups might involve); and (4) Innovate new modeling, bias mitigation, and error measurement techniques in order to address any shortcomings of status quo methods with respect to PWD. In this position paper, we take a step toward the ﬁrst of these goals by reﬂecting on ways in which current key classes of AI systems may necessitate particular consideration with respect to different classes of disability. Systematically studying the extent to which these interactions exist in practice, or demonstrating that they deﬁnitely do not, is an important next step toward creating AI inclusive of PWD; however, articulating the extent of a problem is a necessary precursor to remediation. Furthermore, we note that the question of whether it is even ethical to build certain categories of AI is an important one (and may be dependent on use context). Our mention of various classes of AI is not an endorsement of whether we think such systems should be built, but is simply describing how they may interact with disability. Indeed, there is a larger ethical discussion to be had on how limiting some types of AI with negative associations (like synthetic voices that could be used for deepfakes [11]) might disenfranchise PWD who could beneﬁt from such tech (i.e., by limiting the opportunity to realistically reproduce the voice of someone who can no longer speak).\n\nRISK ASSESSMENT OF EXISTING AI SYSTEMS FOR PWD Here, we group existing classes of AI systems by related functionalities, and identify disability constituencies for whom these systems may be problematic. This risk assessment is meant as a starting point to spark further research, and may not be exhaustive. For example, as new AI technologies are developed they would require consideration with respect to disability. Additionally, while we strove to anticipate ways in which classes of AI may fail for some disability groups, we may not have exhaustively identiﬁed all such groups; indeed, the “long tail” of disability and potential co-occurrence of multiple disabilities are two of many reasons that ensuring AI inclusion for PWD is particularly challenging [73]. Computer Vision Computer vision systems analyze still or video camera inputs to identify patterns, such as the presence and attributes of faces, bodies, or objects. Disabilities that may impact a person’s physical appearance (facial features, facial expressions, body size or proportions, presence of assistive equipment, atypical motion properties) are important to consider when designing and testing the fairness of computer vision algorithms. Face Recognition Face recognition systems include capabilities for identifying the presence of a face and/or making inferences about its properties, including face detection, identiﬁcation (i.e., to guess the identity of a speciﬁc person), veriﬁcation (i.e., to validate a claimed identity), and analysis (e.g., gender classiﬁcation, emotion analysis). Face recognition systems are already used in a wide variety of scenarios, including biometric authentication [3, 52], security systems [21], criminal justice [61], interview support software [34], and social/entertainment applications [23], many of which are controversial. We hypothesize that such techniques may not work well for people with differences in facial features and expressions if they were not considered when gathering training data and evaluating models. For instance, various aspects of facial analysis software may not work well for people with conditions such as Down syndrome, achondroplasia, cleft lip/palate, or other conditions that result in characteristic facial differences. Such systems may also fail for people who are blind, which may not only result in differences in eye anatomy, but may also result in a person wearing medical or cosmetic aids such as dark glasses, and may produce unanticipated behaviors, such as a person not holding their face toward a camera at the expected angle. Emotion processing algorithms may misinterpret the facial expressions of someone with autism or Williams syndrome, who may not emote in a conventional manner; expression interpretation may also be problematic for people who have experienced stroke, Parkinson’s disease, Bell’s Palsy, or other conditions that restrict facial movements. Body Recognition Body recognition systems include capabilities for identifying the presence of a body and/or making inferences about its properties, such as body detection, identiﬁcation, veriﬁcation, and analysis. Body recognition systems can power applications using gesture recognition (e.g., in VR and AR [4, 49] or gaming [47]), or gait analysis (e.g., for biometric authentication [78], sports biomechanics [54], and path predictions used by self-driving vehicles [74]). Body recognition systems may not work well for PWD characterized by body shape, posture, or mobility differences. For example, gesture recognition systems2 are unlikely to work well for people with differences in morphology (e.g., a person with an amputated arm may be unable to perform bimanual gestures, or may grip a device differently than expected; a person with polydactyly’s style of touching a screen may register an unanticipated pattern). Failure of gesture recognition systems is also likely in cases where disability affects the nature of motion itself, such as for someone who experiences tremor or spastic motion [56, 57]. Fatigue may also impact gesture performance (and therefore recognition accuracy) over time, particularly for groups that may be more susceptible to fatigue such as due to disability or advanced age. The scheduling of medications whose main- or side-effects mitigate or amplify motor symptoms such as tremor may also result in differential gesture performance within or across days. People who are unable to move at all or who have severely restricted motion (e.g., people with ALS or quadriplegia), may be locked out of using certain technologies if body recognition is the only permitted interaction. Further, body recognition systems may not work well for people with mobility or morphology differences; for example, if a self-driving car’s pedestrian-detection algorithm does not include examples of people with posture differences such as due to cerebral palsy, Parkinson’s disease, advanced age, or who use wheelchairs during its training and evaluation, it may not correctly identify such people as objects to avoid, or may incorrectly estimate the speed and trajectory of those who move differently than expected, similar to Uber’s recent self-driving car accident that killed a pedestrian walking a bicycle [14]. Object, Scene, and Text Recognition Object, scene, and optical character recognition (OCR) systems recognize common objects, logos, text, handwriting, etc., and output labels, captions, and/or properties (i.e., location, activity, relationship). Systems taking advantage of these capabilities have been widely adopted by PWD, particularly people other sensors, such as capacitive touchscreens [15], accelerometers within devices [30, 42], etc.; body and mobility differences may create problems regardless of sensor type, though different sensor classes may have pros and cons for particular populations. with visual impairments, such as Microsoft SeeingAI [51], Google Lookout [25], LookTel Money Reader [44], KNFB Reader [64], OrCam MyEye [63], etc. Most systems for recognizing objects from photos are trained using datasets of images taken by people who are sighted, and the images are often of high quality since many are taken from social media sites such as Flickr [12]. These image data are known to be biased with regard to geographic areas and household income [17]. When applying the models to process images a blind user captures, the error rates often increase because images taken by people who are blind differ substantially in quality from those taken by people who are sighted due to poor framing, blur, unusual angles, poor lighting, etc. [31]. Similar problems may be observed from pictures taken by people with tremor or other motor disabilities [55]. Further, OCR models for handwriting recognition may not work well for people with tremor or other motor disabilities that impact writing neatness. Additionally, error metrics used to evaluate many vision systems may not be adequate to capture the enduser experience of such tools, particularly for end users with disabilities that may prevent them from verifying the system’s output (i.e., someone who is blind must rely on the output of an object detection system) [45]. Speech Systems We use the term “speech systems” to refer to AI systems that recognize the content (i.e., words) and/or properties (i.e., prosody, speaker demographics) of speech, or that generate speech from symbolic inputs such as text, Speech Synthesis Markup Language (SSML), or other encodings. Disabilities that may impact the content or clarity of a user’s speech, as well as those impacting the ability to perceive sound, may reduce the accuracy and usability of speech systems. Speech Recognition Automatic Speech Recognition (ASR) systems take in speech and output text. ASR systems have the potential to be important accessibility tools for people who are deaf or hard of hearing (DHH), such as by producing captions that can be overlaid as subtitles on videos [24, 76], or possibly even using augmented reality to live-caption face-to-face speech [35]. Speech input is also also useful for people who have difﬁculty using their hands to control traditional input devices [5]. ASR may not work correctly for people with atypical speech. ASR systems are known to have bias; for instance, many systems perform better for men than women [58, 66, 68]. Today, many ASR systems do not work well for some older adults, due to differences in pitch, pacing, and clarity of speech by people of very advanced ages, since they are not commonly represented in the training and evaluation of the systems [67]. People with accents, including accents due to disability (e.g., “deaf accent”), also face challenges using current ASR tools [20, 27, 68], though it is possible to train personalized models for such groups [16, 75]. Speech disabilities such as disarthrya, as well as the use of speech-generating augmentative and alternative communication (AAC) devices, can also negatively impact ASR functionality [38]. Further, people who are unable to speak at all (i.e., some people who are deaf, people with some forms of aphasia), may be locked out of using ASR technologies. Additionally, error metrics used to evaluate many ASR systems, such as Word Error Rate, may not be adequate to capture the end-user experience of such tools, particularly for users with disabilities that may prevent them from verifying the system’s output (i.e., someone who is profoundly deaf must trust the output of ASR captioning). Speech Generation Speech generation technologies include technologies such as text to speech (TTS) systems that aim to generate realistic audio from symbolic inputs such as text, SSML, or other markup, as well as emerging AI tools such as voice fonts [10, 48], which aim to realistically mimic the sound of a particular speaker. TTS systems have been widely deployed in voice assistants such as Cortana, Alexa, Siri, and the Google Assistant; TTS is also key to many assistive technologies, including screen readers used by people who are blind and AAC devices used by people with speech and motor disabilities. Voice banking to create personalized voice fonts may be particularly valued by people with degenerative conditions that result in progressive loss of speaking abilities (e.g., ALS) [19, 38]. System defaults for what constitutes comprehensible speaking rates may need adjustments for particular disability segments; development of error metrics related to comprehension may need inclusion of such populations in order to account for diverse user needs – for instance, people with cognitive or intellectual disabilities may require slower speech rates, whereas people with visual impairments may ﬁnd rates too slow [77]. Text-based prediction techniques are often deeply intertwined with speech generation in the case of AAC technologies; the choice of training and evaluation corpora for prediction may need to be adapted to be relevant to the topical needs and desired speech attributes of AAC users, supporting expressivity and authentic self-representation [38]. Speaker Analysis Speaker analysis systems include capabilities for speaker identiﬁcation, speaker veriﬁcation, and making inferences about the speaker’s attributes such as age, gender, and emotion. Speaker analysis systems have a wide range of applications including biometric authentication [59], enhancing speech transcription [72], and personalization [26]. Speaker analysis systems also have the potential to be important accessibility tools for people who are DHH, such as by supporting sound awareness through visualizations [36]. Speaker recognition and speech analysis tools that make inferences about a user’s personal characteristics (i.e., gender, age) may not work well for PWD that signiﬁcantly impact the sound of speech (e.g., dysarthria). Analysis tools that attempt to infer emotional state from prosodic features are likely to fail for speakers with atypical prosody, such as people with autism or some types of dementia. have accuracy and fairness challenges for people with cognitive and/or intellectual disabilities; systems for minority languages used by disability subcommunities, such as American Sign Language, are also a concern [8]. Text Analysis Text analysis systems take text as input, and may attempt to detect content properties (e.g., key phrases, named entities, language) and/or author properties (e.g., sentiment, personality, demographics). Text analysis is broadly applied in record management, information retrieval, and pattern mining. Text analysis systems have the potential to be helpful for PWD that impact reading and writing, such as dyslexia, dysgraphia, or other cognitive differences, such as through visual illustration and focused highlighting [50] or through intelligent spelling, grammar correction, and word or phrase suggestions [28]. Cognitive and intellectual disabilities are likely to impact the efﬁcacy and utility of many aspects of text analysis systems. For example, there is some evidence that spelling correction and query rewriting tools may not accurately handle dyslexic spelling [53, 65]. Further, people with autism may express emotion differently in writing than people who are neurotypical, resulting in incorrect classiﬁcations about their emotional state or personality. If these metrics are used as input to an automatic hiring system [71] or automatic essay grading systems used with many standardized aptitude tests, text analysis systems can have accuracy and fairness challenges for people with cognitive and/or intellectual disabilities. Integrative AI In addition to the aforementioned classes of systems for vision, speech, and text processing, which were focused on single models, many complex AI systems are architectures integrating several models together to achieve more complex behavior. Here, we discuss two common examples of integrative AI: Information Retrieval and Conversational Agents. Information Retrieval Information retrieval (IR) tools, such as those that power web search engines, rely on AI for a variety of purposes, including query rewriting, autocompletion suggestions, spelling corrections, search result ranking, content summarization, and question answering. The input and output of IR systems can have many formats, e.g., image, video, sound, or text. It is likely that many IR systems may inadvertently amplify existing biases against PWD, such as through returning stereotyped and/or over- and under-represented content in search results (a problem that has been documented with respect to gender in image search results [39] and word embeddings [7]). AI systems for advertising, both content-based (i.e., related to the current search query) and behavior-based (i.e., related to a user’s personal characteristics), are also a key component of many commercial IR systems, as well as other online ecosystems (e.g., social media). Advertising algorithms and other types of recommender systems may hold particular risk for PWD by actively propagating discriminatory behavior such as through differential pricing for products and services and/or differential exposure to employment or other opportunities (an issue for which Facebook recently encountered legal trouble, by allowing housing ads that may have differentiated among protected demographics, including PWD [70]). IR systems may pose particular challenges for people with cognitive or intellectual disabilities if not trained and tested with these groups; for example, people with dyslexia have reported that status quo query completion and result ranking techniques may not match their abilities [53]. Conversational Agents Conversational agents provide conversational experiences to end users for various practical applications, including customer service [69], education [13], and health support [22]. They are also powered by a variety of models, e.g., ASR, text analysis, TTS, and/or speaker analysis. Conversational agents have the potential to reduce users’ workload when completing unfamiliar tasks [29], and could potentially provide cognitive assistance to people with dementia or intellectual disabilities that impact memory or executive functioning [43]. If not carefully built, conversational agents could amplify existing biases against PWD, such as through returning stereotyped content in conversations (e.g., Microsoft shut down the chatbot Tay because it started generating hate speech learned from coordinated malicious users [46]). Further, conversational agents may not work well for people with cognitive and/or intellectual disabilities, resulting in poor user experience. Training conversational agents on corpora that include data from people with a variety of cognitive and intellectual capabilities, as well as testing with similarly diverse audiences, is particularly important. For example, conversational agents may need to correctly interpret atypical spelling or phrasing from users with dyslexia, or may need to adjust their vocabulary level to be understood by someone with dementia. Further, conversational agents may need to support conversation in a user’s preferred expressive medium, which may not be written language for some disability segments – i.e., it may be important to support communication via sign languages (for people who are deaf) or via pictures and/or icons (for people with aphasia or autism). Other AI Techniques In addition to assessing risk factors for particular classes of AI applications, it is also worth considering that many AI techniques and practices that comprise the building blocks of such systems may lead to biases against PWD, such as techniques for outlier detection, practices of evaluating systems through aggregate metrics, deﬁnition of objective functions, and using training data that do not capture the true use cases or the true complexity of the real world. Outlier detection algorithms ﬂag outlier input, typically for punitive action, such as fraud detection. Lack of or low representation in training and evaluation data may erroneously result in people with a variety of disabilities being inadvertently ﬂagged by anomaly detection tools, even when their actions should constitute legitimate system inputs. For example, many systems use task completion time as a signal for automatically determining input legitimacy, ranging from CAPTCHAs that aim to distinguish humans from bots to online crowd labor markets that aim to distinguish legitimate workers from spammers [79]. However, many types of disability might manifest in atypical task performance timing, including the use of screen reader or magniﬁer tools by people with vision impairments, difﬁculty performing quick and accurate motions by people with a variety of motor-limiting conditions, people accessing devices through switch inputs due to motor limitations, slow reading times due to cognitive disabilities such as dyslexia, etc. A common approach in evaluating AI systems is measuring performance with aggregate metrics such as accuracy, area under the curve (AUC), or mean square error (MSE). Aggregate metrics hide how performance varies across groups, in particular performance drops for small classes such as PWD [60]. Objective functions that aim to maximize aggregate metrics will likely fail to prioritize performance for PWD. Recent work has introduced techniques that expand the objective functions for model training with terms that penalize performance discrepancies between subgroups [1]. Most AI systems are trained with existing datasets (i.e., data scraped from public corpora such as Flickr images [12]). In some cases, existing data sets may fail to capture the complexity of the real world and may lack representation of diverse groups, such as PWD. This may lead to blind spots in AI models [41]. Actively curating inclusive datasets may be particularly important not only for training, but also for testing AI systems against known benchmarks.\n\nDISCUSSION\nOur research roadmap for increasing fairness in AI for PWD included four proposed steps; this position paper mostly focused on the ﬁrst: identifying ways in which (lack of) inclusion in training and evaluation of AI systems may negatively impact such systems’ fairness for PWD. To address this, we discussed ways in which common categories of AI may need to account for various types of disabilities. Regarding the types of potential harm caused by unfair AI, most of our examples are related to quality of service [9], like voice-activated smart speakers that may not recognize input from people with speech disabilities. Others are related to harms of allocation [2], like using an incorrect prediction of the emotional state or personality of someone with autism as input into an automatic hiring system, or denigration [46], like erroneously ﬂagging inputs from PWD as invalid outliers. Additional potential harms include stereotyping [7] and overor under-representation [39]; IR systems may inadvertently amplify existing biases against PWD by returning stereotyped and/or poorly represented content in search results. For issues related to allocation, quality of service, and representation, measuring objective fairness metrics through benchmarking could be sufﬁcient to reveal bias, while issues related to stereotyping and denigration might require additional qualitative investigations. More thorough considerations of all types of harms with regard to PWD is important for future work. analogous error cases for other minority user groups; our use of cautionary language such as “may cause” or “is likely” reﬂects this uncertainty. Per point #2 of our research roadmap, systematic testing of the hypotheses we have presented here is a necessary step for future research. Item #3 on our roadmap is the creation of public datasets for testing and benchmarking (and handling the complex ethical issues that creating such datasets for vulnerable user groups might involve); this is another key area for future work. Questions that must be addressed include: Is it acceptable to create datasets representing disability by scraping existing online data sources? How could this be done in a way that preserves users’ privacy, and that ensures accurate ground-truth labeling of disability status? Are there potential harms that aggregating data about disability might expose people to? Could such data adequately cover rare conditions and/or intersectionality (either of co-occuring disabilities or of the intersection of disability with other demographics such as gender, race, geolocation, or socioeconomic status)? If curating data from scratch, how can we encourage contributions from target constituencies, and how can we ensure data collection mechanisms are sufﬁciently accessible? Is informed consent in data collection possible for people with some types of intellectual disabilities, and, if not, what methods can be used to promote fairness in AI for this class of end-user? Will it ever be possible to know one has complete coverage of all relevant disability communities, or will this always be an “unknown unknown” [41]? If, as we suspect may be the case, status quo modeling, bias mitigation, and/or error measurement techniques are inadequate for many scenarios affecting PWD, further research into new techniques will be warranted (item #4 in our proposed roadmap). One challenge to consider may be the extent to which it is possible (or desirable) to develop general models that are fair across varied demographics versus creating personalized models for particular user groups. For example, success in developing accurate ASR for deaf speech has thus far focused on custom models for particular users [16, 75]. The need for personalization may be high given the “long tail” of disability, though the need to train personalized models may present additional barriers for PWD, as well as creating a two-tiered system of people for whom general AI models work by default and those for whom they do not. Involvement of PWD not only in evaluating AI systems, but also in deﬁning meaningful usage scenarios, error metrics, and policies, is critical for the development of fair AI.\n\nCONCLUSION\nIn this position paper, we have reﬂected on the ways in which current classes of AI systems, as well as several techniques that are the building blocks of AI, may limit the efﬁcacy and fairness of these systems for people with disabilities. Ultimately, our goal is the creation of new design guidelines, datasets, algorithmic techniques, and error metrics that can help AI systems realize their enormous potential to beneﬁt PWD, while avoiding the possible pitfalls we have outlined here. We hope this paper provides a research roadmap that can guide AI researchers and practitioners in creating systems that are fair to and effective for PWD.\n\nREFERENCES\n1. Alekh Agarwal, Alina Beygelzimer, Miroslav Dudík,\nJohn Langford, and Hanna Wallach. 2018. A reductions approach to fair classiﬁcation. arXiv preprint arXiv:1803.02453 (2018).\n2. Julia Angwin, Jeff Larson, Surya Mattu, Lauren Kirchner,\nand ProPublica. 2016. Machine Bias: There’s software used across the country to predict future criminals. And it’s biased against blacks. (2016). Retrieved July 3, 2019 from https://www.propublica.org/article/ machine-bias-risk-assessments-in-criminal-sentencing\n3. Apple Inc. 2019a. About Face ID advanced technology.\n(2019). Retrieved July 3, 2019 from https://support.apple.com/en-us/HT208108\n4. Apple Inc. 2019b. Augmented Reality – ARKit 3 – Apple\nDeveloper. (2019). Retrieved July 3, 2019 from https://developer.apple.com/augmented-reality/arkit/\n5. Apple Inc. 2019c. macOS Catalina – Introducing Voice\nControl. Your all-access to all devices. (2019). Retrieved July 3, 2019 from https: //www.apple.com/macos/catalina-preview/#accessibility\n6. Jeffrey P. Bigham and Patrick Carrington. 2018. Learning\nfrom the Front: People with Disabilities as Early Adopters of AI. In HCIC 2018.\n7. Tolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. In Advances in neural information processing systems. 4349–4357.\n8. Danielle Bragg, Oscar Koller, Mary Bellard, Larwan\nBerke, Patrick Boudreault, Annelies Braffort, Naomi Caselli, Matt Huenerfauth, Hernisa Kacorrim, Tessa Verhoef, Christian Vogler, and Meredith Ringel Morris.\n2019. Sign Language Recognition, Generation, and\nTranslation: An Interdisciplinary Perspective. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility. ACM.\n9. Joy Buolamwini and Timnit Gebru. 2018. Gender shades:\nIntersectional accuracy disparities in commercial gender classiﬁcation. In Conference on Fairness, Accountability and Transparency. 77–91.\n10. Min Chu, Yong Zhao, and Sheng Zhao. 2010. Providing\npersonalized voice font for text-to-speech applications. (April 6 2010). US Patent 7,693,719.\n11. CNN Business. 2019. Deepfake videos: Inside the\nPentagon’s race against disinformation. (2019). Retrieved July 3, 2019 from https://www.cnn.com/interactive/2019/ 01/business/pentagons-race-against-deepfakes/\n12. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. 2009. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition. Ieee, 248–255.\n13. Duolingo. 2019. Say hello to the Bots. The most\nadvanced way to learn a language. (2019). Retrieved July 3, 2019 from http://bots.duolingo.com\n14. The Economist. 2018. Why Uber’s self-driving car killed\na pedestrian. (2018). Retrieved July 3, 2019 from https://www.economist.com/the-economist-explains/2018/ 05/29/why-ubers-self-driving-car-killed-a-pedestrian\n15. John Greer Elias, Wayne Carl Westerman, and\nMyra Mary Haggerty. 2010. Multi-touch gesture dictionary. (Nov. 23 2010). US Patent 7,840,912.\n16. Engadget. 2019. Google trains its AI to accommodate\nspeech impairments. (2019). Retrieved July 3, 2019 from https://www.engadget.com/2019/05/07/ google-ai-impaired-speech-recognition/\n17. Facebook Artiﬁcial Intelligence. 2019. Does object\nrecognition work for everyone? A new method to assess bias in CV systems. (2019). Retrieved July 3, 2019 from https://ai.facebook.com/blog/ new-way-to-assess-ai-bias-in-object-recognition-systems/\n18. Michael Feldman, Sorelle A Friedler, John Moeller,\nCarlos Scheidegger, and Suresh Venkatasubramanian.\n2015. Certifying and removing disparate impact. In\nProceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 259–268.\n19. Alexander J. Fiannaca, Ann Paradiso, Jon Campbell, and\nMeredith Ringel Morris. 2018. Voicesetting: Voice Authoring UIs for Improved Expressivity in Augmentative Communication. In Proceedings of the Systems (CHI ’18). ACM, New York, NY, USA, Article 283, 12 pages. DOI:\n20. Raymond Fok, Harmanpreet Kaur, Skanda Palani,\nMartez E. Mott, and Walter S. Lasecki. 2018. Towards More Robust Speech Interactions for Deaf and Hard of Hearing Users. In Proceedings of the 20th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS ’18). ACM, New York, NY, USA,\n21. Australian Border Force. 2019. SmartGates. (2019).\nRetrieved July 3, 2019 from https://www.abf.gov.au/ entering-and-leaving-australia/smartgates\n22. Russell Fulmer, Angela Joerin, Breanna Gentile, Lysanne\nLakerink, and Michiel Rauws. 2018. Using Psychological Artiﬁcial Intelligence (Tess) to Relieve Symptoms of Depression and Anxiety: Randomized Controlled Trial. JMIR mental health 5, 4 (2018), e64.\n23. Google. 2019a. Google Photos Help – Search by people,\nthings, & places in your photos. (2019). Retrieved July 3, 2019 from https://support.google.com/photos/answer/ 6128838?co=GENIE.Platform%3DAndroid&hl=en\n24. Google. 2019b. TouTube Help – Use automatic\ncaptioning. (2019). Retrieved July 3, 2019 from https: //support.google.com/youtube/answer/6373554?hl=en\n25. Google. 2019c. With Lookout, discover your\nsurroundings with the help of AI. (2019). Retrieved July 3, 2019 from https: //www.blog.google/outreach-initiatives/accessibility/ lookout-discover-your-surroundings-help-ai/\n26. Google Assistant. 2017. Tomato, tomahto. Google Home\nnow supports multiple users. (2017). Retrieved July 3, 2019 from https://blog.google/products/assistant/ tomato-tomahto-google-home-now-supports-multiple-users/\n27. Linda G Gottermeier and S Kushalnagar Raja. 2016. User\nEvaluation of Automatic Speech Recognition Systems for Deaf-Hearing Interactions at School and Work. Audiology Today 28, 2 (2016), 20–34.\n28. Grammarly Inc. 2019. Great Writing, Simpliﬁed. (2019).\nRetrieved July 3, 2019 from https://www.grammarly.com\n29. Anhong Guo, Junhan Kong, Michael Rivera, Frank F. Xu,\nand Jeffrey P. Bigham. 2019. StateLens: A Reverse Engineering Solution for Making Existing Dynamic Touchscreens Accessible. In Proceedings of the 32th Annual Symposium on User Interface Software and Technology (UIST ’19). ACM, New York, NY, USA. DOI:\n30. Anhong Guo and Tim Paek. 2016. Exploring Tilt for\nNo-touch, Wrist-only Interactions on Smartwatches. In Proceedings of the 18th International Conference on Human-Computer Interaction with Mobile Devices and Services (MobileHCI ’16). ACM, New York, NY, USA,\n31. Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo,\nChi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P. Bigham. 2018. VizWiz Grand Challenge: Answering Visual Questions from Blind People. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 3608–3617.\n32. Vicki L. Hanson, Anna Cavender, and Shari Trewin. 2015.\nWriting About Accessibility. Interactions 22, 6 (Oct.\n33. Moritz Hardt, Eric Price, Nati Srebro, and others. 2016.\nEquality of opportunity in supervised learning. In Advances in neural information processing systems. 3315–3323.\n34. HireVue. 2019. HireVue - Hiring Intelligence |\nAssessment & Video Interview Software. (2019). Retrieved July 3, 2019 from https://www.hirevue.com\n35. Dhruv Jain, Bonnie Chinh, Leah Findlater, Raja\nKushalnagar, and Jon Froehlich. 2018. Exploring Augmented Reality Approaches to Real-Time Captioning: A Preliminary Autoethnographic Study. In Proceedings of the 2018 ACM Conference Companion Publication on Designing Interactive Systems (DIS ’18 Companion). ACM, New York, NY, USA, 7–11. DOI: Vogler, and Jon E. Froehlich. 2015. Head-Mounted Display Visualizations to Support Sound Awareness for the Deaf and Hard of Hearing. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 241–250. DOI:\n37. Toshihiro Kamishima, Shotaro Akaho, Hideki Asoh, and\nJun Sakuma. 2012. Fairness-aware classiﬁer with prejudice remover regularizer. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases. Springer, 35–50.\n38. Shaun K. Kane, Meredith Ringel Morris, Ann Paradiso,\nand Jon Campbell. 2017. “At Times Avuncular and Cantankerous, with the Reﬂexes of a Mongoose”: Understanding Self-Expression Through Augmentative and Alternative Communication Devices. In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing (CSCW ’17). ACM, New York, NY, USA, 1166–1179. DOI:\n39. Matthew Kay, Cynthia Matuszek, and Sean A. Munson.\n2015. Unequal Representation and Gender Stereotypes in\nImage Search Results for Occupations. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems (CHI ’15). ACM, New York, NY, USA, 3819–3828. DOI:\n40. David Kim, Otmar Hilliges, Shahram Izadi, Alex D.\nButler, Jiawen Chen, Iason Oikonomidis, and Patrick Olivier. 2012. Digits: Freehand 3D Interactions Anywhere Using a Wrist-worn Gloveless Sensor. In Proceedings of the 25th Annual ACM Symposium on User Interface Software and Technology (UIST ’12). ACM, New York, NY, USA, 167–176. DOI:\n41. Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and\nEric Horvitz. 2017. Identifying unknown unknowns in the open world: Representations and policies for guided exploration. In Thirty-First AAAI Conference on Artiﬁcial Intelligence.\n42. Gierad Laput, Robert Xiao, and Chris Harrison. 2016.\nViBand: High-Fidelity Bio-Acoustic Sensing Using Commodity Smartwatch Accelerometers. In Proceedings of the 29th Annual Symposium on User Interface Software and Technology (UIST ’16). ACM, New York, NY, USA, 321–333. DOI:\n43. Clayton Lewis. 2005. HCI for people with cognitive\ndisabilities. ACM SIGACCESS Accessibility and Computing 83 (2005), 12–17.\n44. LookTel. 2019. LookTel Money Reader. (2019).\nRetrieved July 3, 2019 from http://www.looktel.com/moneyreader\n45. Haley MacLeod, Cynthia L. Bennett, Meredith Ringel\nMorris, and Edward Cutrell. 2017. Understanding Blind People’s Experiences with Computer-Generated Captions of Social Media Images. In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems (CHI ’17). ACM, New York, NY, USA, 5988–5999. DOI:\n46. Microsoft. 2016. Learning from Tay’s introduction.\n(2016). Retrieved July 3, 2019 from https://blogs.microsoft.com/blog/2016/03/25/ learning-tays-introduction/\n47. Microsoft. 2019a. Azure Kinect DK – BODY\nTRACKING SDK. (2019). Retrieved July 3, 2019 from https://azure.microsoft.com/en-us/services/kinect-dk/\n48. Microsoft. 2019b. Custom Voice. (2019). Retrieved July\n3, 2019 from https://speech.microsoft.com/customvoice\n49. Microsoft. 2019c. Gestures – Mixed Reaity. (2019).\nRetrieved July 3, 2019 from https://docs.microsoft.com/ en-us/windows/mixed-reality/gestures\n50. Microsoft. 2019d. Immersive Reader – An AI Service that\nhelps users read and comprehend text. (2019). Retrieved July 3, 2019 from https://azure.microsoft.com/en-us/ services/cognitive-services/immersive-reader/\n51. Microsoft. 2019e. Seeing AI. (2019). Retrieved July 3,\n2019 from https://www.microsoft.com/en-us/ai/seeing-ai\n52. Microsoft. 2019f. Windows Hello: Discover facial\nrecognition on Windows 10. (2019). Retrieved July 3, 2019 from https://www.microsoft.com/en-us/windows/windows-hello\n53. Meredith Ringel Morris, Adam Fourney, Abdullah Ali,\nand Laura Vonessen. 2018. Understanding the Needs of Searchers with Dyslexia. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18). ACM, New York, NY, USA, Article 35, 12\n54. Motion Analysis. 2019. Sports Biomechanics. (2019).\nRetrieved July 3, 2019 from https: //motionanalysis.com/industry/sports-biomechanics/\n55. Martez E. Mott, Jane E., Cynthia L. Bennett, Edward\nCutrell, and Meredith Ringel Morris. 2018. Understanding the Accessibility of Smartphone Photography for People with Motor Impairments. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI ’18). ACM, New York, NY, USA, Article 520, 12 pages. DOI:\n56. Martez E. Mott, Radu-Daniel Vatavu, Shaun K. Kane,\nand Jacob O. Wobbrock. 2016. Smart Touch: Improving Touch Accuracy for People with Motor Impairments with Template Matching. In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems (CHI ’16). ACM, New York, NY, USA, 1934–1946. DOI:\n57. Martez E. Mott and Jacob O. Wobbrock. 2019. Cluster\nTouch: Improving Touch Accuracy on Smartphones for People with Motor and Situational Impairments. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (CHI ’19). ACM, New York, NY, USA, Article 27, 14 pages. DOI:\n58. Antony Nicol, Chris Casey, and Stuart MacFarlane. 2002.\nChildren are ready for speech technology-but is the technology ready for them. Interaction Design and Children, Eindhoven, The Netherlands (2002).\n59. Nuance Communications, Inc. 2019. Every voice matters:\nOur system knows who is talking and why. (2019). Retrieved July 3, 2019 from https: //www.nuance.com/automotive/voice-biometrics.html\n60. Besmira Nushi, Ece Kamar, and Eric Horvitz. 2018.\nTowards accountable AI: Hybrid human-machine analyses for characterizing system failure. In Sixth AAAI Conference on Human Computation and Crowdsourcing.\n61. Federal Bureau of Investigation. 2019. Next Generation\nIdentiﬁcation (NGI). (2019). Retrieved July 3, 2019 from https://www.fbi.gov/services/cjis/ fingerprints-and-other-biometrics/ngi\n62. Mike Oliver. 2013. The social model of disability: Thirty\nyears on. Disability & society 28, 7 (2013), 1024–1026.\n63. OrCam. 2019. OrCam MyEye 2 – For the Blind and\nVisually Impaired. (2019). Retrieved July 3, 2019 from https://www.orcam.com/en/myeye2/\n64. KNFB Reader. 2018. KNFB Reader gives you easy\naccess to print and ﬁles, anytime, anywhere. (2018). Retrieved July 3, 2019 from https://knfbreader.com\n65. Luz Rello, Miguel Ballesteros, and Jeffrey P. Bigham.\n2015. A Spellchecker for Dyslexia. In Proceedings of the\n17th International ACM SIGACCESS Conference on Computers &#38; Accessibility (ASSETS ’15). ACM, New York, NY, USA, 39–47. DOI:\n66. James A Rodger and Parag C Pendharkar. 2004. A ﬁeld\nstudy of the impact of gender and user’s technical experience on the performance of voice-activated medical tracking application. International Journal of Human-Computer Studies 60, 5-6 (2004), 529–544.\n67. S. Schlögl, G. Chollet, M. Garschall, M. Tscheligi, and G.\nLegouverneur. 2013. Exploring Voice User Interfaces for Seniors. In Proceedings of the 6th International Conference on PErvasive Technologies Related to Assistive Environments (PETRA ’13). ACM, New York, NY, USA, Article 52, 2 pages. DOI:\n68. Rachael Tatman. 2017. Gender and dialect bias in\nYouTube’s automatic captions. In Proceedings of the First ACL Workshop on Ethics in Natural Language Processing. 53–59.\n69. TechCrunch. 2016. Facebook launches Messenger\nplatform with chatbots. (2016). Retrieved July 3, 2019 from https: //techcrunch.com/2016/04/12/agents-on-messenger/\n70. The New York Times. 2019. Facebook Engages in\nHousing Discrimination With Its Ad Practices, U.S. Says. (2019). Retrieved July 3, 2019 from https://www.nytimes.com/2019/03/28/us/politics/ facebook-housing-discrimination.html\n71. TopResume. 2019. Get to Know the 5 Most Popular\nPre-Employment Personality Tests. (2019). Retrieved July 3, 2019 from https://www.topresume.com/career-advice/ how-to-pass-the-pre-employment-personality-test\n72. Sue E Tranter and Douglas A Reynolds. 2006. An\noverview of automatic speaker diarization systems. IEEE Transactions on audio, speech, and language processing 14, 5 (2006), 1557–1565.\n73. Shari Trewin. 2018. AI Fairness for People with\nDisabilities: Point of View. arXiv preprint arXiv:1811.10670 (2018).\n74. University of Michigan. 2019. Teaching self-driving cars\nto predict pedestrian movement. (2019). Retrieved July 3, 2019 from https://news.umich.edu/ teaching-self-driving-cars-to-predict-pedestrian-movement/\n75. VentureBeat. 2019. How Microsoft is using AI to\nimprove accessibility. (2019). Retrieved July 3, 2019 from https://venturebeat.com/2019/05/06/ how-microsoft-is-using-ai-to-improve-accessibility/\n76. The Verge. 2019. Android Q’s Live Caption feature adds\nreal-time subtitles to any audio or video playing on your phone. (2019). Retrieved July 3, 2019 from https://www.theverge.com/2019/5/7/18528447/ google-android-q-live-caption-video-transcription-io-2019\n77. Alexandra Vtyurina, Adam Fourney, Meredith Ringel\nMorris, Leah Findlater, and Ryen White. 2019. VERSE: Bridging Screen Readers and Voice Assistants for Enhanced Eyes-Free Web Search. In Proceedings of the 21st International ACM SIGACCESS Conference on Computers and Accessibility. ACM.\n78. Liang Wang, Tieniu Tan, Huazhong Ning, and Weiming\nHu. 2003. Silhouette analysis-based gait recognition for human identiﬁcation. IEEE transactions on pattern analysis and machine intelligence 25, 12 (2003), 1505–1518.\n79. Kathryn Zyskowski, Meredith Ringel Morris, Jeffrey P.\nBigham, Mary L. Gray, and Shaun K. Kane. 2015. Accessible Crowdwork?: Understanding the Value in and Challenge of Microtask Employment for People with Disabilities. In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work &#38; Social Computing (CSCW ’15). ACM, New York, NY, USA, 1682–1693. DOI:\n--- FOOTNOTES 1 Microsoft Research, Redmond, WA & New York, NY, USA 2 Human-Computer Interaction Institute, Carnegie Mellon University, Pittsburgh, PA, USA 1Throughout this paper, we use people-ﬁrst language as suggested by 2Many gesture systems use computer vision [40, 49], but some use 2018 CHI Conference on Human Factors in Computing", "persona": "Student", "response": "### 1. Overview\nIn this position url, the authors identify potential areas where Artificial Intelligence (AI) may impact people with disabilities (PWD). Although AI can be extremely beneficial to these populations (the url provides several examples of such benefits), there is a risk of these systems not working properly for PWD or even discriminating against them. This url is an effort towards identifying how inclusion issues for PWD may impact AI, which is only a part of the authors’ broader research agenda. Integrative AI – Information Retrieval (IR): These are complex systems, such as the ones that power web search engines. It is possible that IR amplifies existing bias against PWD. For example, search results can return stereotypical content for PWD, while targeted-advertising may eventually exclude PWD from products or even employment opportunities. The url is then divided into considerations across many AI functionalities and possible risks when used by PWD.\n\n### 2. Background\nThe authors covered computer vision (identification of patterns in still or video camera inputs), speech systems (systems that recognize the content or properties of speech or generate it from diverse inputs), text processing (understanding text data and its context), integrative AI (complex systems based on multiple models), and other AI techniques. Speech Systems – Speech Recognition: Automatic Speech Recognition (ASR) may not work well for “people with atypical speech”. It’s known that such systems works better for men than women, while malfunctioning for people of very advanced ages or with stronger accents. The authors point to speech disabilities, such as dysarthria, that need to be taken into consideration for a fair construction of those systems. Further, ASR locks out people who cannot speak at all.\n\n### 3. Methods\nComputer Vision – Face Recognition: The authors hypothesize that such systems may not work well for people “with differences in facial features and expressions if they were not considered when gathering training data and evaluating models”. For example, people with Down syndrom, achondroplasia, or cleft/lip palate. Systems may also malfunction for blind people, who may not show their faces at an expected angle or who may use dark glasses. Finally, emotion and expression processing algorithms may malfunction for someone with autism, Williams syndrom, who suffered a stroke, Parkinson’s disease or “or other conditions that restrict facial movements”. Computer Vision – Body Recognition: “Body recognition systems may not work well for PWD char- acterized by body shape, posture, or mobility differences”.\n\n### 4. Results\nAmong some examples, the authors point to people who have amputated limbs or someone who experiences tremors or spastic motion. Regarding people with differences in movement, systems may malfunction for “people with posture differences such as due to cerebral palsy, Parkinson’s disease, advanced age, or who use wheelchairs”. The url cites an Uber self-driving car accident, in which the car hit someone walking a bicycle. Computer Vision – Object, Scene, and Text Recognition: Many of these systems are trained in high quality pictures, usually taken by sighted people. It’s to expect that these systems may malfunction while trying to detect objects, scenes, and texts from images taken by a blind user, or someone who has tremors or motor disabilities.\n\n### 5. Takeaways\nSummary contributed by Pablo Nazé, Sr Business Manager of Responsible AI at Fairly AI. MBA grad from Rotman. The authors note that this systematic analysis of interactions between PWD and AI is not an endorsement of any system, and that there may exist an ethical debate of whether some categories of AI should be built. It’s important to note that this analysis is a starting point towards this theme, and may not be exhaustive. Other AI Techniques: For example, outlier detection. These systems usually flag outlier behaviour as negative, tied to punitive action. For example, input legitimacy (use of CAPCTHAs or other mechanisms to separate humans from bots), may not work well for people with atypical performance timing, such as someone with motor disabilities or visual impairments. Text Processing – Text Analysis: Some systems, such as spelling correction and query rewriting tools, may not handle dislexyc spelling. Moreover, since autistic people express emotion differently, systems that infern sentiments from text may also fall short for this population. The authors exposed in this opinion url ways in which AI can negatively affect PWD, which usually reflects in a worse quality of service, underrepresentation, or stereotyping for these populations. Some of the cases mentioned in the url are hypothesis, while some are backed up by evidence. The authors also propose a broader research roadmap for AI fairness regarding PWD, including testing the hypotheses presented, building representative datasets, and innovative new AI techniques “to address any shortcomings of status quo methods with respect to PWD”.", "paper_id": "36", "story_id": "36-03"}
{"paper": "Deep Video Portraits\nPABLO GARRIDO, Technicolor, France\nJUSTUS THIES and MATTHIAS NIESSNER, Technical University of Munich, Germany\nPATRICK PÉREZ, Technicolor, France\nCHRISTIAN RICHARDT, University of Bath, United Kingdom\nMICHAEL ZOLLHÖFER, Stanford University, United States of America\narXiv:1805.11714v1  [cs.CV]  29 May 2018\nInput\nOutput\nInput\nOutput\nFig. 1. Unlike current face reenactment approaches that only modify the expression of a target actor in a video, our novel deep video portrait approach enables\nfull control over the target by transferring the rigid head pose, facial expression and eye motion with a high level of photorealism.\nWe present a novel approach that enables photo-realistic re-animation of\nportrait videos using only an input video. In contrast to existing approaches\nthat are restricted to manipulations of facial expressions only, we are the irst\nto transfer the full 3D head position, head rotation, face expression, eye gaze,\nand eye blinking from a source actor to a portrait video of a target actor. The\ncore of our approach is a generative neural network with a novel space-time\narchitecture. The network takes as input synthetic renderings of a parametric\nface model, based on which it predicts photo-realistic video frames for a\ngiven target actor. The realism in this rendering-to-video transfer is achieved\nby careful adversarial training, and as a result, we can create modiied target\nvideos that mimic the behavior of the synthetically-created input. In order\nto enable source-to-target video re-animation, we render a synthetic target\nvideo with the reconstructed head animation parameters from a source\nvideo, and feed it into the trained network ś thus taking full control of the target. With the ability to freely recombine source and target parameters,\nwe are able to demonstrate a large variety of video rewrite applications\nwithout explicitly modeling hair, body or background. For instance, we can\nreenact the full head using interactive user-controlled editing, and realize\nhigh-idelity visual dubbing. To demonstrate the high quality of our output,\nwe conduct an extensive series of experiments and evaluations, where for\ninstance a user study shows that our video edits are hard to detect.\nCCS Concepts: · Computing methodologies →Computer graphics;\nNeural networks; Appearance and texture representations; Animation; Rendering;\nAdditional Key Words and Phrases: Facial Reenactment, Video Portraits,\nDubbing, Deep Learning, Conditional GAN, Rendering-to-Video Translation\nACM Reference Format:\nHyeongwoo Kim, Pablo Garrido, Ayush Tewari, Weipeng Xu, Justus Thies,\nMatthias Nießner, Patrick Pérez, Christian Richardt, Michael Zollhöfer,\nand Christian Theobalt. 2018. Deep Video Portraits. ACM Trans. Graph. 37, 4,\n\nINTRODUCTION\nSynthesizing and editing video portraits, i.e., videos framed to show a person’s head and upper body, is an important problem in computer graphics, with applications in video editing and movie postproduction, visual efects, visual dubbing, virtual reality, and telepresence, among others. In this paper, we address the problem of synthesizing a photo-realistic video portrait of a target actor that mimics the actions of a source actor, where source and target can be diferent subjects. More speciically, our approach enables a source actor to take full control of the rigid head pose, face expressions and eye motion of the target actor; even face identity can be modiied to some extent. All of these dimensions can be manipulated together or independently. Full target frames, including the entire head and hair, but also a realistic upper body and scene background complying with the modiied head, are automatically synthesized. Recently, many methods have been proposed for face-interior reenactment [Liu et al. 2001; Olszewski et al. 2017; Suwajanakorn et al. 2017; Thies et al. 2015, 2016; Vlasic et al. 2005]. Here, only the face expression can be modiied realistically, but not the full changing background. Many of these methods it a parametric 3D face model to RGB(-D) video [Thies et al. 2015, 2016; Vlasic et al. 2005], and re-render the modiied model as a blended overlay over the target video for reenactment, even in real time [Thies et al. 2015, 2016]. Synthesizing a complete portrait video under full 3D head control is much more challenging. Averbuch-Elor et al. [2017] enable mild head pose changes driven by a source actor based on image warping. They generate reactive dynamic proile pictures from a static target portrait photo, but not fully reenacted videos. Also, large changes in head pose cause artifacts (see Section 7.3), the target gaze cannot be controlled, and the identity of the target person is not fully preserved (mouth appearance is copied from the source actor). Performance-driven 3D head animation methods [Cao et al. 2015, 2014a, 2016; Hu et al. 2017; Ichim et al. 2015; Li et al. 2015; Olszewski et al. 2016; Weise et al. 2011] are related to our work, but have orthogonal methodology and application goals. They typically drive the full head pose of stylized 3D CG avatars based on visual source actor input, e.g., for games or stylized VR environments. Recently, Cao et al. [2016] proposed image-based 3D avatars with dynamic textures based on a real-time face tracker. However, their goal is full 3D animated head control and rendering, often intentionally in a stylized rather than a photo-realistic fashion. We take a diferent approach that directly generates entire photorealistic video portraits in front of general static backgrounds under full control of a target’s head pose, facial expression, and eye motion. We formulate video portrait synthesis and reenactment as a rendering-to-video translation task. Input to our algorithm are synthetic renderings of only the coarse and fully-controllable 3D face interior model of a target actor and separately rendered eye gaze images, which can be robustly and eiciently obtained via a state-of-the-art model-based reconstruction technique. The input is automatically translated into full-frame photo-realistic video output showing the entire upper body and background. Since we only track the face, we cannot actively control the motion of the torso or hair, or control the background, but our rendering-to-video translation network is able to implicitly synthesize a plausible body and background (including some shadows and relections) for a given head pose. This translation problem is tackled using a novel space-time encoderśdecoder deep neural network, which is trained in an adversarial manner. At the core of our approach is a conditional generative adversarial network (cGAN) [Isola et al. 2017], which is speciically tailored to video portrait synthesis. For temporal stability, we use a novel space-time network architecture that takes as input short sequences of conditioning input frames of head and eye gaze in a sliding window manner to synthesize each target video frame. Our target and scene-speciic networks only require a few minutes of portrait video footage of a person for training. To the best of our knowledge, our approach is the irst to synthesize full photo-realistic video portraits of a target person’s upper body, including realistic clothing and hair, and consistent scene background, under full 3D control of the target’s head. To summarize, we make the following technical contributions: • A rendering-to-video translation network that transforms coarse face model renderings into full photo-realistic portrait video output. • A novel space-time encoding as conditional input for temporally coherent video synthesis that represents face geometry, relectance, and motion as well as eye gaze and eye blinks. • A comprehensive evaluation on several applications to demonstrate the lexibility and efectiveness of our approach. We demonstrate the potential and high quality of our method in many intriguing applications, ranging from face reenactment and visual dubbing for foreign language movies to user-guided interactive editing of portrait videos for movie postproduction. A comprehensive comparison to state-of-the-art methods and a user study conirm the high idelity of our results.\n\n3 OVERVIEW Our deep video portraits approach provides full control of the head of a target actor by transferring the rigid head pose, facial expression, and eye motion of a source actor, while preserving the target’s identity and appearance. Full target video frames are synthesized, including consistent upper body posture, hair and background. First, we track the source and target actor using a state-of-the-art monocular face reconstruction approach that uses a parametric face and illumination model (see Section 4). The resulting sequence of lowdimensional parameter vectors represents the actor’s identity, head pose, expression, eye gaze, and the scene lighting for every video frame (Figure 2, left). This allows us to transfer the head pose, expression, and/or eye gaze parameters from the source to the target, as desired. In the next step (Figure 2, middle), we generate new synthetic renderings of the target actor based on the modiied parameters (see Section 5). In addition to a normal color rendering, we also render correspondence maps and eye gaze images. These renderings serve as conditioning input to our novel rendering-to-video translation network (see Section 6), which is trained to convert the synthetic input into photo-realistic output (see Figure 2, right). For temporally coherent results, our network works on space-time volumes of conditioning inputs. To process a complete video, we input the conditioning space-time volumes in a sliding window fashion, and assemble the inal video from the output frames. We evaluate our approach (see Section 7) and show its potential on several video rewrite applications, such as full-head reenactment, gaze redirection, video dubbing, and interactive parameter-based video control. 4 MONOCULAR FACE RECONSTRUCTION We employ a state-of-the-art dense face reconstruction approach that its a parametric model of face and illumination to each video frame. It obtains a meaningful parametric face representation for the source Vs = {Is f | f = 1, . . . , Ns } and target Vt = {It f | f = 1, . . . , Nt } video sequence, where Ns and Nt denote the total number of source and target frames, respectively. Let P• = {P• f | f = 1, . . . , N•} be the corresponding parameter sequence that fully describes the source or target facial performance. The set of reconstructed parameters encode the rigid head pose (rotation R• ∈SO(3) and translation t• ∈R3), facial identity coeicients α • ∈RNα (geometry, Nα = 80) and β• ∈RNβ (relectance, Nβ = 80), expression coeicients δ• ∈RNδ (Nδ =64), gaze direction for both eyes e• ∈R4, and spherical harmonics illumination coeicients γ• ∈R27. Overall, our monocular face tracker reconstructs Np =261 parameters per video frame. In the following, we provide more details on the face tracking algorithm as well as the parametric face representation. Parametric Face Representation. We represent the space of facial identity based on a parametric head model [Blanz and Vetter 1999], and the space of facial expressions via an aine model. Mathematically, we model geometry variation through an aine model v∈R3N that stacks per-vertex deformations of the underlying template mesh with N vertices, as follows: Nδ Õ Nα Õ k=1 αkbgeo k + k=1 δkbexp k . (1) v(α,δ) = ageo + Difuse skin relectance is modeled similarly by a second aine model r∈R3N that stacks the difuse per-vertex albedo: Nβ Õ k=1 βkbref k . (2) r(β) = aref + The vectors ageo ∈R3N and aref ∈R3N store the average facial geometry and corresponding skin relectance, respectively. The geometry basis {bgeo k }Nα k=1 has been computed by applying principal component analysis (PCA) to 200 high-quality face scans [Blanz and Vetter 1999]. The relectance basis {bref k }Nβ k=1 has been obtained in the same manner. For dimensionality reduction, the expression basis {bexp k }Nδ k=1 has been computed using PCA, starting from the blendshapes of Alexander et al. [2010] and Cao et al. [2014b]. Their blendshapes have been transferred to the topology of Blanz and Vetter [1999] using deformation transfer [Sumner and Popović 2004]. Image Formation Model. To render synthetic head images, we assume a full perspective camera that maps model-space 3D points v via camera space ˆv∈R3 to 2D points p=Π(ˆv)∈R2 on the image plane. The perspective mapping Π contains the multiplication with the camera intrinsics and the perspective division. We assume a ixed and identical camera for all scenes, i.e., world and camera space are the same, and the face model accounts for all the scene motion. Based on a distant illumination assumption, we use the spherical harmonics (SH) basis functions Yb : R3 →R to approximate the incoming radiance B from the environment: B2 Õ B(ri, ni,γ) = ri · b=1 γbYb(ni). (3) Here, B is the number of spherical harmonics bands, γb ∈R3 are the SH coeicients, and ri and ni are the relectance and unit normal vector of the i-th vertex, respectively. For difuse materials, an average approximation error below 1 percent is achieved with only B = 3 bands, independent of the illumination [Ramamoorthi and Hanrahan 2001], since the incident radiance is in general a smooth function. This results in B2 =9 parameters per color channel. Dense Face Reconstruction. We employ a dense data-parallel face reconstruction approach to eiciently compute the parameters P• for both source and target videos. Face reconstruction is based on an analysis-by-synthesis approach that maximizes photo-consistency between a synthetic rendering of the model and the input. The reconstruction energy combines terms for dense photo-consistency, landmark alignment and statistical regularization: E(X) = wphotoEphoto(X) + wlandEland(X) + wregEreg(X), (4) with X ={R•, t•,α •, β•,δ•,γ•}. This enables the robust reconstruction of identity (geometry and skin relectance), facial expression, and scene illumination. We use 66 automatically detected facial landmarks of the True Vision Solution tracker1, which is a commercial implementation of Saragih et al. [2011], to deine the sparse alignment term Eland. Similar to Thies et al. [2016], we use a robust ℓ1-norm for dense photometric alignment Ephoto. The regularizer Ereg enforces statistically plausible parameter values based on the assumption of normally distributed data. The eye gaze estimate e• is directly obtained from the landmark tracker. The identity is only estimated in the irst frame and is kept constant afterwards. All other parameters are estimated every frame. For more details on the energy formulation, we refer to Garrido et al. [2016] and Thies et al. [2016]. We use a data-parallel implementation of iteratively re-weighted least squares (IRLS), similar to Thies et al. [2016], to ind the optimal set of parameters. One diference to their work is that we compute and explicitly store the Jacobian J and the residual vector F to global memory based on a data-parallel strategy that launches one thread per matrix/vector element. Afterwards, a dataparallel matrixśmatrix/matrixśvector multiplication computes the right- and left-hand side of the normal equations that have to be solved in each IRLS step. The resulting small linear system (97×97 in tracking mode, 6 DoF rigid pose, 64 expression parameters and 27 SH coeicients) is solved on the CPU using Cholesky factorization in each IRLS step. The reconstruction of a single frame takes 670 ms (all parameters) and 250 ms (without identity, tracking mode). This allows the eicient generation of the training corpus that is required by our space-time rendering-to-video translation network (see Section 6). Contrary to Garrido et al. [2016] and Thies et al. [2016], our model features dimensions to model eyelid closure, so eyelid motion is captured well. 5 SYNTHETIC CONDITIONING INPUT Using the method from Section 4, we reconstruct the face in each frame of the source and unmodiied target video. Next, we obtain the modiied parameter vector for every frame of the target sequence, e.g., for full-head reenactment, we modify the rigid head pose, expression and eye gaze of the target actor. All parameters are copied in a relative manner from the source to the target, i.e., with respect to a neutral reference frame. Then we render synthetic conditioning images of the target actor’s face model under the modiied parameters using hardware rasterization. For higher temporal coherence, our rendering-to-video translation network takes a space-time volume of conditioning images {Cf −o |o=0, . . . , 10} as input, with f being the index of the current frame. We use a temporal window of size Nw =11, with the current frame being at its end. This provides the network a history of the earlier motions. For each frame Cf −o of the window, we generate three diferent conditioning inputs: a color rendering, a correspondence image, and an eye gaze image (see Figure 3). The color rendering shows the modiied target actor model under the estimated target illumination, while keeping the target identity (geometry and skin relectance) ixed. This image provides a good starting point for the following rendering-to-video translation, since in the face region only the 1http://truevisionsolutions.net Deep Video Portraits • 163:5 Diffuse Rendering Correspondence Eye and Gaze Map Fig. 3. The synthetic input used for conditioning our rendering-to-video translation network: (1) colored face rendering under target illumination, (2) correspondence image, and (3) the eye gaze image. delta to a real image has to be learned. In addition to this color input, we also provide a correspondence image encoding the index of the parametric face model’s vertex that projects into each pixel. To this end, we texture the head model with a constant unique gradient texture map, and render it. Finally, we also provide an eye gaze image that solely contains the white region of both eyes and the locations of the pupils as blue circles. This image provides information about the eye gaze direction and blinking to the network. We stack all Nw conditioning inputs of a time window in a 3D tensor X of size W × H × 9Nw (3 images, with 3 channels each), to obtain the input to our rendering-to-video translation network. To process the complete video, we feed the conditioning space-time volumes in a sliding window fashion. The inal generated photorealistic video output is assembled directly from the output frames. 6 RENDERING-TO-VIDEO TRANSLATION The generated conditioning space-time video tensors are the input to our rendering-to-video translation network. The network learns to convert the synthetic input into full frames of a photo-realistic target video, in which the target actor now mimics the head motion, facial expression and eye gaze of the synthetic input. The network learns to synthesize the entire actor in the foreground, i.e., the face for which conditioning input exists, but also all other parts of the actor, such as hair and body, so that they comply with the target head pose. It also synthesizes the appropriately modiied and illed-in background, including even some consistent lighting efects between foreground and background. The network is trained for a speciic target actor and a speciic static, but otherwise general scene background. Our rendering-to-video translation network follows an encoderśdecoder architecture and is trained in an adversarial manner based on a discriminator that is jointly trained. In the following, we explain the network architectures, the used loss functions and the training procedure in detail. Network Architecture. We show the architecture of our renderingto-video translation network in Figure 4. Our conditional generative adversarial network consists of a space-time transformation network T and a discriminator D. The transformation network T takes the W × H × 9Nw space-time tensor X as input and outputs a photoreal image T(X) of the target actor. The temporal input enables the network to take the history of motions into account by inspecting previous conditioning images. The temporal axis of the input tensor is aligned along the network channels, i.e., the convolutions in the irst layer have 9Nw channels. Note, we store all image data in normalized [−1, +1]-space, i.e, black is mapped to [−1, −1, −1]⊤and white is mapped to [+1, +1, +1]⊤. 163:6 •\n\nH. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt\nBilinear Downsampling 32 64 128 Y T X 256 32 64 128 128 64 … D 256 256 T(X) 64 128 256 32 64 128 TanH channels 64 128 256 128 64 9�# 3 … 3 in in stride 2 stride 1 stride 2 4×4 = = = = prob. prob. 4×4 3×3 out out BN LReLu Up Conv Drop Drop Conv BN ReLu Up Refine Refine DeConv BN ReLu Refine Fig. 4. Architecture of our rendering-to-video translation network for an input resolution of 256×256: The encoder has 8 downsampling modules with (64, 128, 256, 512, 512, 512, 512, 512) output channels. The decoder has 8 upsampling modules with (512, 512, 512, 512, 256, 128, 64, 3) output channels. The upsampling modules use the following dropout probabilities (0.5, 0.5, 0.5, 0, 0, 0, 0, 0). The first downsampling and the last upsampling module do not employ batch normalization (BN). The final non-linearity (TanH) brings the output to the employed normalized [−1, +1]-space. Our network consists of two main parts, an encoder for computing a low-dimensional latent representation, and a decoder for synthesizing the output image. We employ skip connections [Ronneberger et al. 2015] to enable the network to transfer ine-scale structure. To generate video frames with suicient resolution, our network also employs a cascaded reinement strategy [Chen and Koltun 2017]. In each downsampling step, we use a convolution (4 × 4, stride 2) followed by batch normalization and a leaky ReLU non-linearity. The upsampling module is speciically designed to produce high-quality output, and has the following structure: irst, the resolution is increased by a factor of two based on deconvolution (4 × 4, upsampling factor of 2), batch normalization, dropout and ReLU. Afterwards, two reinement steps based on convolution (3 × 3, stride 1, stays on the same resolution) and ReLU are applied. The inal hyperbolic tangent non-linearity (TanH) brings the output tensor to the normalized [−1, +1]-space used for storing the image data. For more details, please refer to Figure 4. The input to our discriminator D is the conditioning input tensor X (size W × H × 9Nw), and either the predicted output image T(X) or the ground-truth image, both of size W × H × 3. The employed discriminator is inspired by the PatchGAN classiier, proposed by Isola et al. [2017]. We extended it to take volumes of conditioning images as input. Objective Function. We train in an adversarial manner to ind the best rendering-to-video translation network: T∗= argmin T max D EcGAN(T, D) + λEℓ1(T). (5) This objective function comprises an adversarial loss EcGAN(T, D) and an ℓ1-norm reproduction loss Eℓ1(T). The constant weight of λ=100 balances the contribution of these two terms. The adversarial loss has the following form: EGAN(T, D) = EX,Y \u0002 log D(X, Y) \u0003 + EX \u0002 log \u00001 −D(X, T(X))\u0001\u0003 . (6) We do not inject a noise vector while training our network to produce deterministic outputs. During adversarial training, the discriminator D tries to get better at classifying given images as real or synthetic, while the transformation network T tries to improve in fooling the discriminator. The ℓ1-norm loss penalizes the distance between the synthesized image T(X) and the ground-truth image Y, which encourages the sharpness of the synthesized output: Eℓ1(T) = EX,Y \u0002 ∥Y −T(X)∥1 \u0003 . (7) Training. We construct the training corpus T ={(Xi, Yi)}i based on the tracked video frames of the target video sequence. Typically, two thousand video frames, i.e., about one minute of video footage, are suicient to train our network (see Section 7). Our training corpus consists of Nt −(Nw −1) rendered conditioning space-time volumes Xi and the corresponding ground-truth image Yi (using a window size of Nw =11). We train our networks using the TensorFlow [Abadi et al. 2015] deep learning framework. The gradients for back-propagation are obtained using Adam [Kingma and Ba 2015]. We train for 31,000 iterations with a batch size of 16 (approx. 250 epochs for a training corpus of 2000 frames) using a base learning rate of 0.0002 and irst momentum of 0.5; all other parameters have their default value. We train our networks from scratch, and initialize the weights based on a Normal distribution N(0, 0.2).\n\nRESULTS\nOur approach enables full-frame target video portrait synthesis under full 3D head pose control. We measured the runtime for training and testing on an Intel Xeon E5-2637 with 3.5 GHz (16 GB RAM) and an NVIDIA GeForce GTX Titan Xp (12 GB RAM). Training our network takes 10 hours for a target video resolution of 256×256 pixels, and 42 hours for 512×512 pixels. Tracking the source actor takes 250 ms per frame (without identity), and the rendering-to-video conversion (inference) takes 65 ms per frame for 256×256 pixels, or 196 ms for 512×512 pixels. In the following, we evaluate the design choices of our deep video portrait algorithm, compare to current state-of-the-art reenactment approaches, and show the results of a large-scale web-based user study. We further demonstrate the potential of our approach on several video rewrite applications, such as reenactment under full head and facial expression control, facial expression reenactment only, video dubbing, and live video portrait editing under user control. In total, we applied our approach to 14 diferent target sequences of 13 diferent subjects and used 5 diferent source sequences; see Appendix A for details. A comparison to a simple nearest-neighbor retrieval approach can be found in Figure 6 and in the supplemental video. Our approach requires only a few minutes of target video footage for training. 7.1 Applications Our approach enables us to take full control of the rigid head pose, facial expression, and eye motion of a target actor in a video portrait, thus opening up a wide range of video rewrite applications. All parameter dimensions can be estimated and transfered from a source video sequence or edited manually through an interactive user interface. Source Target Source Target Fig. 5. ualitative results of full-head reenactment: our approach enables full-frame target video portrait synthesis under full 3D head pose control. The output video portraits are photo-realistic and hard to distinguish from real videos. Note that even the shadow in the background of the second row moves consistently with the modified foreground head motion. In the sequence at the top, we only transfer the translation in the camera plane, while we transfer the full 3D translation for the sequence at the botom. For full sequences, please refer to our video. Obama video courtesy of the White House (public domain). Input Ours Nearest Neighbor Fig. 6. Comparison to a nearest-neighbor approach in parameter space (pose and expression). Our results have higher quality and are temporally more coherent (see supplemental video). For the nearest-neighbor approach, it is dificult to find the right trade-of between pose and expression. This leads to many results with one of the two dimensions not being well-matched. The results are also temporally unstable, since the nearest neighbor abruptly changes, especially for small training sets. Reenactment under full head control. Our approach is the irst that can photo-realistically transfer the full 3D head pose (spatial position and rotation), facial expression, as well as eye gaze and eye blinking of a captured source actor to a target actor video. Figure 5 shows some examples of full-head reenactment between diferent source and target actors. Here, we use the full target video for training and the source video as the driving sequence. As can be seen, the output of our approach achieves a high level of realism and faithfully mimics the driving sequence, while still retaining the mannerisms of the original target actor. Note that the shadow in the background moves consistently with the position of the actor in the scene, as shown in Figure 5 (second row). We also demonstrate the high quality of our results and evaluate our approach quantitatively in a self-reenactment scenario, see Figure 7. For the quantitative analysis, Deep Video Portraits • 163:7 we use two thirds of the target video for training and one third for testing. We capture the face in the training and driving video with our model-based tracker, and then render the conditioning images, which serve as input to our network for synthesizing the output. For further details, please refer to Section 7.2. Note that the synthesized results are nearly indistinguishable from the ground truth. Facial Reenactment and Video Dubbing. Besides full-head reenactment, our approach also enables facial reenactment. In this experiment, we replace the expression coeicients of the target actor with those of the source actor before synthesizing the conditioning input to our rendering-to-video translation network. Here, the head pose and position, and eye gaze remain unchanged. Figure 8 shows facial reenactment results. Observe that the face expression in the synthesized target video nicely matches the expression of the source actor in the driving sequence. Please refer to the supplemental video for the complete video sequences. Our approach can also be applied to visual dubbing. In many countries, foreign-language movies are dubbed, i.e., the original voice of an actor is replaced with that of a dubbing actor speaking in another language. Dubbing often causes visual discomfort due to the discrepancy between the actor’s mouth motion and the new audio track. Even professional dubbing studios achieve only approximate audio alignment at best. Visual dubbing aims at altering the mouth motion of the target actor to match the new foreign-language audio track spoken by the dubber. Figure 9 shows results where we modify the facial motion of actors speaking originally in German to adhere to an English translation spoken by a professional dubbing actor, who was ilmed in a dubbing studio [Garrido et al. 2015]. More precisely, we transfer the captured facial expressions. Fig. 7. uantitative evaluation of the photometric re-rendering error. We evaluate our approach quantitatively in a self-reenactment seting, where the ground-truth video portrait is known. We train our rendering-to-video translation network on two thirds of the video sequence, and test on the remaining third. The error maps show per-pixel Euclidean distance in RGB (color channels in [0, 255]); the mean photometric error of the test set is shown in the top-right. The error is consistently low in regions with conditioning input, with higher errors in regions without conditioning, such as the upper body. Obama video courtesy of the White House (public domain). Putin video courtesy of the Kremlin (CC BY). May video courtesy of the UK government (Open Government Licence). Fig. 8. Facial reenactment results of our approach. We transfer the expressions from the source to the target actor, while retaining the head pose (rotation and translation) as well as the eye gaze of the target actor. For the full sequences, please refer to the supplemental video. Obama video courtesy of the White House (public domain). Putin video courtesy of the Kremlin (CC BY). Reagan video courtesy of the National Archives and Records Administration (public domain). Fig. 9. Dubbing comparison on two sequences of Garrido et al. [2015]. For visual dubbing, we transfer the facial expressions of the dubbing actor (‘input’) to the target actor. We compare our results to Garrido et al.’s. Our approach obtains higher quality results in terms of the synthesized mouth shape and mouth interior. Note that our approach also enables full-head reenactment in addition to expression transfer. For the full comparison, we refer to the supplemental video. of the dubbing actor to the target actor, while leaving the original target gaze and eye blinks intact, i.e., we use the original eye gaze images of the tracked target sequence as conditioning. As can be seen, our approach achieves dubbing results of high quality. In fact, we produce images with more realistic mouth interior and more emotional content in the mouth region. Please see the supplemental video for full video results. Interactive Editing of Video Portraits. We built an interactive editor that enables users to reanimate video portraits with live feedback by modifying the parameters of the coarse face model rendered into the conditioning images (see our live demo in the supplemental video). Figure 10 shows a few static snapshots that were taken while the users were playing with our editor. Our approach enables changes of all parameter dimensions, either independently or all together, as shown in Figure 10. More speciically, we show independent changes of the expression, head rotation, head translation, and eye gaze (including eye blinks). Please note the realistic and consistent generation of the torso, head and background. Even shadows or relections appear very consistently in the background. In addition, we show user edits that modify all parameters simultaneously. Our interactive editor runs at approximately 9 fps. While not the focus of this paper, our approach also enables modiications of the geometric facial identity, see Figure 11. These combined modiications show as a proof of concept that our network generalizes beyond the training corpus. 7.2 uantitative Evaluation We performed a quantitative evaluation of the re-rendering quality. First, we evaluate our approach in a self-reenactment setting, where the ground-truth video portrait is known. We train our rendering-tovideo translation network on the irst two thirds of a video sequence and test it on the remaining last third of the video, see Figure 7. The photometric error maps show the per-pixel Euclidean distance in RGB color space, with each channel being in [0, 255]. We performed this test for three diferent videos and the mean photometric errors are 2.88 (Vladimir Putin), 4.76 (Theresa May), and 4.46 (Barack Obama). Our approach obtains consistently low error in regions Reference Expression Gaze Rotation Translation Combined Fig. 10. Interactive editing. Our approach provides full parametric control over video portraits (by controlling head model parameters in conditioning images). This enables modifications of the rigid head pose (rotation and translation), facial expression and eye motion. All of these dimensions can be manipulated together or independently. We also show these modifications live in the supplemental video. Obama video courtesy of the White House (public domain). Reference Identity Change Fig. 11. Identity modification. While not the main focus of our approach, it also enables modification of the facial shape via the geometry shape parameters. This shows that our network picks up the correspondence between the model and the video portrait. Note that the produced outputs are also consistent in regions that are not constrained by the conditioning input, such as the hair and background. Ours Averbuch-Elor17 Input Fig. 12. Comparison to the image reenactment approach of Averbuch-Elor et al. [2017] in the full-head reenactment scenario. Since their method is based on a single target image, they copy the mouth interior from the source to the target, thus not preserving the target’s identity. Our learningbased approach enables larger modifications of the rigid head pose without apparent artifacts, while their warping-based approach distorts the head and background. In addition, ours enables joint control of the eye gaze and eye blinks. The diferences are most evident in the supplemental video. Obama video courtesy of the White House (public domain). Deep Video Portraits • 163:9 Reference Expression Gaze Reference Expression Gaze Rotation Translation Combined Rotation Translation Combined Fig. 13. Comparison to Suwajanakorn et al. [2017]. Their approach produces accurate lip sync with visually imperceptible artifacts, but provides no direct control over facial expressions. Thus, the expressions in the output do not always perfectly match the input (box, mouth), especially for expression changes without audio cue. Our visual dubbing approach accurately transfers the expressions from the source to the target. In addition, our approach provides more control over the target video by also transferring the eye gaze and eye blinks (box, eyes), and the rigid head pose (arrows). Since the source sequence shows more head-pose variation than the target sequence, we scaled the transferred rotation and translation by 0.5 in this experiment. For the full video sequence, we refer to the supplemental video. Obama video courtesy of the White House (public domain). with conditioning input (face) and higher errors are found in regions that are unexplained by the conditioning input. Please note that while the synthesized video portraits slightly difer from the ground truth outside the face region, the synthesized hair and upper body are still plausible, consistent with the face region, and free of visual artifacts. For a complete analysis of these sequences, we refer to the supplemental video. We evaluate our space-time conditioning strategy in Figure 16. Without space-time conditioning, the photometric error is signiicantly higher. The average errors over the complete sequence are 4.9 without vs. 4.5 with temporal conditioning (Barack Obama) and 5.3 without vs. 4.8 with temporal conditioning (Theresa May). In 163:10 •\n\nH. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt\nFig. 14. Comparison to the state-of-the-art facial reenactment approach of Thies et al. [2016]. Our approach achieves expression transfer of similar quality, while also enabling full-head reenactment, i.e., it also transfers the rigid head pose, gaze direction, and eye blinks. For the video result, we refer to the supplemental video. Obama video courtesy of the White House (public domain). addition to a lower photometric error, space-time conditioning also leads to temporally signiicantly more stable video outputs. This can be seen best in the supplemental video. We also evaluate the importance of the training set size. In this experiment, we train our rendering-to-video translation network with 500, 1000, 2000 and 4000 frames of the target sequence, see Figure 15. As can be expected, larger training sets produce better results, and the best results are obtained with the full training set. We also evaluate diferent image resolutions by training our rendering-to-video translation network for resolutions of 256×256, 512×512 and 1024×1024 pixels. We evaluate the quality in the selfreenactment setting, as shown in Figure 17. Generative networks of higher resolution are harder to train and require signiicantly longer training times: 10 hours for 256×256, 42 hours for 512×512, and 110 hours for 1024×1024 (on a Titan Xp). Therefore, we use a resolution of 256×256 pixels for most results. 7.3 Comparisons to the State of the Art We compare our deep video portrait approach to current state-ofthe-art video and image reenactment techniques. Comparison to Thies et al. [2016]. We compare our approach to the state-of-the-art Face2Face facial reenactment method of Thies et al. [2016]. In comparison to Face2Face, our approach achieves expression transfer of similar quality. What distinguishes our approach is the capability for full-head reenactment, i.e., the ability to also transfer the rigid head pose, gaze direction, and eye blinks in addition to the facial expressions, as shown in Figure 14. As can be seen, in our result, the head pose and eye motion nicely matches the source sequence, while the output generated by Face2Face follows the head and eye motion of the original target sequence. Please see the supplemental video for the video result. Comparison to Suwajanakorn et al. [2017]. We also compare to the audio-based dubbing approach of Suwajanakorn et al. [2017], see Figure 13. Their AudioToObama approach produces accurate lip sync with visually imperceptible artifacts, but provides no direct control over facial expressions. Thus, the expressions in the output do not always perfectly match the input (box, mouth), especially for expression changes without an audio cue. Our visual dubbing approach accurately transfers the expressions from the source to the target. In addition, our approach provides more control over the target video by also transferring the eye gaze and eye blinks (box, eyes) and the general rigid head pose (arrows). While their approach is trained on a huge amount of training data (17 hours), our approach only uses a small training dataset (1.3 minutes). The diferences are best visible in the supplemental video. Comparison to Averbuch-Elor et al. [2017]. We compare our approach in the full-head reenactment scenario to the image reenactment approach of Averbuch-Elor et al. [2017], see Figure 12. Their approach does not preserve the identity of the target actor, since they copy the teeth and mouth interior from the source to the target sequence. Our learning-based approach enables larger modiications of the head pose without apparent artifacts, while their warpingbased approach signiicantly distorts the head and background. In addition, we enable the joint modiication of the gaze direction and eye blinks; see supplemental video. 7.4 User Study We conducted two extensive web-based user studies to quantitatively evaluate the realism of our results. We prepared short 5second video clips that we extracted from both real and synthesized videos (see Figure 18), to evaluate three applications of our approach: self-reenactment, same-person-reenactment and visual dubbing. We opted for self-reenactment, same-person-reenactment (two speeches of Barack Obama) and visual dubbing to guarantee that the motion types in the evaluated real and synthesized video pairs are matching. This eliminates the motion type as a confounding factor from the statistical analysis, e.g., having unrealistic motions for a public speech in the synthesized videos would negatively bias the outcome of the study. Our evaluation is focused on the visual quality of the synthesized results. Most video clips have a resolution of 256×256 pixels, but some are 512×512 pixels. In our user study, we presented one video clip at a time, and asked participants to respond to the statement łThis video clip looks real to mež on a 5-point Likert scale (1śstrongly disagree, 2śdisagree, 3śdon’t know, 4śagree, 5śstrongly agree). Video clips are shown in a random order, and each video clip is shown exactly once to assess participants’ irst impression. We recruited 135 and 69 anonymous participants for our two studies, largely from North America and Europe. Fig. 15. uantitative evaluation of the training set size. We train our rendering-to-video translation network with training corpora of diferent sizes. The error maps show the per-pixel distance in RGB color space with each channel being in [0, 255]; the mean photometric error is shown in the top-right. Smaller training sets have larger photometric errors, especially for regions outside of the face. For the full comparison, we refer to the supplemental video. Obama video courtesy of the White House (public domain). May video courtesy of the UK government (Open Government Licence). Fig. 16. uantitative evaluation of the influence of the proposed space-time conditioning input. The error maps show the per-pixel distance in RGB color space with each channel being in [0, 255]; the mean photometric error is shown in the top-right. Without space-time conditioning, the photometric error is higher. Temporal conditioning adds significant temporal stability. This is best seen in the supplemental video. Obama video courtesy of the White House (public domain). May video courtesy of the UK government (Open Government Licence). Fig. 17. uantitative comparison of diferent resolutions. We train three rendering-to-video translation networks for resolutions of 256×256, 512×512 and 1024×1024 pixels. The error maps show the per-pixel distance in RGB color space with each channel being in [0, 255]; the mean photometric error is shown in the top-right. For the full comparison, see our video. May video courtesy of the UK government (Open Government Licence). Deep Video Portraits • 163:11 Fig. 18. We performed a user study to evaluate the quality of our results and see if users can distinguish between real (top) and synthesized video clips (botom). The video clips include self-reenactment, same-person-reenactment, and video dubbing. Putin video courtesy of the Kremlin (CC BY). Obama video courtesy of the White House (public domain). Elizabeth II video courtesy of the Governor General of Canada (public domain). The results in Table 1 show that only 80% of participants rated real 256×256 videos as real, i.e. (strongly) agreeing to the video looking real; it seems that in anticipation of synthetic video clips, participants became overly critical. At the same time, 50% of participants consider our 256×256 results to be real, which increases slightly to 52% for 512×512. Our best result is the self-reenactment of Vladimir Putin at 256×256 resolution, which 65% of participants consider to be real, compared to 78% for the real video. We also evaluated partial and full reenactment by transferring a speech by Barack Obama to another video clip of himself. Table 2 indicates that we achieve better realism ratings with full reenactment comprising facial expressions and pose (50%) compared to transferring only facial expressions (38%). This might be because full-head reenactment keeps expressions and head motion synchronized. Suwajanakorn et al.’s speech-driven reenactment approach [2017] achieves a realism rating of 64% compared to the real source and target video clips, which achieve 70ś86%. Our full-head reenactment results are considered to be at least as real as Suwajanakorn et al.’s by 60% of participants. We inally compared our dubbing results to VDub [Garrido et al. 2015] in Table 3. Overall, 57% of participants gave our results a higher realism rating (and 32% gave the same rating). Our results are again considered to be real by 51% of participants, compared to only 21% for VDub.\n On average, across all scenarios and both studies, our results are considered to be real by 47% of the participants (1,767 ratings), compared to only 80% for real video clips (1,362 ratings). This suggests that our results already fool about 60% of the participants ś a good result given the critical participant pool. However, there is some variation across our results: lower realism ratings were given for well-known personalities like Barack Obama, while higher ratings were given for instance to the unknown dubbing actors.\n\nDISCUSSION\nWhile we have demonstrated highly realistic reenactment results in a large variety of applications and scenarios, our approach is also subject to a few limitations. Similar to all other learning-based approaches, ours works very well inside the span of the training corpus. Extreme target head poses, such as large rotations, or expressions far outside this span can lead to a degradation of the visual quality of the generated video portrait, see Figure 19 and the supplemental video. Since we only track the face with a parametric model, we cannot actively control the motion of the torso or hair, or control the background. The network learns to extrapolate and inds a plausible and consistent upper body and background (including some shadows and relections) for a given head pose. This limitation Fig. 19. Our approach works well within the span of the training corpus. Extreme changes in head pose far outside the training set or strong changes to the facial expression might lead to artifacts in the synthesized video. This is a common limitation of all learning-based approaches. In these cases, artifacts are most prominent outside the face region, as these regions have no conditioning input. May video courtesy of the UK government (Open Government Licence). Malou video courtesy of Louisa Malou (CC BY). could be overcome by also tracking the body and using the underlying body model to generate an extended set of conditioning images. Currently, we are only able to produce medium-resolution output due to memory and training time limitations. The limited output resolution makes it especially diicult to reproduce ine-scale detail, such as individual teeth, in a temporally coherent manner. Yet, recent progress on high-resolution discriminative adversarial networks [Karras et al. 2018; Wang et al. 2017] is promising and could be leveraged to further increase the resolution of the generated output. On a broader scale, and not being a limitation, democratization of advanced high-quality video editing possibilities, ofered by our and other methods, calls for additional care in ensuring veriiable video authenticity, e.g., through invisible watermarking. 9\n\nRELATED WORK\nWe discuss related optimization and learning-based methods that aim at reconstructing, animating and re-writing faces in images and videos, and review relevant image-to-image translation work. For a comprehensive overview of current methods we refer to a recent state-of-the-art report on monocular 3D face reconstruction, tracking and applications [Zollhöfer et al. 2018]. Monocular Face Reconstruction. Face reconstruction methods aim to reconstruct 3D face models of shape and appearance from visual data. Optimization-based methods it a 3D template model, mainly the inner face region, to single images [Blanz et al. 2004; Blanz and Vetter 1999], unstructured image collections [KemelmacherShlizerman 2013; Kemelmacher-Shlizerman et al. 2011; Roth et al. 2017] or video [Cao et al. 2014b; Fyfe et al. 2014; Garrido et al. 2016; Ichim et al. 2015; Shi et al. 2014; Suwajanakorn et al. 2014; Thies et al. 2016; Wu et al. 2016]. Recently, Booth et al. [2018] proposed a largescale parametric face model constructed from almost ten thousand images or image patches to learn a regressor for predicting either 2017; Tran et al. 2017], ine-scale skin details [Cao et al. 2015], or both [Richardson et al. 2017; Sela et al. 2017]. Deep neural networks have been shown to be quite robust for inferring the coarse 3D facial shape and appearance of the inner face region, even when trained on synthetic data [Richardson et al. 2016]. Tewari et al. [2017] showed that encoderśdecoder architectures can be trained fully unsupervised on in-the-wild images by integrating physical image formation into the network. Richardson et al. [2017] trained an end-to-end regressor to recover facial geometry at a coarse and ine-scale level. Sela et al. [2017] use an encoderśdecoder network to infer a detailed depth image and a dense correspondence map, which serve as a basis for non-rigidly deforming a template mesh. Fig. 2. Deep video portraits enable a source actor to fully control a target video portrait. First, a low-dimensional parametric representation (let) of both videos is obtained using monocular face reconstruction. The head pose, expression and eye gaze can now be transferred in parameter space (middle). We do not focus on the modification of the identity and scene illumination (hatched background), since we are interested in reenactment. Finally, we render conditioning input images that are converted to a photo-realistic video portrait of the target actor (right). Obama video courtesy of the White House (public domain). Still, none of these methods creates a fully generative model for the entire head, hair, mouth interior, and eye gaze, like we do. Video-based Facial Reenactment. Facial reenactment methods rewrite the face content of a target actor in a video or image by transferring facial expressions from a source actor. Facial expressions are commonly transferred via dense motion ields [Averbuch-Elor et al. 2017; Liu et al. 2001; Suwajanakorn et al. 2015], parameters [Thies et al. 2016, 2018; Vlasic et al. 2005], or by warping candidate frames that are selected based on the facial motion [Dale et al. 2011], appearance metrics [Kemelmacher-Shlizerman et al. 2010] or both [Garrido et al. 2014; Li et al. 2014]. The methods described above irst reconstruct and track the source and target faces, which are represented as a set of sparse 2D landmarks or dense 3D models. Most approaches only modify the inner region of the face and thus are mainly intended for altering facial expressions, but they do not take full control of a video portrait in terms of rigid head pose, facial expression, and eye gaze. Recently, Wood et al. [2018] proposed an approach for eye gaze redirection based on a itted parametric eye model. Their approach only provides control over the eye region. One notable exception to pure facial reenactment is AverbuchElor et al.’s approach [2017], which enables the reenactment of a portrait image and allows for slight changes in head pose via image warping [Fried et al. 2016]. Since this approach is based on a single target image, it copies the mouth interior from the source to the target, thus preserving the target’s identity only partially. We take advantage of learning from a target video to allow for larger changes in head pose, facial reenactment, and joint control of the eye gaze. Visual Dubbing. Visual dubbing is a particular instance of face reenactment that aims to alter the mouth motion of the target actor to match a new audio track, commonly spoken in a foreign language by a dubbing actor. Here, we can ind speech-driven [Bregler et al. 1997; Chang and Ezzat 2005; Ezzat et al. 2002; Liu and Ostermann 2011; Suwajanakorn et al. 2017] or performance-driven [Garrido et al. 2015; Thies et al. 2016] techniques. Speech-driven dubbing techniques learn a person-speciic phoneme-to-viseme mapping from a training sequence of the actor. These methods produce accurate lip Deep Video Portraits • 163:3 sync with visually imperceptible artifacts, as recently demonstrated by Suwajanakorn et al. [2017]. However, they cannot directly control the target’s facial expressions. Performance-driven techniques overcome this limitation by transferring semantically-meaningful motion parameters and re-rendering the target model with photorealistic relectance [Thies et al. 2016], and ine-scale details [Garrido et al. 2015, 2016]. These approaches generalize better, but do not edit the head pose and still struggle to synthesize photo-realistic mouth deformations. In contrast, our approach learns to synthesize photo-realistic facial motion and actions from coarse renderings, thus enabling the synthesis of expressions and joint modiication of the head pose, with consistent body and background. Image-to-image Translation. Approaches using conditional GANs [Mirza and Osindero 2014], such as Isola et al.’s łpix2pixž [2017], have shown impressive results on image-to-image translation tasks which convert between images of two diferent domains, such as maps and satellite photos. These combine encoderśdecoder architectures [Hinton and Salakhutdinov 2006], often with skip-connections [Ronneberger et al. 2015], with adversarial loss functions [Goodfellow et al. 2014; Radford et al. 2016]. Chen and Koltun [2017] were the irst to demonstrate high-resolution results with 2 megapixel resolution, using cascaded reinement networks without adversarial training. The latest trends show that it is even possible to train highresolution GANs [Karras et al. 2018] and conditional GANs [Wang et al. 2018] at similar resolutions. However, the main challenge is the requirement for paired training data, as corresponding image pairs are often not available. This problem is tackled by CycleGAN [Zhu et al. 2017], DualGAN [Yi et al. 2017], and UNIT [Liu et al. 2017] ś multiple concurrent unsupervised image-to-image translation techniques that only require two sets of unpaired training samples. These techniques have captured the imagination of many people by translating between photographs and paintings, horses and zebras, face photos and depth as well as correspondence maps [Sela et al. 2017], and translation from face photos to cartoon drawings [Taigman et al. 2017]. Ganin et al. [2016] learn photo-realistic gaze manipulation in images. Olszewski et al. [2017] synthesize a realistic inner face texture, but cannot generate a fully controllable output video, including person-speciic hair. Lassner et al. [2017] propose a generative model to synthesize people in clothing, and Ma et al. [2017] generate new images of persons in arbitrary poses using image-to-image translation. In contrast, our approach enables the synthesis of temporally-coherent video portraits that follow the animation of a source actor in terms of head pose, facial expression and eye gaze.\n\nCONCLUSION\nWe presented a new approach to synthesize entire photo-real video portraits of a target actors in front of general static backgrounds. It is the irst to transfer head pose and orientation, face expression, and eye gaze from a source actor to a target actor. The proposed method is based on a novel rendering-to-video translation network that converts a sequence of simple computer graphics renderings into photo-realistic and temporally-coherent video. This mapping is learned based on a novel space-time conditioning volume formulation. We have shown through experiments and a user study that our method outperforms prior work in quality and expands over their possibilities. It thus opens up a new level of capabilities in many applications, like video reenactment for virtual reality and telepresence, interactive video editing, and visual dubbing. We see our approach as a step towards highly realistic synthesis of full-frame video content under control of meaningful parameters. We hope that it will inspire future research in this very challenging ield.\n\nACKNOWLEDGMENTS\nWe are grateful to all our actors. We thank True-VisionSolutions Pty Ltd for kindly providing the 2D face tracker and Adobe for a Premiere Pro CC license. We also thank Supasorn Suwajanakorn and Hadar Averbuch-Elor for the comparisons. This work was supported by ERC Starting Grant CapReal (335545), a TUM-IAS Rudolf Mößbauer Fellowship, a Google Faculty Award, RCUK grant CAMERA (EP/M023281/1), an NVIDIA Corporation GPU Grant, and the Max Planck Center for Visual Computing and Communications (MPC-VCC). A\n\nREFERENCES\nMartín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jefrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geofrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. 2015. TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems. https://www.tensorlow.org/ Software available from tensorlow.org. Oleg Alexander, Mike Rogers, William Lambeth, Jen-Yuan Chiang, Wan-Chun Ma, Chuan-Chang Wang, and Paul Debevec. 2010. The Digital Emily Project: Achieving a Photorealistic Digital Actor. IEEE Computer Graphics and Applications 30, 4 Hadar Averbuch-Elor, Daniel Cohen-Or, Johannes Kopf, and Michael F. Cohen. 2017. Bringing Portraits to Life. ACM Transactions on Graphics (SIGGRAPH Asia) 36, 6 Deep Video Portraits • 163:13 Volker Blanz, Kristina Scherbaum, Thomas Vetter, and Hans-Peter Seidel. 2004. Exchanging Faces in Images. Computer Graphics Forum (Eurographics) 23, 3 (September Volker Blanz and Thomas Vetter. 1999. A Morphable Model for the Synthesis of 3D Faces. In Annual Conference on Computer Graphics and Interactive Techniques (SIGGRAPH). James Booth, Anastasios Roussos, Allan Ponniah, David Dunaway, and Stefanos Zafeiriou. 2018. Large Scale 3D Morphable Models. International Journal of Computer Christoph Bregler, Michele Covell, and Malcolm Slaney. 1997. Video Rewrite: Driving Visual Speech with Audio. In Annual Conference on Computer Graphics and Interactive Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler. 2015. Real-time High-idelity Facial Performance Capture. ACM Transactions on Graphics (SIGGRAPH) 34, 4 (July Chen Cao, Qiming Hou, and Kun Zhou. 2014a. Displaced Dynamic Expression Regression for Real-time Facial Tracking and Animation. ACM Transactions on Graphics Chen Cao, Yanlin Weng, Shun Zhou, Yiying Tong, and Kun Zhou. 2014b. FaceWarehouse: A 3D Facial Expression Database for Visual Computing. IEEE Transactions on 10.1109/TVCG.2013.249 Chen Cao, Hongzhi Wu, Yanlin Weng, Tianjia Shao, and Kun Zhou. 2016. Real-time Facial Animation with Image-based Dynamic Avatars. ACM Transactions on Graphics Yao-Jen Chang and Tony Ezzat. 2005. Transferable Videorealistic Speech Animation. In Symposium on Computer Animation (SCA). 143ś151. 1073368.1073388 Qifeng Chen and Vladlen Koltun. 2017. Photographic Image Synthesis with Cascaded Reinement Networks. In International Conference on Computer Vision (ICCV). 1520ś Kevin Dale, Kalyan Sunkavalli, Micah K. Johnson, Daniel Vlasic, Wojciech Matusik, and Hanspeter Pister. 2011. Video face replacement. ACM Transactions on Graphics 2024164 Tony Ezzat, Gadi Geiger, and Tomaso Poggio. 2002. Trainable Videorealistic Speech Animation. ACM Transactions on Graphics (SIGGRAPH) 21, 3 (July 2002), 388ś398. Ohad Fried, Eli Shechtman, Dan B. Goldman, and Adam Finkelstein. 2016. Perspectiveaware Manipulation of Portrait Photos. ACM Transactions on Graphics (SIGGRAPH) Graham Fyfe, Andrew Jones, Oleg Alexander, Ryosuke Ichikari, and Paul Debevec.\n2014. Driving High-Resolution Facial Scans with Video Performance Capture. ACM\n2638549 Yaroslav Ganin, Daniil Kononenko, Diana Sungatullina, and Victor Lempitsky. 2016. DeepWarp: Photorealistic Image Resynthesis for Gaze Manipulation. In European Conference on Computer Vision (ECCV). 311ś326. 978-3-319-46475-6_20 Pablo Garrido, Levi Valgaerts, Ole Rehmsen, Thorsten Thormaehlen, Patrick Pérez, and Christian Theobalt. 2014. Automatic Face Reenactment. In Conference on Computer 2014.537 Pablo Garrido, Levi Valgaerts, Hamid Sarmadi, Ingmar Steiner, Kiran Varanasi, Patrick Pérez, and Christian Theobalt. 2015. VDub: Modifying Face Video of Actors for Plausible Visual Alignment to a Dubbed Audio Track. Computer Graphics Forum Pablo Garrido, Michael Zollhöfer, Dan Casas, Levi Valgaerts, Kiran Varanasi, Patrick Pérez, and Christian Theobalt. 2016. Reconstruction of Personalized 3D Face Rigs from Monocular Video. ACM Transactions on Graphics 35, 3 (June 2016), 28:1ś15. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative Adversarial Nets. In Advances in Neural Information Processing Systems. Geofrey E. Hinton and Ruslan Salakhutdinov. 2006. Reducing the Dimensionality of Data with Neural Networks. Science 313, 5786 (July 2006), 504ś507. https: Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jaewoo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen-Chun Chen, and Hao Li. 2017. Avatar Digitization from a Single Image for Real-time Rendering. ACM Transactions on Graphics (SIGGRAPH Alexandru Eugen Ichim, Soien Bouaziz, and Mark Pauly. 2015. Dynamic 3D Avatar Creation from Hand-held Video Input. ACM Transactions on Graphics (SIGGRAPH) Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. 2017. Image-to-Image Translation with Conditional Adversarial Networks. In Conference on Computer 163:14 •\nH. Kim, P. Garrido, A. Tewari, W. Xu, J. Thies, M. Nießner, P. Pérez, C. Richardt, M. Zollhöfer, and C. Theobalt\n2017.632 Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive Growing of GANs for Improved Quality, Stability, and Variation. In International Conference on Learning Representations (ICLR). Ira Kemelmacher-Shlizerman. 2013. Internet-Based Morphable Model. In International 2013.404 Ira Kemelmacher-Shlizerman, Aditya Sankar, Eli Shechtman, and Steven M. Seitz. 2010. Being John Malkovich. In European Conference on Computer Vision (ECCV). 341ś353. Ira Kemelmacher-Shlizerman, Eli Shechtman, Rahul Garg, and Steven M. Seitz. 2011. Exploring photobios. ACM Transactions on Graphics (SIGGRAPH) 30, 4 (August Diederik P. Kingma and Jimmy Ba. 2015. Adam: A Method for Stochastic Optimization. In International Conference on Learning Representations (ICLR). Christoph Lassner, Gerard Pons-Moll, and Peter V. Gehler. 2017. A Generative Model of People in Clothing. In International Conference on Computer Vision (ICCV). 853ś862. Hao Li, Laura Trutoiu, Kyle Olszewski, Lingyu Wei, Tristan Trutna, Pei-Lun Hsieh, Aaron Nicholls, and Chongyang Ma. 2015. Facial Performance Sensing Headmounted Display. ACM Transactions on Graphics (SIGGRAPH) 34, 4 (July 2015), Kai Li, Qionghai Dai, Ruiping Wang, Yebin Liu, Feng Xu, and Jue Wang. 2014. A DataDriven Approach for Facial Expression Retargeting in Video. IEEE Transactions 2293064 Kang Liu and Joern Ostermann. 2011. Realistic facial expression synthesis for an imagebased talking head. In International Conference on Multimedia and Expo (ICME). Ming-Yu Liu, Thomas Breuel, and Jan Kautz. 2017. Unsupervised Image-to-Image Translation Networks. In Advances in Neural Information Processing Systems. Zicheng Liu, Ying Shan, and Zhengyou Zhang. 2001. Expressive Expression Mapping with Ratio Images. In Annual Conference on Computer Graphics and Interactive Liqian Ma, Qianru Sun, Xu Jia, Bernt Schiele, Tinne Tuytelaars, and Luc Van Gool. 2017. Pose Guided Person Image Generation. In Advances in Neural Information Processing Systems. Mehdi Mirza and Simon Osindero. 2014. Conditional Generative Adversarial Nets. (2014). https://arxiv.org/abs/1411.1784 arXiv:1411.1784. Kyle Olszewski, Zimo Li, Chao Yang, Yi Zhou, Ronald Yu, Zeng Huang, Sitao Xiang, Shunsuke Saito, Pushmeet Kohli, and Hao Li. 2017. Realistic Dynamic Facial Textures from a Single Image using GANs. In International Conference on Computer Vision Kyle Olszewski, Joseph J. Lim, Shunsuke Saito, and Hao Li. 2016. High-idelity Facial and Speech Animation for VR HMDs. ACM Transactions on Graphics (SIGGRAPH Alec Radford, Luke Metz, and Soumith Chintala. 2016. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks. In International Conference on Learning Representations (ICLR). Ravi Ramamoorthi and Pat Hanrahan. 2001. An eicient representation for irradiance environment maps. In Annual Conference on Computer Graphics and Interactive Elad Richardson, Matan Sela, and Ron Kimmel. 2016. 3D Face Reconstruction by Learning from Synthetic Data. In International Conference on 3D Vision (3DV). 460ś Elad Richardson, Matan Sela, Roy Or-El, and Ron Kimmel. 2017. Learning Detailed Face Reconstruction from a Single Image. In Conference on Computer Vision and Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI). 234ś241. https: Joseph Roth, Yiying Tong Tong, and Xiaoming Liu. 2017. Adaptive 3D Face Reconstruction from Unconstrained Photo Collections. IEEE Transactions on Pattern Analysis and Machine Intelligence 39, 11 (November 2017), 2127ś2141. https: Jason M. Saragih, Simon Lucey, and Jefrey F. Cohn. 2011. Real-time avatar animation from a single image. In International Conference on Automatic Face and Gesture Matan Sela, Elad Richardson, and Ron Kimmel. 2017. Unrestricted Facial Geometry Reconstruction Using Image-to-Image Translation. In International Conference on Fuhao Shi, Hsiang-Tao Wu, Xin Tong, and Jinxiang Chai. 2014. Automatic Acquisition of High-idelity Facial Performances Using Monocular Videos. ACM Transactions 1145/2661229.2661290 Robert W. Sumner and Jovan Popović. 2004. Deformation Transfer for Triangle Meshes. ACM Transactions on Graphics (SIGGRAPH) 23, 3 (August 2004), 399ś405. https: Supasorn Suwajanakorn, Ira Kemelmacher-Shlizerman, and Steven M. Seitz. 2014. Total Moving Face Reconstruction. In European Conference on Computer Vision (ECCV) (Lecture Notes in Computer Science), Vol. 8692. 796ś812. 978-3-319-10593-2_52 Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. 2015. What Makes Tom Hanks Look Like Tom Hanks. In International Conference on Computer Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. 2017. Synthesizing Obama: Learning Lip Sync from Audio. ACM Transactions on Graphics Yaniv Taigman, Adam Polyak, and Lior Wolf. 2017. Unsupervised Cross-Domain Image Generation. In International Conference on Learning Representations (ICLR). Ayush Tewari, Michael Zollhöfer, Hyeongwoo Kim, Pablo Garrido, Florian Bernard, Patrick Pérez, and Christian Theobalt. 2017. MoFA: Model-based Deep Convolutional Face Autoencoder for Unsupervised Monocular Reconstruction. In International 2017.401 Justus Thies, Michael Zollhöfer, Matthias Nießner, Levi Valgaerts, Marc Stamminger, and Christian Theobalt. 2015. Real-time Expression Transfer for Facial Reenactment. ACM Transactions on Graphics (SIGGRAPH Asia) 34, 6 (November 2015), 183:1ś14. Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. 2016. Face2Face: Real-Time Face Capture and Reenactment of RGB Videos. In Conference on Computer Vision and Pattern Recognition (CVPR). 2387ś2395. https: Justus Thies, Michael Zollhöfer, Marc Stamminger, Christian Theobalt, and Matthias Nießner. 2018. FaceVR: Real-Time Facial Reenactment and Eye Gaze Control in Virtual Reality. ACM Transactions on Graphics (2018). Anh Tuan Tran, Tal Hassner, Iacopo Masi, and Gerard Medioni. 2017. Regressing Robust and Discriminative 3D Morphable Models with a very Deep Neural Network. In Conference on Computer Vision and Pattern Recognition (CVPR). 1493ś1502. https: Daniel Vlasic, Matthew Brand, Hanspeter Pister, and Jovan Popović. 2005. Face Transfer with Multilinear Models. ACM Transactions on Graphics (SIGGRAPH) 24, 3 (July Chao Wang, Haiyong Zheng, Zhibin Yu, Ziqiang Zheng, Zhaorui Gu, and Bing Zheng.\n2017. Discriminative Region Proposal Adversarial Networks for High-Quality Imageto-Image Translation. (2017). https://arxiv.org/abs/1711.09554 arXiv:1711.09554.\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. 2018. High-Resolution Image Synthesis and Semantic Manipulation with Conditional GANs. In Conference on Computer Vision and Pattern Recognition (CVPR). Thibaut Weise, Soien Bouaziz, Hao Li, and Mark Pauly. 2011. Realtime Performancebased Facial Animation. ACM Transactions on Graphics (SIGGRAPH) 30, 4 (July Erroll Wood, Tadas Baltrušaitis, Louis-Philippe Morency, Peter Robinson, and Andreas Bulling. 2018. GazeDirector: Fully articulated eye gaze redirection in video. Computer Chenglei Wu, Derek Bradley, Markus Gross, and Thabo Beeler. 2016. An AnatomicallyConstrained Local Deformation Model for Monocular Face Capture. ACM Transac- 2897824.2925882 Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. 2017. DualGAN: Unsupervised Dual Learning for Image-to-Image Translation. In International Conference on Computer Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. 2017. Unpaired Image-toImage Translation using Cycle-Consistent Adversarial Networks. In International 2017.244 Michael Zollhöfer, Justus Thies, Pablo Garrido, Derek Bradley, Thabo Beeler, Patrick Pérez, Marc Stamminger, Matthias Nießner, and Christian Theobalt. 2018. State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications. Computer Received January 2018; revised April 2018; inal version May 2018; accepted May 2018\n--- FOOTNOTES HYEONGWOO KIM, Max Planck Institute for Informatics, Germany AYUSH TEWARI and WEIPENG XU, Max Planck Institute for Informatics, Germany CHRISTIAN THEOBALT, Max Planck Institute for Informatics, Germany Authors’ addresses: Hyeongwoo Kim, Max Planck Institute for Informatics, Campus Xu, wxu@mpi-inf.mpg.de, Max Planck Institute for Informatics, Campus E1.4, Saarbrücken, 66123, Germany; Justus Thies, justus.thies@tum.de; Matthias Nießner, America, zollhoefer@cs.stanford.edu; Christian Theobalt, Max Planck Institute for This is the author’s version of the work. It is posted here for your personal use. Not for 3D head pose, including a consistent upper body and a consistently 3D scans. Learning-based approaches leverage a large corpus of 3D face shape and appearance [Richardson et al. 2016; Tewari et al.\n\nAPPENDIX\nThis appendix describes all the used datasets, see Table 4 (target actors) and Table 5 (source actors). Table 4. Target videos: Name and length of sequences (in frames). Malou video courtesy of Louisa Malou (CC BY). May video courtesy of the UK government (Open Government Licence). Obama video courtesy of the White House (public domain). Putin video courtesy of the Kremlin (CC BY). Reagan video courtesy of the National Archives and Records Administration (public domain). Elizabeth II video courtesy of the Governor General of Canada (public domain). Reagan video courtesy of the National Archives and Records Administration (public domain). Wolf video courtesy of Tom Wolf (CC BY). Ingmar Malou May Obama1 Obama2 3,000 15,000 5,000 2,000 3,613 Putin Elizabeth II Reagan Thomas Wolf 4,000 1,500 6,984 2,239 15,000 DB1 DB2 DB3 DB4 8,000 18,138 6,500 30,024 Table 5. Source videos: Name and length of sequences (in frames). Obama video courtesy of the White House (public domain). Obama3 David1 David2 DB5 DB6 1,945 4,611 3,323 3,824 2,380", "persona": "Student", "response": "### 1. Overview\nSynthesizing and editing video portraits—i.e., videos framed to show a person’s head and upper body—is an important problem in computer graphics, with applications in video editing and movie postproduction, visual effects, visual dubbing, virtual reality, and telepresence, among others. The problem of synthesizing a photo-realistic video portrait of a target actor that mimics the actions of a source actor—and especially where the source and target actors can be different subjects—is still an open problem. There hasn’t been an approach that enables one to take full control of the rigid head pose, face expressions, and eye motion of the target actor; even face identity can be modified to some extent. Until now. The core method presented in the url provides full control over the head of a target actor by transferring the rigid head pose, facial expressions, and eye motion of a source actor, while preserving the target’s identity and appearance. On top of that, full video of the target is synthesized, including consistent upper body posture, hair, and background. This approach enables us to take full control of the rigid head pose, facial expression, and eye motion of a target actor in a video portrait, thus opening up a wide range of video rewrite applications. This approach is the first that can photo-realistically transfer the full 3D head pose (spatial position and rotation), facial expression, as well as eye gaze and eye burling of a captured source actor to a target actor video. 3.2 Facial Reenactment and Video Dubbing Video dubbing could also be applied by modifying the facial motion of actors speaking originally in another language to an ensign translation, spoken by a professional dubbing actor in a dubbing studio. More precisely, the captured facial expressions of the dubbing actor could be transferred to the target actor, while leaving the original target gaze and eye burls intact. In contrast to existing approaches that are restricted to manipulations of facial expressions only, the authors are the first to transfer the full 3D head position, head rotation, face expression, eye gaze, and eye burling from a source actor to a portrait video of a target actor. In this post, I’m going to review “Deep Video Portraits”, which presents a novel approach that enables photo-realistic re-animation of portrait videos using only an input video.\n\n### 2. Background\nFirst, the source and target actors are being tracked using a state-of-the-art face reconstruction approach from a single image, and a 3D morphable model (3DMM) is derived to best fit the source and target actors. 2.2 Face Reconstruction from a single image 3D morphable models are used for face analysis because the intrinsic properties of 3D faces provide a representation that’s immune to intra-personal variations, such as pose and illumination. Given a single facial input image, a 3DMM can recover 3D face (shape and texture) and scene properties (pose and illumination) via a fitting process. Below are more details on the parametric face representation and the fitting process. Where a_{geo} ∈ R^(3N) stores the average facial geometry. The geometry bases b_k for the geometry has been computed by applying principal component analysis (PCA) to 200 high-quality face scans, and b_k for the expressions has been obtained in the same manner on blendshapes. Till then, see you in the next post! 😄\n\n### 3. Methods\nIn this post, I’ll cover two things: First, a short definition of a DeepFake. Second, an overview of the url “Deep Video Portraits” in the words of the authors. 1. Defining DeepFakes The resulting sequence of low-dimensional parameter vectors represents the actor’s identity, head pose, expression, eye gaze, and the scene lighting for every video frame. Then, the head pose, expressions and/or eye gaze parameters from the source are taken and mixed with the illumination and identity parameters of the target. This allows the network to generate a full-head reenactment while preserving the actor’s identity and look. The authors employ a state-of-the-art dense face reconstruction approach that fits a parametric model of the face and illumination to each video frame. It obtains a meaningful parametric face representation for both the source and the target, given an input video sequence. The meaningful parametric face representation consists of a set of parameters P.\n\n### 4. Results\n2. Deep Video Portraits , which could be denoted as the corresponding parameter sequence that fully describes the source or target facial performance. The set of reconstructed parameters P encode the rigid head pose, facial identity coefficients, expressions coefficients, gaze direction for both eyes, and spherical harmonics illumination coefficients. Overall, the face reconstruction process estimates 261 parameters per video frame.\n\n### 5. Takeaways\nNext, new synthetic renderings of the target actor are generated based on the mixed parameters. These renderings are the input to the url’s novel “rendering-to-video translation network”, which is trained to convert the synthetic input into photo-realistic output. 2.3 Synthetic Conditioning Input All of the images are stacked to obtain the input to the rendering-to-video translation network. 2.4 Rendering-To-Video Translation The generated conditioning space-time stacked images are the input to the rendering-to-video translation network. It also synthesizes the appropriately modified and filled-in background, even including some consistent lighting effects between the foreground and background. As can be seen, the output of their approach achieves a high level of realism and faithfully mimics the driving sequence, while still retaining the mannerisms of the original target actor. Besides full-head reenactment, the approach also enables facial reenactment. In this experiment, the authors replaced the expression coefficients of the target actor with those of the source actor before synthesizing the conditioning input to the rendering-to-video translation network. The authors have shown, through experiments and a user study, that their method outperforms prior work, both in terms of model performance and expanded capabilities. This opens doors to many applications, like video reenactment for virtual reality and telepresence, interactive video editing, and visual dubbing. For the enthusiastic reader: The training objective function comprises a conditioned adversarial loss and L1 photometric loss.", "paper_id": "14", "story_id": "14-01"}
{"paper": "Jukebox: A Generative Model for Music Prafulla Dhariwal * 1 Heewoo Jun * 1 Christine Payne * 1 Jong Wook Kim 1 Alec Radford 1 Ilya Sutskever 1\n\nAbstract\nWe introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multiscale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-ﬁdelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples, along with model weights and code. arXiv:2005.00341v1  [eess.AS]  30 Apr 2020\n\n1. Introduction\nMusic is an integral part of human culture, existing from the earliest periods of human civilization and evolving into a wide diversity of forms. It evokes a unique human spirit in its creation, and the question of whether computers can ever capture this creative process has fascinated computer scientists for decades. We have had algorithms generating piano sheet music (Hiller Jr & Isaacson, 1957; Moorer, 1972; Hadjeres et al., 2017; Huang et al., 2017), digital vocoders generating a singer’s voice (Bonada & Serra, 2007; Saino et al., 2006; Blaauw & Bonada, 2017) and also synthesizers producing timbres for various musical instruments (Engel et al., 2017; 2019). Each captures a speciﬁc aspect of music generation: melody, composition, timbre, and the human voice singing. However, a single system to do it all remains elusive. The ﬁeld of generative models has made tremendous progress in the last few years. One of the aims of generative modeling is to capture the salient aspects of the data and to generate new instances indistinguishable from the true data The hypothesis is that by learning to produce the data we can learn the best features of the data1. We are surrounded by highly complex distributions in the visual, audio, and text domain, and in recent years we have devel- oped advances in text generation (Radford et al.), speech generation (Xie et al., 2017) and image generation (Brock et al., 2019; Razavi et al., 2019). The rate of progress in this ﬁeld has been rapid, where only a few years ago we had algorithms producing blurry faces (Kingma & Welling, 2014; Goodfellow et al., 2014) but now we now can generate high-resolution faces indistinguishable from real ones (Zhang et al., 2019b). Generative models have been applied to the music generation task too. Earlier models generated music symbolically in the form of a pianoroll, which speciﬁes the timing, pitch, velocity, and instrument of each note to be played. (Yang et al., 2017; Dong et al., 2018; Huang et al., 2019a; Payne, 2019; Roberts et al., 2018; Wu et al., 2019). The symbolic approach makes the modeling problem easier by working on the problem in the lower-dimensional space. However, it constrains the music that can be generated to being a speciﬁc sequence of notes and a ﬁxed set of instruments to render with. In parallel, researchers have been pursuing the nonsymbolic approach, where they try to produce music directly as a piece of audio. This makes the problem more challenging, as the space of raw audio is extremely high dimensional with a high amount of information content to model. There has been some success, with models producing piano pieces either in the raw audio domain (Oord et al., 2016; Mehri et al., 2017; Yamamoto et al., 2020) or in the spectrogram domain (Vasquez & Lewis, 2019). The key bottleneck is that modeling the raw audio directly introduces extremely long-range dependencies, making it computationally challenging to learn the high-level semantics of music. A way to reduce the difﬁculty is to learn a lower-dimensional encoding of the audio with the goal of losing the less important information but retaining most of the musical information. This approach has demonstrated some success in generating short instrumental pieces restricted to a set of a few instruments (Oord et al., 2017; Dieleman et al., 2018). In this work, we show that we can use state-of-the-art deep generative models to produce a single system capable of generating diverse high-ﬁdelity music in the raw audio domain, with long-range coherence spanning multiple minutes. Our approach uses a hierarchical VQ-VAE architecture (Razavi et al., 2019) to compress audio into a discrete space, with a loss function designed to retain the maximum amount of musical information, while doing so at increasing levels of compression. We use an autoregressive Sparse Transformer (Child et al., 2019; Vaswani et al., 2017) trained with maximum-likelihood estimation over this compressed space, and also train autoregressive upsamplers to recreate the lost information at each level of compression. We show that our models can produce songs from highly diverse genres of music like rock, hip-hop, and jazz. They can capture melody, rhythm, long-range composition, and timbres for a wide variety of instruments, as well as the styles and voices of singers to be produced with the music. We can also generate novel completions of existing songs. Our approach allows the option to inﬂuence the generation process: by swapping the top prior with a conditional prior, we can condition on lyrics to tell the singer what to sing, or on midi to control the composition. We release our model weights and training and sampling code at https://github.com/openai/jukebox.\n\n2. Background\nWe consider music in the raw audio domain represented as a continuous waveform x ∈[−1, 1]T , where the number of samples T is the product of the audio duration and the sampling rate typically ranging from 16 kHz to 48 kHz. For music, CD quality audio, 44.1 kHz samples stored in 16 bit precision, is typically enough to capture the range of frequencies perceptible to humans. As an example, a fourminute-long audio segment will have an input length of ∼10 million, where each position can have 16 bits of information. In comparison, a high-resolution RGB image with 1024 × 1024 pixels has an input length of ∼3 million, and each position has 24 bits of information. This makes learning a generative model for music extremely computationally demanding with increasingly longer durations; we have to capture a wide range of musical structures from timbre to global coherence while simultaneously modeling a large amount of diversity. 2.1. VQ-VAE To make this task feasible, we use the VQ-VAE (Oord et al., 2017; Dieleman et al., 2018; Razavi et al., 2019) to compress raw audio to a lower-dimensional space. A one-dimensional VQ-VAE learns to encode an input sequence x = ⟨xt⟩T t=1 using a sequence of discrete tokens z = ⟨zs ∈[K]⟩S s=1, where K denotes the vocabulary size and we call the ratio T/S the hop length. It consists of an encoder E(x) which encodes x into a sequence of latent vectors h = ⟨hs⟩S s=1, a bottleneck that quantizes hs 7→ezs by mapping each hs to its nearest vector ezs from a codebook C = {ek}K k=1, and a decoder D(e) that decodes the embedding vectors back to the input space. It is thus an auto-encoder with a discretization bottleneck. The VQ-VAE is trained using the following objective: L = Lrecons + Lcodebook + βLcommit (1) t ∥xt −D(ezt)∥2 2 (2) Lrecons = 1 T P s ∥sg [hs] −ezs∥2 2 (3) Lcodebook = 1 S P s ∥hs −sg [ezs]∥2 2 (4) Lcommit = 1 S P where sg denotes the stop-gradient operation, which passes zero gradient during backpropagation. The reconstruction loss Lrecons penalizes for the distance between the input x and the reconstructed output bx = D(ez), and Lcodebook penalizes the codebook for the distance between the encodings h and their nearest neighbors ez from the codebook. To stabilize the encoder, we also add Lcommit to prevent the encodings from ﬂuctuating too much, where the weight β controls the amount of contribution of this loss. To speed up training, the codebook loss Lcodebook instead uses EMA updates over the codebook variables. Razavi et al. (2019) extends this to a hierarchical model where they train a single encoder and decoder but break up the latent sequence h into a multi-level representation [h(1), · · · , h(L)] with decreasing sequence lengths, each learning its own codebook C(l). They use non-autoregressive encoder-decoders and jointly train all levels with a simple mean-squared loss.\n\n3. Music VQ-VAE\nInspired by the results from the hierarchical VQ-VAE model (Razavi et al., 2019) for images, we consider applying the same technique to model raw audio using three different levels of abstraction, as illustrated in Figure 1. At each level, we use residual networks consisting of WaveNet-style noncausal 1-D dilated convolutions, interleaved with downsampling and upsampling 1-D convolutions to match different hop lengths. A detailed description of the architecture is provided in Appendix B.1. We make a number of modiﬁcations to our VQ-VAE compared to the ones in (Oord et al., 2017; Razavi et al., 2019), as described in the following subsections. 3.1. Random restarts for embeddings VQ-VAEs are known to suffer from codebook collapse, wherein all encodings get mapped to a single or few embedding vectors while the other embedding vectors in the codebook are not used, reducing the information capacity of the bottleneck. To prevent this, we use random restarts: when the mean usage of a codebook vector falls below a threshold, we randomly reset it to one of the encoder outputs from the current batch. This ensures all vectors in the Codebook ek Vector Quantization Encode Vector Quantization Encode Vector Quantization Encode ht = E(xt) xt zt = argmink ǁ ht – ek ǁ Figure 1. We ﬁrst train three separate VQ-VAE models with different temporal resolutions. At each level, the input audio is segmented and encoded into latent vectors ht, which are then quantized to the closest codebook vectors ezt. The code zt is a discrete representation of the audio that we later train our prior on. The decoder takes the sequence of codebook vectors and reconstructs the audio. The top level learns the highest degree of abstraction, since it is encoding longer audio per token while keeping the codebook size the same. Audio can be reconstructed using the codes at any one of the abstraction levels, where the least abstract bottom-level codes result in the highest-quality audio, as shown in Figure 4. For the detailed structure of each component, see Figure 7. codebook are being used and thus have a gradient to learn from, mitigating codebook collapse. 3.2. Separated Autoencoders When using the hierarchical VQ-VAE from (Razavi et al., 2019) for raw audio, we observed that the bottlenecked top level is utilized very little and sometimes experiences a complete collapse, as the model decides to pass all information through the less bottlenecked lower levels. To maximize the amount of information stored at each level, we simply train separate autoencoders with varying hop lengths. Discrete codes from each level can be treated as independent encodings of the input at different levels of compression. 3.3. Spectral Loss When using only the sample-level reconstruction loss, the model learns to reconstruct low frequencies only. To capture mid-to-high frequencies, we add a spectral loss which is deﬁned as Lspec = ∥|STFT(x)| −|STFT(bx)|∥2 It encourages the model to match the spectral components without paying attention to phase which is more difﬁcult to learn. This is similar to the use of power loss (Oord et al., 2018) and spectral convergence (Arık et al., 2018b) when training parallel decoders for raw audio. One difference between the latter approach and ours is that we are no longer optimizing the spectral signal-to-noise ratio; dividing by the magnitude of the signal results in numerical instability for mostly silent inputs. To prevent the model from overﬁtting to a particular choice of the STFT parameters, Codebook Lookup Decode Codebook Lookup Decode Codebook Lookup Decode ezt x̂ t = D(ezt) we use the sum of the spectral losses Lspec calculated over multiple STFT parameters that trade-off time and frequency resolutions (Yamamoto et al., 2020).\n\n4. Music Priors and Upsamplers\nAfter training the VQ-VAE, we need to learn a prior p(z) over the compressed space to generate samples. We break up the prior model as p(z) = p(ztop, zmiddle, zbottom) (5) = p(ztop)p(zmiddle|ztop)p(zbottom|zmiddle, ztop) (6) and train separate models for the top-level prior p(ztop), and upsamplers p(zmiddle|ztop) and p(zbottom|zmiddle, ztop). Each of these is an autoregressive modeling problem in the discrete token space produced by the VQ-VAE. We use Transformers with sparse attention (Vaswani et al., 2017; Child et al., 2019) as they are currently the SOTA in autoregressive modeling. We propose a simpliﬁed version which we call the Scalable Transformer, that is easier to implement and scale (see Appendix A for details). For the upsamplers, we need to provide the autoregressive Transformers with conditioning information from the codes of the upper levels. To do so, we use a deep residual WaveNet (Xie et al., 2017) followed by an upsampling strided convolution and a layer norm (Ba et al., 2016), and add the output as extra positional information to the embeddings of the current level. We condition the lower levels only on the chunk of upper level codes that correspond to the same segment of raw audio. At each level, we use Transformers over the same context length of discrete codes, which correspond to increasing the raw audio length with larger hop lengths, and modeling longer temporal dependencies at the higher levels while keeping the same computational footprint for training each level. As our VQ-VAE is convolutional, we can use the same VQ-VAE to produce codes for arbitrary lengths of audio. 4.1. Artist, Genre, and Timing Conditioning Our generative model can be made more controllable by providing additional conditioning signals while training. For our ﬁrst models, we provide artist and genre labels for the songs. This has two advantages: ﬁrst, it reduces the entropy of the audio prediction, so the model is able to achieve better quality in any particular style. Second, at generation time, we are able to steer the model to generate in a style of our choosing. Additionally, we attach a timing signal for each segment at training time. This signal includes the total duration of the piece, the start time of that particular sample and how much fraction of the song that has elapsed. This allows the model to learn audio patterns that depend on the overall structure, such as spoken or instrumental introductions and applause at the end of a piece. 4.2. Lyrics Conditioning While the conditioned models above are able to generate songs of diverse genres and artistic styles, singing voices generated by those models, while often sung in a compelling melody, are mostly composed of babbling, rarely producing recognizable English words. In order to be able to control the generative model with lyrics, we provide more context at training time by conditioning the model on the lyrics corresponding to each audio segment, allowing the model to produce singing simultaneosly with the music. Lyrics-to-singing (LTS) task: The conditioning signal only includes the text of the lyrics, without timing or vocalisation information. We thus have to model the temporal alignment of lyrics and singing, the artists voice and also the diversity of ways one can sing a phrase depending on the pitch, melody, rhythm and even genre of the song. The conditioning data isn’t precise as the lyrics data often contains textual references to repeated sections like “chorus” or mismatching portions of lyrics with the corresponding music. There is also no separation between lead vocals, accompanying vocals and the background music in target audio. This makes the Lyrics-to-singing (LTS) task signiﬁcantly more challenging than the corresponding Text-to-speech (TTS) task. Conditioning Information Top-Level Prior Middle Upsampler Bottom Upsampler VQ-VAE Decoder (a) Ancestral sampling: Priors for the VQ-VAE codes are trained using a cascade of Transformer models, shown in blue. Each model takes conditioning information such as genre, artist, timing, and lyrics, and the upsampler models are also conditioned on the codes from the upper levels. To generate music, the VQ-VAE codes are sampled from top to bottom using the conditioning information for control, after which the VQ-VAE decoder can convert the bottom-level codes to audio. time new samples (b) Windowed sampling: To generate music longer than the model’s context length (12 in this ﬁgure), we repeatedly sample continuations at each level, using overlapping windows of previous codes as the context. The overlap amount is a hyperparameter, and the ﬁgure shows an example of 75% overlap with hop length 3. Generate Decode Encode Primed Audio Generated Audio (c) Primed sampling: The model can generate continuations of an existing audio signal by converting it into the VQ-VAE codes and sampling the subsequent codes in each level. responding to the audio during training, we began with a simple heuristics of aligning the characters of the lyrics to linearly span the duration of each song, and pass a ﬁxed-side window of characters centered around the current segment during training. While this simple strategy of linear alignment worked surprisingly well, we found that it fails for certain genres such as hip-hop with fast lyrics. To address this, we use Spleeter (Hennequin et al., 2019) to extract vocals from each song and run NUS AutoLyricsAlign (Gupta et al., 2020) on the extracted vocals to obtain a word-level alignments of the lyrics, allowing us to more accurately provide the lyrics for a given chunk of audio. We choose a large enough window so that the actual lyrics have a high probability of being inside the window. Encoder-decoder model: We use an encoder-decoder style model to condition on the characters of the lyrics, with the encoder producing features from the lyrics which are attended to by the decoder which produces the top level music tokens. The lyrics encoder is a Transformer with an autoregressive modeling loss for lyrics, and its last level is used as features of the lyrics. In the music decoder, we interleave a few additional layers with encoder-decoder attention where the queries from the music tokens are only allowed to attend to keys and values from the lyrics tokens. These layers attend on the activation from the last layer of the lyrics encoder (see Figure 8c). In Figure 3, we see that the attention pattern learned by one of these layers corresponds to the alignment between the lyrics and the singing. 4.3. Decoder Pretraining To reduce computation required to train the lyrics conditional model, we use a pretrained unconditional top-level prior as our decoder and introduce the lyrics encoder using model surgery (Berner et al., 2019). We initialize the output projection weights in the MLP and the attention layers of these residual blocks to zeros (Zhang et al., 2019a), so that the added layers perform the identity function at initialization. Thus, at initialization the model behaves identically as the pretrained decoder, but there is still a gradient with respect to the encoder state and parameters2, allowing the model to learn to use the encoder. 4.4. Sampling After we have trained our VQ-VAE, upsamplers, and top level priors, we can then use them to sample novel songs. Ancestral sampling: We ﬁrst generate the top level codes one token at a time by the usual ancestral sampling process (see Figure 2a): generating the ﬁrst token, then passing all 1.0 500 0.8 400 Lyrics token position 0.6 300 0.4 200 0.2 100 0 0.0 0 1600 3200 4800 6400 8000 Music token position Figure 3. Lyrics-singing alignment learned by one of the encoderdecoder attention layers. The x-axis is the position of music queries, and the y-axis is the position of lyric keys. The positions attended to by the decoder correspond to the characters being sung. previously generated tokens into the model as inputs and outputting the next token conditioned on all previous tokens. We then run our conditioning wavenet on the top level codes to produce the conditioning information for the middle level and sample ancestrally from it too, and do the same for the bottom level. Windowed sampling: To sample segments longer than the context length, we use windowed sampling, where we move ahead our sampling window by half our context and continue sampling conditioned on this half context (See Figure 2b). We can trade off speed for quality by using a smaller hop length here. Primed sampling: Instead of sampling the entire token sequence from the model, we can also run a forward pass of the VQ-VAE to obtain the top, middle, and bottom level codes corresponding to a segment from an actual song, as shown in Figure 2c. We can use these as the initial tokens in our ancestral sampling process and continue sampling from these to produce novel completions of the song.\n\n5. Experiments\n5.1. Dataset We scraped a new dataset of 1.2 million songs (600k of which in English), paired with the lyrics and metadata from LyricWiki (LyricWiki). The metadata includes artist, album, genre, and year of the release, along with common moods or playlist keywords associated with each song. We train on 32 bit, 44.1 kHz raw audio and perform data augmentation by randomly downmixing the right and left channels to produce mono channel audio. 128x respectively, with a codebook size of 2048 for each level. The VQ-VAE has 2 million parameters and is trained on 9-second audio clips on 256 V100 for 3 days. We used exponential moving average to update the codebook following Razavi et al. (2019). For our prior and upsampler models, we use a context of 8192 tokens of VQ-VAE codes, which corresponds to approximately 24, 6, and 1.5 seconds of raw audio at the top, middle, and bottom level, respectively. The upsamplers have one billion parameters and are trained on 128 V100s for 2 weeks, and the top-level prior has 5 billion parameters and is trained on 512 V100s for 4 weeks. We use Adam with learning rate 0.00015 and weight decay of 0.002. For lyrics conditioning, we reuse the prior and add a small encoder, after which we train the model on our models and training are provided in Appendix B.3. 5.3. Samples We trained a sequence of models with increasing sample quality. Our ﬁrst model was trained on the MAESTRO dataset using 22 kHz VQ-VAE codes and relatively small prior models. We observed that this could generate high ﬁdelity classical music samples with piano and occasional violin. We then collected a larger and more diverse dataset of songs with genre and artist labels. The same model when trained on this new dataset was able to produce diverse samples other than classical music, and demonstrated musicality and coherence over more than a minute. Despite the novelty of being able to generate generally high ﬁdelity and coherent songs, sample quality was still limited by a number of factors. First, the use of 22 kHz sampling rate along with small upsamplers introduced noise both in the upsampling and decoding steps, which we hear as grainy texture. We improved ﬁdelity by using 44 kHz VQ-VAE and 1B parameter upsamplers in all subsequent experiments at the expense of longer rendering time. Second, the 1B top-level prior was not big enough to produce singing and diverse musical timbres. We ﬁrst explored increasing the model size to 5 billion parameters. Larger capacity allowed better modeling of the broader distribution of songs, resulting in samples with better musicality, longer coherence and initial singing. While there is an overall qualitative improvement, the unconditional model still struggled to sing recognizable words. Training a seq2seq model with lyric conditioning and limiting the dataset only to songs primarily in English made singing both intelligible and controllable. The ﬁnal model, which we call Jukebox, uses all these improvements. Because everyone experiences music differently, it is generally tricky and not very meaningful to evaluate samples by the mean opinion score or FID-like metrics. We manually evaluate coherence, musicality, diver- sity, and novelty of generated samples. The links to curated examples are embedded in text. Coherence: We ﬁnd the samples stay very coherent musically through the context length of the top-level prior (approximately 24 seconds), and they maintain similar harmonies and textures as we slide the window to generate longer samples. However, because the top-level does not have the context of the entire song, we do not hear long term musical patterns, and we would never hear choruses or melodies that repeat. The generations progress through beginnings of songs (for example applause or slow instrumental warm-ups), through sections that sound chorus-like, through instrumental interludes, and then fading or otherwise wrapping up at the end. The top-level prior always knows what fraction of the song is complete time-wise, so it is able to imitate appropriate beginnings, middles and ends. Musicality: The samples frequently imitate familiar musical harmonies and the lyrics are usually set in ways that are very natural. Frequently the highest or longest notes of the melody match words that a human singer would choose to emphasize, and the lyrics are almost always rendered in ways that capture the prosody of the phrases. This is noticeable in hip hop generations, where the model reliably captures the rhythm of spoken text. We do ﬁnd that the generated melodies are usually less interesting than human composed melodies. In particular, we do not hear the antecedent and consequent pattern familiar to many human melodies, and we rarely hear choruses that are melodically memorable. Diversity: Likelihood training encourages covering of all modes, so we expect the model to produce diverse samples. – Re-renditions: We generate multiple samples conditioned on artist and lyrics combinations that exist in our training data. While occasionally drum and bass lines or melodic intervals echo the original versions, we ﬁnd that none of the generated samples is noticeably similar to the original songs. We also generate multiple songs conditioned on the same artist and lyrics as Sample 1 to obtain Samples 9–12. All ﬁve sound interesting in their own ways with different moods and melodies with Sample 10 playing a harmonic at 00:14 as part of a blues riff, showing that the model has learned a wide range of singing and playing styles. – Completions: We prime the model with 12 seconds of existing songs and ask it to complete them in the same styles. When the priming samples include singing, the continuations are more likely to imitate the original tunes and rhythms. Songs primed with more generic or common intros tend to be more diverse. Even generated samples that are close to the originals early on deviate completely into new musical material after about 30 seconds. Re-renditions and completions are interesting and diverse, but overall, there is still room for improvement in music quality compared to the original songs. – Full tree: To understand diversity in a more systematic way, we generate multiple continuations from the same segment. We start with a one-minute sample and independently sample four times per one-minute extension. By the three minute mark, there are 16 completions. We can think of this branching tree as exploring different possibilities obtained by ancestral sampling. In the generated songs in the link, we hear diversity in singing and development even when the same initial segment is used. We note that this particular sample follows the lyrics more successfully than many. For certain genres like hip hop and rap, where linearly moving the window does not yield good lyrics alignment, the chance of obtaining plausible singing is lower. Novelty: With the ability to condition on various styles, lyrics, and raw audio, we would like Jukebox to be a useful tool for both professional musicians and music enthusiasts alike. In this section, we are interested in exploring capabilities and applications of Jukebox. – Novel styles: We generate songs in an unusual genre typically not associated with an artist. In general, we ﬁnd that it is fairly difﬁcult to generalize to a novel style of singing while using the same voice as the artist embedding overpowers other information. In Joe Bonamassa and Frank Sinatra samples, we hear a modest variation in instrumentation, energy, and ambience depending on the genre embedding. However, our attempts to mix country singer Alan Jackson with unusual genres like hip hop and punk did not seem to move the samples away from a country style in meaningful ways. – Novel voices: We pick artists whose voices are reproduced reasonably well by the model, and interpolate their style embeddings to synthesize new voices. Some blending, for instance, between Frank Sinatra and Alan Jackson in Sample 4, still sounds similar to Frank Sinatra. In most cases, the model renders in a vaguely recognizable but distinct voice that preserves different vocal attributes. Samples 1 and 2 conditioned on the Céline Dion embeddings divided by two have slightly different timbre and tone but capture her unique vibrato. We also experiment with changing the style embedding in the middle of a song to create a duet (Sample 7). This is another way of guiding generation during sampling. Continuing in another voice works best when the segment ends in an interlude; otherwise, the model blends voices in the middle of a word or a sentence. – Novel lyrics: We ask Jukebox to sing poems and novel verses generated by GPT-2 (Radford et al.) to demonstrate that it can indeed sing new lyrics. While the training data consists of song lyrics with limited vocabulary and constrained structure, the model has learned to follow along most prompts and sing even new words that are reasonably pronounceable (including technical terms from the deep learning literature). To get the best results, however, we ﬁnd that it is useful to spell out difﬁcult words or acronyms as they are spoken. The generations are noticeably higher quality if the text matches the distribution of lyrics for the given artist, both in terms of length, and of rhyming or rhythmic qualities. For example, hip hop lyrics tend to be longer than most other genres, and the commonly emphasized syllables easily form clear rhythms. – Novel riffs: Another useful application of Jukebox is the ability to record an incomplete idea and explore various continuations without ever needing to tabulate in symbolic representations, which would lose details of timbre and mood. We curate recordings of novel riffs by our in-house musicians and prime the model during sampling. Sample 6 starts with a musical style not widely used in Elton John’s songs. The model still carries out the tune and develops it further. Similarly, the beginning of Sample 1 is a progressive jazz piece with a 5/4 polymeter, which has never been used in hip hop. Despite this novelty, the rhythm persists throughout the song and is incorporated naturally with rapping. 5.4. VQ-VAE Ablations Spectral convergence (dB) Level Hop length Without restart With restart Bottom 8 −21.1 −23.0 Middle 32 −12.4 −12.4 Top 128 −8.3 −8.3 Table 1. Reconstruction ﬁdelity degrades with higher compression. Restarting dead codes near random encoder outputs mitigates learning suboptimal codes. Codebook size Spectral convergence (dB) 256 −15.9 2048 −23.0 No quantization −40.5 Table 2. Bottom-level VQ-VAE reconstruction results with different codebook sizes. Using larger codebooks helps reconstruction because it allows more information to be encoded at the bottleneck layers. Removing the bottleneck entirely yields almost perfect reconstruction. 16k 8k 4k 2k 1k 500 16k 8k 4k 2k 1k 500 16k 8k 4k 2k 1k 500 16k 8k 4k 2k 1k 500 16k 8k 4k 2k 1k 500 Figure 4. Comparison of reconstructions from different VQ-VAEs, x-axis is time and y-axis is frequency. The columns from left to right are bottom-, middle-, and top-level reconstructions at hop lengths 8, 32, and 128 respectively, visualized as Mel spectrograms. The ﬁrst row is the ground-truth, and the second row shows the spectrograms of audio outputs from our VQ-VAE. In the third row, we remove the spectral loss, and see that the middle and top level lose high-frequency information. In the fourth row, we use a hierarchical VQ-VAE (Razavi et al., 2019) instead of separate auto-encoders (Figure 1), and we see the middle and top levels are not used for encoding pertinent information. Finally, the ﬁfth row shows a baseline with the Opus codec that encodes audio at constant bitrates comparable to our VQ-VAE. It also fails to capture higher frequencies and adds noticeable artifacts at the highest level of compression. Ablation Spectral convergence (dB) None −8.3 Without spectral loss −6.3 With single autoencoder 2.9 Table 3. Top-level codes are generally difﬁcult to train well without spectral loss or with a single hierarchical autoencoder. Resulting reconstructions may lose some to most of information. We compare raw audio VQ-VAEs when trained with varying compression ratios, objectives, and architectures. As we use nonautoregressive decoders with continuous representation for output, we report spectral convergence (Sturmel 16k 8k 4k 2k 1k 500 & Daudet, 2011), which measures the amount of spectral error relative to signal, as test error and proxy for reconstruction ﬁdelity. We evaluate on 5000 held-out 3-second audio segments and report the average in decibels. All models in this section are trained with a batch size of 32, 3-second audio clips sampled at 44 kHz. As before, we use hop lengths of 8, 32, and 128 for the bottom, middle and top level respectively. In Table 1, we see that increasing the hop size results in higher reconstruction error. Figure 4 indeed shows that a signiﬁcant amount of information, especially higher frequencies, is missing at middle and top levels across all ablations we ran. This is expected as audio is compressed more with Codebook entropy (bits) 10 9 8 0 100k 200k 300k 400k 500k Number of training steps Figure 5. Entropy of codebook with 2048 codes, i.e 11 bits, over training. Reviving dead codes near random encoder outputs ensures good codebook utilization from the start of training. larger hop sizes. To mitigate codebook collapse, we restart dead codes near random encoder embeddings. In Figure 5, we see that this yields higher codebook usage even from early on in training. Models trained without random restarts can converge to the same test error and codebook usage but require more training steps. With poor initialization, these models sometimes end up with suboptimal codes hurting reconstruction ﬁdelity. Codebook size also matters, as it sets a limit on channel capacity through the bottleneck layers. In Table 2, we ﬁnd that reconstruction error increases considerably when the codebook size is reduced from 2048 to 256. We also compare with a model that uses continuous representations without vector quantization. We can think of this model as using a vastly large codebook with all encoder embeddings. This achieves almost perfect reconstruction with negligible spectral error. When the model is trained with L2 loss only, reconstructions tend to sound muddy from missing high frequencies, and this problem is exacerbated as hop size is increased. In Figure 4, we see that top-level codes trained without spectral loss do not capture much information beyond 2 kHz, and obtain worse reconstructions (Table 3). However, we observe that while spectral loss helps encode more information, it also adds distortion artifacts which we hear as scratchy noise. Lastly, we train a raw audio hierarchical VQ-VAE (Razavi et al., 2019) and ﬁnd that it is generally difﬁcult to push information to higher levels. This model is trained twice as long as the previous models, but middle and top-level reconstructions as shown in Figure 4 are not capturing much. It is possible that higher level codes may have collapsed before bottom level starts to reconstruct the audio well. Making the bottom layers explicitly model residuals pushed more information to the top. But, we found separate autoencoders to be cleaner and more effective.\n\n6. Related Work\nGenerative modeling in deep learning: Generative models aim to learn the distribution of data by either explicitly by modeling the distribution or implicitly by constructing means to sample from it (Goodfellow, 2016). Modeling the interdependency within high-dimensional data was traditionally considered extremely difﬁcult, but starting with Deep Boltzmann Machines (Salakhutdinov & Hinton, 2009), various kinds of deep generative models have been introduced. Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) use generator and discriminator networks that contest each other to make the generated samples as indistinguishable as possible from the data, and they are renowned for their ability to generate high-quality pictures (Zhang et al., 2019b; Brock et al., 2019). Autoregressive generative models such as NADE (Uria et al., 2016), PixelCNN (Van den Oord et al., 2016), and Transformers (Vaswani et al., 2017) use the chain rule of probability to factorize the joint distribution of data into a product of simpler distributions, and ﬂow-based models (Dinh et al., 2015; 2017; Rezende & Mohamed, 2015; Kingma & Dhariwal, 2018) learn a series of invertible transformations that maps the data distribution with a simpler one such as a Gaussian distribution. Autoregressive ﬂows (Papamakarios et al., 2017; Kingma et al., 2016) combine the two ideas to achieve faster density estimation or data generation. Variational autoencoders (VAEs) (Rezende et al., 2014; Kingma & Welling, 2014) impose a Gaussian prior on the latent code in an encoder-decoder setup from which data can be sampled. with restart without restart Generative models for music: Generative modeling of symbolic music dates back to more than half a century, when Hiller Jr & Isaacson (1957) introduced the ﬁrst computergenerated music based on Markov chains. There exists a variety of earlier approaches using rule-based systems (Moorer, 1972), chaos and self-similarity (Pressing, 1988), cellular automata (Beyls, 1989), concatenative synthesis (Jehan, 2005), and constraint programming (Anders & Miranda, 2011). More recent data-driven approaches include DeepBach (Hadjeres et al., 2017) and Coconet (Huang et al., 2017) which use Gibbs sampling to produce notes in the style of Bach chorals, MidiNet (Yang et al., 2017) and MuseGAN (Dong et al., 2018) which use generative adversarial networks, MusicVAE (Roberts et al., 2018) and HRNN (Wu et al., 2019) which use hierarchical recurrent networks, and Music Transformer (Huang et al., 2019a) and MuseNet (Payne, 2019) which use Transformers to autoregressively predict MIDI note events. There also have been a number of approaches for synthesizing music conditioned on symbolic music information, such as NSynth (Engel et al., 2017) which uses WaveNet-style autoencoder, Mel2Mel (Kim et al., 2019) and Wave2Midi2Wave (Hawthorne et al., 2019) which synthesize music using WaveNet conditioned on a piano roll representation, and GanSynth (Engel et al., 2019) which uses generative adversarial networks to produce magnitude spectrograms together with instananeous frequencies for easier spectrogram inversion. Generative models for music can also be used for music style transfer, as seen in Midi-VAE (Brunner et al., 2018) which uses a variational autoencoder to transfer styles between classical and jazz music, LakhNES (Donahue et al., 2019) which uses a Transformer architecture to generate chiptune music, and Universal Music Translator Network (Mor et al., 2019) which uses a denoising autoencoder that can disentangle musical style and content. Sample-level generation of audio: In recent years, a variety of generative models for raw audio have been introduced. WaveNet (Oord et al., 2016) performs autoregressive sampleby-sample probabilistic modeling of raw waveform using a series of dilated convolutions to exponentially increase the context length. It can produce realistic audio either unconditionally or by conditioning on acoustic features or spectrograms. The autoregressive nature of WaveNet makes the sampling notoriously slow, and it uses a categorical distribution for audio samples which introduces quantization noise. Parallel WaveNet (Oord et al., 2018) improves upon this by instead using a mixture of logistics distribution, a continuous probability distribution, and performing probability density distillation which learns a parallel feed-forward network from a pre-trained autoregressive model, allowing faster sampling of high ﬁdelity audio. ClariNet (Ping et al., 2019) achieves similar audio quality using a simple Gaussian distribution instead and thus having a closed-form loss function, eliminating the need for Monte-Carlo sampling. SampleRNN (Mehri et al., 2017) uses a multi-scale, hierarchical recurrent neural network with convolutional upsampling to model long-range complex structures. WaveRNN (Kalchbrenner et al., 2018) uses recurrent neural networks that operate separately on the most signiﬁcant and the least signiﬁcant bytes, which can be efﬁciently deployed in mobile devices while having comparable audio quality to WaveNet. WaveGlow (Prenger et al., 2019) is a ﬂow-based model for parallel sample-level audio synthesis, which can be trained with a straightforward maximum-likelihood estimation and thus is advantageous to the two-stage training process needed for distillation. Parallel WaveGAN (Yamamoto et al., 2020) and MelGAN (Kumar et al., 2019) are GAN-based approaches directly modeling audio waveforms, achieving similar quality as WaveNet and WaveGlow models with signiﬁcantly fewer parameters. While the approaches above serve as sophisticated generative models for raw audio to be conditioned on a compact and controllable representation of audio such as Mel spectrograms, MelNet (Vasquez & Lewis, 2019) takes a different approach of hierarchically generating accurate high-resolution Mel spec- trograms, after which a simple gradient-based optimization can produce high-ﬁdelity audio. VQ-VAE: Oord et al. (2017) introduced VQ-VAE, an approach of downsampling extremely long context inputs to a shorter-length discrete latent encoding using a vector quantization, and they showed that it can generate both highquality images and audio, as well as learn unsupervized representations of phonemes. Razavi et al. (2019) extended the above model by introducing a hierarchy of discrete representations for images and showed that the resulting model can learn to separate high-level semantics into the highest level of discrete codes which have the largest receptive ﬁeld, while capturing local features like textures in the lower levels with smaller receptive ﬁelds. They used the hierarchical model to generate high-diversity and high-ﬁdelity images for the conditional ImageNet and FFHQ datasets. Dieleman et al. (2018) tried variants of this approach where instead of a single encoder there are successive encoders that each further compress the lossy discrete encodings from the previous levels. A downside of this approach is that information is lost at each step and requires separate training for each VQ-VAE level, and it leads to a hierarchy collapse problem. De Fauw et al. (2019) used AR decoders which are known to cause the problem of ignoring the latent variables, and they suggested ways to mitigate it. The feed-forward decoders from (Razavi et al., 2019) do not suffer from this issue, and thus we use their approach. Speech synthesis: Producing natural human voice entails an understanding of linguistic features, mapping of sounds, and steerability of expression. Many text-to-speech (TTS) systems rely on highly engineered features (Klatt, 1980), carefully curated sound segments (Hunt & Black, 1996), statistical parametric modeling (Zen et al., 2009), and often complex pipelines as described in (Arık et al., 2017). These approaches are fairly involved and produce unnatural or inarticulate voices. More recent works like Deep Voice 3 (Ping et al., 2018), Tacotron 2 (Shen et al., 2018), and Char2Wav (Sotelo et al., 2017) learn speech synthesis endto-end using sequence-to-sequence architecture (Sutskever et al., 2014). The design space is vast, but in general, typical approaches comprise of a bidirectional encoder, a decoder, and a vocoder to build text representations, audio features, and the ﬁnal raw waveforms. To generate multiple voices, text-to-speech models can also condition on the speaker identity (Oord et al., 2016; Gibiansky et al., 2017; Jia et al., 2018) as well as text prompt. By learning and manipulating auxiliary embeddings, models can mimic a new voice (Arık et al., 2018a; Taigman et al., 2018) at test time. These methods, however, require labeled data. Ideas like clustering (Dehak et al., 2011), priming (Wang et al., 2018), and variational autoencoders (Hsu et al., 2019; Akuzawa et al., 2018) have been used to learn broader styles of speech and control expressivity in an unsupervised way. There are also works on synthesizing singing by additionally controlling pitch and timbre. Similar to TTS literature, early works use concatenative methods (Bonada & Serra, 2007) that join short segments of curated singing, and statistical parametric methods (Saino et al., 2006; Oura et al., 2010) which allow modeling of timbre from training data. Both approaches impose fairly strong assumptions resulting in noticeable artifacts. (Blaauw & Bonada, 2017) train a neural TTS model with a parametric vocoder to separate pitch and timbre which can be controlled at generation time.\n\n7. Future work\nWhile our approach represents a step forward in the ability to generate coherent long raw audio music samples, we recognize several directions for future work. Great music generation should be high quality over all time scales: it should have a developing musical and emotional structure across the entire piece, local notes and harmonies that always make sense, nuanced and appropriate small timbral and textural details, and audio recording quality that balances and blends the multiple voices well, and without unwanted noise. We view our current model as stronger on the mid-range time scales: often the model generates samples that locally sound very good, with interesting and diverse harmonies, rhythms, instruments, and voices. We have frequently been very impressed how the melody and rhythm generated suits a particular lyric extremely well. However, while the samples stay consistent over longer time scales, we notice they don’t have traditional larger music structures (such as choruses that repeat, or melodies that have a question and answer form). Additionally, on the smallest scale, we sometimes hear audio noise or scratchiness. Beyond the quality of the samples, we also would look to diversify the languages and styles the model is able to generate. Our current model has been trained only on songs whose primary language as detected by (Sites, 2013) is English. In the future, we would look to include other languages and artists. We believe this will be of interest both for generating strictly in those styles, and because historically we have seen much creativity and development coming from unusual blends of existing musical styles. Finally, we consider it very important that computer music generation also serves as a tool for human musicians, and increasingly those interested in music but without formal training. While we are able to steer our current model somewhat through lyric and midi conditioning, we can imagine many other possible ways for humans to inﬂuence the generations, including indicating the mood or dynamic at various sections, or controlling when drums, singers, or other instruments should play. The current model takes around an hour to generate 1 minute of top level tokens. The upsampling process is very slow, as it proceeds sequentially through the sample. Currently it takes around 8 hours to upsample one minute of top level tokens. We can create a human-in-the-loop co-composition process at the top level only, using the VQ-VAE decoders to get a fast upsampling of the top level tokens to hear a very rough sense of what the model generates. The top-level model generates multiple samples, the person picks a favorite (listening to the rough VQ-VAE decoding), and then the model continues generating multiple samples continuing the favorite. This process would be signiﬁcantly improved with faster generation and Transformer upsampling steps. Our models have fast parallel evaluation of likelihood but slow autoregressive sampling. We can instead use a model with fast parallel sampling but slow autoregressive likelihood evaluation (Kingma et al., 2016), and distill the information from our current model into it (Oord et al., 2018). The distillation works by generating samples from the parallel sampler and evaluating it likelihood and entropy using the parallel likelihood evaluator, and then optimising the sampler by minimising the KL divergence of it from our current model. 8. Conclusion\nWe have introduced Jukebox, a model that generates raw audio music imitating many different styles and artists. We can condition this music on speciﬁc artists and genres, and can optionally specify the lyrics for the sample. We laid out the details necessary to train a Hierarchical VQ-VAE to compress the music effectively into tokens. While previous work has generated raw audio music in the 20–30 second range, our model is capable of generating pieces that are multiple minutes long, and with recognizable singing in natural-sounding voices.\n\n9. Acknowledgement\nWe would like to thank John Schulman and Will Guss for producing and performing novel riffs for our sampling experiments, and Rewon Child, Aditya Ramesh, Ryan Lowe and Jack Clark for providing feedback for initial drafts of this paper.\n\nReferences\nAkuzawa, K., Iwasawa, Y., and Matsuo, Y. Expressive speech synthesis via modeling expressions with variational autoencoder. In INTERSPEECH, 2018. Arık, S. Ö., Chrzanowski, M., Coates, A., Diamos, G., Gibiansky, A., Kang, Y., Li, X., Miller, J., Ng, A., Raiman, J., Sengupta, S., and Shoeybi, M. Deep Voice: Real-time neural text-to-speech. In International Conference on Machine Learning, pp. 195–204, 2017. Arık, S. Ö., Chen, J., Peng, K., Ping, W., and Zhou, Y. Neural voice cloning with a few samples. In Advances in Neural Information Processing Systems, pp. 10019–\n10029. 2018a.\nArık, S. Ö., Jun, H., and Diamos, G. Fast spectrogram inversion using multi-head convolutional neural networks. IEEE Signal Processing Letters, 26(1):94–98, 2018b. Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Berner, C., Brockman, G., Chan, B., Cheung, V., D˛ebiak, P., Dennison, C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., et al. Dota 2 with large scale deep reinforcement learning. arXiv preprint arXiv:1912.06680, 2019. Beyls, P. The musical universe of cellular automata. In International Computer Music Conference, pp. 34–41, 1989. Blaauw, M. and Bonada, J. A neural parametric singing synthesizer. In INTERSPEECH, 2017. Bonada, J. and Serra, X. Synthesis of the singing voice by performance sampling and spectral models. IEEE signal processing magazine, 24(2):67–79, 2007. Brock, A., Donahue, J., and Simonyan, K. Large scale GAN training for high ﬁdelity natural image synthesis. In International Conference on Learning Representations, 2019. Brunner, G., Konrad, A., Wang, Y., and Wattenhofer, R. MIDI-VAE: modeling dynamics and instrumentation of music with applications to style transfer. In International Society for Music Information Retrieval Conference, pp. 747–754, 2018. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. De Fauw, J., Dieleman, S., and Simonyan, K. Hierarchical autoregressive image models with auxiliary decoders. arXiv preprint arXiv:1903.04933, 2019. Dehak, N., Kenny, P. J., Dehak, R., Dumouchel, P., and Ouellet, P. Front-end factor analysis for speaker veriﬁcation. IEEE Transactions on Audio, Speech, and Language Processing, 19(4):788–798, 2011. Dieleman, S., van den Oord, A., and Simonyan, K. The challenge of realistic music generation: modelling raw audio at scale. In Advances in Neural Information Processing Systems, pp. 7989–7999, 2018. Dinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear independent components estimation. In International Conference in Learning Representations, Workshop, 2015. Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using Real NVP. In International Conference in Learning Representations, 2017. Donahue, C., Mao, H. H., Li, Y. E., Cottrell, G. W., and McAuley, J. J. LakhNES: Improving multi-instrumental music generation with cross-domain pre-training. In International Society for Music Information Retrieval Conference, pp. 685–692, 2019. Dong, H.-W., Hsiao, W.-Y., Yang, L.-C., and Yang, Y.-H. MuseGAN: Multi-track sequential generative adversarial networks for symbolic music generation and accompaniment. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018. Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi, M., Eck, D., and Simonyan, K. Neural audio synthesis of musical notes with wavenet autoencoders. In International Conference on Machine Learning, pp. 1068–1077, 2017. Engel, J., Agrawal, K. K., Chen, S., Gulrajani, I., Donahue, C., and Roberts, A. GANSynth: Adversarial neural audio synthesis. In International Conference on Learning Representations, 2019. Gibiansky, A., Arık, S. Ö., Diamos, G., Miller, J., Peng, K., Ping, W., Raiman, J., and Zhou, Y. Deep Voice 2: Multispeaker neural text-to-speech. In Advances in neural information processing systems, pp. 2962–2970, 2017. Goodfellow, I. NIPS 2016 tutorial: Generative adversarial networks. In Neural Information Processing Systems, Tutorial, 2016. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. Generative adversarial nets. In Advances in neural\ninformation processing systems, pp. 2672–2680, 2014. Gupta, C., Yılmaz, E., and Li, H. Automatic lyrics transcription in polyphonic music: Does background music help? In International Conference on Acoustics, Speech, and Signal Processing, 2020. Hadjeres, G., Pachet, F., and Nielsen, F. Deepbach: a steerable model for bach chorales generation. In International Conference on Machine Learning, pp. 1362–1371. JMLR. org, 2017. Hawthorne, C., Stasyuk, A., Roberts, A., Simon, I., Huang, C.-Z. A., Dieleman, S., Elsen, E., Engel, J., and Eck, D. Enabling factorized piano music modeling and generation with the MAESTRO dataset. In International Conference on Learning Representations, 2019. Hennequin, R., Khlif, A., Voituret, F., and Moussallam, M. Spleeter: A fast and state-of-the art music source separation tool with pre-trained models. Late-Breaking/Demo ISMIR 2019, November 2019. Deezer Research. Hiller Jr, L. A. and Isaacson, L. M. Musical composition with a high speed digital computer. In Audio Engineering Society Convention 9. Audio Engineering Society, 1957. Ho, J., Kalchbrenner, N., Weissenborn, D., and Salimans, T. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180, 2019. Hsu, W.-N., Zhang, Y., Weiss, R. J., Zen, H., Wu, Y., Wang, Y., Cao, Y., Jia, Y., Chen, Z., Shen, J., Nguyen, P., and Pang, R. Hierarchical generative modeling for controllable speech synthesis. In International Conference on Learning Representations, 2019. Huang, C. A., Cooijmans, T., Roberts, A., Courville, A. C., and Eck, D. Counterpoint by convolution. In International Society for Music Information Retrieval Conference, pp. 211–218, 2017. Huang, C.-Z. A., Vaswani, A., Uszkoreit, J., Shazeer, N., Simon, I., Hawthorne, C., Dai, A. M., Hoffman, M. D., Dinculescu, M., and Eck, D. Music Transformer: Generating music with long-term structure. In International Conference on Learning Representations, 2019a. Huang, Y., Cheng, Y., Bapna, A., Firat, O., Chen, D., Chen, M., Lee, H., Ngiam, J., Le, Q. V., Wu, Y., et al. Gpipe: Efﬁcient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Processing Systems, pp. 103–112, 2019b. Hunt, A. J. and Black, A. W. Unit selection in a concatenative speech synthesis system using a large speech database. In IEEE International Conference on Acoustics, Speech, and Signal Processing Conference, pp. 373–376, 1996. and Planning, Program in Media Arts and Sciences, 2005. Jia, Y., Zhang, Y., Weiss, R., Wang, Q., Shen, J., Ren, F., Chen, z., Nguyen, P., Pang, R., Lopez Moreno, I., and Wu, Y. Transfer learning from speaker veriﬁcation to multispeaker text-to-speech synthesis. In Advances in Neural Information Processing Systems, pp. 4480–4490. 2018. Kalchbrenner, N., Elsen, E., Simonyan, K., Noury, S., Casagrande, N., Lockhart, E., Stimberg, F., Oord, A., Dieleman, S., and Kavukcuoglu, K. Efﬁcient neural audio synthesis. In International Conference on Machine Learning, pp. 2410–2419, 2018. Kim, J. W., Bittner, R., Kumar, A., and Bello, J. P. Neural music synthesis for ﬂexible timbre control. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 176–180, 2019. Kingma, D. P. and Dhariwal, P. Glow: Generative ﬂow with invertible 1x1 convolutions. In Advances in Neural Information Processing Systems, pp. 10215–10224, 2018. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. Kingma, D. P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I., and Welling, M. Improved variational inference with inverse autoregressive ﬂow. In Advances in neural information processing systems, pp. 4743–4751, 2016. Klatt, D. H. Software for a cascade/parallel formant synthesizer. Journal of the Acoustical Society of America, 67 (3):971–995, 1980. Kumar, K., Kumar, R., de Boissiere, T., Gestin, L., Teoh,\nW. Z., Sotelo, J., de Brébisson, A., Bengio, Y., and\nCourville, A. C. MelGAN: Generative adversarial networks for conditional waveform synthesis. In Advances in Neural Information Processing Systems, pp. 14881– 14892, 2019. LyricWiki. URL https://lyrics.fandom.com/ wiki/LyricWiki. Mehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S., Sotelo, J., Courville, A., and Bengio, Y. SampleRNN: An unconditional end-to-end neural audio generation model. In International Conference on Learning Representations, 2017. Moorer, J. A. Music and computer composition. Communications of the ACM, 15(2):104–113, 1972. Mor, N., Wolf, L., Polyak, A., and Taigman, Y. Autoencoderbased music translation. In International Conference on Learning Representations, 2019. Oord, A. v. d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A., and Kavukcuoglu, K. WaveNet: A generative model for raw audio. arXiv preprint arXiv:1609.03499, 2016. Oord, A. v. d., Vinyals, O., and Kavukcuoglu, K. Neural discrete representation learning. In Neural Information Processing Systems, 2017. Oord, A. v. d., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., van den Driessche, G., Lockhart, E., Cobo, L., Stimberg, F., Casagrande, N., Grewe, D., Noury, S., Dieleman, S., Elsen, E., Kalchbrenner, N., Zen, H., Graves, A., King, H., Walters, T., Belov, D., and Hassabis,\nD. Parallel WaveNet: Fast high-ﬁdelity speech synthesis.\nIn International Conference on Machine Learning, pp. 3918–3926, 2018. Oura, K., Mase, A., Yamada, T., Muto, S., Nankaku, Y., and Tokuda, K. Recent development of the HMM-based singing voice synthesis system – Sinsy. 2010. Papamakarios, G., Pavlakou, T., and Murray, I. Masked autoregressive ﬂow for density estimation. In Advances in Neural Information Processing Systems, pp. 2338–2347, 2017. Payne, C. Musenet. OpenAI blog, 2019. URL https: //openai.com/blog/musenet. Ping, W., Peng, K., Gibiansky, A., Arik, S. O., Kannan, A., Narang, S., Raiman, J., and Miller, J. Deep Voice 3: 2000-speaker neural text-to-speech. In International Conference on Learning Representations, 2018. Ping, W., Peng, K., and Chen, J. Clarinet: Parallel wave generation in end-to-end text-to-speech. In International Conference on Learning Representations, 2019. Prenger, R., Valle, R., and Catanzaro, B. WaveGlow: A ﬂow-based generative network for speech synthesis. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 3617–3621, 2019. Pressing, J. Nonlinear maps as generators of musical design. Computer Music Journal, 12(2):35–46, 1988. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners. Razavi, A., van den Oord, A., and Vinyals, O. Generating diverse high-ﬁdelity images with vq-vae-2. In Advances in Neural Information Processing Systems, pp. 14837– 14847, 2019. Rezende, D. and Mohamed, S. Variational inference with normalizing ﬂows. In International Conference on Machine Learning, pp. 1530–1538, 2015. Rezende, D. J., Mohamed, S., and Wierstra, D. Stochastic backpropagation and approximate inference in deep generative models. In International Conference on Machine Learning, pp. 1278–1286, 2014. Roberts, A., Engel, J., Raffel, C., Hawthorne, C., and Eck,\nD. A hierarchical latent vector model for learning longterm structure in music. In International Conference on\nMachine Learning, pp. 4364–4373, 2018. Saino, K., Zen, H., Nankaku, Y., Lee, A., and Tokuda,\nK. An HMM-based singing voice synthesis system. In\nINTERSPEECH, 2006. Salakhutdinov, R. and Hinton, G. Deep boltzmann machines. In Artiﬁcial intelligence and statistics, pp. 448–455, 2009. Shen, J., Pang, R., Weiss, R. J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R., Saurous, R. A., Agiomvrgiannakis, Y., and Wu, Y. Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions. In IEEE International Conference on Acoustics, Speech and Signal Processing, pp. 4779–4783, 2018. Sites, D. Compact language detector 2. 2013. URL https: //github.com/CLD2Owners/cld2. Sotelo, J., Mehri, S., Kumar, K., Santos, J. F., Kastner, K., Courville, A. C., and Bengio, Y. Char2Wav: End-toend speech synthesis. In International Conference on Learning Representations, 2017. Sturmel, N. and Daudet, L. Signal reconstruction from stft magnitude: A state of the art. International Conference on Digital Audio Effects, DAFx, 2011. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pp. 3104–3112, 2014. Taigman, Y., Wolf, L., Polyak, A., and Nachmani, E. VoiceLoop: Voice ﬁtting and synthesis via a phonological loop. In International Conference on Learning Representations, 2018. Uria, B., Côté, M.-A., Gregor, K., Murray, I., and Larochelle, H. Neural autoregressive distribution estimation. The Journal of Machine Learning Research, 17 (1):7184–7220, 2016. Van den Oord, A., Kalchbrenner, N., Espeholt, L., Vinyals, O., Graves, A., et al. Conditional image generation with pixelcnn decoders. In Advances in neural information processing systems, pp. 4790–4798, 2016. Vasquez, S. and Lewis, M. MelNet: A generative model for audio in the frequency domain. arXiv preprint arXiv:1906.01083, 2019. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in Neural Information Processing Systems, pp. 5998–6008, 2017. Wang, Y., Stanton, D., Zhang, Y., Skerry-Ryan, R., Battenberg, E., Shor, J., Xiao, Y., Ren, F., Jia, Y., and Saurous,\nR. A. Style Tokens: Unsupervised style modeling, control\nand transfer in end-to-end speech synthesis. In International Conference on Machine Learning, 2018. Wu, J., Hu, C., Wang, Y., Hu, X., and Zhu, J. A hierarchical recurrent neural network for symbolic melody generation. IEEE Transactions on Cybernetics, 2019. Xie, S., Girshick, R., Dollár, P., Tu, Z., and He, K. Aggregated residual transformations for deep neural networks. In IEEE Conference on Computer Vision and Pattern Recognition, pp. 1492–1500, 2017. Yamamoto, R., Song, E., and Kim, J.-M. Parallel WaveGAN: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram. In International Conference on Acoustics, Speech, and Signal Processing, 2020. Yang, L., Chou, S., and Yang, Y. Midinet: A convolutional generative adversarial network for symbolic-domain music generation. In International Society for Music Information Retrieval Conference, pp. 324–331, 2017. Zen, H., Tokuda, K., and Black, A. W. Review: Statistical parametric speech synthesis. Speech Communication, 51 (11):1039–1064, 2009. Zhang, H., Dauphin, Y. N., and Ma, T. Fixup initialization: Residual learning without normalization. In International Conference on Machine Learning, 2019a. Zhang, H., Goodfellow, I., Metaxas, D., and Odena, A. Self-attention generative adversarial networks. In International Conference on Machine Learning, 2019b.\nA. Scalable Transformer\nWe make the Sparse Transformer (Child et al., 2019) more scalable and easier to implement by a few small changes. We implement a simpler attention pattern that has the same performance without needing custom kernels to implement. We simplify the initialization by using the same initalization scale in the whole model without rescaling the weights based on fan-in and depth, and we optimize the memory footprint with fully half-precision training, i.e. storing the model weights, gradients and the optimizer states in half precision and performing computations in half precision as well. To cope with the narrower dynamic range of the fp16 format, we use dynamic scaling of the gradient and Adam optimizer states. Axis-aligned attention patterns: The Sparse Transformer (Child et al., 2019) sparsiﬁes the attention pattern by reshaping the input sequence into a 2-D sequence of shape (blocks, block length) to use factorized attention. They observe that the strided attention pattern works best for images and audio because it does not have the state bottleneck of the ﬁxed attention. However, their implementation require specialized CUDA kernels. We can obtain a similar pattern by doing masked row, masked column, and unmasked previous-row attention. While the masked row captures the local context, the masked column and unmasked previous-row attention captures the context of all previous rows. We observe the same computational speed as well as training loss with this pattern. Each of these can be implemented directly as a dense attention by transposing or slicing the input sequence along appropriate axes, and thus do not require special CUDA kernels to implement. This can be easily extended to video too. Complementary to our work, a similar pattern was introduced in (Ho et al., 2019) where they also used axis-aligned attention but instead used a two-stream architecture. Half-precision parameters and optimizer state with dynamic scaling: To allow training large models, (Child et al., 2019) uses recompute with gradient checkpointing, performs computations using half precision activations and gradients, and uses dynamic loss scaling. While this speeds up training on Volta cores, one still has a high memory usage from storing the parameters and Adam state in full ﬂoat precision. To scale our models further, we store our matmul parameters and their Adam state in half precision, thus halving our memory usage. We use a single parameter s to set the scale of all weights and initialize all matmul and input/output embeddings3 to N(0, s), and position embeddings to N(0, 2s). The initialization ensures all parameters are in a similar dynamic range, and allows us to train in half preci- Masked Row Attention Masked Column Attention Unmasked Previous-Row Attention (a) Three axis-aligned attention patterns are sparse attention patterns that allow autoregressive generative modeling while only using simple Python-level array manipulation. Masked row and column attention patterns use autoregressive masks, whereas unmasked previous-row attention is fully visible. (b) Combining two of the attention patterns, each position can attend to any of the previous positions, while not causing a state bottleneck as in ﬁxed sparse attention (Child et al., 2019). Figure 6. Axis-aligned attention patterns sion completely without loss in training performance. For the Adam state tensors (m_t, v_t) we do dynamic scaling. For each iteration and for every parameter, we rescale its state tensors before casting so that their maximum corresponds to the maximum value of the ﬂoat16 range, thus maximizing the use of the ﬂoat16 range. Thus, we store the state m_t as the tuple (scale, (m_t/scale).half()), where scale = m_t.max()/float16.max(), and similarly for v_t. The above lets us ﬁt models of size 1B parameters into memory for our large context of 8192 tokens. To train even larger models, we use GPipe (Huang et al., 2019b).\nB. Experimental details\nB.1. Music VQ-VAE We have three separate raw audio VQ-VAEs to produce discrete codes at varying hop sizes for the bottom, middle, and top priors. All autoencoders comprise non-causal, dilated 1-D convolutions, and are trained independently using nonautoregressive reconstruction losses. Basic building blocks in these networks share the same architecture, as shown in Figure 7. Each encoder block consists of a downsampling convolution, a residual network, and a 1D convolution with a kernel size of 3. Dilation is grown by a factor of 3 in these residual networks to increase the receptive ﬁeld. The decoder block mirrors this exactly with a 1D convolution with the kernel size of 3, a residual network with dilation contracting across depth, and an upsampling transposed convolution. Here, all resampling convolutions use a kernel size of 4 and stride 2 so that each building block changes the hop length by a factor of 2. To get higher compression in time, we simply stack more of these blocks. For example, using seven blocks yields a hop length of 128 for the top level autoencoder. Each residual network has four residual blocks in the middle and top VQ-VAEs resulting in a receptive ﬁeld of 120 ms and 480 ms for the respective discrete tokens. Because increasing the residual depth helped improve reconstruction quality slightly, we doubled the number of residual blocks for the bottom level. This dramatically increases the receptive ﬁeld to about 2 seconds per code but the actual receptive ﬁeld is mostly local. We also experimented with having a single decoder and modeling the residuals to separate out learned representations as in (Razavi et al., 2019), hoping upsampling priors would simply ﬁll in local musical structure. However, pushing information to the top level was quite challenging as the bottommost level reconstructs almost perfectly early on in training. When we add auxiliary objectives to encourage the top to be used more, the top-level codes add serious distortions to the ﬁnal output. A similar challenge is shown in (Dieleman et al., 2018). B.2. Music Priors and Upsamplers Architectural details of our music prior and upsampler models are depicted in Figure 8. They perform autoregressive modeling of tokens at each level, conditioned on information such as artist and genre, as well as the tokens from the upper level in the case of the upsamplers (Figure 8a). Each artist and genre are learned as embedding vectors, whose sum is provided as the very ﬁrst token in each sequence. In addition, positional embedding is learned as a function of each position’s absolute and relative timing in the duration of the song. In upsampler models, upper-level tokens ×L ×D Dilated Conv1D Conv1D + xt ht Conv1D (a) The encoder compresses the raw audio input into a sequence of embeddings. The length of this latent representation relative to the raw audio duration determines the amount of compression, and is an important factor for the trade-off between ﬁdelity and coherence. Codebook ht ezt Codebook Nearest-Neighbor Search zt Lookup Gradient Passthrough (b) The bottleneck takes the sequence of embeddings from the encoder and maps it into a sequence of code vectors from the codebook. This sequence of code indices is used as a discrete representation to be modeled by the priors. Larger codebooks improve ﬁdelity but may be more difﬁcult to compress. ×L ×D Dilated Conv1D Conv1D + Transposed Conv1D ezt x̂ t Conv1D (c) The decoder reconstructs the raw audio from latent representations. It is a mirror of the encoder where dilations constracts by a factor of 3 down to 1 at the last block. The ﬁnal Conv1D projects to the desired number of audio channels and also acts as a smoothing operation after a sequence of transposed convolutions. Figure 7. Components of the VQ-VAE model are upsampled by the conditioner network, using WaveNetstyle dilated convolutions followed by a transposed 1-D convolutional layer (Figure 8b). When the model is trained on lyrics, the top-level prior takes lyrics data corresponding to each audio segment and uses them to train an encoder-decoder Transformer as shown in Figure 8c. All transformer stacks use sparse self-attention layers with the three factorized attention types (row, column, and previous-row) repeating, and encoder-decoder attention layers, when present, are interleaved with the other attention types. Each layer consists of residual connections of an attention and an MLP feedforward network, each prepended by layer normalization (see Figure 8d). Upper-Level Tokens Conditioner Timing Data Time Embedding z1:T–1 Artist & Genre Lyrics Scalable Transformer z1:T (a) The structure of our prior models, performing next-token prediction at each level. The Transformer takes the embeddings of the tokens z1:T −1 prepended by the sum of the artist and genre embeddings, in addition to the time embedding that encodes relative and absolute timing of the segments in the duration of the song. The upsampler priors additionally take the tokens from the upper level, which are fed to the conditioner network and added to the input sequence. The top-level prior takes lyrics as conditioning information as well (see Figure 8c). Only in the Top Level VQ Codes Lyrics Lyrics Token Embedding VQ Code Embedding Row Attention Layer Row Attention Layer Column Attention Layer Column Attention Layer Previous-Row Attention Layer Previous-Row Attention Layer Row Attention Layer Encoder-Decoder Attention Layer Column Attention Layer Row Attention Layer Previous-Row Attention Layer Column Attention Layer ⋮ Previous-Row Attention Layer Row Attention Layer Encoder-Decoder Attention Layer ⋮ ⋮ Column Attention Layer Previous-Row Attention Layer Encoder-Decoder Attention Layer Row Attention Layer Column Attention Layer Lyrics Token Embedding Previous-Row Attention Layer Encoder-Decoder Attention Layer Next-Token Prediction VQ Code Embedding Next-Token Prediction (c) The Scalable Transformer architecture, shown with the lyrics Transformer used in the top-level prior. The Transformer layers use the three factorized attention types alternatingly, i.e. repeating row, column, and previous-row attentions. In the top-level prior, the VQ Transformer additionally includes interleaved encoder-decoder attention layers that apply lyrics conditioning by attending on the activation of the last encoder layer. Not Used in the Top Level Token Embedding ×D Dilated Conv1D Conv1D + Transposed Conv1D (b) The conditioner network takes the tokens from the upper level, and their embedding vectors go through non-causal WaveNet-like layers with increasingly dilated convolutions. The transposed 1-D convolution upsamples the sequence to the higher temporal resolution of the current level. Layer Norm Encoder Features Attention ×6 + ×3 Layer Norm MLP ×3 + (d) Each Transformer layer is a residual attention block, which performs two residual operations, attention and MLP, each prepended with layer normalization. Depending on the layer’s type, it uses either one of the three factorized attentions or encoder-decoder attention taking the lyrics features from the encoder. B.3. Hyperparameters For all Transformers’ residual blocks, we use MLP blocks with the same width as the model width, and attention blocks with queries, keys, and values with width 0.25 times the model width. For all convolutional residual blocks, we use convolutions with same channels as the model width. Sample rate 44100 Sample length 393216 Hop lengths 8, 32, 128 Embedding width 64 Residual block width 64, 32, 32 Residual blocks (per 2x downsample) 8, 4, 4 Conv ﬁlter size 3 Conv channels 32 Dilation growth rate 3 Commit weight β 0.02 Codebook EMA γ 0.99 Codebook size 2048 Spectral loss STFT bins 2048, 1024, 512 Spectral loss STFT hop length 240, 120, 50 Spectral loss STFT window size 1200, 600, 240 Initialization scale 0.02 Batch size 256 Training steps 384618 Learning rate 0.0003 Sample length 1048576 Context length 8192 Transformer width 4800 Transformer self-attention layers 72 Attention heads 8 Factorized attention shape (128, 64) Lyrics encoder tokens 512 Lyrics encoder width 1280 Lyrics encoder layers 18 Lyrics encoder attention heads 4 Lyrics encoder factored attention shape (32, 16) Encoder-Decoder attention layers 7 Initialization scale 0.002 Encoder initialization scale 0.014 Batch size 512 Training steps 310500 Learning rate 0.00015 Adam β2 0.925 Weight decay 0.002 Table 4. VQ-VAE hyperparameters Sample length 262144, 65536 Context length 8192 Transformer width 1920 Transformer layers 72 Attention heads 1 Factorized attention shape (128, 64) Conditioner residual block width 1024 Conditioner residual blocks 16 Conditioner conv ﬁlter size 3 Conditioner conv channels 1024 Conditioner dilation growth rate 3 Conditioner dilation cycle 8 Initialization scale 0.004, 0.008 Batch size 192, 184 Training steps 265000, 279000 Learning rate 0.0003 Adam β2 0.95 Weight decay 0.01 Table 6. Top-level prior hyperparameters Table 5. Middle- and bottom-level upsampler hyperparameters B.4. t-SNE Plot of Artists Barry White Tony Bennett Natalie Cole Barrington Levy Fats Domino Lou Rawls Donna Summer The Platters Dinah Washington J immy Cliff Sean Paul Shaggy Bobby Bland Luther Vandross The Weeknd Lonnie J ohnson Ariana Grande Dean Martin Akon Ed Sheeran Neil Diamond\nT. Pain\nBuck Owens Kanye West O.S.T.R. Migos The Beatles Van Morrison The Beach Boys Pink Floyd Billy Ray Cyrus The Cure The Smashing Pumpkins P!nk Linkin Park Figure 9. t-SNE of (artist, genre) embedding. The overall clustering shows very clearly how genres are related to one another. The broadest of all, pop, is situated in the middle of rock, country, blues, hip hop, and many more. Soundtrack and classical form their own island. Within a genre, we see a similar trend among artists. John Lennon, Paul McCartney, George Harrison and Ringo Starr are clustered around The Beatles. Cheap Trick which has a number of Beatles covers is also found near. Because we are showing only about 400 artists here, not all neighboring artists may be related. For an interactive version, we point to our blog post. Ray Noble Jazz R&B R&B Soul Cab Calloway Hip Hop Country Blues Louis Prima Soundtrack Pop Rock The Mills Brothers Classical Reggae Walter Gieseking Nina Simone Yann Tiersen Glenn Gould Bessie Smith Howard Shore Garrick Ohlsson Franz Schubert Ramin Djawadi Yo-Yo Ma Vangelis Henry Mancini J ohann Sebastian Bach Andrea Bocelli Hank Snow Fleetwood Mac ELO Rod Stewart Fleetwood Mac ELO Rod Stewart Eddy Arnold The Beatles George Harrison The Beatles George Harrison Paul McCartney John Lennon Neil Young Neil Young Eagles Eagles Marty Robbins Paul McCartney John Lennon Ringo Starr Ringo Starr Cheap Trick Cheap Trick Aerosmith Aerosmith Status Quo Queen Status Quo Pearl Jam Pearl Jam Queen\n--- FOOTNOTES 512 V100s for 2 weeks. The detailed hyperparameters for Jehan, T. Creating music by listening. PhD thesis, Massachusetts Institute of Technology, School of Architecture 1B upsamplers 5B prior", "persona": "General Public", "response": "### 1. Overview\nWhat if you could create an original song—complete with vocals and instrumentation—in the style of your favorite artist, using just a few lines of text? With AI music generators like OpenAI’s Jukebox, that kind of creativity is now within reach. Whether you're experimenting with new sounds or looking for inspiration, Jukebox makes it easy to explore and produce music in entirely new ways. Jukebox is an AI-powered tool that generates music in a variety of genres and artistic styles. Developed by OpenAI, Jukebox has been trained using a vast dataset containing over 1.2 million songs. As a result, it can generate musical styles spanning from reggae to R&B, jazz, hip-hop, pop, classical, country, and blues. Jukebox can also imitate the style of popular artists and bands, such as Frank Sinatra, Beyoncé, or the Beatles, to help you produce new songs. Released in 2020, Jukebox is an experimental research model. While it’s no longer actively maintained, it remains a compelling tool for AI music generation and creative experimentation. Though AI tools like Jukebox aren’t perfect, they can still provide inspiration to help you produce good songs quickly. With these tools, you can explore music-making in different genres, try out various styles, and find your unique sound. Keep reading to learn how to set up Jukebox and use it for music generation.\n\n### 2. Background\nAdditionally, running Jukebox is resource-intensive, as it requires a relatively powerful computer with enough VRAM and GPU. Most PCs are not equipped to run the Jukebox model. As a result, the best way to install and run Jukebox is via a cloud environment like Google Colab—which we’ll use in this tutorial. Raw audio processing Practical applications Disclosure: Upwork is an OpenAI partner, giving OpenAI customers and other businesses direct access to trusted expert independent professionals experienced in working with OpenAI technologies.\n\n### 3. Methods\nBefore going any further, let’s discuss the technologies behind OpenAI’s Jukebox. 5. Next, run the “!pip install -r requirements.txt” command to install the required dependencies for the project. Specifically, this will install fire, tqdm, soundfile, unidecode, numba, librosa, and mpi4py libraries. Note this process may take some time depending on your internet connection quality. 6. After the dependencies are added to the project, proceed and install the OpenAI Jukebox model by running the following code. Note that this installation might consume more RAM than that assigned to your Google Colab account. Consider upgrading to higher pricing plans to access more resources. ‍ ‍\n\n### 4. Results\nFeature/Tool Jukebox (OpenAI) Suno AudioCraft (Meta) MuseNet (OpenAI) Developer OpenAI Suno AI Meta OpenAI Status Research project, no longer maintained Actively maintained Open-source, maintained Deprecated Generates vocals Yes Yes No (instrumentals only) No (MIDI-style output) Generates lyrics Yes (optional input) Yes No No Audio quality Experimental, often noisy High quality Clean instrumentals MIDI, no audio Ease of Use Technical setup required Web-based, user-friendly Requires technical setup No longer available Customization High (requires code) Limited High (code-based) Minimal Model access Open-source (via GitHub) Closed Open-source (GitHub) Not available Best for Developers, AI researchers Creators, casual users Developers, researchers Historical interest Understanding datasets and metadata Once the new raw audio has been generated in the neural net, it needs to be enhanced further through a process known as upsampling. The upsampling exercise takes more time and consumes significant resources. For instance, upsampling raw audio using Jukebox in Google Colab can take 10-12 hours, depending on the audio length. The final output is a song that makes some sense but is unlikely to be entirely perfect. Understanding the encoder-decoder framework These algorithms slowly build upon the raw data, generating new outputs and improving the overall audio quality. However, increasing the batch size will lead to increased RAM usage.\n\n### 5. Takeaways\nUnderstanding core concepts Metadata, on the other hand, outlines specific details about a song. This includes the artist, album, release date, name, genre, and track number. The metadata provides context to the AI model, aiding in the production of more accurate and context-aware music. Exploring advanced features If you’re using a cloud environment with a pay-as-you-go pricing plan, you may have to pay more for additional computational resources.", "paper_id": "01", "story_id": "01-03"}
{"paper": "A comprehensive diferential diagnosis is a cornerstone of medical care that is often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by large language models present new opportunities to assist and automate aspects of this process1 . Here we introduce the Articulate Medical Intelligence Explorer (AMIE), a large language model that is optimized for diagnostic reasoning, and evaluate its ability to generate a diferential diagnosis alone or as an aid to clinicians. Twenty clinicians evaluated 302 challenging, real-world medical cases sourced from published case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: assistance from search engines and standard medical resources; or assistance from AMIE in addition to these tools. All clinicians provided a baseline, unassisted diferential diagnosis prior to using the respective assistive tools. AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% versus 33.6%, P = 0.04). Comparing the two assisted study arms, the diferential diagnosis quality score was higher for clinicians assisted by AMIE (top-10 accuracy 51.7%) compared with clinicians without its assistance (36.1%; McNemar’s test: 45.7, P < 0.01) and clinicians with search (44.4%; McNemar’s test: 4.75, P = 0.03). Further, clinicians assisted by AMIE arrived at more comprehensive diferential lists than those without assistance from AMIE. Our study suggests that AMIE has potential to improve clinicians’ diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients’ access to specialist-level expertise.\n\nAn accurate diagnosis is a critical component of effective medical care. Building artificial intelligence (AI) systems that are capable of performing or assisting clinicians in this important task has been a long-standing grand challenge2 . Whereas prior focus has been on evaluating a machine’s ability to accurately output a diagnosis1,3–5 , real-world clinical practice involves an iterative and interactive process of reasoning about a differential diagnosis (DDx), weighing multiple diagnostic possibilities in the light of increasing amounts of clinical information over time. Deep learning has been applied to promising effect for generating DDx in a number of specialties including radiology4 , ophthalmology5  and dermatology3 , but such systems lack the interactive capabilities to fluently assist a user through communication in natural language. The emergence of large language models (LLMs) presents an opportunity to design novel interactive tools and interfaces to aid DDx. These models have demonstrated the ability to perform complex language comprehension and reasoning tasks, generating coherent text and thereby enabling a large variety of real-world applications6–9 . Both general-purpose LLMs (GPT-4) and medical domain-specialized LLMs (Med-PaLM 2) have demonstrated strong performance in standardized and multiple-choice medical benchmarks10,11. Such evaluations represent a natural starting point for probing the model’s medical knowledge and capabilities but do not measure utility in real-world scenarios for care delivery—for example, in challenging medical cases faced by trained physicians. It is also not obvious how these models might actively assist clinicians in the development of a DDx. Recent work has begun to assess the standalone performance of these models on challenging case reports that involve complex deduction and diagnosis1,12–14, but has stopped short of evaluating how they can assist clinicians, augment performance and empower them to provide better care. Here we introduce AMIE, an LLM that is optimized for clinical diagnostic reasoning to generate a DDx for challenging, real-world medical cases. Beyond measuring standalone performance, we integrated this model into an interactive interface to measure how well AMIE could assist clinicians in developing a DDx. Using a set of challenging real-world case reports from the New England Journal of Medicine (NEJM) clinicopathological conferences (CPCs), we compared clinicians’ ability to form a DDx with the assistance of AMIE versus with access to traditional information retrieval tools (such as internet searches and books). AMIE achieved impressive performance in both generating DDx lists that contained the correct diagnosis (top-10 accuracy) and in identifying the correct final diagnosis as the most likely in the list (top-1 accuracy). Under automated model-based evaluation, the quality and the accuracy of the DDx list produced by AMIE was found to be significantly better than the state-of-the-art GPT-4 model available at the time of the experiments1 . Perhaps more importantly, AMIE also improved the diagnostic capability of clinicians as measured by the quality of their DDx lists for the evaluated cases. LLMs optimized for the safety-critical medical domain such as ours present a novel paradigm for assisting clinicians because of the potential for variation in the ways in which a given individual may converse with the system and utilize it in collaborative reasoning. A detailed explanation of the cases, their components, how they were fed to the model, the randomization scheme of AMIE versus the standard practice and information on the expert raters of the model and how the outputs were evaluated by blind expert raters, can be found in Methods. In evaluating the quality of the DDx lists we used several criteria, inspired by the approach taken in ref. 1 and extended to draw additional insight from the clinicians. First, we measured whether the final diagnosis matched an entry in the DDx list and in which position (top-n accuracy). Second, we used the quality score from Bond et al.15 and created appropriateness and comprehensiveness scales. Combined, these measures assess overall DDx quality, appropriateness and comprehensiveness. When using AMIE for assistance, clinicians asked, on average (mean), 2.92 questions in the interface (median 2, interquartile range (IQR) 1–4). On average (mean), clinician questions consisted of 9.39 words (median 10, IQR 6–12) and 54.31 characters (median 61, IQR 39-63). AMIE’s responses, on average (mean), consisted of 237.60 words (median 198, IQR 127–332) and 1,540.81 characters (median 1,276; IQR 815–2210). In the Search condition, the most popular tools were UpToDate (used in 34% of tasks), Google Search (30%) and PubMed (22%). Although clinicians were allowed to use additional tools in the AMIE condition, this was far less frequent (less than 5% of tasks).\n\nDDx performance of AMIE The DDx lists produced by our language model achieved strong quality, appropriateness and comprehensiveness scores (see Fig. 1). The median quality score was 5 (‘DDx includes the correct diagnosis’) with 54% of DDx lists achieving that score. The number of cases that scored 5 (that is, the DDx included the top diagnosis) was statistically significantly higher for AMIE compared with clinicians without assistance (McNemar’s test: 64.4, P < 0.01). The mean appropriateness score was 4.43 out of 5 (s.d. 0.92). The median comprehensiveness score was 4 (‘The DDx contains all candidates that are reasonable’) with 55% of the DDx lists achieving that score. The mean appropriateness score of AMIE (4.34) was significantly higher than that of unassisted clinicians (3.74) (paired t-test 8.52, P < 0.001, Wilcoxon signed-rank test: 2,857.5, P < 0.001) and assisted clinicians in either the Search (3.80) (paired t-test 7.23, P < 0.001, Wilcoxon signed-rank test: 3,308.5, P < 0.001) or LLM (4.06) (paired t-test 4.98, P < 0.001, P < 0.001, Wilcoxon signed-rank test: 2,752.0, P < 0.001) conditions. For computing top-n accuracy, if any of the first n diagnoses in an individual DDx were marked correct by the language model, the differential was considered to be correct. We computed the proportion of correct DDx lists across all cases to compute the top-n accuracy (for n from 1 to 10) for each DDx. AMIE reliably generated DDx lists that perform well against the ground truth diagnosis (Fig. 2). AMIE provided the correct diagnosis in 177 (59%) of the DDx lists and in 89 (29%) of the lists it was at the top of the list. These scores are higher than those achieved by the clinicians in any of the conditions. The top-10 accuracy of AMIE (59.1%) was significantly higher than the top-10 accuracy for the unassisted clinicians (33.6%) (P = 0.04) (Tables 1 and 2). Figure 2 shows the top-n accuracy based on human and the automated metric. The results are broadly similar, illustrating that despite the final diagnoses often being complex and nuanced, the automated metric faithfully captures the distinction between a DDx list that includes the correct diagnosis and one that does not. The clinicians in the study were not required to give a full list of ten diagnoses for every case. Clinicians in conditions I and II were required to given a minimum of three diagnoses. The median number provided was six. The performance at n = 6 is of particular relevance. Not all clinicians provided six diagnoses, as a result we conducted a variable top-n experiment, where for each case n was set to the number of diagnoses provided by the human clinicians. The variable top-n performance of AMIE was 59.4%—this is similar to the performance at n = 9 and at n = 10. As a result, AMIE’s output of a full list of ten diagnoses did not place it at an advantage compared to the clinicians. AMIE as a DDx assistant Of the DDx lists created before assistance 37% (Search condition) and 29% (AMIE condition) achieved a quality score of 5 (Fig. 1). For comparison, 49% of those created with assistance from AMIE scored 5. The number of cases that scored 5 (that is, the DDx included the top diagnosis) was statistically higher for clinicians assisted by AMIE compared with clinicians without assistance (McNemar’s test: 48.3, P < 0.01) and clinicians with Search assistance (5.45, P = 0.02). For comprehensiveness, the number of cases that scored 4 (that is, The DDx contains all candidates that are reasonable) was statistically higher for clinicians assisted by AMIE compared with clinicians without assistance (McNemar’s test: 185.8, P < 0.01) and clinicians with Search assistance (185.8, P < 0.01). As a consistency check, the number of cases that scored 4 was not statistically higher for clinicians in the Search condition (I) baseline and AMIE condition (II) baseline (McNemar’s test: 1.47, P = 0.23). The mean appropriateness score after assistance with AMIE (4.06) was significantly higher than after assistance with Search (3.80) (paired t-test 3.32, P = 0.001) and the baseline (3.74) (paired t-test 4.79, P < 0.001). To summarize, with the support of AMIE, the quality, appropriateness and comprehensiveness scores for the DDx lists were greater than for the lists prior to assistance (see Fig. 1). The top-n accuracy of the clinicians increased with assistance from AMIE compared to without (see Fig. 2). A Sankey diagram illustrates the effect of the two forms of assistance (Search and AMIE) on top-10 accuracy (Fig. 3). In the AMIE condition, 73 cases that did not feature the final diagnosis prior to using the tool included it after assistance from AMIE. This result is in contrast to only 37 cases in the Search condition. Comparing the two assisted study arms, the DDx quality score was higher for clinicians assisted by AMIE (top-10 accuracy 51.7%) compared with clinicians without its assistance (36.1%) (McNemar’s test: 45.7, P < 0.01) and clinicians with search (44.4%) (4.75, P = 0.03). Task duration with AMIE and Search The time taken to generate updated DDx lists in the Search condition versus the AMIE condition were similar (Search: 7.19 ± 5.33 min, AMIE: 7.29 ± 6.41 min (mean ± s.d.)). These were not significantly different (paired t-test P = 0.807), which is surprising as the clinicians all had experience using internet search and other information retrieval tools, yet they were using the AMIE interface for the first time. We had hypothesized that they would take longer using AMIE owing to the initial learning curve. Length of DDx lists with AMIE and Search When unassisted, the median length of the DDx lists was 6 (IQR 5–9); the mean was 6.41 (s.d. 2.39). With search the median DDx list length was 7 (IQR 5–10); the mean was 6.92 (s.d. 2.52). With AMIE, the median DDx list length was 8 (IQR 6–10); the mean was 7.58 (s.d. 2.33). With assistance from AMIE, the length of the DDx lists was longer than without assistance (paired t-test: 7.13, P < 0.001) and longer than the DDx lists with assistance from search (paired t-test: 3.15, P = 0.002). AMIE comparison with GPT-4 As we did not have the same set of human raters who evaluated the differentials produced by GPT-41  and AMIE, we cannot compare top-10 accuracy numbers directly. Therefore, in our study design, we evaluate performance on that 70-case subset (reported in ref. 1) using the automated metric (which is shown above to be relatively consistent with human evaluation). AMIE performs better with regard to top-n accuracy for n > 1, with the gap being most prominent for n > 2 (Fig. 4). This suggests potentially significant improvements in quality and comprehensiveness of the differentials produced by AMIE. For n = 1, GPT-4 performs marginally better but not statistically significantly\n\nDiscussion\nWe used a popular series of complex diagnostic challenges to evaluate an LLM optimized for clinical reasoning and diagnosis (AMIE); both in a standalone capacity and under randomized comparisons as an assistive tool for physicians. In standalone performance, AMIE generated more appropriate and comprehensive DDx lists than physicians when they were unassisted, with its DDx lists being more likely to include the final diagnosis than DDx lists from a board-certified internal medicine physician, regardless of what position in the DDx list was considered (that is, top-n accuracy with n ranging from 1 to 10). Clinicians using Table 2 | Top-1 and top-10 accuracy of DDx lists produced with AMIE and Search assistance by speciality Model only Human AMIE Before assistance After Search assistance After AMIE assistance Metrics Top-1↑ Top-10↑ Top-1↑ Top-10↑ Top-1↑ Top-10↑ Top-1↑ Top-10↑ AMIE as an assistant produced a DDx with higher top-n accuracy, and DDx with greater quality, appropriateness and comprehensiveness compared with the status quo for clinical practice (use of internet search and other resources). The NEJM CPCs examined here are well-known for being unique and challenging clinical conundrums. Within this distinctive setting, AMIE outperformed an unassisted board-certified physician in both top-1 and top-n accuracy. Whereas the CPCs have long been used as benchmarks for difficult diagnosis, it is also well-known that performance in CPCs in no way reflects a broader measure of competence in a physician’s duties16. Furthermore, the act of forming a DDx comprises many other steps that are not scrutinized in this study, including the goal-directed acquisition of information under uncertainty (which is known to be challenging for AI systems despite recent technical progress in this direction17–19). We are therefore very cautious in extrapolating our findings towards any implications about the utility of AMIE as a standalone diagnostic tool. Nevertheless, our controlled evaluation mirrored the findings of other recent works exploring the performance of LLMs and pre-LLM ‘DDx generators’ in smaller subsets of the NEJM CPCs, which have shown the potential for automated technology to reach the correct DDx with superior performance to standalone physicians in these challenging cases1,12,13,20. Although this represents a step beyond historical attempts at automating DDx in NEJM CPCs, in which computerized approaches were deemed overtly unreliable for practical use21, such studies also undertook limited consideration of the quality of DDx generated by these automated systems or their role as assistive tools. Our work extends previous observations by showing not only that AMIE was more likely to arrive at a correct answer or provide the correct answer in a list, but also that its DDx were determined by an independent rater to be of higher appropriateness and comprehensiveness than those produced by board-certified physicians with access to references and search. In our study, clinicians had access to both images and tabular data in redacted case reports, whereas AMIE was only provided with the main body of the text. Although AMIE outperformed the clinicians despite this limitation, it is unknown whether and how much this gap would widen if AMIE had access to the figures and tables. Furthermore, n = 94 n = 83 n = 109 n = 97 n = 156 n = 73 n = 11 n = 208 n = 146 n = 193 n = 168 n = 135 Not in DDx list In DDx list Fig. 3 | Sankey diagram showing effect of assistance. a, In the AMIE arm, the final correct diagnosis appeared in the DDx list only after assistance in 73 cases. b, In the Search arm, the final correct diagnosis appeared in the DDx list only after assistance in 37 cases. In a small minority of cases in both arms (AMIE arm: 11 (a); Search arm: 12 (b)), the final diagnosis appeared in the DDx list before assistance but was not in the list after assistance. the integration of multimodal inputs by LLMs is an area of novel research22,23, with a large potential number of data modalities to consider and little precedent for how information from multiple modalities should be integrated over time for a single case by AI systems. The repeated examination of NEJM CPCs by automated systems highlights its promise as a ‘benchmark’ for evaluation and development of LLMs. Benchmarking enables comparisons of models with one another and the ability to evaluate a model’s performance improvements or degradation over time. However, consistency in using CPCs as a scalable benchmark is challenging if we are reliant on using human judgement to establish whether a candidate DDx matches the ground truth. We utilized an automated approach for comparing AMIE to a baseline LLM performance (GPT-4). Our estimates varied from recently published estimates in other studies, despite using the same subset of cases1. Direct comparisons of different technologies would ideally be conducted by more extensive and blinded human evaluation, including work to ensure reproducibility of the human evaluation protocol, analysis of inter-rater reliability and the use of metrics that reflect the quality, appropriateness and comprehensiveness of LLM differentials in addition to estimations of accuracy. Our estimates of top-1 and top-10 accuracy, although impressive at close to 30% and 60%, respectively, highlight noteworthy room for improvement for LLMs, especially for complex cases that are non-pathognomonic (that is, cases that do not have a sign or symptom that defines a diagnosis). However, as noted above, the CPCs represent ‘diagnostic puzzles’ rather than real-world examples of common clinical workflows, and it is therefore important to consider more realistic settings in which LLMs might prove of practical value in medicine. One such example is the potential for LLMs to assist clinicians in complex diagnoses. Deep learning tools have shown considerable promise in many areas of medicine, but are overwhelmingly used as assistive rather than autonomous tools24, given the safety-critical nature of medical practice and the many issues of robustness25 and fairness26–28 seen in deployment. Furthermore, observations of standalone diagnostic accuracy often do not guarantee that an AI tool will improve performance in real-world settings as an assistive tool, and it remains unclear how AI and human decision-making should be optimally integrated in medicine29. For LLMs in particular, the known incidence of hallucination and confabulation30 might mislead clinicians into inaccurate diagnosis, replicating or even extending findings in other clinical settings that AI systems might actually degrade the performance of clinicians rather than necessarily improving outcomes. This highlights the importance of focused study of LLMs in assistive scenarios. We explored this specifically in NEJM CPCs and found that AMIE increased the number of appropriate DDx produced by a clinician when used as an assistive tool in addition to overall top-n accuracy, suggesting that AMIE’s primary assistive potential may be due to making the scope of DDx more complete. Given the potential for misleading information to arise from AI systems, including in convincing dialogue, clinicians must appreciate the fundamental limitations of these models and not lose sight of their primacy in the provider–patient relationship and their ultimate authority and responsibility for the diagnostic and therapeutic management of their patients. Such thoughtful and effective LLM use should not be unintuitive to most clinicians. Aiding the diagnostic process could reasonably occur in an emergency room upon presentation (during potentially time-sensitive moments), upon admission to the medical ward, or by a consulting service after a patient has been admitted or in outpatient clinics. Our findings suggest that future research should more rigorously explore how LLMs augment clinicians’ DDx in many such specific scenarios, where the risks and benefits might vary. Despite being a novel tool, the use of AMIE did not seem to add inefficiency or increase the amount of time spent on solving each CPC compared with the use of Search or other conventional information. This suggests that the conversational interface was unobtrusive and intuitive. Consistent with this, the interviewed clinicians all described it as ‘easy’ to use, and were positive about the use and implications of the AMIE interface. Enhancing efficiency while maintaining or improving quality are generally accepted goals of improving healthcare delivery, alongside improving provider experience31, and our study showed significant potential in this regard, as clinicians also reported feeling more confident in their DDx lists after using the model. The clinicians described search becoming difficult when they did not know how to start or narrow down the query; qualitatively, the reports indicate that AMIE was easier to use in this regard. However, there are many human factors, social elements and other complex considerations in these use cases, and it is critical to ensure that efforts are made to avoid inequities in access to avoid exacerbating existing health disparities. Clinicians frequently expressed excitement about using AMIE, but were also aware of the shortcomings of language models and had concerns about confabulations in particular if used by individuals who were not trained or instructed to avoid such questions. However, our work did not explore many other important aspects of human–AI interaction, which require further study in safety-critical settings such as this. For example, we did not explore the extent to which clinicians n = 134 n = 37 n = 12 n = 156 70 70 60 60 Accuracy (%) Accuracy (%) 50 50 40 40 30 GPT-4 DDx AMIE DDx 30 GPT-4 DDx AMIE DDx 20 20 1 2 3 4 5 6 Top-k (MP2-rated) 7 8 9 10 1 2 3 4 5 6 Top-k (GPT-4-rated) 7 8 9 10 1 2 3 4 5 6 Top-k (GPT-4-rated) 7 8 9 10 Fig. 4 | Top-n accuracy in DDx lists from different LLMs. Comparison of the percentage of DDx lists that included the final diagnosis for AMIE versus GPT-4 for 70 cases. We used Med-PaLM 210, GPT-46 and AMIE as the raters—all resulted in similar trends. Points reflect the mean; shaded areas show ±1 s.d. from the mean across 10 trials. trusted the outputs of the model or their understanding of its training and limitations, or undertake focused ‘onboarding’ or training in its use, which are all known to be important modifiers of the benefits derived by clinicians from AI assistants32. The CPC challenges themselves do not enable a rigorous exploration of the possible effects of AI assistance on health equity and fairness; a further study of how these aspects of clinicians’ DDx is affected by LLM assistance is needed. AI systems are known to be able to express uncertainty33 and defer appropriately to clinicians34, which might significantly improve the balance between trust and skepticism needed for effective AI assistance in medicine. Qualitative feedback suggested that there remains room for targeted improvement of LLMs as assistive diagnostic tools, with one clinician noting that “It was most helpful for simpler cases that were specific keywords or pathognomonic signs”, but for more complex cases it still tended to draw conclusions from isolated symptoms rather than viewing the case holistically. The assistive effect of these LLMs could potentially ‘upskill’ clinical providers, particularly in enabling them to broaden and enhance the quality of their DDx. As corroborated via our clinician interviews after their experience with AMIE, such upskilling could be relevant for education or training purposes to support providers across a skill continuum ranging from trainees to attending providers. The upskilling capabilities could also extend to locations where specialist medical training is less common (such as in lower and middle income countries). However, our findings may not generalize to these scenarios, given that we utilized a pool of 20 clinicians with a mean experience of 11.5 years. This may not adequately represent the diverse set of users who are seeking to benefit from LLMs as a diagnostic aid. Our qualitative findings from semi-structured interviews with clinicians highlight the collaborative nature of the diagnostic reasoning process and the importance of clinical judgement when using an LLM. Whereas AMIE was capable of generating a broad DDx in isolation, the clinicians’ expertise enabled them to filter these suggestions when they were using the tool, discarding those they deemed to be inaccurate or irrelevant and leading to a more comprehensive and considered final differential list. This active evaluation and filtering process could explain the gap between standalone AMIE performance and clinician performance when assisted by the tool, with several specific factors highlighted: (1) anchoring bias: clinicians tended to anchor on their initial, unassisted DDx. This is consistent with known anchoring biases and might be exacerbated by the two-stage study design; (2) LLM suggestibility: several clinicians noted that AMIE could be led down alternative diagnostic paths by their follow-up questions and that this could lead to inaccurate conclusions that clinicians recognized as not being supported by the evidence; (3) trust calibration: clinicians highlighted the importance of the model being able to communicate when it is unsure, as this would probably have influenced the extent to which they trusted and incorporated AMIE’s suggestions. 70 60 Accuracy (%) 50 40 30 GPT-4 DDx AMIE DDx 20\n\nConclusion\nGenerating a DDx is a critical step in clinical case management, and the capabilities of LLMs present new opportunities for assistive tooling to help with this task. Tables 1 and 2 Our randomized study showed that AMIE was a helpful AI tool for DDx generation for generalist clinicians. Clinician participants indicated its utility for learning and education, and additional work is needed to understand its suitability for clinical settings.\n\nOnline content Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability 1. Kanjee, Z., Crowe, B. & Rodman, A. Accuracy of a generative artificial intelligence model in a complex diagnostic challenge. JAMA 330, 78–80 (2023). 2. Szolovits, P. & Pauker, S. G. Categorical and probabilistic reasoning in medical diagnosis. Artif. Intell. 11, 115–144 (1978). 3. Liu, Y. et al. A deep learning system for differential diagnosis of skin diseases. Nat. Med. 26, 900–908 (2020). 4. Rauschecker, A. M. et al. Artificial intelligence system approaching neuroradiologist-level differential diagnosis accuracy at brain MRI. Radiology 295, 626–637 (2020). 5. Balas, M. & Ing, E. B. Conversational AI models for ophthalmic diagnosis: comparison of ChatGPT and the Isabel pro differential diagnosis generator. JFO Op. Ophthalmol. 1, 100005 (2023). 6. GPT-4 Technical Report 2303.08774 (OpenAI, 2023). 7. (2023). 8. Scao, T. L. et al. BLOOM: a 176b-parameter open-access multilingual language model. 9. Touvron, H. et al. Llama 2: open foundation and fine-tuned chat models. Preprint at 10. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023). 11. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4 on medical 12. Eriksen, A. V., Moller, S. & Ryg, J. Use of GPT-4 to diagnose complex clinical cases. NEJM AI 13. Buckley, T., Diao, J. A., Rajpurkar, P., Rodman, A. & Manrai, A. K. Multimodal foundation 10.48550/arXiv.2311.05591 (2024). 14. 025-08866-7 (2025). 15. Bond, W. F. et al. Differential diagnosis generators: an evaluation of currently available computer programs. J. Gen. Intern. Med. 27, 213–219 (2012). 16. Ledley, R. S. & Lusted, L. B. Reasoning foundations of medical diagnosis: symbolic logic, probability, and value theory aid our understanding of how physicians reason. Science 130, 9–21 (1959). Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. © The Author(s) 2025\n\nNEJM CPC case reports The case records of the Massachusetts General Hospital (MGH) are published, lightly edited transcriptions of the CPCs of the MGH (Boston, MA). In the CPC, a patient case presentation is described and then an expert physician is asked to provide a DDx and a final diagnosis, along with their diagnostic reasoning, based only on the patient’s provided medical history and preliminary test results. The published cases, organized generally as diagnostic puzzles culminating in a definitive, pathology-confirmed diagnosis, are published regularly in the NEJM. We leverage these case reports, licensed from the NEJM, to evaluate AMIE’s capability to generate a DDx alone and, separately, to aid clinicians in generation of their own differential. For this latter task, we developed a user interface for clinicians to interact with AMIE. A set of 326 case texts from the NEJM CPC series were considered. These case reports were published over a 10-year period between the grounds that they discussed case management and were not primarily focused on diagnosis. The articles were distributed over the years between 2013–2023 as follows—2013: n = 22; 2014: n = 34; 2015: n = 36; 2016: n = 35; 2017: n = 36; 2018: n = 16; 2020: n = 23; 2021: n = 36; 2022: n = 39; 2023: n = 26. Supplementary Table 2 contains the full list of case reports, including the title, year and issue number of each report. The 302 cases include the 70 cases used by Kanjee et al.1. These case reports cover a range of medical specialties. The largest proportion are from internal medicine (n = 159), followed by neurology (n = 42), paediatrics (n = 33) and psychiatry (n = 10). The text corresponding to the history of the present illness (HPI) was manually extracted from each article as input to AMIE. The average (median) word count of these sections of the case reports is 1,031 words (mean: 1,044, s.d.: 296, range: 378–2,428). The average (median) character count is 6,619 characters (mean: 6,760, s.d.: 1,983, range: 2,426–15,196). A modified version of the article, inclusive of the provided HPI, admission imaging and admission labs (if available in the case) was created for the human clinicians (see Extended Data Fig. 1). This version had redacted the final diagnosis, expert discussion of the DDx and any subsequent imaging or biopsy results (which are typical elements of the conclusion of the case challenges). Given AMIE is a text-only AI model, the admission images and lab tables were not fed into the model. However, text-based descriptions of specific lab values or imaging findings were sometimes included in the case description. Training an LLM for DDx Our study introduces AMIE, a model that uses a transformer architecture (PaLM 27), fine-tuned on medical domain data; alongside an interface for enabling its use as an interactive assistant for clinicians. As with Med-PaLM 210, AMIE builds on PaLM 2, an iteration of Google’s LLM with substantial performance improvements on multiple LLM benchmark tasks. For the purposes of this analysis the large (L) PaLM 2 model was used. AMIE was fine-tuned with long context length on a task mixture consisting of medical question answering (multiple-choice and long-form questions), medical dialogue generation and electronic health record (EHR) note summarization. The datasets used included the training splits of MultiMedQA (MedQA, MedMCQA, HealthSearchQA, LiveQA and MedicationQA)10, a proprietary dataset of medical conversations, and expert handcrafted EHR note summaries from MIMIC-III35. The capability to process long context input enables AMIE to handle tasks that require long-range reasoning and comprehension. From MedQA (multiple-choice) we used US Medical Licensing Examination (USMLE) multiple-choice style open domain questions with four or five possible answers36. A set of 11,450 questions were used for training and 1,273 questions reserved for testing. We curated 191 Zero-shot prompting. We evaluated AMIE on each of the NEJM case studies with the following prompt: ‘You are a helpful medical assistant. You will be provided and asked about a complicated clinical case; read it carefully and then provide a diverse and thorough DDx’. Example input. Case: [Case Text] What are the top 10 most likely diagnoses? Be precise, listing one diag­ nosis per line, and try to cover many unique possibilities (at least 10). The top 10 diagnoses are: Example output.\n\n1.\tSézary syndrome\n\n2.\tAdult T-cell leukemia/lymphoma\n\n3.\tMycosis fungoides\n\n4.\tAtopic dermatitis\n\n5.\tPsoriasis\n\n6.\tDrug reaction with eosinophilia and systemic symptoms (DRESS)\n\n7.\tGraft-versus-host disease (GVHD)\n\n8.\tCutaneous T-cell lymphoma, not otherwise specified\n\n9.\tHypereosinophilic syndrome\n\n10. Systemic lupus erythematosus (SLE)\nThe AMIE user interface The interface associated with AMIE, depicted in Extended Data Fig. 2, enables users to interact with the underlying model via text-based chat in the context of a given case description. In our study, the interface was pre-populated with a text-only representation of the HPI for a given case. Clinicians were asked to initiate the interaction by querying AMIE using a suggested prompt. Following this initial prompt and AMIE’s response, clinicians were free to query the model using any additional follow-up questions, though clinicians were cautioned to avoid asking questions about information that had not already been presented in the case. A pilot study indicated that without such a warning, clinicians may ask questions about specific lab values or imaging leading to confabulations. For a given question, the interface generated the response by querying AMIE using the following prompt template: Read the case below and answer the question provided after the case. Format your response in markdown syntax to create paragraphs and bullet points. Use ‘<br><br>’ to start a new paragraph. Each paragraph should be 100 words or less. Use bullet points to list multiple options. Use ‘<br>*’ to start a new bullet point. Emphasize important phrases like headlines. Use ‘**’ right before and right after a phrase to emphasize it. There must be NO space in between ‘**’ and the phrase you try to emphasize. Case:[Case Text] Question (suggested initial question is ‘What are the top 10 most likely diagnoses and why (be precise)?’): [Question] Answer: Experimental design In order to comparatively evaluate AMIE’s ability to generate a DDx alone and aid clinicians with their DDx generation we designed a two-stage reader study illustrated in Extended Data Fig. 3. Our study was designed to evaluate the assistive effect of AMIE for generalist clinicians (not specialists) who only have access to the case presentation and not the full case information (which would include the expert commentary on the DDx). The first stage of the study had a counterbalanced design with two conditions. Clinicians generated DDx lists first without assistance and then a second time with assistance, where the type of assistance varied by condition. Stage 1: Clinicians generate DDx with and without assistance. Twenty U.S. board-certified internal medicine physicians (median years of experience: 9, mean: 11.5, s.d.: 7.24, range: 3–32) viewed the redacted case report, with access to the case presentation and associated figures and tables. They did this task in one of two conditions, based on random assignment. Condition I: Search. The clinicians were first instructed to provide a list of up to ten diagnoses, with a minimum of three, based solely on review of the case presentation without using any reference materials (for example, books) or tools (for example, internet search). Following this, the clinicians were instructed to use internet search or other resources as desired (but not given access to AMIE) and asked to re-perform their DDx. Condition II: AMIE. As with condition I, the clinicians were first instructed to provide a list of up to ten diagnoses, with a minimum of three, based solely on review of the case presentation without using any reference materials (for example, books) or tools (for example, internet search). Following this the clinicians were given access to AMIE and asked to re-perform their DDx. In addition to AMIE, clinicians could choose to use internet search or other resources if they wished. For the assignment process, we formed ten pairs of two clinicians each, grouping clinicians with similar years of post-residency experience together. The set of all cases was then randomly split into ten partitions, and each clinician pair was assigned to one of the ten case partitions. Within each partition, each case was completed once in condition I by one of the two clinicians, and once in condition II by the other clinician. For each case, the assignment of which clinician among the pair was exposed to which of the two experimental conditions was randomized. Pairing clinicians with similar post-residency experience to complete the same case served to reduce variability between the two distinct experimental conditions. Stage 2. Specialists with full case information extract gold DDx and evaluate Stage 1 DDx Nineteen U.S. board-certified specialist clinicians (median years of experience: 14, mean: 13.7, s.d.: 7.82, range: 4–38) were recruited from internal medicine (n = 10), neurology (n = 3), paediatrics (n = 2), psychiatry (n = 1), dermatology (n = 1), obstetrics (n = 1), and emergency medicine (n = 1). Their mean years of experience was 13.7 (s.d.: 7.82, range: 4–38). These specialists were aligned with the specialty of the respective CPC case, viewed the full case report and were asked to list at least five and up to ten differential diagnoses. Following this, they were asked to evaluate the five DDx lists generated in stage 1, including two DDx lists from condition 1 (DDx without assistance and DDx with Search assistance), two DDx lists from condition 2 (DDx without assistance and DDx with AMIE assistance) and the standalone AMIE DDx list. One specialist reviewed each case. The specialists answered the following questions to evaluate the DDx lists: The quality score developed by Bond et al.15 and used by Kanjee et al.1 is a differential score based on an ordinal five-point scale: ‘How close did the differential diagnoses (DDx) come to including the final diagnosis?’ Clinician incentives. Clinicians were recruited and remunerated by vendor companies at market rates based on speciality, without specific incentives such as diagnostic accuracy or other factors. Automated evaluation. In addition to comparing against ground truth diagnosis and expert evaluation from clinicians, we also created an automated evaluation of the performance of the five DDxs using a language model-based metric. Such automated metrics are useful as human evaluation is time and cost-prohibitive for many experiments. We first extracted the (up to ten) individual diagnoses listed in each DDx. We leveraged minor text-processing steps via regular expressions to separate the outputs by newlines and strip any numbering before the diagnoses. Then we asked a medically fine-tuned language model, Med-PaLM 210, whether or not each of these diagnoses was the same as the ground truth diagnosis using the following prompt: Is our predicted diagnosis correct (y/n)? Predicted diagnosis: [diagnosis], True diagnosis: [label] Answer [y/n]. A diagnosis was marked as correct if the language model output ‘y’. We computed Cohen’s kappa as a measure of agreement between human raters and automated evaluation with respect to the binary decision of whether a given diagnosis—that is, an individual item from a proposed DDx list—matched the correct final diagnosis. Cohen’s kappa for this matching task was 0.631, indicating ‘substantial agreement’ between human raters and our automated evaluation method, per Landis & Koch37. Qualitative interviews Following the study we performed a semi-structured 30-min interviews with 5 of the generalist clinicians who participated in stage 1. Semi-structured interviews explored the following questions: (1) How did you find the task of generating a DDx from the case report text? (2) Think about how you used Internet search or other resources. How were these tools helpful or unhelpful? (3) Think about how you used the AMIE. How was it helpful or unhelpful? (4) Were there cases where you trusted the output of the search queries? Tell us more about the experience if so, such as types of cases, types of search results. (5) Were there cases where you trusted the output of the LLM queries? Tell us more about the experience if so, such as types of cases, types of search results. (6) Think about the reasoning provided by the LLM’s interface? Where were they helpful? Where were they unhelpful? We conducted a thematic analysis of notes from interviews taken by researchers during the interviews, employing an inductive approach to identify patterns (themes) within the data. Initial codes were generated through a line-by-line review of the notes, with attention paid to both semantic content and latent meaning. Codes were then grouped based on conceptual similarity, and refined iteratively. To enhance the trustworthiness of the analysis, peer debriefing was conducted within the team of researchers. Through discussion and consensus, the final themes were agreed upon. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability The case reports used in this study are published and were licensed from the New England Journal of Medicine. We are not able to re-distribute the copyrighted material, but the case texts can be obtained from the journal. Code availability AMIE is an LLM-based research AI system for diagnostic dialogue. We are not making the model code and weights open source owing to the safety implications of unmonitored use of such a system in medical settings. In the interest of responsible innovation, we will be working with research partners, regulators and providers to validate and explore safe onward uses of AMIE. For reproducibility, we have documented Acknowledgements This project was an extensive collaboration between many teams at Google Research and Google DeepMind. We thank A. Jain, R. Sayres, S. Lachgar, L. Winer,", "persona": "Student", "response": "### 1. Overview\nImagine an AI system that not only helps doctors with difficult diagnoses but even surpasses them in accuracy. In a groundbreaking study published in Nature, Google Research unveiled AMIE (Articulate Medical Intelligence Explorer), a specialized large language model fine-tuned on vast clinical datasets. Tested on complex real-world cases, AMIE didn't just assist — it outperformed experienced physicians, correctly listing diagnoses nearly twice as often. Even more remarkably, a companion study showed AMIE excelling in diagnostic conversations with simulated patients, rated higher than doctors in both quality and empathy. But is this the dawn of AI doctors replacing humans? Not quite. While AMIE demonstrates astonishing prowess in pattern recognition and reasoning over text-based data, real-world medicine involves human interaction, physical exams, and nuanced decision-making far beyond data crunching. Still, with proper integration, systems like AMIE could dramatically reduce diagnostic burdens, streamline healthcare, and expand access — especially in underserved areas.\n\n### 2. Background\nThe Surprising Results This second paper is here: Conclusion These developments aren't just futuristic speculation; they demand immediate attention from medical associations, policymakers, and educators alike. Google's New AI System Outperforms Physicians in Complex Diagnoses Imagine going to the doctor with a baffling set of symptoms. Getting the right diagnosis quickly is crucial, but sometimes even experienced physicians face challenges piecing together the puzzle. Sometimes it might not be something serious at all; others a deep investigation might be required.\n\n### 3. Methods\nThe findings were striking. When AMIE worked alone, just analyzing the text of the case reports, its diagnostic accuracy was significantly higher than that of experienced physicians working without assistance! AMIE included the correct diagnosis in its top-10 list almost 60% of the time, compared to about 34% for the unassisted doctors.\n\n### 4. Results\nNote that the paper itself indicates that AMIE outperformed GPT-4 on automated evaluations for this task, highlighting the benefits of domain-specific optimization. Notably too, but negatively, the paper does not compare AMIE's performance against other general LLMs, not even Google's own \"smart\" models like Gemini 2.5 Pro. That's quite disappointing, and I can't understand how the reviewers of this paper overlooked this! The analysis of the data produced in the study involved multiple metrics beyond simple accuracy, most notably the top-n accuracy (which asks: was the correct diagnosis in the top 1, 3, 5, or 10?), quality scores (how close was the list to the final diagnosis?), appropriateness, and comprehensiveness — the latter two rated by independent specialist physicians blinded to the source of the diagnostic lists. And the results aren't by a small margin: In 159 simulated cases, specialist physicians rated the AI superior to primary care physicians on 30 out of 32 metrics, while test patients preferred the AMIE on 25 of 26 measures.\n\n### 5. Takeaways\nHow was AMIE trained to achieve such mastery? How did it fare against tools like Google search and even GPT-4? What ethical, practical, and legal challenges does this innovation raise? In my full deep dive, I break down exactly how AMIE works, what the studies revealed, and what all of us — patients, doctors, and policymakers — need to start thinking about now. Very intriguingly, and in favor of the AI system, AMIE alone slightly outperformed doctors who were assisted by AMIE itself! While doctors using AMIE improved their accuracy significantly compared to using standard tools like Google searches (reaching over 51% accuracy), the AI on its own still edged them out slightly on this specific metric for these challenging cases. Measuring Performance Measuring performance and accuracy in the produced diagnoses isn't trivial, and is interesting for you reader with a Data Science mindset. In their work, the researchers didn't just assess AMIE in isolation; rather they employed a randomized controlled setup whereby AMIE was compared against unassisted clinicians, clinicians assisted by standard search tools (like Google, PubMed, etc.), and clinicians assisted by AMIE itself (who could also use search tools, though they did so less often). This wide evaluation provides a more robust picture than a single accuracy number; and the comparison against both unassisted performance and standard tools helps quantify the actual added value of the AI. The finding that AMIE alone slightly outperformed the AMIE-assisted human experts is puzzling. Logically, adding a skilled doctor's judgment to a powerful AI should yield the best results (as previous studies with have shown, in fact). And indeed, doctors with AMIE did significantly better than doctors without it, producing more comprehensive and accurate diagnostic lists.", "paper_id": "59", "story_id": "59-03"}
{"paper": "A comprehensive diferential diagnosis is a cornerstone of medical care that is often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by large language models present new opportunities to assist and automate aspects of this process1 . Here we introduce the Articulate Medical Intelligence Explorer (AMIE), a large language model that is optimized for diagnostic reasoning, and evaluate its ability to generate a diferential diagnosis alone or as an aid to clinicians. Twenty clinicians evaluated 302 challenging, real-world medical cases sourced from published case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: assistance from search engines and standard medical resources; or assistance from AMIE in addition to these tools. All clinicians provided a baseline, unassisted diferential diagnosis prior to using the respective assistive tools. AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% versus 33.6%, P = 0.04). Comparing the two assisted study arms, the diferential diagnosis quality score was higher for clinicians assisted by AMIE (top-10 accuracy 51.7%) compared with clinicians without its assistance (36.1%; McNemar’s test: 45.7, P < 0.01) and clinicians with search (44.4%; McNemar’s test: 4.75, P = 0.03). Further, clinicians assisted by AMIE arrived at more comprehensive diferential lists than those without assistance from AMIE. Our study suggests that AMIE has potential to improve clinicians’ diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients’ access to specialist-level expertise.\n\nAn accurate diagnosis is a critical component of effective medical care. Building artificial intelligence (AI) systems that are capable of performing or assisting clinicians in this important task has been a long-standing grand challenge2 . Whereas prior focus has been on evaluating a machine’s ability to accurately output a diagnosis1,3–5 , real-world clinical practice involves an iterative and interactive process of reasoning about a differential diagnosis (DDx), weighing multiple diagnostic possibilities in the light of increasing amounts of clinical information over time. Deep learning has been applied to promising effect for generating DDx in a number of specialties including radiology4 , ophthalmology5  and dermatology3 , but such systems lack the interactive capabilities to fluently assist a user through communication in natural language. The emergence of large language models (LLMs) presents an opportunity to design novel interactive tools and interfaces to aid DDx. These models have demonstrated the ability to perform complex language comprehension and reasoning tasks, generating coherent text and thereby enabling a large variety of real-world applications6–9 . Both general-purpose LLMs (GPT-4) and medical domain-specialized LLMs (Med-PaLM 2) have demonstrated strong performance in standardized and multiple-choice medical benchmarks10,11. Such evaluations represent a natural starting point for probing the model’s medical knowledge and capabilities but do not measure utility in real-world scenarios for care delivery—for example, in challenging medical cases faced by trained physicians. It is also not obvious how these models might actively assist clinicians in the development of a DDx. Recent work has begun to assess the standalone performance of these models on challenging case reports that involve complex deduction and diagnosis1,12–14, but has stopped short of evaluating how they can assist clinicians, augment performance and empower them to provide better care. Here we introduce AMIE, an LLM that is optimized for clinical diagnostic reasoning to generate a DDx for challenging, real-world medical cases. Beyond measuring standalone performance, we integrated this model into an interactive interface to measure how well AMIE could assist clinicians in developing a DDx. Using a set of challenging real-world case reports from the New England Journal of Medicine (NEJM) clinicopathological conferences (CPCs), we compared clinicians’ ability to form a DDx with the assistance of AMIE versus with access to traditional information retrieval tools (such as internet searches and books). AMIE achieved impressive performance in both generating DDx lists that contained the correct diagnosis (top-10 accuracy) and in identifying the correct final diagnosis as the most likely in the list (top-1 accuracy). Under automated model-based evaluation, the quality and the accuracy of the DDx list produced by AMIE was found to be significantly better than the state-of-the-art GPT-4 model available at the time of the experiments1 . Perhaps more importantly, AMIE also improved the diagnostic capability of clinicians as measured by the quality of their DDx lists for the evaluated cases. LLMs optimized for the safety-critical medical domain such as ours present a novel paradigm for assisting clinicians because of the potential for variation in the ways in which a given individual may converse with the system and utilize it in collaborative reasoning. A detailed explanation of the cases, their components, how they were fed to the model, the randomization scheme of AMIE versus the standard practice and information on the expert raters of the model and how the outputs were evaluated by blind expert raters, can be found in Methods. In evaluating the quality of the DDx lists we used several criteria, inspired by the approach taken in ref. 1 and extended to draw additional insight from the clinicians. First, we measured whether the final diagnosis matched an entry in the DDx list and in which position (top-n accuracy). Second, we used the quality score from Bond et al.15 and created appropriateness and comprehensiveness scales. Combined, these measures assess overall DDx quality, appropriateness and comprehensiveness. When using AMIE for assistance, clinicians asked, on average (mean), 2.92 questions in the interface (median 2, interquartile range (IQR) 1–4). On average (mean), clinician questions consisted of 9.39 words (median 10, IQR 6–12) and 54.31 characters (median 61, IQR 39-63). AMIE’s responses, on average (mean), consisted of 237.60 words (median 198, IQR 127–332) and 1,540.81 characters (median 1,276; IQR 815–2210). In the Search condition, the most popular tools were UpToDate (used in 34% of tasks), Google Search (30%) and PubMed (22%). Although clinicians were allowed to use additional tools in the AMIE condition, this was far less frequent (less than 5% of tasks).\n\nDDx performance of AMIE The DDx lists produced by our language model achieved strong quality, appropriateness and comprehensiveness scores (see Fig. 1). The median quality score was 5 (‘DDx includes the correct diagnosis’) with 54% of DDx lists achieving that score. The number of cases that scored 5 (that is, the DDx included the top diagnosis) was statistically significantly higher for AMIE compared with clinicians without assistance (McNemar’s test: 64.4, P < 0.01). The mean appropriateness score was 4.43 out of 5 (s.d. 0.92). The median comprehensiveness score was 4 (‘The DDx contains all candidates that are reasonable’) with 55% of the DDx lists achieving that score. The mean appropriateness score of AMIE (4.34) was significantly higher than that of unassisted clinicians (3.74) (paired t-test 8.52, P < 0.001, Wilcoxon signed-rank test: 2,857.5, P < 0.001) and assisted clinicians in either the Search (3.80) (paired t-test 7.23, P < 0.001, Wilcoxon signed-rank test: 3,308.5, P < 0.001) or LLM (4.06) (paired t-test 4.98, P < 0.001, P < 0.001, Wilcoxon signed-rank test: 2,752.0, P < 0.001) conditions. For computing top-n accuracy, if any of the first n diagnoses in an individual DDx were marked correct by the language model, the differential was considered to be correct. We computed the proportion of correct DDx lists across all cases to compute the top-n accuracy (for n from 1 to 10) for each DDx. AMIE reliably generated DDx lists that perform well against the ground truth diagnosis (Fig. 2). AMIE provided the correct diagnosis in 177 (59%) of the DDx lists and in 89 (29%) of the lists it was at the top of the list. These scores are higher than those achieved by the clinicians in any of the conditions. The top-10 accuracy of AMIE (59.1%) was significantly higher than the top-10 accuracy for the unassisted clinicians (33.6%) (P = 0.04) (Tables 1 and 2). Figure 2 shows the top-n accuracy based on human and the automated metric. The results are broadly similar, illustrating that despite the final diagnoses often being complex and nuanced, the automated metric faithfully captures the distinction between a DDx list that includes the correct diagnosis and one that does not. The clinicians in the study were not required to give a full list of ten diagnoses for every case. Clinicians in conditions I and II were required to given a minimum of three diagnoses. The median number provided was six. The performance at n = 6 is of particular relevance. Not all clinicians provided six diagnoses, as a result we conducted a variable top-n experiment, where for each case n was set to the number of diagnoses provided by the human clinicians. The variable top-n performance of AMIE was 59.4%—this is similar to the performance at n = 9 and at n = 10. As a result, AMIE’s output of a full list of ten diagnoses did not place it at an advantage compared to the clinicians. AMIE as a DDx assistant Of the DDx lists created before assistance 37% (Search condition) and 29% (AMIE condition) achieved a quality score of 5 (Fig. 1). For comparison, 49% of those created with assistance from AMIE scored 5. The number of cases that scored 5 (that is, the DDx included the top diagnosis) was statistically higher for clinicians assisted by AMIE compared with clinicians without assistance (McNemar’s test: 48.3, P < 0.01) and clinicians with Search assistance (5.45, P = 0.02). For comprehensiveness, the number of cases that scored 4 (that is, The DDx contains all candidates that are reasonable) was statistically higher for clinicians assisted by AMIE compared with clinicians without assistance (McNemar’s test: 185.8, P < 0.01) and clinicians with Search assistance (185.8, P < 0.01). As a consistency check, the number of cases that scored 4 was not statistically higher for clinicians in the Search condition (I) baseline and AMIE condition (II) baseline (McNemar’s test: 1.47, P = 0.23). The mean appropriateness score after assistance with AMIE (4.06) was significantly higher than after assistance with Search (3.80) (paired t-test 3.32, P = 0.001) and the baseline (3.74) (paired t-test 4.79, P < 0.001). To summarize, with the support of AMIE, the quality, appropriateness and comprehensiveness scores for the DDx lists were greater than for the lists prior to assistance (see Fig. 1). The top-n accuracy of the clinicians increased with assistance from AMIE compared to without (see Fig. 2). A Sankey diagram illustrates the effect of the two forms of assistance (Search and AMIE) on top-10 accuracy (Fig. 3). In the AMIE condition, 73 cases that did not feature the final diagnosis prior to using the tool included it after assistance from AMIE. This result is in contrast to only 37 cases in the Search condition. Comparing the two assisted study arms, the DDx quality score was higher for clinicians assisted by AMIE (top-10 accuracy 51.7%) compared with clinicians without its assistance (36.1%) (McNemar’s test: 45.7, P < 0.01) and clinicians with search (44.4%) (4.75, P = 0.03). Task duration with AMIE and Search The time taken to generate updated DDx lists in the Search condition versus the AMIE condition were similar (Search: 7.19 ± 5.33 min, AMIE: 7.29 ± 6.41 min (mean ± s.d.)). These were not significantly different (paired t-test P = 0.807), which is surprising as the clinicians all had experience using internet search and other information retrieval tools, yet they were using the AMIE interface for the first time. We had hypothesized that they would take longer using AMIE owing to the initial learning curve. Length of DDx lists with AMIE and Search When unassisted, the median length of the DDx lists was 6 (IQR 5–9); the mean was 6.41 (s.d. 2.39). With search the median DDx list length was 7 (IQR 5–10); the mean was 6.92 (s.d. 2.52). With AMIE, the median DDx list length was 8 (IQR 6–10); the mean was 7.58 (s.d. 2.33). With assistance from AMIE, the length of the DDx lists was longer than without assistance (paired t-test: 7.13, P < 0.001) and longer than the DDx lists with assistance from search (paired t-test: 3.15, P = 0.002). AMIE comparison with GPT-4 As we did not have the same set of human raters who evaluated the differentials produced by GPT-41  and AMIE, we cannot compare top-10 accuracy numbers directly. Therefore, in our study design, we evaluate performance on that 70-case subset (reported in ref. 1) using the automated metric (which is shown above to be relatively consistent with human evaluation). AMIE performs better with regard to top-n accuracy for n > 1, with the gap being most prominent for n > 2 (Fig. 4). This suggests potentially significant improvements in quality and comprehensiveness of the differentials produced by AMIE. For n = 1, GPT-4 performs marginally better but not statistically significantly\n\nDiscussion\nWe used a popular series of complex diagnostic challenges to evaluate an LLM optimized for clinical reasoning and diagnosis (AMIE); both in a standalone capacity and under randomized comparisons as an assistive tool for physicians. In standalone performance, AMIE generated more appropriate and comprehensive DDx lists than physicians when they were unassisted, with its DDx lists being more likely to include the final diagnosis than DDx lists from a board-certified internal medicine physician, regardless of what position in the DDx list was considered (that is, top-n accuracy with n ranging from 1 to 10). Clinicians using Table 2 | Top-1 and top-10 accuracy of DDx lists produced with AMIE and Search assistance by speciality Model only Human AMIE Before assistance After Search assistance After AMIE assistance Metrics Top-1↑ Top-10↑ Top-1↑ Top-10↑ Top-1↑ Top-10↑ Top-1↑ Top-10↑ AMIE as an assistant produced a DDx with higher top-n accuracy, and DDx with greater quality, appropriateness and comprehensiveness compared with the status quo for clinical practice (use of internet search and other resources). The NEJM CPCs examined here are well-known for being unique and challenging clinical conundrums. Within this distinctive setting, AMIE outperformed an unassisted board-certified physician in both top-1 and top-n accuracy. Whereas the CPCs have long been used as benchmarks for difficult diagnosis, it is also well-known that performance in CPCs in no way reflects a broader measure of competence in a physician’s duties16. Furthermore, the act of forming a DDx comprises many other steps that are not scrutinized in this study, including the goal-directed acquisition of information under uncertainty (which is known to be challenging for AI systems despite recent technical progress in this direction17–19). We are therefore very cautious in extrapolating our findings towards any implications about the utility of AMIE as a standalone diagnostic tool. Nevertheless, our controlled evaluation mirrored the findings of other recent works exploring the performance of LLMs and pre-LLM ‘DDx generators’ in smaller subsets of the NEJM CPCs, which have shown the potential for automated technology to reach the correct DDx with superior performance to standalone physicians in these challenging cases1,12,13,20. Although this represents a step beyond historical attempts at automating DDx in NEJM CPCs, in which computerized approaches were deemed overtly unreliable for practical use21, such studies also undertook limited consideration of the quality of DDx generated by these automated systems or their role as assistive tools. Our work extends previous observations by showing not only that AMIE was more likely to arrive at a correct answer or provide the correct answer in a list, but also that its DDx were determined by an independent rater to be of higher appropriateness and comprehensiveness than those produced by board-certified physicians with access to references and search. In our study, clinicians had access to both images and tabular data in redacted case reports, whereas AMIE was only provided with the main body of the text. Although AMIE outperformed the clinicians despite this limitation, it is unknown whether and how much this gap would widen if AMIE had access to the figures and tables. Furthermore, n = 94 n = 83 n = 109 n = 97 n = 156 n = 73 n = 11 n = 208 n = 146 n = 193 n = 168 n = 135 Not in DDx list In DDx list Fig. 3 | Sankey diagram showing effect of assistance. a, In the AMIE arm, the final correct diagnosis appeared in the DDx list only after assistance in 73 cases. b, In the Search arm, the final correct diagnosis appeared in the DDx list only after assistance in 37 cases. In a small minority of cases in both arms (AMIE arm: 11 (a); Search arm: 12 (b)), the final diagnosis appeared in the DDx list before assistance but was not in the list after assistance. the integration of multimodal inputs by LLMs is an area of novel research22,23, with a large potential number of data modalities to consider and little precedent for how information from multiple modalities should be integrated over time for a single case by AI systems. The repeated examination of NEJM CPCs by automated systems highlights its promise as a ‘benchmark’ for evaluation and development of LLMs. Benchmarking enables comparisons of models with one another and the ability to evaluate a model’s performance improvements or degradation over time. However, consistency in using CPCs as a scalable benchmark is challenging if we are reliant on using human judgement to establish whether a candidate DDx matches the ground truth. We utilized an automated approach for comparing AMIE to a baseline LLM performance (GPT-4). Our estimates varied from recently published estimates in other studies, despite using the same subset of cases1. Direct comparisons of different technologies would ideally be conducted by more extensive and blinded human evaluation, including work to ensure reproducibility of the human evaluation protocol, analysis of inter-rater reliability and the use of metrics that reflect the quality, appropriateness and comprehensiveness of LLM differentials in addition to estimations of accuracy. Our estimates of top-1 and top-10 accuracy, although impressive at close to 30% and 60%, respectively, highlight noteworthy room for improvement for LLMs, especially for complex cases that are non-pathognomonic (that is, cases that do not have a sign or symptom that defines a diagnosis). However, as noted above, the CPCs represent ‘diagnostic puzzles’ rather than real-world examples of common clinical workflows, and it is therefore important to consider more realistic settings in which LLMs might prove of practical value in medicine. One such example is the potential for LLMs to assist clinicians in complex diagnoses. Deep learning tools have shown considerable promise in many areas of medicine, but are overwhelmingly used as assistive rather than autonomous tools24, given the safety-critical nature of medical practice and the many issues of robustness25 and fairness26–28 seen in deployment. Furthermore, observations of standalone diagnostic accuracy often do not guarantee that an AI tool will improve performance in real-world settings as an assistive tool, and it remains unclear how AI and human decision-making should be optimally integrated in medicine29. For LLMs in particular, the known incidence of hallucination and confabulation30 might mislead clinicians into inaccurate diagnosis, replicating or even extending findings in other clinical settings that AI systems might actually degrade the performance of clinicians rather than necessarily improving outcomes. This highlights the importance of focused study of LLMs in assistive scenarios. We explored this specifically in NEJM CPCs and found that AMIE increased the number of appropriate DDx produced by a clinician when used as an assistive tool in addition to overall top-n accuracy, suggesting that AMIE’s primary assistive potential may be due to making the scope of DDx more complete. Given the potential for misleading information to arise from AI systems, including in convincing dialogue, clinicians must appreciate the fundamental limitations of these models and not lose sight of their primacy in the provider–patient relationship and their ultimate authority and responsibility for the diagnostic and therapeutic management of their patients. Such thoughtful and effective LLM use should not be unintuitive to most clinicians. Aiding the diagnostic process could reasonably occur in an emergency room upon presentation (during potentially time-sensitive moments), upon admission to the medical ward, or by a consulting service after a patient has been admitted or in outpatient clinics. Our findings suggest that future research should more rigorously explore how LLMs augment clinicians’ DDx in many such specific scenarios, where the risks and benefits might vary. Despite being a novel tool, the use of AMIE did not seem to add inefficiency or increase the amount of time spent on solving each CPC compared with the use of Search or other conventional information. This suggests that the conversational interface was unobtrusive and intuitive. Consistent with this, the interviewed clinicians all described it as ‘easy’ to use, and were positive about the use and implications of the AMIE interface. Enhancing efficiency while maintaining or improving quality are generally accepted goals of improving healthcare delivery, alongside improving provider experience31, and our study showed significant potential in this regard, as clinicians also reported feeling more confident in their DDx lists after using the model. The clinicians described search becoming difficult when they did not know how to start or narrow down the query; qualitatively, the reports indicate that AMIE was easier to use in this regard. However, there are many human factors, social elements and other complex considerations in these use cases, and it is critical to ensure that efforts are made to avoid inequities in access to avoid exacerbating existing health disparities. Clinicians frequently expressed excitement about using AMIE, but were also aware of the shortcomings of language models and had concerns about confabulations in particular if used by individuals who were not trained or instructed to avoid such questions. However, our work did not explore many other important aspects of human–AI interaction, which require further study in safety-critical settings such as this. For example, we did not explore the extent to which clinicians n = 134 n = 37 n = 12 n = 156 70 70 60 60 Accuracy (%) Accuracy (%) 50 50 40 40 30 GPT-4 DDx AMIE DDx 30 GPT-4 DDx AMIE DDx 20 20 1 2 3 4 5 6 Top-k (MP2-rated) 7 8 9 10 1 2 3 4 5 6 Top-k (GPT-4-rated) 7 8 9 10 1 2 3 4 5 6 Top-k (GPT-4-rated) 7 8 9 10 Fig. 4 | Top-n accuracy in DDx lists from different LLMs. Comparison of the percentage of DDx lists that included the final diagnosis for AMIE versus GPT-4 for 70 cases. We used Med-PaLM 210, GPT-46 and AMIE as the raters—all resulted in similar trends. Points reflect the mean; shaded areas show ±1 s.d. from the mean across 10 trials. trusted the outputs of the model or their understanding of its training and limitations, or undertake focused ‘onboarding’ or training in its use, which are all known to be important modifiers of the benefits derived by clinicians from AI assistants32. The CPC challenges themselves do not enable a rigorous exploration of the possible effects of AI assistance on health equity and fairness; a further study of how these aspects of clinicians’ DDx is affected by LLM assistance is needed. AI systems are known to be able to express uncertainty33 and defer appropriately to clinicians34, which might significantly improve the balance between trust and skepticism needed for effective AI assistance in medicine. Qualitative feedback suggested that there remains room for targeted improvement of LLMs as assistive diagnostic tools, with one clinician noting that “It was most helpful for simpler cases that were specific keywords or pathognomonic signs”, but for more complex cases it still tended to draw conclusions from isolated symptoms rather than viewing the case holistically. The assistive effect of these LLMs could potentially ‘upskill’ clinical providers, particularly in enabling them to broaden and enhance the quality of their DDx. As corroborated via our clinician interviews after their experience with AMIE, such upskilling could be relevant for education or training purposes to support providers across a skill continuum ranging from trainees to attending providers. The upskilling capabilities could also extend to locations where specialist medical training is less common (such as in lower and middle income countries). However, our findings may not generalize to these scenarios, given that we utilized a pool of 20 clinicians with a mean experience of 11.5 years. This may not adequately represent the diverse set of users who are seeking to benefit from LLMs as a diagnostic aid. Our qualitative findings from semi-structured interviews with clinicians highlight the collaborative nature of the diagnostic reasoning process and the importance of clinical judgement when using an LLM. Whereas AMIE was capable of generating a broad DDx in isolation, the clinicians’ expertise enabled them to filter these suggestions when they were using the tool, discarding those they deemed to be inaccurate or irrelevant and leading to a more comprehensive and considered final differential list. This active evaluation and filtering process could explain the gap between standalone AMIE performance and clinician performance when assisted by the tool, with several specific factors highlighted: (1) anchoring bias: clinicians tended to anchor on their initial, unassisted DDx. This is consistent with known anchoring biases and might be exacerbated by the two-stage study design; (2) LLM suggestibility: several clinicians noted that AMIE could be led down alternative diagnostic paths by their follow-up questions and that this could lead to inaccurate conclusions that clinicians recognized as not being supported by the evidence; (3) trust calibration: clinicians highlighted the importance of the model being able to communicate when it is unsure, as this would probably have influenced the extent to which they trusted and incorporated AMIE’s suggestions. 70 60 Accuracy (%) 50 40 30 GPT-4 DDx AMIE DDx 20\n\nConclusion\nGenerating a DDx is a critical step in clinical case management, and the capabilities of LLMs present new opportunities for assistive tooling to help with this task. Tables 1 and 2 Our randomized study showed that AMIE was a helpful AI tool for DDx generation for generalist clinicians. Clinician participants indicated its utility for learning and education, and additional work is needed to understand its suitability for clinical settings.\n\nOnline content Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability 1. Kanjee, Z., Crowe, B. & Rodman, A. Accuracy of a generative artificial intelligence model in a complex diagnostic challenge. JAMA 330, 78–80 (2023). 2. Szolovits, P. & Pauker, S. G. Categorical and probabilistic reasoning in medical diagnosis. Artif. Intell. 11, 115–144 (1978). 3. Liu, Y. et al. A deep learning system for differential diagnosis of skin diseases. Nat. Med. 26, 900–908 (2020). 4. Rauschecker, A. M. et al. Artificial intelligence system approaching neuroradiologist-level differential diagnosis accuracy at brain MRI. Radiology 295, 626–637 (2020). 5. Balas, M. & Ing, E. B. Conversational AI models for ophthalmic diagnosis: comparison of ChatGPT and the Isabel pro differential diagnosis generator. JFO Op. Ophthalmol. 1, 100005 (2023). 6. GPT-4 Technical Report 2303.08774 (OpenAI, 2023). 7. (2023). 8. Scao, T. L. et al. BLOOM: a 176b-parameter open-access multilingual language model. 9. Touvron, H. et al. Llama 2: open foundation and fine-tuned chat models. Preprint at 10. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023). 11. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4 on medical 12. Eriksen, A. V., Moller, S. & Ryg, J. Use of GPT-4 to diagnose complex clinical cases. NEJM AI 13. Buckley, T., Diao, J. A., Rajpurkar, P., Rodman, A. & Manrai, A. K. Multimodal foundation 10.48550/arXiv.2311.05591 (2024). 14. 025-08866-7 (2025). 15. Bond, W. F. et al. Differential diagnosis generators: an evaluation of currently available computer programs. J. Gen. Intern. Med. 27, 213–219 (2012). 16. Ledley, R. S. & Lusted, L. B. Reasoning foundations of medical diagnosis: symbolic logic, probability, and value theory aid our understanding of how physicians reason. Science 130, 9–21 (1959). Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. © The Author(s) 2025\n\nNEJM CPC case reports The case records of the Massachusetts General Hospital (MGH) are published, lightly edited transcriptions of the CPCs of the MGH (Boston, MA). In the CPC, a patient case presentation is described and then an expert physician is asked to provide a DDx and a final diagnosis, along with their diagnostic reasoning, based only on the patient’s provided medical history and preliminary test results. The published cases, organized generally as diagnostic puzzles culminating in a definitive, pathology-confirmed diagnosis, are published regularly in the NEJM. We leverage these case reports, licensed from the NEJM, to evaluate AMIE’s capability to generate a DDx alone and, separately, to aid clinicians in generation of their own differential. For this latter task, we developed a user interface for clinicians to interact with AMIE. A set of 326 case texts from the NEJM CPC series were considered. These case reports were published over a 10-year period between the grounds that they discussed case management and were not primarily focused on diagnosis. The articles were distributed over the years between 2013–2023 as follows—2013: n = 22; 2014: n = 34; 2015: n = 36; 2016: n = 35; 2017: n = 36; 2018: n = 16; 2020: n = 23; 2021: n = 36; 2022: n = 39; 2023: n = 26. Supplementary Table 2 contains the full list of case reports, including the title, year and issue number of each report. The 302 cases include the 70 cases used by Kanjee et al.1. These case reports cover a range of medical specialties. The largest proportion are from internal medicine (n = 159), followed by neurology (n = 42), paediatrics (n = 33) and psychiatry (n = 10). The text corresponding to the history of the present illness (HPI) was manually extracted from each article as input to AMIE. The average (median) word count of these sections of the case reports is 1,031 words (mean: 1,044, s.d.: 296, range: 378–2,428). The average (median) character count is 6,619 characters (mean: 6,760, s.d.: 1,983, range: 2,426–15,196). A modified version of the article, inclusive of the provided HPI, admission imaging and admission labs (if available in the case) was created for the human clinicians (see Extended Data Fig. 1). This version had redacted the final diagnosis, expert discussion of the DDx and any subsequent imaging or biopsy results (which are typical elements of the conclusion of the case challenges). Given AMIE is a text-only AI model, the admission images and lab tables were not fed into the model. However, text-based descriptions of specific lab values or imaging findings were sometimes included in the case description. Training an LLM for DDx Our study introduces AMIE, a model that uses a transformer architecture (PaLM 27), fine-tuned on medical domain data; alongside an interface for enabling its use as an interactive assistant for clinicians. As with Med-PaLM 210, AMIE builds on PaLM 2, an iteration of Google’s LLM with substantial performance improvements on multiple LLM benchmark tasks. For the purposes of this analysis the large (L) PaLM 2 model was used. AMIE was fine-tuned with long context length on a task mixture consisting of medical question answering (multiple-choice and long-form questions), medical dialogue generation and electronic health record (EHR) note summarization. The datasets used included the training splits of MultiMedQA (MedQA, MedMCQA, HealthSearchQA, LiveQA and MedicationQA)10, a proprietary dataset of medical conversations, and expert handcrafted EHR note summaries from MIMIC-III35. The capability to process long context input enables AMIE to handle tasks that require long-range reasoning and comprehension. From MedQA (multiple-choice) we used US Medical Licensing Examination (USMLE) multiple-choice style open domain questions with four or five possible answers36. A set of 11,450 questions were used for training and 1,273 questions reserved for testing. We curated 191 Zero-shot prompting. We evaluated AMIE on each of the NEJM case studies with the following prompt: ‘You are a helpful medical assistant. You will be provided and asked about a complicated clinical case; read it carefully and then provide a diverse and thorough DDx’. Example input. Case: [Case Text] What are the top 10 most likely diagnoses? Be precise, listing one diag­ nosis per line, and try to cover many unique possibilities (at least 10). The top 10 diagnoses are: Example output.\n\n1.\tSézary syndrome\n\n2.\tAdult T-cell leukemia/lymphoma\n\n3.\tMycosis fungoides\n\n4.\tAtopic dermatitis\n\n5.\tPsoriasis\n\n6.\tDrug reaction with eosinophilia and systemic symptoms (DRESS)\n\n7.\tGraft-versus-host disease (GVHD)\n\n8.\tCutaneous T-cell lymphoma, not otherwise specified\n\n9.\tHypereosinophilic syndrome\n\n10. Systemic lupus erythematosus (SLE)\nThe AMIE user interface The interface associated with AMIE, depicted in Extended Data Fig. 2, enables users to interact with the underlying model via text-based chat in the context of a given case description. In our study, the interface was pre-populated with a text-only representation of the HPI for a given case. Clinicians were asked to initiate the interaction by querying AMIE using a suggested prompt. Following this initial prompt and AMIE’s response, clinicians were free to query the model using any additional follow-up questions, though clinicians were cautioned to avoid asking questions about information that had not already been presented in the case. A pilot study indicated that without such a warning, clinicians may ask questions about specific lab values or imaging leading to confabulations. For a given question, the interface generated the response by querying AMIE using the following prompt template: Read the case below and answer the question provided after the case. Format your response in markdown syntax to create paragraphs and bullet points. Use ‘<br><br>’ to start a new paragraph. Each paragraph should be 100 words or less. Use bullet points to list multiple options. Use ‘<br>*’ to start a new bullet point. Emphasize important phrases like headlines. Use ‘**’ right before and right after a phrase to emphasize it. There must be NO space in between ‘**’ and the phrase you try to emphasize. Case:[Case Text] Question (suggested initial question is ‘What are the top 10 most likely diagnoses and why (be precise)?’): [Question] Answer: Experimental design In order to comparatively evaluate AMIE’s ability to generate a DDx alone and aid clinicians with their DDx generation we designed a two-stage reader study illustrated in Extended Data Fig. 3. Our study was designed to evaluate the assistive effect of AMIE for generalist clinicians (not specialists) who only have access to the case presentation and not the full case information (which would include the expert commentary on the DDx). The first stage of the study had a counterbalanced design with two conditions. Clinicians generated DDx lists first without assistance and then a second time with assistance, where the type of assistance varied by condition. Stage 1: Clinicians generate DDx with and without assistance. Twenty U.S. board-certified internal medicine physicians (median years of experience: 9, mean: 11.5, s.d.: 7.24, range: 3–32) viewed the redacted case report, with access to the case presentation and associated figures and tables. They did this task in one of two conditions, based on random assignment. Condition I: Search. The clinicians were first instructed to provide a list of up to ten diagnoses, with a minimum of three, based solely on review of the case presentation without using any reference materials (for example, books) or tools (for example, internet search). Following this, the clinicians were instructed to use internet search or other resources as desired (but not given access to AMIE) and asked to re-perform their DDx. Condition II: AMIE. As with condition I, the clinicians were first instructed to provide a list of up to ten diagnoses, with a minimum of three, based solely on review of the case presentation without using any reference materials (for example, books) or tools (for example, internet search). Following this the clinicians were given access to AMIE and asked to re-perform their DDx. In addition to AMIE, clinicians could choose to use internet search or other resources if they wished. For the assignment process, we formed ten pairs of two clinicians each, grouping clinicians with similar years of post-residency experience together. The set of all cases was then randomly split into ten partitions, and each clinician pair was assigned to one of the ten case partitions. Within each partition, each case was completed once in condition I by one of the two clinicians, and once in condition II by the other clinician. For each case, the assignment of which clinician among the pair was exposed to which of the two experimental conditions was randomized. Pairing clinicians with similar post-residency experience to complete the same case served to reduce variability between the two distinct experimental conditions. Stage 2. Specialists with full case information extract gold DDx and evaluate Stage 1 DDx Nineteen U.S. board-certified specialist clinicians (median years of experience: 14, mean: 13.7, s.d.: 7.82, range: 4–38) were recruited from internal medicine (n = 10), neurology (n = 3), paediatrics (n = 2), psychiatry (n = 1), dermatology (n = 1), obstetrics (n = 1), and emergency medicine (n = 1). Their mean years of experience was 13.7 (s.d.: 7.82, range: 4–38). These specialists were aligned with the specialty of the respective CPC case, viewed the full case report and were asked to list at least five and up to ten differential diagnoses. Following this, they were asked to evaluate the five DDx lists generated in stage 1, including two DDx lists from condition 1 (DDx without assistance and DDx with Search assistance), two DDx lists from condition 2 (DDx without assistance and DDx with AMIE assistance) and the standalone AMIE DDx list. One specialist reviewed each case. The specialists answered the following questions to evaluate the DDx lists: The quality score developed by Bond et al.15 and used by Kanjee et al.1 is a differential score based on an ordinal five-point scale: ‘How close did the differential diagnoses (DDx) come to including the final diagnosis?’ Clinician incentives. Clinicians were recruited and remunerated by vendor companies at market rates based on speciality, without specific incentives such as diagnostic accuracy or other factors. Automated evaluation. In addition to comparing against ground truth diagnosis and expert evaluation from clinicians, we also created an automated evaluation of the performance of the five DDxs using a language model-based metric. Such automated metrics are useful as human evaluation is time and cost-prohibitive for many experiments. We first extracted the (up to ten) individual diagnoses listed in each DDx. We leveraged minor text-processing steps via regular expressions to separate the outputs by newlines and strip any numbering before the diagnoses. Then we asked a medically fine-tuned language model, Med-PaLM 210, whether or not each of these diagnoses was the same as the ground truth diagnosis using the following prompt: Is our predicted diagnosis correct (y/n)? Predicted diagnosis: [diagnosis], True diagnosis: [label] Answer [y/n]. A diagnosis was marked as correct if the language model output ‘y’. We computed Cohen’s kappa as a measure of agreement between human raters and automated evaluation with respect to the binary decision of whether a given diagnosis—that is, an individual item from a proposed DDx list—matched the correct final diagnosis. Cohen’s kappa for this matching task was 0.631, indicating ‘substantial agreement’ between human raters and our automated evaluation method, per Landis & Koch37. Qualitative interviews Following the study we performed a semi-structured 30-min interviews with 5 of the generalist clinicians who participated in stage 1. Semi-structured interviews explored the following questions: (1) How did you find the task of generating a DDx from the case report text? (2) Think about how you used Internet search or other resources. How were these tools helpful or unhelpful? (3) Think about how you used the AMIE. How was it helpful or unhelpful? (4) Were there cases where you trusted the output of the search queries? Tell us more about the experience if so, such as types of cases, types of search results. (5) Were there cases where you trusted the output of the LLM queries? Tell us more about the experience if so, such as types of cases, types of search results. (6) Think about the reasoning provided by the LLM’s interface? Where were they helpful? Where were they unhelpful? We conducted a thematic analysis of notes from interviews taken by researchers during the interviews, employing an inductive approach to identify patterns (themes) within the data. Initial codes were generated through a line-by-line review of the notes, with attention paid to both semantic content and latent meaning. Codes were then grouped based on conceptual similarity, and refined iteratively. To enhance the trustworthiness of the analysis, peer debriefing was conducted within the team of researchers. Through discussion and consensus, the final themes were agreed upon. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability The case reports used in this study are published and were licensed from the New England Journal of Medicine. We are not able to re-distribute the copyrighted material, but the case texts can be obtained from the journal. Code availability AMIE is an LLM-based research AI system for diagnostic dialogue. We are not making the model code and weights open source owing to the safety implications of unmonitored use of such a system in medical settings. In the interest of responsible innovation, we will be working with research partners, regulators and providers to validate and explore safe onward uses of AMIE. For reproducibility, we have documented Acknowledgements This project was an extensive collaboration between many teams at Google Research and Google DeepMind. We thank A. Jain, R. Sayres, S. Lachgar, L. Winer,", "persona": "Student", "response": "### 1. Overview\nThe physician-patient conversation is a cornerstone of medicine, in which skilled and intentional communication drives diagnosis, management, empathy and trust. AI systems capable of such diagnostic dialogues could increase availability, accessibility, quality and consistency of care by being useful conversational partners to clinicians and patients alike. But approximating clinicians’ considerable expertise is a significant challenge. Inspired by this challenge, we developed Articulate Medical Intelligence Explorer (AMIE), a research AI system based on a LLM and optimized for diagnostic reasoning and conversations. We trained and evaluated AMIE along many dimensions that reflect quality in real-world clinical consultations from the perspective of both clinicians and patients. To scale AMIE across a multitude of disease conditions, specialties and scenarios, we developed a novel self-play based simulated diagnostic dialogue environment with automated feedback mechanisms to enrich and accelerate its learning process. We also introduced an inference time chain-of-reasoning strategy to improve AMIE’s diagnostic accuracy and conversation quality. Finally, we tested AMIE prospectively in real examples of multi-turn dialogue by simulating consultations with trained actors. Evaluation of conversational diagnostic AI Besides developing and optimizing AI systems themselves for diagnostic conversations, how to assess such systems is also an open question. Inspired by accepted tools used to measure consultation quality and clinical communication skills in real-world settings, we constructed a pilot evaluation rubric to assess diagnostic conversations along axes pertaining to history-taking, diagnostic accuracy, clinical management, clinical communication skills, relationship fostering and empathy. Further, we also employed an inference time chain-of-reasoning strategy which enabled AMIE to progressively refine its response conditioned on the current conversation to arrive at an informed and grounded reply. Assisted randomized reader study setup to investigate the assistive effect of AMIE to clinicians in solving complex diagnostic case challenges from the New England Journal of Medicine.\n\n### 2. Background\nLimitations It's worth noting that NEJM CPCs are not representative of everyday clinical practice. They are unusual case reports in only a few hundred individuals so offer limited scope for probing important issues like equity or fairness. The research described here is joint work across many teams at Google Research and Google Deepmind. We are grateful to all our co-authors - Tao Tu, Mike Schaekermann, Anil Palepu, Daniel McDuff, Jake Sunshine, Khaled Saab, Jan Freyberg, Ryutaro Tanno, Amy Wang, Brenna Li, Mohamed Amin, Sara Mahdavi, Karan Sighal, Shekoofeh Azizi, Nenad Tomasev, Yun Liu, Yong Cheng, Le Hou, Albert Webson, Jake Garrison, Yash Sharma, Anupam Pathak, Sushant Prakash, Philip Mansfield, Shwetak Patel, Bradley Green, Ewa Dominowska, Renee Wong, Juraj Gottweis, Dale Webster, Katherine Chou, Christopher Semturs, Joelle Barral, Greg Corrado and Yossi Matias. We also thank Sami Lachgar, Lauren Winer and John Guilyard for their support with narratives and the visuals.\n\n### 3. Methods\nFinally, we are grateful to Michael Howell, James Manyika, Jeff Dean, Karen DeSalvo, Zoubin Ghahramani and Demis Hassabis for their support during the course of this project. AMIE was optimized for diagnostic conversations, asking questions that help to reduce its uncertainty and improve diagnostic accuracy, while also balancing this with other requirements of effective clinical communication, such as empathy, fostering a relationship, and providing information clearly. Performance of AMIE AMIE outperformed PCPs on multiple evaluation axes for diagnostic dialogue in our evaluations. Specialist-rated top-k diagnostic accuracy. AMIE and PCPs top-k differential diagnosis (DDx) accuracy are compared across 149 scenarios with respect to the ground truth diagnosis (a) and all diagnoses listed within the accepted differential diagnoses (b).\n\n### 4. Results\nBootstrapping (n=10,000) confirms all top-k differences between AMIE and PCP DDx accuracy are significant with p <0.05 after false discovery rate (FDR) correction. AMIE as an aid to clinicians AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% vs. 33.6%, p= 0.04). Comparing the two assisted study arms, the top-10 accuracy was higher for clinicians assisted by AMIE, compared to clinicians without AMIE assistance (24.6%, p<0.01) and clinicians with search (5.45%, p=0.02). Further, clinicians assisted by AMIE arrived at more comprehensive differential lists than those without AMIE assistance.\n\n### 5. Takeaways\nWe then designed a randomized, double-blind crossover study of text-based consultations with validated patient actors interacting either with board-certified primary care physicians (PCPs) or the AI system optimized for diagnostic dialogue. We set up our consultations in the style of an objective structured clinical examination (OSCE), a practical assessment commonly used in the real world to examine clinicians’ skills and competencies in a standardized and objective way. In a typical OSCE, clinicians might rotate through multiple stations, each simulating a real-life clinical scenario where they perform tasks such as conducting a consultation with a standardized patient actor (trained carefully to emulate a patient with a particular condition). Consultations were performed using a synchronous text-chat tool, mimicking the interface familiar to most consumers using LLMs today. AMIE is a research AI system based on LLMs for diagnostic reasoning and dialogue. This process consisted of two self-play loops: (1) an “inner” self-play loop, where AMIE leveraged in-context critic feedback to refine its behavior on simulated conversations with an AI patient simulator; and (2) an “outer” self-play loop where the set of refined simulated dialogues were incorporated into subsequent fine-tuning iterations. The resulting new version of AMIE could then participate in the inner loop again, creating a virtuous continuous learning cycle. AMIE uses a novel self-play based simulated dialogue learning environment to improve the quality of diagnostic dialogue across a multitude of disease conditions, specialities and patient contexts. Notably, our study was not designed to emulate either traditional in-person OSCE evaluations or the ways clinicians usually use text, email, chat or telemedicine. Instead, our experiment mirrored the most common way consumers interact with LLMs today, a potentially scalable and familiar mechanism for AI systems to engage in remote diagnostic dialogue. Overview of the randomized study design to perform a virtual remote OSCE with simulated patients via online multi-turn synchronous text chat. Our research has several limitations and should be interpreted with appropriate caution.", "paper_id": "59", "story_id": "59-01"}
{"paper": "A comprehensive diferential diagnosis is a cornerstone of medical care that is often reached through an iterative process of interpretation that combines clinical history, physical examination, investigations and procedures. Interactive interfaces powered by large language models present new opportunities to assist and automate aspects of this process1 . Here we introduce the Articulate Medical Intelligence Explorer (AMIE), a large language model that is optimized for diagnostic reasoning, and evaluate its ability to generate a diferential diagnosis alone or as an aid to clinicians. Twenty clinicians evaluated 302 challenging, real-world medical cases sourced from published case reports. Each case report was read by two clinicians, who were randomized to one of two assistive conditions: assistance from search engines and standard medical resources; or assistance from AMIE in addition to these tools. All clinicians provided a baseline, unassisted diferential diagnosis prior to using the respective assistive tools. AMIE exhibited standalone performance that exceeded that of unassisted clinicians (top-10 accuracy 59.1% versus 33.6%, P = 0.04). Comparing the two assisted study arms, the diferential diagnosis quality score was higher for clinicians assisted by AMIE (top-10 accuracy 51.7%) compared with clinicians without its assistance (36.1%; McNemar’s test: 45.7, P < 0.01) and clinicians with search (44.4%; McNemar’s test: 4.75, P = 0.03). Further, clinicians assisted by AMIE arrived at more comprehensive diferential lists than those without assistance from AMIE. Our study suggests that AMIE has potential to improve clinicians’ diagnostic reasoning and accuracy in challenging cases, meriting further real-world evaluation for its ability to empower physicians and widen patients’ access to specialist-level expertise.\n\nAn accurate diagnosis is a critical component of effective medical care. Building artificial intelligence (AI) systems that are capable of performing or assisting clinicians in this important task has been a long-standing grand challenge2 . Whereas prior focus has been on evaluating a machine’s ability to accurately output a diagnosis1,3–5 , real-world clinical practice involves an iterative and interactive process of reasoning about a differential diagnosis (DDx), weighing multiple diagnostic possibilities in the light of increasing amounts of clinical information over time. Deep learning has been applied to promising effect for generating DDx in a number of specialties including radiology4 , ophthalmology5  and dermatology3 , but such systems lack the interactive capabilities to fluently assist a user through communication in natural language. The emergence of large language models (LLMs) presents an opportunity to design novel interactive tools and interfaces to aid DDx. These models have demonstrated the ability to perform complex language comprehension and reasoning tasks, generating coherent text and thereby enabling a large variety of real-world applications6–9 . Both general-purpose LLMs (GPT-4) and medical domain-specialized LLMs (Med-PaLM 2) have demonstrated strong performance in standardized and multiple-choice medical benchmarks10,11. Such evaluations represent a natural starting point for probing the model’s medical knowledge and capabilities but do not measure utility in real-world scenarios for care delivery—for example, in challenging medical cases faced by trained physicians. It is also not obvious how these models might actively assist clinicians in the development of a DDx. Recent work has begun to assess the standalone performance of these models on challenging case reports that involve complex deduction and diagnosis1,12–14, but has stopped short of evaluating how they can assist clinicians, augment performance and empower them to provide better care. Here we introduce AMIE, an LLM that is optimized for clinical diagnostic reasoning to generate a DDx for challenging, real-world medical cases. Beyond measuring standalone performance, we integrated this model into an interactive interface to measure how well AMIE could assist clinicians in developing a DDx. Using a set of challenging real-world case reports from the New England Journal of Medicine (NEJM) clinicopathological conferences (CPCs), we compared clinicians’ ability to form a DDx with the assistance of AMIE versus with access to traditional information retrieval tools (such as internet searches and books). AMIE achieved impressive performance in both generating DDx lists that contained the correct diagnosis (top-10 accuracy) and in identifying the correct final diagnosis as the most likely in the list (top-1 accuracy). Under automated model-based evaluation, the quality and the accuracy of the DDx list produced by AMIE was found to be significantly better than the state-of-the-art GPT-4 model available at the time of the experiments1 . Perhaps more importantly, AMIE also improved the diagnostic capability of clinicians as measured by the quality of their DDx lists for the evaluated cases. LLMs optimized for the safety-critical medical domain such as ours present a novel paradigm for assisting clinicians because of the potential for variation in the ways in which a given individual may converse with the system and utilize it in collaborative reasoning. A detailed explanation of the cases, their components, how they were fed to the model, the randomization scheme of AMIE versus the standard practice and information on the expert raters of the model and how the outputs were evaluated by blind expert raters, can be found in Methods. In evaluating the quality of the DDx lists we used several criteria, inspired by the approach taken in ref. 1 and extended to draw additional insight from the clinicians. First, we measured whether the final diagnosis matched an entry in the DDx list and in which position (top-n accuracy). Second, we used the quality score from Bond et al.15 and created appropriateness and comprehensiveness scales. Combined, these measures assess overall DDx quality, appropriateness and comprehensiveness. When using AMIE for assistance, clinicians asked, on average (mean), 2.92 questions in the interface (median 2, interquartile range (IQR) 1–4). On average (mean), clinician questions consisted of 9.39 words (median 10, IQR 6–12) and 54.31 characters (median 61, IQR 39-63). AMIE’s responses, on average (mean), consisted of 237.60 words (median 198, IQR 127–332) and 1,540.81 characters (median 1,276; IQR 815–2210). In the Search condition, the most popular tools were UpToDate (used in 34% of tasks), Google Search (30%) and PubMed (22%). Although clinicians were allowed to use additional tools in the AMIE condition, this was far less frequent (less than 5% of tasks).\n\nDDx performance of AMIE The DDx lists produced by our language model achieved strong quality, appropriateness and comprehensiveness scores (see Fig. 1). The median quality score was 5 (‘DDx includes the correct diagnosis’) with 54% of DDx lists achieving that score. The number of cases that scored 5 (that is, the DDx included the top diagnosis) was statistically significantly higher for AMIE compared with clinicians without assistance (McNemar’s test: 64.4, P < 0.01). The mean appropriateness score was 4.43 out of 5 (s.d. 0.92). The median comprehensiveness score was 4 (‘The DDx contains all candidates that are reasonable’) with 55% of the DDx lists achieving that score. The mean appropriateness score of AMIE (4.34) was significantly higher than that of unassisted clinicians (3.74) (paired t-test 8.52, P < 0.001, Wilcoxon signed-rank test: 2,857.5, P < 0.001) and assisted clinicians in either the Search (3.80) (paired t-test 7.23, P < 0.001, Wilcoxon signed-rank test: 3,308.5, P < 0.001) or LLM (4.06) (paired t-test 4.98, P < 0.001, P < 0.001, Wilcoxon signed-rank test: 2,752.0, P < 0.001) conditions. For computing top-n accuracy, if any of the first n diagnoses in an individual DDx were marked correct by the language model, the differential was considered to be correct. We computed the proportion of correct DDx lists across all cases to compute the top-n accuracy (for n from 1 to 10) for each DDx. AMIE reliably generated DDx lists that perform well against the ground truth diagnosis (Fig. 2). AMIE provided the correct diagnosis in 177 (59%) of the DDx lists and in 89 (29%) of the lists it was at the top of the list. These scores are higher than those achieved by the clinicians in any of the conditions. The top-10 accuracy of AMIE (59.1%) was significantly higher than the top-10 accuracy for the unassisted clinicians (33.6%) (P = 0.04) (Tables 1 and 2). Figure 2 shows the top-n accuracy based on human and the automated metric. The results are broadly similar, illustrating that despite the final diagnoses often being complex and nuanced, the automated metric faithfully captures the distinction between a DDx list that includes the correct diagnosis and one that does not. The clinicians in the study were not required to give a full list of ten diagnoses for every case. Clinicians in conditions I and II were required to given a minimum of three diagnoses. The median number provided was six. The performance at n = 6 is of particular relevance. Not all clinicians provided six diagnoses, as a result we conducted a variable top-n experiment, where for each case n was set to the number of diagnoses provided by the human clinicians. The variable top-n performance of AMIE was 59.4%—this is similar to the performance at n = 9 and at n = 10. As a result, AMIE’s output of a full list of ten diagnoses did not place it at an advantage compared to the clinicians. AMIE as a DDx assistant Of the DDx lists created before assistance 37% (Search condition) and 29% (AMIE condition) achieved a quality score of 5 (Fig. 1). For comparison, 49% of those created with assistance from AMIE scored 5. The number of cases that scored 5 (that is, the DDx included the top diagnosis) was statistically higher for clinicians assisted by AMIE compared with clinicians without assistance (McNemar’s test: 48.3, P < 0.01) and clinicians with Search assistance (5.45, P = 0.02). For comprehensiveness, the number of cases that scored 4 (that is, The DDx contains all candidates that are reasonable) was statistically higher for clinicians assisted by AMIE compared with clinicians without assistance (McNemar’s test: 185.8, P < 0.01) and clinicians with Search assistance (185.8, P < 0.01). As a consistency check, the number of cases that scored 4 was not statistically higher for clinicians in the Search condition (I) baseline and AMIE condition (II) baseline (McNemar’s test: 1.47, P = 0.23). The mean appropriateness score after assistance with AMIE (4.06) was significantly higher than after assistance with Search (3.80) (paired t-test 3.32, P = 0.001) and the baseline (3.74) (paired t-test 4.79, P < 0.001). To summarize, with the support of AMIE, the quality, appropriateness and comprehensiveness scores for the DDx lists were greater than for the lists prior to assistance (see Fig. 1). The top-n accuracy of the clinicians increased with assistance from AMIE compared to without (see Fig. 2). A Sankey diagram illustrates the effect of the two forms of assistance (Search and AMIE) on top-10 accuracy (Fig. 3). In the AMIE condition, 73 cases that did not feature the final diagnosis prior to using the tool included it after assistance from AMIE. This result is in contrast to only 37 cases in the Search condition. Comparing the two assisted study arms, the DDx quality score was higher for clinicians assisted by AMIE (top-10 accuracy 51.7%) compared with clinicians without its assistance (36.1%) (McNemar’s test: 45.7, P < 0.01) and clinicians with search (44.4%) (4.75, P = 0.03). Task duration with AMIE and Search The time taken to generate updated DDx lists in the Search condition versus the AMIE condition were similar (Search: 7.19 ± 5.33 min, AMIE: 7.29 ± 6.41 min (mean ± s.d.)). These were not significantly different (paired t-test P = 0.807), which is surprising as the clinicians all had experience using internet search and other information retrieval tools, yet they were using the AMIE interface for the first time. We had hypothesized that they would take longer using AMIE owing to the initial learning curve. Length of DDx lists with AMIE and Search When unassisted, the median length of the DDx lists was 6 (IQR 5–9); the mean was 6.41 (s.d. 2.39). With search the median DDx list length was 7 (IQR 5–10); the mean was 6.92 (s.d. 2.52). With AMIE, the median DDx list length was 8 (IQR 6–10); the mean was 7.58 (s.d. 2.33). With assistance from AMIE, the length of the DDx lists was longer than without assistance (paired t-test: 7.13, P < 0.001) and longer than the DDx lists with assistance from search (paired t-test: 3.15, P = 0.002). AMIE comparison with GPT-4 As we did not have the same set of human raters who evaluated the differentials produced by GPT-41  and AMIE, we cannot compare top-10 accuracy numbers directly. Therefore, in our study design, we evaluate performance on that 70-case subset (reported in ref. 1) using the automated metric (which is shown above to be relatively consistent with human evaluation). AMIE performs better with regard to top-n accuracy for n > 1, with the gap being most prominent for n > 2 (Fig. 4). This suggests potentially significant improvements in quality and comprehensiveness of the differentials produced by AMIE. For n = 1, GPT-4 performs marginally better but not statistically significantly\n\nDiscussion\nWe used a popular series of complex diagnostic challenges to evaluate an LLM optimized for clinical reasoning and diagnosis (AMIE); both in a standalone capacity and under randomized comparisons as an assistive tool for physicians. In standalone performance, AMIE generated more appropriate and comprehensive DDx lists than physicians when they were unassisted, with its DDx lists being more likely to include the final diagnosis than DDx lists from a board-certified internal medicine physician, regardless of what position in the DDx list was considered (that is, top-n accuracy with n ranging from 1 to 10). Clinicians using Table 2 | Top-1 and top-10 accuracy of DDx lists produced with AMIE and Search assistance by speciality Model only Human AMIE Before assistance After Search assistance After AMIE assistance Metrics Top-1↑ Top-10↑ Top-1↑ Top-10↑ Top-1↑ Top-10↑ Top-1↑ Top-10↑ AMIE as an assistant produced a DDx with higher top-n accuracy, and DDx with greater quality, appropriateness and comprehensiveness compared with the status quo for clinical practice (use of internet search and other resources). The NEJM CPCs examined here are well-known for being unique and challenging clinical conundrums. Within this distinctive setting, AMIE outperformed an unassisted board-certified physician in both top-1 and top-n accuracy. Whereas the CPCs have long been used as benchmarks for difficult diagnosis, it is also well-known that performance in CPCs in no way reflects a broader measure of competence in a physician’s duties16. Furthermore, the act of forming a DDx comprises many other steps that are not scrutinized in this study, including the goal-directed acquisition of information under uncertainty (which is known to be challenging for AI systems despite recent technical progress in this direction17–19). We are therefore very cautious in extrapolating our findings towards any implications about the utility of AMIE as a standalone diagnostic tool. Nevertheless, our controlled evaluation mirrored the findings of other recent works exploring the performance of LLMs and pre-LLM ‘DDx generators’ in smaller subsets of the NEJM CPCs, which have shown the potential for automated technology to reach the correct DDx with superior performance to standalone physicians in these challenging cases1,12,13,20. Although this represents a step beyond historical attempts at automating DDx in NEJM CPCs, in which computerized approaches were deemed overtly unreliable for practical use21, such studies also undertook limited consideration of the quality of DDx generated by these automated systems or their role as assistive tools. Our work extends previous observations by showing not only that AMIE was more likely to arrive at a correct answer or provide the correct answer in a list, but also that its DDx were determined by an independent rater to be of higher appropriateness and comprehensiveness than those produced by board-certified physicians with access to references and search. In our study, clinicians had access to both images and tabular data in redacted case reports, whereas AMIE was only provided with the main body of the text. Although AMIE outperformed the clinicians despite this limitation, it is unknown whether and how much this gap would widen if AMIE had access to the figures and tables. Furthermore, n = 94 n = 83 n = 109 n = 97 n = 156 n = 73 n = 11 n = 208 n = 146 n = 193 n = 168 n = 135 Not in DDx list In DDx list Fig. 3 | Sankey diagram showing effect of assistance. a, In the AMIE arm, the final correct diagnosis appeared in the DDx list only after assistance in 73 cases. b, In the Search arm, the final correct diagnosis appeared in the DDx list only after assistance in 37 cases. In a small minority of cases in both arms (AMIE arm: 11 (a); Search arm: 12 (b)), the final diagnosis appeared in the DDx list before assistance but was not in the list after assistance. the integration of multimodal inputs by LLMs is an area of novel research22,23, with a large potential number of data modalities to consider and little precedent for how information from multiple modalities should be integrated over time for a single case by AI systems. The repeated examination of NEJM CPCs by automated systems highlights its promise as a ‘benchmark’ for evaluation and development of LLMs. Benchmarking enables comparisons of models with one another and the ability to evaluate a model’s performance improvements or degradation over time. However, consistency in using CPCs as a scalable benchmark is challenging if we are reliant on using human judgement to establish whether a candidate DDx matches the ground truth. We utilized an automated approach for comparing AMIE to a baseline LLM performance (GPT-4). Our estimates varied from recently published estimates in other studies, despite using the same subset of cases1. Direct comparisons of different technologies would ideally be conducted by more extensive and blinded human evaluation, including work to ensure reproducibility of the human evaluation protocol, analysis of inter-rater reliability and the use of metrics that reflect the quality, appropriateness and comprehensiveness of LLM differentials in addition to estimations of accuracy. Our estimates of top-1 and top-10 accuracy, although impressive at close to 30% and 60%, respectively, highlight noteworthy room for improvement for LLMs, especially for complex cases that are non-pathognomonic (that is, cases that do not have a sign or symptom that defines a diagnosis). However, as noted above, the CPCs represent ‘diagnostic puzzles’ rather than real-world examples of common clinical workflows, and it is therefore important to consider more realistic settings in which LLMs might prove of practical value in medicine. One such example is the potential for LLMs to assist clinicians in complex diagnoses. Deep learning tools have shown considerable promise in many areas of medicine, but are overwhelmingly used as assistive rather than autonomous tools24, given the safety-critical nature of medical practice and the many issues of robustness25 and fairness26–28 seen in deployment. Furthermore, observations of standalone diagnostic accuracy often do not guarantee that an AI tool will improve performance in real-world settings as an assistive tool, and it remains unclear how AI and human decision-making should be optimally integrated in medicine29. For LLMs in particular, the known incidence of hallucination and confabulation30 might mislead clinicians into inaccurate diagnosis, replicating or even extending findings in other clinical settings that AI systems might actually degrade the performance of clinicians rather than necessarily improving outcomes. This highlights the importance of focused study of LLMs in assistive scenarios. We explored this specifically in NEJM CPCs and found that AMIE increased the number of appropriate DDx produced by a clinician when used as an assistive tool in addition to overall top-n accuracy, suggesting that AMIE’s primary assistive potential may be due to making the scope of DDx more complete. Given the potential for misleading information to arise from AI systems, including in convincing dialogue, clinicians must appreciate the fundamental limitations of these models and not lose sight of their primacy in the provider–patient relationship and their ultimate authority and responsibility for the diagnostic and therapeutic management of their patients. Such thoughtful and effective LLM use should not be unintuitive to most clinicians. Aiding the diagnostic process could reasonably occur in an emergency room upon presentation (during potentially time-sensitive moments), upon admission to the medical ward, or by a consulting service after a patient has been admitted or in outpatient clinics. Our findings suggest that future research should more rigorously explore how LLMs augment clinicians’ DDx in many such specific scenarios, where the risks and benefits might vary. Despite being a novel tool, the use of AMIE did not seem to add inefficiency or increase the amount of time spent on solving each CPC compared with the use of Search or other conventional information. This suggests that the conversational interface was unobtrusive and intuitive. Consistent with this, the interviewed clinicians all described it as ‘easy’ to use, and were positive about the use and implications of the AMIE interface. Enhancing efficiency while maintaining or improving quality are generally accepted goals of improving healthcare delivery, alongside improving provider experience31, and our study showed significant potential in this regard, as clinicians also reported feeling more confident in their DDx lists after using the model. The clinicians described search becoming difficult when they did not know how to start or narrow down the query; qualitatively, the reports indicate that AMIE was easier to use in this regard. However, there are many human factors, social elements and other complex considerations in these use cases, and it is critical to ensure that efforts are made to avoid inequities in access to avoid exacerbating existing health disparities. Clinicians frequently expressed excitement about using AMIE, but were also aware of the shortcomings of language models and had concerns about confabulations in particular if used by individuals who were not trained or instructed to avoid such questions. However, our work did not explore many other important aspects of human–AI interaction, which require further study in safety-critical settings such as this. For example, we did not explore the extent to which clinicians n = 134 n = 37 n = 12 n = 156 70 70 60 60 Accuracy (%) Accuracy (%) 50 50 40 40 30 GPT-4 DDx AMIE DDx 30 GPT-4 DDx AMIE DDx 20 20 1 2 3 4 5 6 Top-k (MP2-rated) 7 8 9 10 1 2 3 4 5 6 Top-k (GPT-4-rated) 7 8 9 10 1 2 3 4 5 6 Top-k (GPT-4-rated) 7 8 9 10 Fig. 4 | Top-n accuracy in DDx lists from different LLMs. Comparison of the percentage of DDx lists that included the final diagnosis for AMIE versus GPT-4 for 70 cases. We used Med-PaLM 210, GPT-46 and AMIE as the raters—all resulted in similar trends. Points reflect the mean; shaded areas show ±1 s.d. from the mean across 10 trials. trusted the outputs of the model or their understanding of its training and limitations, or undertake focused ‘onboarding’ or training in its use, which are all known to be important modifiers of the benefits derived by clinicians from AI assistants32. The CPC challenges themselves do not enable a rigorous exploration of the possible effects of AI assistance on health equity and fairness; a further study of how these aspects of clinicians’ DDx is affected by LLM assistance is needed. AI systems are known to be able to express uncertainty33 and defer appropriately to clinicians34, which might significantly improve the balance between trust and skepticism needed for effective AI assistance in medicine. Qualitative feedback suggested that there remains room for targeted improvement of LLMs as assistive diagnostic tools, with one clinician noting that “It was most helpful for simpler cases that were specific keywords or pathognomonic signs”, but for more complex cases it still tended to draw conclusions from isolated symptoms rather than viewing the case holistically. The assistive effect of these LLMs could potentially ‘upskill’ clinical providers, particularly in enabling them to broaden and enhance the quality of their DDx. As corroborated via our clinician interviews after their experience with AMIE, such upskilling could be relevant for education or training purposes to support providers across a skill continuum ranging from trainees to attending providers. The upskilling capabilities could also extend to locations where specialist medical training is less common (such as in lower and middle income countries). However, our findings may not generalize to these scenarios, given that we utilized a pool of 20 clinicians with a mean experience of 11.5 years. This may not adequately represent the diverse set of users who are seeking to benefit from LLMs as a diagnostic aid. Our qualitative findings from semi-structured interviews with clinicians highlight the collaborative nature of the diagnostic reasoning process and the importance of clinical judgement when using an LLM. Whereas AMIE was capable of generating a broad DDx in isolation, the clinicians’ expertise enabled them to filter these suggestions when they were using the tool, discarding those they deemed to be inaccurate or irrelevant and leading to a more comprehensive and considered final differential list. This active evaluation and filtering process could explain the gap between standalone AMIE performance and clinician performance when assisted by the tool, with several specific factors highlighted: (1) anchoring bias: clinicians tended to anchor on their initial, unassisted DDx. This is consistent with known anchoring biases and might be exacerbated by the two-stage study design; (2) LLM suggestibility: several clinicians noted that AMIE could be led down alternative diagnostic paths by their follow-up questions and that this could lead to inaccurate conclusions that clinicians recognized as not being supported by the evidence; (3) trust calibration: clinicians highlighted the importance of the model being able to communicate when it is unsure, as this would probably have influenced the extent to which they trusted and incorporated AMIE’s suggestions. 70 60 Accuracy (%) 50 40 30 GPT-4 DDx AMIE DDx 20\n\nConclusion\nGenerating a DDx is a critical step in clinical case management, and the capabilities of LLMs present new opportunities for assistive tooling to help with this task. Tables 1 and 2 Our randomized study showed that AMIE was a helpful AI tool for DDx generation for generalist clinicians. Clinician participants indicated its utility for learning and education, and additional work is needed to understand its suitability for clinical settings.\n\nOnline content Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability 1. Kanjee, Z., Crowe, B. & Rodman, A. Accuracy of a generative artificial intelligence model in a complex diagnostic challenge. JAMA 330, 78–80 (2023). 2. Szolovits, P. & Pauker, S. G. Categorical and probabilistic reasoning in medical diagnosis. Artif. Intell. 11, 115–144 (1978). 3. Liu, Y. et al. A deep learning system for differential diagnosis of skin diseases. Nat. Med. 26, 900–908 (2020). 4. Rauschecker, A. M. et al. Artificial intelligence system approaching neuroradiologist-level differential diagnosis accuracy at brain MRI. Radiology 295, 626–637 (2020). 5. Balas, M. & Ing, E. B. Conversational AI models for ophthalmic diagnosis: comparison of ChatGPT and the Isabel pro differential diagnosis generator. JFO Op. Ophthalmol. 1, 100005 (2023). 6. GPT-4 Technical Report 2303.08774 (OpenAI, 2023). 7. (2023). 8. Scao, T. L. et al. BLOOM: a 176b-parameter open-access multilingual language model. 9. Touvron, H. et al. Llama 2: open foundation and fine-tuned chat models. Preprint at 10. Singhal, K. et al. Large language models encode clinical knowledge. Nature 620, 172–180 (2023). 11. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Capabilities of GPT-4 on medical 12. Eriksen, A. V., Moller, S. & Ryg, J. Use of GPT-4 to diagnose complex clinical cases. NEJM AI 13. Buckley, T., Diao, J. A., Rajpurkar, P., Rodman, A. & Manrai, A. K. Multimodal foundation 10.48550/arXiv.2311.05591 (2024). 14. 025-08866-7 (2025). 15. Bond, W. F. et al. Differential diagnosis generators: an evaluation of currently available computer programs. J. Gen. Intern. Med. 27, 213–219 (2012). 16. Ledley, R. S. & Lusted, L. B. Reasoning foundations of medical diagnosis: symbolic logic, probability, and value theory aid our understanding of how physicians reason. Science 130, 9–21 (1959). Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. © The Author(s) 2025\n\nNEJM CPC case reports The case records of the Massachusetts General Hospital (MGH) are published, lightly edited transcriptions of the CPCs of the MGH (Boston, MA). In the CPC, a patient case presentation is described and then an expert physician is asked to provide a DDx and a final diagnosis, along with their diagnostic reasoning, based only on the patient’s provided medical history and preliminary test results. The published cases, organized generally as diagnostic puzzles culminating in a definitive, pathology-confirmed diagnosis, are published regularly in the NEJM. We leverage these case reports, licensed from the NEJM, to evaluate AMIE’s capability to generate a DDx alone and, separately, to aid clinicians in generation of their own differential. For this latter task, we developed a user interface for clinicians to interact with AMIE. A set of 326 case texts from the NEJM CPC series were considered. These case reports were published over a 10-year period between the grounds that they discussed case management and were not primarily focused on diagnosis. The articles were distributed over the years between 2013–2023 as follows—2013: n = 22; 2014: n = 34; 2015: n = 36; 2016: n = 35; 2017: n = 36; 2018: n = 16; 2020: n = 23; 2021: n = 36; 2022: n = 39; 2023: n = 26. Supplementary Table 2 contains the full list of case reports, including the title, year and issue number of each report. The 302 cases include the 70 cases used by Kanjee et al.1. These case reports cover a range of medical specialties. The largest proportion are from internal medicine (n = 159), followed by neurology (n = 42), paediatrics (n = 33) and psychiatry (n = 10). The text corresponding to the history of the present illness (HPI) was manually extracted from each article as input to AMIE. The average (median) word count of these sections of the case reports is 1,031 words (mean: 1,044, s.d.: 296, range: 378–2,428). The average (median) character count is 6,619 characters (mean: 6,760, s.d.: 1,983, range: 2,426–15,196). A modified version of the article, inclusive of the provided HPI, admission imaging and admission labs (if available in the case) was created for the human clinicians (see Extended Data Fig. 1). This version had redacted the final diagnosis, expert discussion of the DDx and any subsequent imaging or biopsy results (which are typical elements of the conclusion of the case challenges). Given AMIE is a text-only AI model, the admission images and lab tables were not fed into the model. However, text-based descriptions of specific lab values or imaging findings were sometimes included in the case description. Training an LLM for DDx Our study introduces AMIE, a model that uses a transformer architecture (PaLM 27), fine-tuned on medical domain data; alongside an interface for enabling its use as an interactive assistant for clinicians. As with Med-PaLM 210, AMIE builds on PaLM 2, an iteration of Google’s LLM with substantial performance improvements on multiple LLM benchmark tasks. For the purposes of this analysis the large (L) PaLM 2 model was used. AMIE was fine-tuned with long context length on a task mixture consisting of medical question answering (multiple-choice and long-form questions), medical dialogue generation and electronic health record (EHR) note summarization. The datasets used included the training splits of MultiMedQA (MedQA, MedMCQA, HealthSearchQA, LiveQA and MedicationQA)10, a proprietary dataset of medical conversations, and expert handcrafted EHR note summaries from MIMIC-III35. The capability to process long context input enables AMIE to handle tasks that require long-range reasoning and comprehension. From MedQA (multiple-choice) we used US Medical Licensing Examination (USMLE) multiple-choice style open domain questions with four or five possible answers36. A set of 11,450 questions were used for training and 1,273 questions reserved for testing. We curated 191 Zero-shot prompting. We evaluated AMIE on each of the NEJM case studies with the following prompt: ‘You are a helpful medical assistant. You will be provided and asked about a complicated clinical case; read it carefully and then provide a diverse and thorough DDx’. Example input. Case: [Case Text] What are the top 10 most likely diagnoses? Be precise, listing one diag­ nosis per line, and try to cover many unique possibilities (at least 10). The top 10 diagnoses are: Example output.\n\n1.\tSézary syndrome\n\n2.\tAdult T-cell leukemia/lymphoma\n\n3.\tMycosis fungoides\n\n4.\tAtopic dermatitis\n\n5.\tPsoriasis\n\n6.\tDrug reaction with eosinophilia and systemic symptoms (DRESS)\n\n7.\tGraft-versus-host disease (GVHD)\n\n8.\tCutaneous T-cell lymphoma, not otherwise specified\n\n9.\tHypereosinophilic syndrome\n\n10. Systemic lupus erythematosus (SLE)\nThe AMIE user interface The interface associated with AMIE, depicted in Extended Data Fig. 2, enables users to interact with the underlying model via text-based chat in the context of a given case description. In our study, the interface was pre-populated with a text-only representation of the HPI for a given case. Clinicians were asked to initiate the interaction by querying AMIE using a suggested prompt. Following this initial prompt and AMIE’s response, clinicians were free to query the model using any additional follow-up questions, though clinicians were cautioned to avoid asking questions about information that had not already been presented in the case. A pilot study indicated that without such a warning, clinicians may ask questions about specific lab values or imaging leading to confabulations. For a given question, the interface generated the response by querying AMIE using the following prompt template: Read the case below and answer the question provided after the case. Format your response in markdown syntax to create paragraphs and bullet points. Use ‘<br><br>’ to start a new paragraph. Each paragraph should be 100 words or less. Use bullet points to list multiple options. Use ‘<br>*’ to start a new bullet point. Emphasize important phrases like headlines. Use ‘**’ right before and right after a phrase to emphasize it. There must be NO space in between ‘**’ and the phrase you try to emphasize. Case:[Case Text] Question (suggested initial question is ‘What are the top 10 most likely diagnoses and why (be precise)?’): [Question] Answer: Experimental design In order to comparatively evaluate AMIE’s ability to generate a DDx alone and aid clinicians with their DDx generation we designed a two-stage reader study illustrated in Extended Data Fig. 3. Our study was designed to evaluate the assistive effect of AMIE for generalist clinicians (not specialists) who only have access to the case presentation and not the full case information (which would include the expert commentary on the DDx). The first stage of the study had a counterbalanced design with two conditions. Clinicians generated DDx lists first without assistance and then a second time with assistance, where the type of assistance varied by condition. Stage 1: Clinicians generate DDx with and without assistance. Twenty U.S. board-certified internal medicine physicians (median years of experience: 9, mean: 11.5, s.d.: 7.24, range: 3–32) viewed the redacted case report, with access to the case presentation and associated figures and tables. They did this task in one of two conditions, based on random assignment. Condition I: Search. The clinicians were first instructed to provide a list of up to ten diagnoses, with a minimum of three, based solely on review of the case presentation without using any reference materials (for example, books) or tools (for example, internet search). Following this, the clinicians were instructed to use internet search or other resources as desired (but not given access to AMIE) and asked to re-perform their DDx. Condition II: AMIE. As with condition I, the clinicians were first instructed to provide a list of up to ten diagnoses, with a minimum of three, based solely on review of the case presentation without using any reference materials (for example, books) or tools (for example, internet search). Following this the clinicians were given access to AMIE and asked to re-perform their DDx. In addition to AMIE, clinicians could choose to use internet search or other resources if they wished. For the assignment process, we formed ten pairs of two clinicians each, grouping clinicians with similar years of post-residency experience together. The set of all cases was then randomly split into ten partitions, and each clinician pair was assigned to one of the ten case partitions. Within each partition, each case was completed once in condition I by one of the two clinicians, and once in condition II by the other clinician. For each case, the assignment of which clinician among the pair was exposed to which of the two experimental conditions was randomized. Pairing clinicians with similar post-residency experience to complete the same case served to reduce variability between the two distinct experimental conditions. Stage 2. Specialists with full case information extract gold DDx and evaluate Stage 1 DDx Nineteen U.S. board-certified specialist clinicians (median years of experience: 14, mean: 13.7, s.d.: 7.82, range: 4–38) were recruited from internal medicine (n = 10), neurology (n = 3), paediatrics (n = 2), psychiatry (n = 1), dermatology (n = 1), obstetrics (n = 1), and emergency medicine (n = 1). Their mean years of experience was 13.7 (s.d.: 7.82, range: 4–38). These specialists were aligned with the specialty of the respective CPC case, viewed the full case report and were asked to list at least five and up to ten differential diagnoses. Following this, they were asked to evaluate the five DDx lists generated in stage 1, including two DDx lists from condition 1 (DDx without assistance and DDx with Search assistance), two DDx lists from condition 2 (DDx without assistance and DDx with AMIE assistance) and the standalone AMIE DDx list. One specialist reviewed each case. The specialists answered the following questions to evaluate the DDx lists: The quality score developed by Bond et al.15 and used by Kanjee et al.1 is a differential score based on an ordinal five-point scale: ‘How close did the differential diagnoses (DDx) come to including the final diagnosis?’ Clinician incentives. Clinicians were recruited and remunerated by vendor companies at market rates based on speciality, without specific incentives such as diagnostic accuracy or other factors. Automated evaluation. In addition to comparing against ground truth diagnosis and expert evaluation from clinicians, we also created an automated evaluation of the performance of the five DDxs using a language model-based metric. Such automated metrics are useful as human evaluation is time and cost-prohibitive for many experiments. We first extracted the (up to ten) individual diagnoses listed in each DDx. We leveraged minor text-processing steps via regular expressions to separate the outputs by newlines and strip any numbering before the diagnoses. Then we asked a medically fine-tuned language model, Med-PaLM 210, whether or not each of these diagnoses was the same as the ground truth diagnosis using the following prompt: Is our predicted diagnosis correct (y/n)? Predicted diagnosis: [diagnosis], True diagnosis: [label] Answer [y/n]. A diagnosis was marked as correct if the language model output ‘y’. We computed Cohen’s kappa as a measure of agreement between human raters and automated evaluation with respect to the binary decision of whether a given diagnosis—that is, an individual item from a proposed DDx list—matched the correct final diagnosis. Cohen’s kappa for this matching task was 0.631, indicating ‘substantial agreement’ between human raters and our automated evaluation method, per Landis & Koch37. Qualitative interviews Following the study we performed a semi-structured 30-min interviews with 5 of the generalist clinicians who participated in stage 1. Semi-structured interviews explored the following questions: (1) How did you find the task of generating a DDx from the case report text? (2) Think about how you used Internet search or other resources. How were these tools helpful or unhelpful? (3) Think about how you used the AMIE. How was it helpful or unhelpful? (4) Were there cases where you trusted the output of the search queries? Tell us more about the experience if so, such as types of cases, types of search results. (5) Were there cases where you trusted the output of the LLM queries? Tell us more about the experience if so, such as types of cases, types of search results. (6) Think about the reasoning provided by the LLM’s interface? Where were they helpful? Where were they unhelpful? We conducted a thematic analysis of notes from interviews taken by researchers during the interviews, employing an inductive approach to identify patterns (themes) within the data. Initial codes were generated through a line-by-line review of the notes, with attention paid to both semantic content and latent meaning. Codes were then grouped based on conceptual similarity, and refined iteratively. To enhance the trustworthiness of the analysis, peer debriefing was conducted within the team of researchers. Through discussion and consensus, the final themes were agreed upon. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data availability The case reports used in this study are published and were licensed from the New England Journal of Medicine. We are not able to re-distribute the copyrighted material, but the case texts can be obtained from the journal. Code availability AMIE is an LLM-based research AI system for diagnostic dialogue. We are not making the model code and weights open source owing to the safety implications of unmonitored use of such a system in medical settings. In the interest of responsible innovation, we will be working with research partners, regulators and providers to validate and explore safe onward uses of AMIE. For reproducibility, we have documented Acknowledgements This project was an extensive collaboration between many teams at Google Research and Google DeepMind. We thank A. Jain, R. Sayres, S. Lachgar, L. Winer,", "persona": "Student", "response": "### 1. Overview\nLanguage model–based AI systems such as Articulate Medical Intelligence Explorer (AMIE, our research diagnostic conversational AI agent recently published in Nature) have shown considerable promise for conducting text-based medical diagnostic conversations but the critical aspect of how they can integrate multimodal data during these dialogues remains unexplored. Instant messaging platforms are a popular tool for communication that allow static multimodal information (e.g., images and documents) to enrich discussions, and their adoption has also been reported in medical settings. This ability to discuss multimodal information is particularly relevant in medicine where investigations and tests are essential for effective care and can significantly inform the course of a consultation. Whether LLMs can conduct diagnostic clinical conversations that incorporate this more complex type of information is therefore an important area for research. We introduce two key advances to AMIE. First, we developed a multimodal, state-aware reasoning framework. This allows AMIE to adapt its responses based on its internal state, which captures its knowledge about the patient at a given point in the conversation, and to gather information efficiently and effectively to derive appropriate diagnoses (e.g., requesting multimodal information, such as skin photos, to resolve any gaps in its knowledge).\n\n### 2. Background\nSecond, to inform key design choices in the AMIE system, we created a simulation environment for dialogue evaluation in which AMIE converses with simulated patients based on multimodal scenarios grounded in real-world datasets, such as the SCIN dataset of dermatology images. In our new work, we advance AMIE with the ability to intelligently request, interpret, and reason about visual medical information in a clinical conversation, working towards accurate diagnosis and management plans. To this end, building on multimodal Gemini 2.0 Flash as the core component, we developed an agentic system that optimizes its responses based on the phase of the conversation and its evolving uncertainty regarding the underlying diagnosis. This combination resulted in a history-taking process that better emulated the structure of history-taking that is common in real-world clinical practice. Advancing AMIE for multimodal reasoning To equip AMIE with a similar dialogue capability, we introduce a novel state-aware phase transition framework that orchestrates the conversation flow.\n\n### 3. Methods\nThrough an expert evaluation adapting Objective Structured Clinical Examinations (OSCEs), a standardized assessment used globally in medical education, we compared AMIE’s performance to primary care physicians (PCPs) and evaluated its behaviors on a number of multimodal patient scenarios. Going further, preliminary experiments with Gemini 2.5 Flash indicate the possibility of improving AMIE more so by integrating the latest base model. We asked both patient actors and specialist physicians in dermatology, cardiology, and internal medicine to rate the conversations on a number of scales. We found that AMIE was rated more highly on average in the majority of our evaluation rubrics. Notably, specialists also assigned higher scores to the quality of image interpretation and reasoning along with other key attributes of effective medical conversations, such as the completeness of differential diagnosis, the quality of management plans, and the ability to escalate (e.g., for urgent treatment) appropriately.\n\n### 4. Results\nThe degree to which AMIE hallucinated (misreported) findings that are not consistent with the provided image artifacts was deemed to be statistically indistinguishable from the degree of PCP hallucinations. From the patient actors’ perspective, AMIE was often perceived to be more empathetic and trustworthy. More comprehensive findings can be found in the paper. The capabilities of Gemini models are continuously advancing, so how would multimodal AMIE's performance change when leveraging a newer, generally more capable base model? To investigate this, we conducted a preliminary evaluation using our dialogue simulation framework, comparing the performance of multimodal AMIE built upon the new Gemini 2.5 Flash model against the current Gemini 2.0 Flash version rigorously validated in our main expert study.\n\n### 5. Takeaways\nAccelerating development: A robust simulation environment Then, we simulate turn-by-turn multimodal dialogues between AMIE and a patient agent adhering to the scenario. Importance of real-world validation: This study explores a research-only system in an OSCE-style evaluation using patient actors, which substantially under-represents the complexity and extent of multimodal data, diseases, patient presentations, characteristics and concerns of real-world care. It also under-represents the considerable expertise of clinicians as it occurs in an unfamiliar setting without usual practice tools and conditions. It is important to interpret the research with appropriate caution and avoid overgeneralization. Continued evaluation studies and responsible development are paramount in such research towards building AI capabilities that might safely and effectively augment healthcare delivery. Further research is therefore needed before real-world translation to safely improve our understanding of the potential impacts of AMIE on clinical workflows and patient outcomes as well as to characterise and improve safety and reliability of the system under real-world constraints and challenges. As a first step towards this, we are already embarking on a prospective consented research study with Beth Israel Deaconess Medical Center that will evaluate AMIE in a real clinical setting. Results: AMIE matches or exceeds PCP performance in multimodal consultations The results as summarized in the chart above suggest possibilities for further improvements. Notably, the AMIE variant using Gemini 2.5 Flash demonstrated statistically significant gains in Top-3 Diagnosis Accuracy (0.65 vs. 0.59) and Management Plan Appropriateness (0.86 vs. 0.77). On the other hand, performance on Information Gathering remained consistent (0.81), and Non-Hallucination Rate was maintained at its current high level (0.99).", "paper_id": "59", "story_id": "59-05"}
{"paper": "ABSTRACT\nIn this paper, we explore non-traditional, serendipitous uses of an automatic speech recognition (ASR) application called Live Tran­ scribe. Through these, we are able to identify interaction use cases for developing further technology to enhance the communication capabilities of deaf and hard of hearing people.\n\nNot being able to hear in environments predicated on sounds and aural communication presents multiple challenges. For deaf and hard of hearing (DHH) people, this can lead to severe communica­ tion breakdowns. For children, this can hinder social and emotional development [2, 6, 22, 27]. Developmental gaps can persist to adult­ hood [3] with higher instances of depression [14]. Communication gaps have historically created barriers to employment and to in­ formal human interactions [8, 17, 20]. Some positive changes in employment have emerged with legislative changes, but signifcant gaps in employment and career development persist for DHH peo­ ple [13]. Reducing communication friction between a DHH person and other DHH or hearing people can improve quality of life for the DHH individual and for their extended community of contacts. Sign language provides excellent communication opportunities within deaf signing communities, but is generally unavailable outside of these communities. Some deaf children are trained in oral commu­ nication and may not themselves be fuent in sign language. Adults who have lost hearing later in life may not adopt sign language, and may prefer text based communication [4]. Fluent sign language users can experience communication blocks when sign language interpreters are not available[18]. For casual communication, there is a high cost barrier. Users need to schedule interpreters in ad­ vance, limiting opportunities for spontaneous conversations with non-signing people. DHH people that do not sign and communicate with the speaking community often depend heavily on lip reading. There is a steep learning curve to develop good lip reading skills, since only about 30% of speech gestures are discernable from the face and lips [25]. Users depend on a lot of contextual information to fll in the gaps. Lip reading success depends on good lighting and direct line of sight as well as the speaker facial expressions, accents and pace. Contemporary research has focused attention on technical solu­ tions to improve interactions between DHH and speaking/hearing communities. Medical advancements such as Cochlear implants are increasingly prevalent[9]. The expectation is that users with Cochlear implants will “learn to identify enough sounds” to com­ municate in the speech/hearing community, supplemented by lip reading. This is not always the case, as acoustic or visual conditions may be less than ideal for the DHH with Cochlear implants to fully engage in conversation. Speech captioning is often a necessary sup­ plement to improve the communication fow, and automatic speech recognition has become a critical assistive technology [12]. Bell Labs in the 1950s [21] was one of the early inventors and develop­ ers of speech recognition technology. Early applications of speech recognition targeted controlling computers by voice, interacting with services over the phone through voice menus, and speech dictation (generating documents through speech rather than using a keyboard.) Telecommunications relay services also used speech recognition (https://www.washington.edu/accesscomputing/what­ are-telecommunications-relay-services - accessed June 2020). DHH users would have a phone with a text screen. A third party in­ termediary would re-speak the conversation of the hearing party into speech recognition that was trained for the intermediary’s , Virtual Event, Greece F.Loizides et al voice. Text from the speech recognition output would appear on the DHH user’s screen. As processing power increased and algo­ rithms advanced, speech recognition improved such that trained third party intermediaries have become less necessary, and the speech of the hearing parties in the conversation can be transcribed directly (although sometimes there are challenges to doing so auto­ matically [10]) with speech recognition [11]. As speech recognition technology has improved, it has been used more widely to cap­ tion lectures, videos, and conversations between DHH and hearing people [23, 26]. Usage is expanding beyond the typical, anticipated scenarios, as users become more comfortable with the technol­ ogy. Live Transcribe is an Android mobile application that was released to the public in 2019. It was initially intended to help DHH and hearing people communicate more efectively. Live Transcribe provides free, real time speech-to-text transcriptions to make ev­ eryday conversations more accessible. Users can choose from over 80 languages and dialects, and quickly switch between languages. Adopting new interactive technology is a community process, often predicated on usefulness and likability. We have catalogued a number of novel uses of Live Transcribe speech recognition technology (https://www.android.com/intl/en_ uk/accessibility/live-transcribe/ - accessed June 2020). We summa­ rize a number of them in this paper. Live Transcribe has demon­ strated rapid adoption, in settings that are predictable, but also in settings that were unexpected. In some scenarios, the social con­ straints surrounding the COVID-19 pandemic have spawned new use cases. The authors are either involved in the development of Live Transcribe or active users of the application with several of the experiences being a frst hand account of the author’s use of the app. The reports were gathered by the Live Transcribe team at Google. Both Googlers and non-Googlers who used the app sent notes about some of their experiences about how they use the technology to the Live Transcribe team. Some of the reports came from researchers at Gallaudet University in Washington DC in the United States. Gal­ laudet University is a university for the Deaf and hard-of-hearing that collaborated with Google to make the app. More specifcally, the app was inspired by a dinner in a poorly lit restaurant with Dimitri Kanevsky and Sara Basson’s family in New York. Dimitri is deaf. It was too dark for Dimitri to be able to lip-read. Sara’s son took out his Android phone and voice typed in English into Google Docs. Dimitri read what Sara’s son was saying and got the idea for a transcription app that would type out what people were saying in real time. Dimitri Kanevsky is one of the co-creators of Live Transcribe along with Chet Gnegy. Susanna Zaraysky uses the app with her father in Russian and English to communicate through the glass doors of his nursing home. Prior to the Covid lockdown, she used the app when her father’s hearing aids weren’t working or for him to follow along better in doctor’s visits when he didn’t hear clearly. Olga Prilepova uses Live Transcribe both in English with people in California and in Russian with people in Moldova when visiting. To avoid holding a device in one’s hand while speaking in Live Transcribe, she has come up with ways to wear devices on one’s body to display the transcriptions and how to use multiple devices to display transcriptions during video calls. Sagar Savla is the Product Manager for Live Transcribe and other sound and accessibility research eforts. He was motivated by wanting to help his grandmother with her age-onset hearing loss. Fernando Loizides is an assistant professor of computer science and human-computer interaction who is gathering use cases for Live Transcribe and looking for paths to better interaction for accessi­ bility. He uses Live Transcribe in traditional and non-traditional ways to gather live transcription in meetings and to “catch-up” to conversation of a meeting that has already started by re-reading the previous minutes’ transcription.\n\nAny technology that is novel, faces a hurdle, big or small, of be­ ing accepted and used. This is modelled and frame-worked by the technology acceptance model [16]. Technology acceptance and adoption varies depending on several variables such as perhaps (among others) age, previous experience and exposure, trustworthi­ ness, perceived usefulness and likeability. In the following examples, we demonstrate diferent scenarios where technology acceptance and adoption is demonstrated across diferent scenarios. 2.1 Employment opportunities that were previously unavailable A hearing colleague went to a sports event. He needed directions for parking and asked an usher. The usher initially did not respond. When he got her attention, she pulled out her phone and started using Live Transcribe to converse. As a deaf person communicating with a hearing audience within the event context, the fexibility associated with Live Transcribe enabled her to fulfl a job as an usher. 2.2 Ability for deaf people to spontaneously communicate with a hearing person Two deaf men on a business trip were walking to their hotel from the conference site. They were signing to each other and not planning to communicate with a hearing person at that time. A woman that seemed distressed approached them, asking for help. It was too dark to read her lips and they couldn’t understand her. One of the deaf travellers took out his Android phone, opened Live Transcribe, and asked her to repeat her problem. Live Transcribe transcribed her perfectly. She was lost and couldn’t fnd her hotel. The two deaf travellers were able to accompany her back to where she needed to be. The woman was able to adapt seamlessly to Live Transcribe and was extremely grateful for the help in getting to her hotel safely. 2.3 Shared responsibility for communication: Hearing users enabling Live Transcribe A deaf user of Live Transcribe works in technical support, assisting employees with technical problems. In this particular instance, her phone had broken, and she did not know how she would commu­ nicate with her colleagues. Several hearing colleagues had already installed Live Transcribe on their phones, enabling her to commu­ nicate with them. This also freed her from her typical burden of constantly carrying her phone with her. Breaking Boundaries with Live Transcribe: Expanding Use Cases Beyond Standard Captioning Scenarios , Virtual Event, Greece Figure 1: Using Live Transcribe to ’listen In’ on the children’s conversation 2.4 Deeper understanding between deaf parents and hearing children There are complex relationships and psychological factors between parents and children, when some are DHH and others are hearing [24]. In the case where the parents are deaf, this can lead to the children being the primary spokespeople of the family and can sometimes leave the children feeling as though they have been deprived of their childhood [15]. In a family where all members are hearing, parents can engage in direct conversation with their children as well as overhearing “side” conversations (See Figure 1). A deaf professor commented that Live Transcribe enabled the parents to better understand how their children communicated, and how they helped each other with math. Live Transcribe enabled the parents to follow along to casual conversations as passive observers rather than active participants. As the parent puts it, “Now that’s a nice application. I fnally can see how the kids help each other with math”. 2.5 Changing the communication style of a hearing person speaking to deaf people A colleague indicated that she had communicated for many years with a deaf friend that depended on lipreading. Over the years, she had become aware of which sounds were more visible on the lips than others (only about 30% of English sounds are lip visible.) She had unconsciously become adept at opting for words with more visible sounds, given a choice. She also learned how to build up con­ text, so that less familiar words would be recognisable. As a result, her own language choices would shift when she communicated with him. If there were many unusual and unpredictable words that needed to be communicated, she shifted to written/chat formats. Nonetheless, she couldn’t be certain whether 100 per cent of what she was saying was intelligible. With Live Transcribe, verbal com­ munication became more natural. She no longer monitors word choices, consciously or subconsciously. Since she can also view the Live Transcribe output, she knows what is or is not accurately understood, and when to repeat or re-state. COVID-19 mask wearing rendered their previous communication methods impossible (e.g., standing close to their interlocutors and lip read­ ing). These examples show how DHH users, their family members, and strangers adapted to using Live Transcribe, including people previously unfamiliar with the app, in order to increase inclusivity [1]. 3.1 DHH people that typically lip read, interacting with hearing people wearing masks Mask wearing is pervasive during the COVID-19 pandemic, making lip reading impossible for DHH people that depend on it. A video (accessed June 2020) has become popular on Facebook, showing Live Transcribe providing conversational access when lipreading is unavailable (https://www.facebook.com/drew.ledson/videos/vb. 1090025427/10221234434025640/?type=2&theater). 3.2 Communication through closed (glass) doors A hearing colleague had a number of family celebrations in March 2020: It was the 40th anniversary of their departure from the former Soviet Union. It was also her parents’ 50th wedding anniversary. Due to COVID-19, normal celebrations became impossible. Her father was in a nursing home, and she knew that he would not be able to hear them speaking to him from outside the nursing home, even with his hearing aids. She opted to try Live Transcribe on a tablet. “One of the nurses wheeled my dad in his wheelchair to the glass doors of the nursing home,” she said. Her father wasn’t expecting to see her, or his grandchildren, standing outside (See Figure 2 Left). They were able to hear and understand him. He was able to read the transcription on the tablet, as his granddaughter said “We miss you and we love you”. It wasn’t the big party at a nice restaurant they had originally planned, but it provided an opportunity to recognise a meaningful time for their family. As quarantine procedures at the nursing home became stricter, even visiting by the glass door proved impossible. The family replicated the “glass door” experience using a smartphone and video chat (See Figure 2 Right). They held the tablet with transcription such that her father could see the speaker and the Russian transcribed speech at the same time. They commented on the signifcance of using this technology to celebrate their family’s departure from a country that didn’t allow people to communicate freely. 3.3 Speaker feedback leads to better speech A deaf patient with a Cochlear implant needed to see their audiolo­ gist at Johns Hopkins in Baltimore due to an emergency situation. It was the frst day that the clinic had started seeing clients again after Maryland had lifted some restrictions, and many protocols for mak­ ing the screening process for COVID-19 symptoms had been hastily put into place without regard for accessibility. At the entrance to the clinic, people in full Personal Protective Equipment (PPE) asked a set of screening questions, but it was impossible to understand anything they were saying. PPE also afects the accuracy of auto­ matic speech recognition, so simply using Live Transcribe on their phone didn’t quite work. The patient explained the situation, and , Virtual Event, Greece F.Loizides et al Figure 2: Left: Communicating through the glass with Live Transcribe Right: Speaking using Live Transcribe in Russian on their tablet while the father watches them via video call on the phone propped up on a homemade phone stand. by holding the microphone really close to the worker in PPE, the worker could see the output in real-time and also correct herself in real-time if the PPE resulted in a garbled response. This feedback mechanism was critical to making this communication work. 3.4 Managing distractions Online or face-to-face meetings provide an easy way for several par­ ties to communicate in real time. Online meetings are the prevalent mode during COVID-19 lockdown, and it is easy to get distracted by other stimuli when working alone. One hearing user commented that they often switch to another window to reply to an email or a chat. When they switch back to the meeting, they realize that they have missed a portion of the conversation. With Live Transcribe, they are able to “catch up” on the portion of the conversation they have missed. They now use Live Transcribe more frequently as a meeting tool to provide note taking. 3.5 Experimenting with communication at a distance A deaf colleague with a Cochlear implant had to ship two laptops and set up packing and shipping in the store. The process took about 20 minutes. He could not lip read given the store employee’s mask, and did not want to ask the store employee to lower his mask. Nor did he want to engage in prolonged close contact by back-and­ forth writing. He wasn’t sure how well Live Transcribe would work, given the 6 feet (2 meters) of separation between them. He put his phone on the counter so that it was in visual range for himself as well as the store employee. The transcription worked well, and it was helpful that the employee could see his own transcription in real time.\n\nCAPTIONING SPEECH AS A TOOL FOR SPEECH IMPROVEMENT FOR DHH USERS DHH people often have particular speech characteristics associated with hearing loss, often referred to as “deaf speech”. Research has explored the characteristics of deaf speech and various methods to improve articulation of deaf speakers, which is typically managed by speech therapists [5, 7, 19]. Practice with a speech therapist is inherently intermittent, however, and the therapist is not always available to correct a speaker’s pronunciation. In the following experiences, Live Transcribe has been used as a tool to determine whether the deaf speaker’s articulation is sufciently accurate to be recognized correctly. Where it is not recognized correctly, the deaf speaker can dynamically tweak their own pronunciation until it is recognizable. Figure 3: CART on a computer screen, Live Transcribe on a smartphone, and the round Roger Select (an assistive listen­ ing device) 4.1 Real time transcription to train DHH users with Cochlear implants Cochlear implants [9] can provide DHH people with some access to auditory information. Often users need to train themselves on interpreting the sound provided through the Cochlear implant. One user commented that he has historically done this training with speech transcription provided by CART writers; stenographers that caption speech to create access for DHH people (See Figure 3). The output from a CART writer generally lags the actual speech by a couple of seconds. The CART writer needs to hear the speech, and convert it to stenotype. A DHH Cochlear implant user commented that training is more complicated when there is a lag between what he hears, and what he reads as the caption. With Live Transcribe, the latency is minor. The word that he hears is still in auditory memory when the caption for the word appears. He shared his setup in the image below. He continues to use CART which appears on his computer screen. He believes CART provides highest accuracy when other people speaking have strong accents. In addition he has Live Transcribe on display, for auditory reinforcement with low latency. 5.1 Wearables Breaking Boundaries with Live Transcribe: Expanding Use Cases Beyond Standard Captioning Scenarios , Virtual Event, Greece Figure 4: Wearable innovations for Live Transcribe Interaction. Left: head and chest wear. Center: chest wear. Right: wrist wear. phone limits the person’s ability to do two-handed tasks. Also, if the captioning appears on a phone in the user’s hand, then the user will appear less engaged with other people in the conversation. Staring at the phone in hand can erroneously signal lack of engagement. Fi­ nally, the DHH person staring at a phone will miss other useful cues from the speaker that are gleaned from lip reading, body language, and other emotional indicators. We have begun experimenting with a number of display alternatives. The hat-mounted phone setup uses a sticky magnetic plate, a magnet and a tiny Palm phone (See Figure 4 - Left). Wearing and displaying Live Transcribe for others to view. One can monitor transcription by watching another phone with Live Transcribe in their hands (See Figure 4 - Center). This self-loop allows directing audio from a speaker to a mic, and tran­ scribes audio that is played on the phone (including phone calls). A bracelet can be made from a piece of fabric cut from a shirt sleeve. We experimented with several wrist/hand mounts using a running wristband or a thin glove (See Figure 4 - Right). Figure 5: Using a stand for hand’s free interaction 5.2 Touch-free access to captions We put a Live Transcribe tablet on a boom mic stand, for use by an immobile user. There are many ways to mount a tablet. Here, we used sticky-back velcro on a board (See Figure 5). This enables limited mobility DHH people to have a line of sight with the Live Transcribe tablet and be able to communicate with others in the room and through calls.\n\nIn this paper, we presented several experiences of non-standard use of an automatic speech recognition application, Live Tran­ scribe. These experiences serve as use cases to explore both the interaction behavioural models of DHH and hearing people. We have discovered social and physical barriers that were broken down through these use cases. and identifed where the lives of DHH people and their interactions were enriched. In future, we hope to identify more cases where automatic speech transcription , Virtual Event, Greece in Live Transcribe and other speech recognition tools advances communications beyond one to one interactions. A recent project, code named Euphonia,(https://ai.googleblog.com/2019/08/project­ euphonias-personalized-speech.html - accessed June 2020) allows users to train speech recognition models for non-standard speech. We also plan to capture experiences with other features in Live Tran­ scribe (such as environment sound detection, noise level indicator, haptic feedback) that were not presented in this paper.\n\nACKNOWLEDGMENTS\nWe would like to thank all the Live Transcribe developers, designers, testers, and leaders within Google and elsewhere. We would also like to thank everyone who has contributed their experiences for this article.\n\nREFERENCES\n[1] Vicki A Aaberg. 2012. A path to greater inclusivity through understanding implicit attitudes toward disability. Journal of Nursing Education 51, 9 (2012), 505–510. [2] Drew Arnold and Ann Tremblay. 1979. Interaction of deaf and hearing preschool children. Journal of Communication Disorders 12, 3 (1979), 245–251. [3] Gunnel AM Backenroth. 1995. Deaf people’s perception of social interaction in working life. International Journal of Rehabilitation Research 18, 1 (1995), 76–81. [4] Larwan Berke, Christopher Caulfeld, and Matt Huenerfauth. 2017. Deaf and hard-of-hearing perspectives on imperfect automatic speech recognition for captioning one-on-one meetings. In Proceedings of the 19th International ACM SIGACCESS Conference on Computers and Accessibility. 155–164. [5] Lynne E Bernstein, Edward T Auer, and Paula E Tucker. 2001. Enhanced speechreading in deaf adults: Can short-term training/practice close the gap for hearing adults? Journal of speech language and hearing research 44, 1 (2001), 5–18. [6] Rosemary Calderon and Mark Greenberg. 2011. Social and emotional develop­ ment of deaf children: Family, school, and program efects. The Oxford handbook of deaf studies, language, and education 1 (2011), 188–99. [7] Ruth Campbell and Helen Wright. 1988. Deafness, spelling and rhyme: How spelling supports written word and picture rhyming skills in deaf subjects. The Quarterly Journal of Experimental Psychology Section A 40, 4 (1988), 771–788. [8] John B Christiansen and Judy Egelston-Dodd. 1982. Socioeconomic Status of the Deaf Population. Vol. 4. Gallaudet College. [9] Graeme Clark. 2004. Cochlear implants. In Speech processing in the auditory system. Springer, 422–462. [10] Martin Cooke, Phil Green, Ljubomir Josifovski, and Ascension Vizinho. 2001. Robust automatic speech recognition with missing and unreliable acoustic data. Speech communication 34, 3 (2001), 267–285. [11] Kosei Fume, Taira Ashikawa, Nayuko Watanabe, and Hiroshi Fujimura. 2018. Implementation of Automatic Captioning System to Enhance the Accessibility of Meetings. In International Conference on Computers Helping People with Special Needs. Springer, 187–194. [12] Biing-Hwang Juang and Lawrence R Rabiner. 2005. Automatic speech of Technology. Atlanta Rutgers University and the University of California. Santa Barbara 1 (2005), 67. [13] Ronald R Kelly, Andrew B Quagliata, Richard DeMartino, and Victor Perotti. 2016. 21st-century deaf workers: Going beyond\" just employed\" to career growth and entrepreneurship. (2016). [14] Marit H Kvam, Mitchell Loeb, and Kristian Tambs. 2007. Mental health in deaf adults: symptoms of anxiety and depression among hearing and deaf individuals. The Journal of deaf studies and deaf education 12, 1 (2007), 1–7. [15] Harlan L Lane, Robert Hofmeister, and Benjamin J Bahan. 1996. A journey into the DEAF-WORLD. Dawn Sign Press. [16] Younghwa Lee, Kenneth A Kozar, and Kai RT Larsen. 2003. The technology acceptance model: Past, present, and future. Communications of the Association for information systems 12, 1 (2003), 50. [17] Janet E MacLeod-Gallinger. 1992. The career status of deaf women: A comparative look. American Annals of the Deaf (1992), 315–325. [18] Donna Jo Napoli and Rachel Sutton-Spence. 2010. Limitations on simultaneity in sign language. Language 86, 3 (2010), 647–662. [19] Nancy A Neef and Brian A Iwata. 1985. The development of generative lipreading skills in deaf persons using cued speech training. Analysis and intervention in developmental disabilities 5, 4 (1985), 289–305. F.Loizides et al [20] David L Passmore. 1982. Vocational and Economic Implications of Deafness. Journal of Epsilon Pi Tau 8, 2 (1982), 34–38. [21] Roberto Pieraccini and ICSI Director. 2012. From AUDREY to Siri. Is speech recognition a solved problem (2012), 23. [22] Robert L Schum. 1991. Communication and social growth: a developmental model of social behavior in deaf children. Ear and hearing 12, 5 (1991), 320–327. [23] Matthew Seita and Matt Huenerfauth. 2020. Deaf Individuals’ Views on Speak­ ing Behaviors of Hearing Peers when Using an Automatic Captioning App. In Extended Abstracts of the 2020 CHI Conference on Human Factors in Computing Systems. 1–8. [24] Jenny L Singleton and Matthew D Tittle. 2000. Deaf parents and their hearing children. Journal of Deaf studies and Deaf education 5, 3 (2000), 221–236. [25] Quentin Summerfeld. 1992. Lipreading and audio-visual speech perception. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences 335, 1273 (1992), 71–78. [26] Emily Q Wang and Anne Marie Piper. 2018. Accessibility in Action: Co-Located Collaboration Among Deaf and Hearing Professionals. Proceedings of the ACM on Human-Computer Interaction 2, CSCW (2018), 1–25. [27] Mary T Weiner and Margery Miller. 2006. Deaf children and bullying: Directions for future research. American Annals of the Deaf 151, 1 (2006), 61–70.\n--- FOOTNOTES 1 INTRODUCTION AND MOTIVATION © 2020 Copyright held by the owner/author(s). 2 COMMUNITY ACCEPTANCE AND 3 EMERGENCY COMMUNICATION DURING 4 CAPTIONING SPEECH AS A TOOL FOR 5 HANDS-FREE INTERACTION 6 SUMMARY AND FUTURE WORK recognition–a brief history of the technology development. Georgia Institute", "persona": "Student", "response": "### 1. Overview\nNot being able to hear in environments predicated on sounds and aural communication presents multiple challenges. For deaf and hard of hearing (DHH) people, this can lead to severe communication breakdowns. For children, this can hinder social and emotional development [2, 6, 22, 27]. Developmental gaps can persist to adulthood [3] with higher instances of depression [14]. Communication gaps have historically created barriers to employment and to informal human interactions [8, 17, 20]. Some positive changes in employment have emerged with legislative changes, but significant gaps in employment and career development persist for DHH people [13]. Reducing communication friction between a DHH person and other DHH or hearing people can improve quality of life for the DHH individual and for their extended community of contacts. Sign language provides excellent communication opportunities within deaf signing communities, but is generally unavailable outside of these communities. Some deaf children are trained in oral communication and may not themselves be fluent in sign language. Adults who have lost hearing later in life may not adopt sign language, and may prefer text based communication [4]. Fluent sign language users can experience communication blocks when sign language interpreters are not available[18]. For casual communication, there is a high cost barrier.\n\n### 2. Background\n1 INTRODUCTION AND MOTIVATION 2 COMMUNITY ACCEPTANCE AND ADOPTION Any technology that is novel, faces a hurdle, big or small, of being accepted and used. This is modelled and frame-worked by the technology acceptance model [16]. Technology acceptance and adoption varies depending on several variables such as perhaps (among others) age, previous experience and exposure, trustworthiness, perceived usefulness and likeability. In the following examples, we demonstrate different scenarios where technology acceptance and adoption is demonstrated across different scenarios. A hearing colleague went to a sports event. He needed directions for parking and asked an usher. The usher initially did not respond. When he got her attention, she pulled out her phone and started using Live Transcribe to converse. As a deaf person communicating with a hearing audience within the event context, the flexibility associated with Live Transcribe enabled her to fulfil a job as an usher. Two deaf men on a business trip were walking to their hotel from the conference site. They were signing to each other and not planning to communicate with a hearing person at that time. A woman that seemed distressed approached them, asking for help.\n\n### 3. Methods\nContemporary research has focused attention on technical solutions to improve interactions between DHH and speaking/hearing communities. Medical advancements such as Cochlear implants are increasingly prevalent[9]. The expectation is that users with Cochlear implants will “learn to identify enough sounds” to communicate in the speech/hearing community, supplemented by lip reading. This is not always the case, as acoustic or visual conditions may be less than ideal for the DHH with Cochlear implants to fully engage in conversation. Speech captioning is often a necessary supplement to improve the communication flow, and automatic speech recognition has become a critical assistive technology [12]. Bell Labs in the 1950s [21] was one of the early inventors and developers of speech recognition technology. Early applications of speech recognition targeted controlling computers by voice, interacting with services over the phone through voice menus, and speech dictation (generating documents through speech rather than using a keyboard.) Telecommunications relay services also used speech recognition (https://www.washington.edu/accesscomputing/what-are-telecommunications-relay-services - accessed June 2020).\n\n### 4. Results\nDHH users would have a phone with a text screen. A third party intermediary would re-speak the conversation of the hearing party into speech recognition that was trained for the intermediary's voice. Text from the speech recognition output would appear on the DHH user's screen. As processing power increased and algorithms advanced, speech recognition improved such that trained third party intermediaries have become less necessary, and the speech of the hearing parties in the conversation can be transcribed directly (although sometimes there are challenges to doing so automatically [10]) with speech recognition [11]. As speech recognition technology has improved, it has been used more widely to caption lectures, videos, and conversations between DHH and hearing people [23, 26].\n\n### 5. Takeaways\n5.1 Wearables In this paper, we presented several experiences of non-standard use of an automatic speech recognition application, Live Transcribe. These experiences serve as use cases to explore both the interaction behavioural models of DHH and hearing people. We have discovered social and physical barriers that were broken down through these use cases. and identified where the lives of DHH people and their interactions were enriched. In future, we hope to identify more cases where automatic speech transcription in Live Transcribe and other speech recognition tools advances communications beyond one to one interactions. A recent project, code named Euphonia,(https://ai.googleblog.com/2019/08/project-euphonias-personalized-speech.html - accessed June 2020) allows users to train speech recognition models for non-standard speech. We also plan to capture experiences with other features in Live Transcribe (such as environment sound detection, noise level indicator, haptic feedback) that were not presented in this paper.", "paper_id": "33", "story_id": "33-01"}
{"paper": "Show and Tell: A Neural Image Caption Generator Oriol Vinyals Google Alexander Toshev Google vinyals@google.com toshev@google.com\n\nAbstract\nAutomatically describing the content of an image is a fundamental problem in artiﬁcial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the ﬂuency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. For instance, while the current state-of-the-art BLEU-1 score (the higher the better) on the Pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. We also show BLEU-1 score improvements on Flickr30k, from 56 to 66, and on SBU, from 19 to 28. Lastly, on the newly released COCO dataset, we achieve a BLEU-4 of 27.7, which is the current state-of-the-art.\n\n1. Introduction\nBeing able to automatically describe the content of an image using properly formed English sentences is a very challenging task, but it could have great impact, for instance by helping visually impaired people better understand the content of images on the web. This task is signiﬁcantly harder, for example, than the well-studied image classiﬁcation or object recognition tasks, which have been a main focus in the computer vision community [27]. Indeed, a description must capture not only the objects contained in an image, but it also must express how these objects relate to each other as well as their attributes and the activities they are involved in. Moreover, the above semantic knowledge has to be expressed in a natural language like English, which means that a language model is needed in addition to visual understanding. Most previous attempts have proposed to stitch together Samy Bengio Google Dumitru Erhan Google bengio@google.com dumitru@google.com A group of people shopping at an outdoor market. ! There are many vegetables at the fruit stand. Language ! Generating! Vision! Deep CNN RNN Figure 1. NIC, our model, is based end-to-end on a neural network consisting of a vision CNN followed by a language generating RNN. It generates complete sentences in natural language from an input image, as shown on the example above. existing solutions of the above sub-problems, in order to go from an image to its description [6, 16]. In contrast, we would like to present in this work a single joint model that takes an image I as input, and is trained to maximize the likelihood p(S|I) of producing a target sequence of words S = {S1, S2, . . .} where each word St comes from a given dictionary, that describes the image adequately. The main inspiration of our work comes from recent advances in machine translation, where the task is to transform a sentence S written in a source language, into its translation T in the target language, by maximizing p(T|S). For many years, machine translation was also achieved by a series of separate tasks (translating words individually, aligning words, reordering, etc), but recent work has shown that translation can be done in a much simpler way using Recurrent Neural Networks (RNNs) [3, 2, 30] and still reach state-of-the-art performance. An “encoder” RNN reads the source sentence and transforms it into a rich ﬁxed-length vector representation, which in turn in used as the initial hidden state of a “decoder” RNN that generates the target sentence. Here, we propose to follow this elegant recipe, replacing the encoder RNN by a deep convolution neural network (CNN). Over the last few years it has been convincingly shown that CNNs can produce a rich representation of the input image by embedding it to a ﬁxed-length vector, such that this representation can be used for a variety of vision tasks [28]. Hence, it is natural to use a CNN as an image “encoder”, by ﬁrst pre-training it for an image classiﬁcation task and using the last hidden layer as an input to the RNN decoder that generates sentences (see Fig. 1). We call this model the Neural Image Caption, or NIC. Our contributions are as follows. First, we present an end-to-end system for the problem. It is a neural net which is fully trainable using stochastic gradient descent. Second, our model combines state-of-art sub-networks for vision and language models. These can be pre-trained on larger corpora and thus can take advantage of additional data. Finally, it yields signiﬁcantly better performance compared to state-of-the-art approaches; for instance, on the Pascal dataset, NIC yielded a BLEU score of 59, to be compared to the current state-of-the-art of 25, while human performance reaches 69. On Flickr30k, we improve from 56 to 66, and on SBU, from 19 to 28.\n\n3. Model\nIn this paper, we propose a neural and probabilistic framework to generate descriptions from images. Recent advances in statistical machine translation have shown that, given a powerful sequence model, it is possible to achieve state-of-the-art results by directly maximizing the probability of the correct translation given an input sentence in an “end-to-end” fashion – both for training and inference. These models make use of a recurrent neural network which encodes the variable length input into a ﬁxed dimensional vector, and uses this representation to “decode” it to the desired output sentence. Thus, it is natural to use the same approach where, given an image (instead of an input sentence in the source language), one applies the same principle of “translating” it into its description. Thus, we propose to directly maximize the probability of the correct description given the image by using the following formulation: S its correct transcription. Since S represents any sentence, its length is unbounded. Thus, it is common to apply the chain rule to model the joint probability over S0, . . . , SN, where N is the length of this particular example as N X log p(S|I) = t=0 log p(St|I, S0, . . . , St−1) (2) where we dropped the dependency on θ for convenience. At training time, (S, I) is a training example pair, and we optimize the sum of the log probabilities as described in (2) over the whole training set using stochastic gradient descent (further training details are given in Section 4). It is natural to model p(St|I, S0, . . . , St−1) with a Recurrent Neural Network (RNN), where the variable number of words we condition upon up to t −1 is expressed by a ﬁxed length hidden state or memory ht. This memory is updated after seeing a new input xt by using a non-linear function f: ht+1 = f(ht, xt) . (3) To make the above RNN more concrete two crucial design choices are to be made: what is the exact form of f and how are the images and words fed as inputs xt. For f we use a Long-Short Term Memory (LSTM) net, which has shown state-of-the art performance on sequence tasks such as translation. This model is outlined in the next section. For the representation of images, we use a Convolutional Neural Network (CNN). They have been widely used and studied for image tasks, and are currently state-of-the art for object recognition and detection. Our particular choice of CNN uses a novel approach to batch normalization and yields the current best performance on the ILSVRC 2014 classiﬁcation competition [12]. Furthermore, they have been shown to generalize to other tasks such as scene classiﬁcation by means of transfer learning [4]. The words are represented with an embedding model. 3.1. LSTM-based Sentence Generator The choice of f in (3) is governed by its ability to deal with vanishing and exploding gradients [10], the most common challenge in designing and training RNNs. To address this challenge, a particular form of recurrent nets, called LSTM, was introduced [10] and applied with great success to translation [3, 30] and sequence generation [9]. The core of the LSTM model is a memory cell c encoding knowledge at every time step of what inputs have been observed up to this step (see Figure 2) . The behavior of the cell is controlled by “gates” – layers which are applied multiplicatively and thus can either keep a value from the gated layer if the gate is 1 or zero this value if the gate is 0. In particular, three gates are being used which control whether to forget the current cell value (forget gate f), if it should word prediction softmax LSTM memory block mt σ forget ct-1 gate f output σ c gate f ct σ input gate i updating h term x input Figure 2. LSTM: the memory block contains a cell c which is controlled by three gates. In blue we show the recurrent connections – the output m at time t −1 is fed back to the memory at time t via the three gates; the cell value is fed back via the forget gate; the predicted word at time t −1 is fed back in addition to the memory output m at time t into the Softmax for word prediction. read its input (input gate i) and whether to output the new cell value (output gate o). The deﬁnition of the gates and cell update and output are as follows: it = σ(Wixxt + Wimmt−1) (4) ft = σ(Wfxxt + Wfmmt−1) (5) ot = σ(Woxxt + Wommt−1) (6) ct = ft ⊙ct−1 + it ⊙h(Wcxxt + Wcmmt−1)(7) mt = ot ⊙ct (8) pt+1 = Softmax(mt) (9) where ⊙represents the product with a gate value, and the various W matrices are trained parameters. Such multiplicative gates make it possible to train the LSTM robustly as these gates deal well with exploding and vanishing gradients [10]. The nonlinearities are sigmoid σ(·) and hyperbolic tangent h(·). The last equation mt is what is used to feed to a Softmax, which will produce a probability distribution pt over all words. Training The LSTM model is trained to predict each word of the sentence after it has seen the image as well as all preceding words as deﬁned by p(St|I, S0, . . . , St−1). For this purpose, it is instructive to think of the LSTM in unrolled form – a copy of the LSTM memory is created for the log p1(S1) log p2(S2) log pN(SN) p1 pN p2 ... LSTM LSTM LSTM LSTM WeS0 WeS1 WeSN-1 S1 SN-1 S0 image Figure 3. LSTM model combined with a CNN image embedder (as deﬁned in [12]) and word embeddings. The unrolled connections between the LSTM memories are in blue and they correspond to the recurrent connections in Figure 2. All LSTMs share the same parameters. image and each sentence word such that all LSTMs share the same parameters and the output mt−1 of the LSTM at time t −1 is fed to the LSTM at time t (see Figure 3). All recurrent connections are transformed to feed-forward connections in the unrolled version. In more detail, if we denote by I the input image and by S = (S0, . . . , SN) a true sentence describing this image, the unrolling procedure reads: x−1 = CNN(I) (10) xt = WeSt, t ∈{0 . . . N −1} (11) pt+1 = LSTM(xt), t ∈{0 . . . N −1} (12) where we represent each word as a one-hot vector St of dimension equal to the size of the dictionary. Note that we denote by S0 a special start word and by SN a special stop word which designates the start and end of the sentence. In particular by emitting the stop word the LSTM signals that a complete sentence has been generated. Both the image and the words are mapped to the same space, the image by using a vision CNN, the words by using word embedding We. The image I is only input once, at t = −1, to inform the LSTM about the image contents. We empirically veriﬁed that feeding the image at each time step as an extra input yields inferior results, as the network can explicitly exploit noise in the image and overﬁts more easily. Our loss is the sum of the negative log likelihood of the correct word at each step as follows: N X L(I, S) = − t=1 log pt(St) . (13) Inference There are multiple approaches that can be used to generate a sentence given an image, with NIC. The ﬁrst one is Sampling where we just sample the ﬁrst word according to p1, then provide the corresponding embedding as input and sample p2, continuing like this until we sample the special end-of-sentence token or some maximum length. The second one is BeamSearch: iteratively consider the set of the k best sentences up to time t as candidates to generate sentences of size t + 1, and keep only the resulting best k of them. This better approximates S = arg maxS′ p(S′|I). We used the BeamSearch approach in the following experiments, with a beam of size 20. Using a beam size of 1 (i.e., greedy search) did degrade our results by 2 BLEU points on average.\n\n4. Experiments\nWe performed an extensive set of experiments to assess the effectiveness of our model using several metrics, data sources, and model architectures, in order to compare to prior art. 4.1. Evaluation Metrics Although it is sometimes not clear whether a description should be deemed successful or not given an image, prior art has proposed several evaluation metrics. The most reliable (but time consuming) is to ask for raters to give a subjective score on the usefulness of each description given the image. In this paper, we used this to reinforce that some of the automatic metrics indeed correlate with this subjective score, following the guidelines proposed in [11], which asks the graders to evaluate each generated sentence with a scale from 1 to 41. For this metric, we set up an Amazon Mechanical Turk experiment. Each image was rated by 2 workers. The typical level of agreement between workers is 65%. In case of disagreement we simply average the scores and record the average as the score. For variance analysis, we perform bootstrapping (re-sampling the results with replacement and computing means/standard deviation over the resampled results). Like [11] we report the fraction of scores which are larger or equal than a set of predeﬁned thresholds. The rest of the metrics can be computed automatically assuming one has access to groundtruth, i.e. human generated descriptions. The most commonly used metric so far in the image description literature has been the BLEU score [25], which is a form of precision of word n-grams between generated and reference sentences 2. Even though with an unrelated description, with a score of 4 being the best and 1 being the worst. compute precision at the unigram level, whereas BLEU-n is a geometric average of precision over 1- to n-grams. this metric has some obvious drawbacks, it has been shown to correlate well with human evaluations. In this work, we corroborate this as well, as we show in Section 4.3. An extensive evaluation protocol, as well as the generated outputs of our system, can be found at http://nic. droppages.com/. Besides BLEU, one can use the perplexity of the model for a given transcription (which is closely related to our objective function in (1)). The perplexity is the geometric mean of the inverse probability for each predicted word. We used this metric to perform choices regarding model selection and hyperparameter tuning in our held-out set, but we do not report it since BLEU is always preferred 3. A much more detailed discussion regarding metrics can be found in [31], and research groups working on this topic have been reporting other metrics which are deemed more appropriate for evaluating caption. We report two such metrics - METEOR and Cider - hoping for much more discussion and research to arise regarding the choice of metric. Lastly, the current literature on image description has also been using the proxy task of ranking a set of available descriptions with respect to a given image (see for instance [14]). Doing so has the advantage that one can use known ranking metrics like recall@k. On the other hand, transforming the description generation task into a ranking task is unsatisfactory: as the complexity of images to describe grows, together with its dictionary, the number of possible sentences grows exponentially with the size of the dictionary, and the likelihood that a predeﬁned sentence will ﬁt a new image will go down unless the number of such sentences also grows exponentially, which is not realistic; not to mention the underlying computational complexity of evaluating efﬁciently such a large corpus of stored sentences for each image. The same argument has been used in speech recognition, where one has to produce the sentence corresponding to a given acoustic sequence; while early attempts concentrated on classiﬁcation of isolated phonemes or words, state-of-the-art approaches for this task are now generative and can produce sentences from a large dictionary. Now that our models can generate descriptions of reasonable quality, and despite the ambiguities of evaluating an image description (where there could be multiple valid descriptions not in the groundtruth) we believe we should concentrate on evaluation metrics for the generation task rather than for ranking. 4.2. Datasets For evaluation we use a number of datasets which consist of images and sentences in English describing these images. The statistics of the datasets are as follows: Dataset name size train valid. test Pascal VOC 2008 [6] - - 1000 Flickr8k [26] 6000 1000 1000 Flickr30k [33] 28000 1000 1000 MSCOCO [20] 82783 40504 40775 SBU [24] - - With the exception of SBU, each image has been annotated by labelers with 5 sentences that are relatively visual and unbiased. SBU consists of descriptions given by image owners when they uploaded them to Flickr. As such they are not guaranteed to be visual or unbiased and thus this dataset has more noise. The Pascal dataset is customary used for testing only after a system has been trained on different data such as any of the other four dataset. In the case of SBU, we hold out 1000 images for testing and train on the rest as used by [18]. Similarly, we reserve 4K random images from the MSCOCO validation set as test, called COCO-4k, and use it to report results in the following section. 4.3. Results Since our model is data driven and trained end-to-end, and given the abundance of datasets, we wanted to answer questions such as “how dataset size affects generalization”, “what kinds of transfer learning it would be able to achieve”, and “how it would deal with weakly labeled examples”. As a result, we performed experiments on ﬁve different datasets, explained in Section 4.2, which enabled us to understand our model in depth. 4.3.1 Training Details Many of the challenges that we faced when training our models had to do with overﬁtting. Indeed, purely supervised approaches require large amounts of data, but the datasets that are of high quality have less than 100000 images. The task of assigning a description is strictly harder than object classiﬁcation and data driven approaches have only recently become dominant thanks to datasets as large as ImageNet (with ten times more data than the datasets we described in this paper, with the exception of SBU). As a result, we believe that, even with the results we obtained which are quite good, the advantage of our method versus most current human-engineered approaches will only increase in the next few years as training set sizes will grow. Nonetheless, we explored several techniques to deal with overﬁtting. The most obvious way to not overﬁt is to initialize the weights of the CNN component of our system to a pretrained model (e.g., on ImageNet). We did this in all the experiments (similar to [8]), and it did help quite a lot in terms of generalization. Another set of weights that could be sensibly initialized are We, the word embeddings. We tried initializing them from a large news corpus [22], but no signiﬁcant gains were observed, and we decided to just leave them uninitialized for simplicity. Lastly, we did some model level overﬁtting-avoiding techniques. We tried dropout [34] and ensembling models, as well as exploring the size (i.e., capacity) of the model by trading off number of hidden units versus depth. Dropout and ensembling gave a few BLEU points improvement, and that is what we report throughout the paper. We trained all sets of weights using stochastic gradient descent with ﬁxed learning rate and no momentum. All weights were randomly initialized except for the CNN weights, which we left unchanged because changing them had a negative impact. We used 512 dimensions for the embeddings and the size of the LSTM memory. Descriptions were preprocessed with basic tokenization, keeping all words that appeared at least 5 times in the training set. 4.3.2 Generation Results We report our main results on all the relevant datasets in Tables 1 and 2. Since PASCAL does not have a training set, we used the system trained using MSCOCO (arguably the largest and highest quality dataset for this task). The stateof-the-art results for PASCAL and SBU did not use image features based on deep learning, so arguably a big improvement on those scores comes from that change alone. The Flickr datasets have been used recently [11, 21, 14], but mostly evaluated in a retrieval framework. A notable exception is [21], where they did both retrieval and generation, and which yields the best performance on the Flickr datasets up to now. Human scores in Table 2 were computed by comparing one of the human captions against the other four. We do this for each of the ﬁve raters, and average their BLEU scores. Since this gives a slight advantage to our system, given the BLEU score is computed against ﬁve reference sentences and not four, we add back to the human scores the average difference of having ﬁve references instead of four. Given that the ﬁeld has seen signiﬁcant advances in the last years, we do think it is more meaningful to report BLEU-4, which is the standard in machine translation moving forward. Additionally, we report metrics shown to correlate better with human evaluations in Table 14. Despite recent efforts on better evaluation metrics [31], our model fares strongly versus human raters. However, when evaluating our captions using human raters (see Section 4.3.6), our model fares much more poorly, suggesting more work Metric BLEU-4 METEOR CIDER NIC 27.7 23.7 85.5 Random 4.6 9.0 5.1 Nearest Neighbor 9.9 15.7 36.5 Human 21.7 25.2 85.4 Table 1. Scores on the MSCOCO development set. Approach PASCAL Flickr Flickr SBU (xfer) 30k 8k Im2Text [24] 11 TreeTalk [18] 19 BabyTalk [16] 25 Tri5Sem [11] 48 m-RNN [21] 55 58 MNLM [14]5 56 51 SOTA 25 56 58 19 NIC 59 66 63 28 Human 69 68 70 Table 2. BLEU-1 scores. We only report previous work results when available. SOTA stands for the current state-of-the-art. is needed towards better metrics. On the ofﬁcial test set for which labels are only available through the ofﬁcial website, our model had a 27.2 BLEU-4. 4.3.3 Transfer Learning, Data Size and Label Quality Since we have trained many models and we have several testing sets, we wanted to study whether we could transfer a model to a different dataset, and how much the mismatch in domain would be compensated with e.g. higher quality labels or more training data. The most obvious case for transfer learning and data size is between Flickr30k and Flickr8k. The two datasets are similarly labeled as they were created by the same group. Indeed, when training on Flickr30k (with about 4 times more training data), the results obtained are 4 BLEU points better. It is clear that in this case, we see gains by adding more training data since the whole process is data-driven and overﬁtting prone. MSCOCO is even bigger (5 times more training data than Flickr30k), but since the collection process was done differently, there are likely more differences in vocabulary and a larger mismatch. Indeed, all the BLEU scores degrade by 10 points. Nonetheless, the descriptions are still reasonable. Since PASCAL has no ofﬁcial training set and was collected independently of Flickr and MSCOCO, we report transfer learning from MSCOCO (in Table 2). Doing transfer learning from Flickr30k yielded worse results with BLEU-1 at 53 (cf. 59). Lastly, even though SBU has weak labeling (i.e., the labels were captions and not human generated descriptions), the task is much harder with a much larger and noisier vocabulary. However, much more data is available for training. When running the MSCOCO model on SBU, our performance degrades from 28 down to 16. 4.3.4 Generation Diversity Discussion Having trained a generative model that gives p(S|I), an obvious question is whether the model generates novel captions, and whether the generated captions are both diverse and high quality. Table 3 shows some samples when returning the N-best list from our beam search decoder instead of the best hypothesis. Notice how the samples are diverse and may show different aspects from the same image. The agreement in BLEU score between the top 15 generated sentences is 58, which is similar to that of humans among them. This indicates the amount of diversity our model generates. In bold are the sentences that are not present in the training set. If we take the best candidate, the sentence is present in the training set 80% of the times. This is not too surprising given that the amount of training data is quite small, so it is relatively easy for the model to pick “exemplar” sentences and use them to generate descriptions. If we instead analyze the top 15 generated sentences, about half of the times we see a completely novel description, but still with a similar BLEU score, indicating that they are of enough quality, yet they provide a healthy diversity. A man throwing a frisbee in a park. A man holding a frisbee in his hand. A man standing in the grass with a frisbee. A close up of a sandwich on a plate. A close up of a plate of food with french fries. A white plate topped with a cut in half sandwich. A display case ﬁlled with lots of donuts. A display case ﬁlled with lots of cakes. A bakery display case ﬁlled with lots of donuts. Table 3. N-best examples from the MSCOCO test set. Bold lines indicate a novel sentence not present in the training set. 4.3.5 Ranking Results While we think ranking is an unsatisfactory way to evaluate description generation from images, many papers report ranking scores, using the set of testing captions as candidates to rank given a test image. The approach that works best on these metrics (MNLM), speciﬁcally implemented a ranking-aware loss. Nevertheless, NIC is doing surprisingly well on both ranking tasks (ranking descriptions given images, and ranking images given descriptions), as can be seen in Tables 4 and 5. Note that for the Image Annotation task, we normalized our scores similar to what [21] used. Approach Image Annotation Image Search R@1 R@10 Med r R@1 R@10 Med r DeFrag [13] 13 44 14 10 43 15 m-RNN [21] 15 49 11 12 42 15 MNLM [14] 18 55 8 13 52 10 NIC 20 61 6 19 64 5 Table 4. Recall@k and median rank on Flickr8k. Approach Image Annotation Image Search R@1 R@10 Med r R@1 R@10 Med r DeFrag [13] 16 55 8 10 45 13 m-RNN [21] 18 51 10 13 42 16 MNLM [14] 23 63 5 17 57 8 NIC 17 56 7 17 57 7 Table 5. Recall@k and median rank on Flickr30k. Figure 4. Flickr-8k: NIC: predictions produced by NIC on the Flickr8k test set (average score: 2.37); Pascal: NIC: (average score: 2.45); COCO-1k: NIC: A subset of 1000 images from the MSCOCO test set with descriptions produced by NIC (average score: 2.72); Flickr-8k: ref: these are results from [11] on Flickr8k rated using the same protocol, as a baseline (average score: 2.08); Flickr-8k: GT: we rated the groundtruth labels from Flickr8k using the same protocol. This provides us with a “calibration” of the scores (average score: 3.89) 4.3.6 Human Evaluation Figure 4 shows the result of the human evaluations of the descriptions provided by NIC, as well as a reference system and groundtruth on various datasets. We can see that NIC is better than the reference system, but clearly worse than the groundtruth, as expected. This shows that BLEU is not a perfect metric, as it does not capture well the difference between NIC and human descriptions assessed by raters. Examples of rated images can be seen in Figure 5. It is interesting to see, for instance in the second image of the ﬁrst column, how the model was able to notice the frisbee given its size. Figure 5. A selection of evaluation results, grouped by human rating. 4.3.7 Analysis of Embeddings In order to represent the previous word St−1 as input to the decoding LSTM producing St, we use word embedding vectors [22], which have the advantage of being independent of the size of the dictionary (contrary to a simpler onehot-encoding approach). Furthermore, these word embeddings can be jointly trained with the rest of the model. It is remarkable to see how the learned representations have captured some semantic from the statistics of the language. Table 4.3.7 shows, for a few example words, the nearest other words found in the learned embedding space. Note how some of the relationships learned by the model will help the vision component. Indeed, having “horse”, “pony”, and “donkey” close to each other will encourage the CNN to extract features that are relevant to horse-looking animals. We hypothesize that, in the extreme case where we see very few examples of a class (e.g., “unicorn”), its proximity to other word embeddings (e.g., “horse”) should provide a lot more information that would be completely lost with more traditional bag-of-words based approaches.\n\n2. Related Work\nThe problem of generating natural language descriptions from visual data has long been studied in computer vision, but mainly for video [7, 32]. This has led to complex systems composed of visual primitive recognizers combined with a structured formal language, e.g. And-Or Graphs or logic systems, which are further converted to natural language via rule-based systems. Such systems are heavily hand-designed, relatively brittle and have been demonstrated only on limited domains, e.g. trafﬁc scenes or sports. The problem of still image description with natural text has gained interest more recently. Leveraging recent advances in recognition of objects, their attributes and locations, allows us to drive natural language generation systems, though these are limited in their expressivity. Farhadi et al. [6] use detections to infer a triplet of scene elements which is converted to text using templates. Similarly, Li et al. [19] start off with detections and piece together a ﬁnal description using phrases containing detected objects and relationships. A more complex graph of detections beyond triplets is used by Kulkani et al. [16], but with template-based text generation. More powerful language models based on language parsing have been used as well [23, 1, 17, 18, 5]. The above approaches have been able to describe images “in the wild”, but they are heavily handdesigned and rigid when it comes to text generation. A large body of work has addressed the problem of ranking descriptions for a given image [11, 8, 24]. Such approaches are based on the idea of co-embedding of images and text in the same vector space. For an image query, descriptions are retrieved which lie close to the image in the embedding space. Most closely, neural networks are used to co-embed images and sentences together [29] or even image crops and subsentences [13] but do not attempt to generate novel descriptions. In general, the above approaches cannot describe previously unseen compositions of objects, even though the individual objects might have been observed in the training data. Moreover, they avoid addressing the problem of evaluating how good a generated description is. In this work we combine deep convolutional nets for image classiﬁcation [12] with recurrent networks for sequence modeling [10], to create a single network that generates descriptions of images. The RNN is trained in the context of this single “end-to-end” network. The model is inspired by recent successes of sequence generation in machine translation [3, 2, 30], with the difference that instead of starting with a sentence, we provide an image processed by a convolutional net. The closest works are by Kiros et al. [15] who use a neural net, but a feedforward one, to predict the next word given the image and previous words. A recent work by Mao et al. [21] uses a recurrent NN for the same prediction task. This is very similar to the present proposal but there are a number of important differences: we use a more powerful RNN model, and provide the visual input to the RNN model directly, which makes it possible for the RNN to keep track of the objects that have been explained by the text. As a result of these seemingly insigniﬁcant differences, our system achieves substantially better results on the established benchmarks. Lastly, Kiros et al. [14] propose to construct a joint multimodal embedding space by using a powerful computer vision model and an LSTM that encodes text. In contrast to our approach, they use two separate pathways (one for images, one for text) to deﬁne a joint embedding, and, even though they can generate text, their approach is highly tuned for ranking.\n\n5. Conclusion\nWord Neighbors car van, cab, suv, vehicule, jeep boy toddler, gentleman, daughter, son street road, streets, highway, freeway horse pony, donkey, pig, goat, mule computer computers, pc, crt, chip, compute Table 6. Nearest neighbors of a few example words a reasonable description in plain English. NIC is based on a convolution neural network that encodes an image into a compact representation, followed by a recurrent neural network that generates a corresponding sentence. The model is trained to maximize the likelihood of the sentence given the image. Experiments on several datasets show the robustness of NIC in terms of qualitative results (the generated sentences are very reasonable) and quantitative evaluations, using either ranking metrics or BLEU, a metric used in machine translation to evaluate the quality of generated sentences. It is clear from these experiments that, as the size of the available datasets for image description increases, so will the performance of approaches like NIC. Furthermore, it will be interesting to see how one can use unsupervised data, both from images alone and text alone, to improve image description approaches.\n\nReferences\n[1] A. Aker and R. Gaizauskas. Generating image descriptions using dependency relational patterns. In ACL, 2010. 2 [2] D. Bahdanau, K. Cho, and Y. Bengio. Neural machine translation by jointly learning to align and translate. arXiv:1409.0473, 2014. 1, 2 [3] K. Cho, B. van Merrienboer, C. Gulcehre, F. Bougares,\nH. Schwenk, and Y. Bengio. Learning phrase representations\nusing RNN encoder-decoder for statistical machine translation. In EMNLP, 2014. 1, 2, 3 [4] J. Donahue, Y. Jia, O. Vinyals, J. Hoffman, N. Zhang,\nE. Tzeng, and T. Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In ICML, 2014.\n3 [5] D. Elliott and F. Keller. Image description using visual dependency representations. In EMNLP, 2013. 2 [6] A. Farhadi,\nM. Hejrati,\nM. A. Sadeghi,\nP. Young,\nC. Rashtchian, J. Hockenmaier, and D. Forsyth. Every picture tells a story: Generating sentences from images.\nIn ECCV, 2010. 1, 2, 5 [7] R. Gerber and H.-H. Nagel. Knowledge representation for the generation of quantiﬁed natural language descriptions of vehicle trafﬁc in image sequences. In ICIP. IEEE, 1996. 2 [8] Y. Gong, L. Wang, M. Hodosh, J. Hockenmaier, and\nS. Lazebnik. Improving image-sentence embeddings using\nlarge weakly annotated photo collections. In ECCV, 2014. 2, 5 [9] A. Graves. Generating sequences with recurrent neural networks. arXiv:1308.0850, 2013. 3 [10] S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural Computation, 9(8), 1997. 2, 3 [11] M. Hodosh, P. Young, and J. Hockenmaier. Framing image description as a ranking task: Data, models and evaluation metrics. JAIR, 47, 2013. 2, 4, 6, 7 [12] S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In arXiv:1502.03167, 2015. 2, 3, 4 [13] A. Karpathy, A. Joulin, and L. Fei-Fei. Deep fragment embeddings for bidirectional image sentence mapping. NIPS,\n2014. 2, 7\n[14] R. Kiros, R. Salakhutdinov, and R. S. Zemel. Unifying visual-semantic embeddings with multimodal neural language models. In arXiv:1411.2539, 2014. 2, 5, 6, 7 [15] R. Kiros and R. Z. R. Salakhutdinov. Multimodal neural language models. In NIPS Deep Learning Workshop, 2013. 2 [16] G. Kulkarni, V. Premraj, S. Dhar, S. Li, Y. Choi, A. C. Berg, and T. L. Berg. Baby talk: Understanding and generating simple image descriptions. In CVPR, 2011. 1, 2, 6 [17] P. Kuznetsova, V. Ordonez, A. C. Berg, T. L. Berg, and\nY. Choi. Collective generation of natural image descriptions.\nIn ACL, 2012. 2 [18] P. Kuznetsova, V. Ordonez, T. Berg, and Y. Choi. Treetalk: Composition and compression of trees for image descriptions. ACL, 2(10), 2014. 2, 5, 6 [19] S. Li, G. Kulkarni, T. L. Berg, A. C. Berg, and Y. Choi. Composing simple image descriptions using web-scale n-grams. In Conference on Computational Natural Language Learning, 2011. 2 [20] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ar, and C. L. Zitnick. Microsoft coco: Common objects in context. arXiv:1405.0312, 2014. 5 [21] J. Mao, W. Xu, Y. Yang, J. Wang, and A. Yuille. Explain images with multimodal recurrent neural networks. In arXiv:1410.1090, 2014. 2, 6, 7 [22] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of word representations in vector space. In ICLR,\n2013. 6, 8\n[23] M. Mitchell, X. Han, J. Dodge, A. Mensch, A. Goyal, A. C. Berg, K. Yamaguchi, T. L. Berg, K. Stratos, and H. D. III. Midge: Generating image descriptions from computer vision detections. In EACL, 2012. 2 [24] V. Ordonez, G. Kulkarni, and T. L. Berg. Im2text: Describing images using 1 million captioned photographs. In NIPS,\n2011. 2, 5, 6\n[25] K. Papineni, S. Roukos, T. Ward, and W. J. Zhu. BLEU: A method for automatic evaluation of machine translation. In ACL, 2002. 4 [26] C. Rashtchian, P. Young, M. Hodosh, and J. Hockenmaier. Collecting image annotations using amazon’s mechanical turk. In NAACL HLT Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk, pages 139– 147, 2010. 5 [27] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet Large Scale Visual\nRecognition Challenge, 2014. 1 [28] P. Sermanet, D. Eigen, X. Zhang, M. Mathieu, R. Fergus, and Y. LeCun. Overfeat: Integrated recognition, localization and detection using convolutional networks. arXiv preprint arXiv:1312.6229, 2013. 2 [29] R. Socher, A. Karpathy, Q. V. Le, C. Manning, and A. Y. Ng. Grounded compositional semantics for ﬁnding and describing images with sentences. In ACL, 2014. 2 [30] I. Sutskever, O. Vinyals, and Q. V. Le. Sequence to sequence learning with neural networks. In NIPS, 2014. 1, 2, 3 [31] R. Vedantam, C. L. Zitnick, and D. Parikh. CIDEr: Consensus-based image description evaluation. In arXiv:1411.5726, 2015. 5, 6 [32] B. Z. Yao, X. Yang, L. Lin, M. W. Lee, and S.-C. Zhu. I2t: Image parsing to text description. Proceedings of the IEEE, 98(8), 2010. 2 [33] P. Young, A. Lai, M. Hodosh, and J. Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. In ACL,\n2014. 5\n[34] W. Zaremba, I. Sutskever, and O. Vinyals. Recurrent neural network regularization. In arXiv:1409.2329, 2014. 6\n--- FOOTNOTES 1 The raters are asked whether the image is described without any errors, described with minor errors, with a somewhat related description, or 2In this literature, most previous work report BLEU-1, i.e., they only 1M", "persona": "Student", "response": "### 1. Overview\n1.3. BN-Inception / Inception-v2 as CNN The image I is only input once, at t=-1. CNN is initialized by pre-trained ImageNet, and left unchanged. The particular choice of CNN is BN-Inception / Inception-v2 which yields the good performance on the ILSVRC 2014 classification competition. 2.2.\n\n### 2. Background\nOutline 1.4. Overview 2.1. Datasets BLEU Transfer learning is tried, but when running the MSCOCO model on SBU, our performance degrades from 28 down to 16. where θ are the parameters of our model, I is an image, and S its correct transcription.\n\n### 3. Methods\nSince S represents any sentence, its length is unbounded. Thus, it is common to apply the chain rule to model the joint probability over S0, …, SN, where N is the length of this particular example as: 1.2. LSTM as RNN It is natural to model p(St|I, S0, …, St-1) with a Recurrent Neural Network (RNN). where xt and ht are the input and the hidden state at time t. f is LSTM as it can obtain SOTA performance on sequence tasks. The core ability of LSTM is to either keep a value from the gated layer if the gate is 1 or zero this value if the gate is 0. S0 is a special start word and SN is a special stop word. During training, the loss is the sum of the negative log likelihood of the correct word at each step as follows: We is randomly initialized, since initializing using large corpus has no significant gain. 1.6. Inference During inference, BeamSearch is used which iteratively considers the set of the k best sentences up to time t as candidates to generate sentences of size t+1, and keep only the resulting best k of them.\n\n### 4. Results\nExperimental Results 1.1. Objective 1.5. Training A beam of size 20 is used . Using a beam size of 1 (i.e., greedy search) did degrade our results by 2 BLEU points on average. 2. Experimental Results With the exception of SBU, each image has been annotated by labelers with 5 sentences that are relatively visual and unbiased. Human scores were computed by comparing one of the human captions against the other four. If the best candidate is taken, the sentence is present in the training set 80% of the times.\n\n### 5. Takeaways\nNeural Image Caption (NIC) Network Architecture 1. Neural Image Caption (NIC) Network Architecture NIC is to directly maximize the probability of the correct description given the image: Each word is represented as a one-hot vector St of dimension equal to the size of the dictionary. NIC outperforms SOTA approaches such as m-RNN by large margin. 2.4. Qualitative Results", "paper_id": "27", "story_id": "27-02"}
{"paper": "Abstract\nAccelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background. 1\n\nIntroduction\nThe excellent soft tissue contrast and ﬂexibility of magnetic resonance imaging (MRI) makes it a very powerful diagnostic tool for a wide range of disorders, including neurological, musculoskeletal, and oncological diseases. However, the long acquisition time in MRI, which can easily exceed 30 minutes, leads to low patient throughput, problems with patient comfort and compliance, artifacts from patient motion, and high exam costs. As a consequence, increasing imaging speed has been a major ongoing research goal since the advent of MRI in the 1970s. Increases in imaging speed have been achieved through both hardware developments (such as improved magnetic ﬁeld gradients) and software advances (such as new pulse sequences). One noteworthy development in this context is parallel imaging, introduced in the 1990s, which allows multiple data points to be sampled simultaneously, rather than in a traditional sequential order [39, 26, 9]. The introduction of compressed sensing (CS) in 2006 [2, 23] promised another breakthrough in the reduction of MR scan time. At their core, CS techniques speed up the MR acquisition by acquiring less measurement data than has previously been required to reconstruct diagnostic quality images. Since undersampling of this kind violates the Nyquist-Shannon sampling theorem, aliasing artifacts are introduced which must be eliminated in the course of image reconstruction. This can be achieved by incorporating additional a priori knowledge during the image reconstruction process. The last two years have seen the rapid development of machine learning approaches for MR image reconstruction, which hold great promise for further acceleration of MR image acquisition [10, 48, 11, 35, 60]. Some of the ﬁrst work on this subject was presented at the 2016 annual meeting of the International Society for Magnetic Resonance in Medicine (ISMRM). The 2017 ISMRM annual meeting included, for the ﬁrst time, a dedicated session on machine learning for image reconstruction, and presentations on the subject at the 2018 annual meeting spanned multiple focused sessions, including a dedicated category for abstracts. Despite this substantial increase in research activity, the ﬁeld of MR image reconstruction still lacks large-scale, public datasets with consistent evaluation metrics and baselines. Many MR image reconstruction studies use datasets that are not openly available to the research community. This makes it challenging to reproduce and validate comparisons of different approaches, and it restricts access to work on this important problem to researchers associated with or cooperating with large academic medical centers where such data is available. In contrast, research in computer vision applications such as object classiﬁcation has greatly beneﬁted from the availability of large-scale datasets associated with challenges such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [31]. Such challenges have served as a catalyst for the recent explosion in research activity on deep learning [21]. The goal of the fastMRI dataset is to provide a ﬁrst step towards enabling similar breakthroughs in the machine-learning-based reconstruction of accelerated MR images. In this work we describe the ﬁrst large-scale release of raw MRI data that includes 8344 volumes, consisting of 167,375 slices1, associated with in vivo examinations from a range of MRI systems. In addition, we are releasing processed MR images in DICOM format from 20,000 knee and brain examinations from a representative clinical patient population, consisting of more than 1.57 million slices. Prior to providing details about the dataset and about target reconstruction tasks with associated benchmarks, we begin with a brief primer on MR image acquisition and reconstruction, in order to enable non-MRI-experts to get up to speed quickly on the information content of the dataset. In general, both the fastMRI dataset and this paper aim to connect the data science and the MRI research communities, with the overall goal of advancing the state of the art in accelerated MRI.\n\n2 Introduction to MR Image Acquisition and Reconstruction MR imaging is an indirect process, whereby cross-sectional images of the subject’s anatomy are produced from frequency and phase measurements instead of direct, spatially-resolved measurements. A measuring instrument, known as a receiver coil, is placed in proximity to the area to be imaged (Figure 1). During imaging, a sequence of spatially- and temporally-varying magnetic ﬁelds, called a “pulse sequence,” is applied by the MRI machine. This induces the body to emit resonant electromagnetic response ﬁelds which are measured by the receiver coil. The measurements typically correspond to points along a prescribed path through the multidimensional Fourier-space representation of an imaged body. This Fourier space is known as k-space in the medical imaging community. In the most basic usage of MR imaging, the full Fourier-space representation of a region is captured by a sequence of samples that tile the space up to a speciﬁed maximum frequency. Figure 1: The receiver coil housing and its positioning on a patient for a knee MR examination. The spatially-resolved image m can be estimated from the full k-space y by performing an inverse multidimensional Fourier transform: where ˆm is a noise-corrupted estimate of the true image m. The number of samples captured in k-space is a limiting factor for the speed of MR imaging. Fewer samples can be captured by sampling up to a lower maximum frequency, however this produces images of lower spatial resolution. An alternative undersampling approach involves omitting some number of k-space samples within a given maximum frequency range, which then results in aliasing artifacts. In order to remove these artifacts and infer the true underlying spatial structure of the imaged subject, one may apply a number of possible reconstruction strategies. 2.1 Parallel MR Imaging In parallel MR imaging, multiple receiver coils are used, each of which produces a separate k-space measurement matrix. Each of these matrices is different, since the view each coil provides of the imaged volume is modulated by the differential sensitivity that coil exhibits to MR signal arising from different regions. In other words, each coil measures Fourier components of the imaged volume multiplied by a complex-valued position-dependent coil sensitivity map Si. The measured k-space signal yi for coil i in an array of nc coils is given by where the multiplication is entry-wise. This is illustrated in Figure 2b, which shows the absolute value of the inverse discrete Fourier transform (DFT) of fully-sampled complex-valued k-space signals for each coil in a 15-element coil array. Each coil is typically highly sensitive in one region, and its sensitivity falls offsigniﬁcantly in other regions. If the sensitivity maps are known, and the k-space sampling is full (i.e., satisfying the Nyquist sampling condition), then the set of linear relations between m and each yi deﬁnes a linear system that is overdetermined by a factor of nc. It may be inverted using a pseudoinverse operation to produce a reconstruction of m, as long as the linear system is full rank. The quality of this reconstruction will depend on the measurement noise, since the signal-to-noise ratio is poor in parts of the volume where the coil sensitivity is low. In accelerated parallel imaging, each coil’s k-space signal is undersampled. As long as the total number of measurements across all coils exceeds the number of image voxels to be reconstructed, an unregularized least squares solution can still be used, leading to a theoretical nc-fold ˆm = F−1(y), (1) yi = F (Sim) + noise, (2) (a) k-space data from 15 coils (b) Individual coil spatial images from fully sampled data (d) Ground truth, cropped to central region and vertically ﬂipped (e) Unregularized reconstruction Figure 2: Multi-coil MRI reconstruction (c) Coil sensitivity map magnitudes given by ESPIRiT (f) Total variation penalty reconstruction (g) Baseline model reconstruction speedup over fully-sampled single-coil imaging. Each extra coil effectively produces an additional “sensitivity-encoded” measurement of the volume [26], which augments the frequency and phase encoded measurements obtained from the sequential application of magnetic ﬁeld gradients in the MR pulse sequence. Estimates of coil sensitivity patterns, required for inversion of the undersampled multi-coil linear system, may be generated from separate low-resolution calibration scans. They may also be derived directly from the k-space measurements by fully sampling a comparatively small central region of k-space, which corresponds to low spatial frequencies. In practice, the use of sub-sampling results in signiﬁcant ampliﬁcation of noise, and regularization is usually needed. In cases where a tight imaging ﬁeld of view is used, or at imaging depths exceeding the dimensions of the individual coils, the sensitivity patterns of different coils spread out, thereby lowering the effective rank of the linear system, increasing noise ampliﬁcation associated with the inverse operation, and limiting the maximum practical acceleration. As a result, in the clinic, parallel imaging acceleration factors are typically on the order of two to three. 2.2 Machine Learning Reconstruction of Undersampled MRI Data Classical approaches to MRI reconstruction solve a regularized inverse optimization problem to ﬁnd the spatially-resolved image from the sub-sampled k-space data, in both the single-coil and the multi-coil case. We describe the classical approach in more detail in Section 6. In the machine learning approach, a reconstruction function is learned from input and output pair tuples (y, m) drawn from a population. The goal is to ﬁnd a function B that minimizes the risk (i.e., expected loss) over the population distribution: We discuss error metrics that may be used as loss functions L in Section 5. In practice this optimization problem must be approximated with the empirical risk using a sample {(m(i), y(i))}ndata i=1 from the population, with respect to a loss function L:\n\n3 Prior Public Datasets The availability of public datasets has played an important role in advancing research in medical imaging, providing benchmarks to compare different approaches and leading to more impactful contributions. Early works such as DDSM [13], SLIVER07 [14] and CAUSE07 [8] triggered increasing efforts to collect new larger-scale biomedical datasets, which resulted in over one hundred public releases (counting the entries on https://grand-challenge.org/) to advance medical image analysis research. The vast majority of these datasets, which include a range of medical imaging modalities, are designed to test the limits of current methods in the tasks of segmentation, classiﬁcation, and detection. Datasets such as BraTS [24], LUNA [37], ChestX-ray [50], DeepLesion [55], and Camelyon [1], UK biobank2, ADNI (Alzheimers Disease Neuroimaging Initiative)3 and ˆm = B(y) (3) B∗= arg min B R(B), where R(B) = E(y,m) [L (B (y) , m)] . ndata X Rempirical(B) = 1 ndata i=1 L \u0010 B \u0010 y(i)\u0011 , m(i)\u0011 . (4) Dataset Volumes Body part MR scan type NYU dataset [11] 100 knee PD, T2 Stanford dataset 2D FSE 89 knee PD Stanford dataset 3D FSE [34] 20 knee PD Stanford undersampled dataset 38 knee PD fastMRI dataset 1594 knee PD Table 1: Publicly available MRI datasets containing k-space data TCIA (The Cancer Imaging Archive)4. However, none of the most prominent public MRI datasets include k-space imaging data. However, the current lack of large-scale reference standards for MR image reconstruction hinders progress in this important area. Most research uses synthetic k-space data that is not directly acquired but rather obtained from post-processing of already-reconstructed images [5, 38, 57, 56, 27]. Research using small-scale proprietary raw k-space datasets is also common [15, 36, 35, 33, 22]. In order to address the above-mentioned shortcomings, recent efforts have been devoted to collecting and publicly releasing datasets containing raw (unprocessed) k-space data; see, e.g., [34, 11]. However, the size of these existing datasets remains small. As an example, Table 1 lists publicly available knee MR datasets containing raw k-space data. Although datasets such as these provide a valuable test bed for signal processing methods, larger datasets encompassing different anatomy are required to fully realize the potential of deep learning. 4 The fastMRI Dataset and Associated Tasks The fastMRI dataset (http://fastmri.med.nyu.edu/) contains four types of data from MRI acquisitions of knees and brains Raw multi-coil k-space data: unprocessed complex-valued multi-coil MR measurements. Emulated single-coil k-space data: combined k-space data derived from multi-coil k-space data in such as way as to approximate single-coil acquisitions, for evaluation of single-coil reconstruction algorithms. Ground-truth images: real-valued images reconstructed from fully-sampled multi-coil acquisitions using the simple root-sum-of-squares method detailed below. These may be used as references to evaluate the quality of reconstructions. DICOM images: spatially-resolved images for which the raw data was discarded during the acquisition process. These images are provided to represent a larger variety of machines and settings than are present in the raw data. This data was designed to enable two distinct types of tasks: 1. Single-coil reconstruction task: reconstruct images approximating the ground-truth from\nundersampled single-coil data. 2. Multi-coil reconstruction task: reconstruct images approximating the ground-truth from undersampled multi-coil data. For each task we provide an oﬃcial split of the k-space data and ground-truth images into training and validation subsets that contain fully-sampled acquisitions, as well as test and challenge subsets which contain k-space data that have been subjected to undersampling masks as described below. Ground-truth images are not being released for the test and challenge datasets. During training of a machine-learning model, the training k-space data should be programmatically masked following the same procedure. The challenge subsets are not being released at the time of writing and are reserved for future challenges associated with the fastMRI dataset. The rationale for having a single-coil reconstruction task (and for providing simulated single-coil data), even though reconstruction from multi-coil data is expected to be more precise, is twofold: (i) to lower the barrier of entry for researchers who may not be familiar with MRI data, since the use of a single coil removes a layer of complexity, and (ii) to include a task that is relevant for the single-coil MRI machines still in use throughout the world. The DICOM images may be useful as additional data for training. Their distribution is different from that of the ground-truth images, since they were acquired with a larger diversity of scanners, manners of acquisition, reconstruction methods, and post-processing algorithms, so the application of transfer-learning techniques may be necessary. Most DICOM images are the result of accelerated parallel imaging acquisitions and corresponding reconstructions, with image quality that differs from that of putative fully-sampled acquisitions and reconstructions. The ground-truth images may, in many cases, represent a higher standard of image quality than the clinical gold standard, for which full sampling is not routine or even practical. 4.1 Anonymization Curation of the datasets described here was part of a study approved by the NYU School of Medicine Institutional Review Board. Raw data was anonymized via conversion to the vendorneutral ISMRMRD format [18]. DICOM data was anonymized using the RSNA clinical trial processor. We performed manual inspection of each DICOM image for the presence of unexpected protected health information (PHI), manual checking of metadata in raw data ﬁles, as well as spot checking of all metadata and image content. 4.2 Knee k-space Data Multi-coil raw data was stored for 1,594 scans acquired for the purpose of diagnostic knee MRI. For each scan, a single fully sampled MRI volume was acquired on one of three clinical 3T systems (Siemens Magnetom Skyra, Prisma and Biograph mMR) or one clinical 1.5T system (Siemens Magnetom Aera). Data acquisition used a 15 channel knee coil array and conventional Cartesian two pulse sequences, yielding coronal proton-density weighting with (PDFS, 798 scans) and without (PD, 796 scans) fat suppression (see Figure 3). Sequence parameters are, as per standard clinical protocol, matched as closely as possible between the systems. The following sequence parameters were used: Echo train length 4, matrix size 320 × 320, in-plane resolution 0.5mm×0.5mm, slice thickness 3mm, no gap between slices. Timing varied between systems, with repetition time (TR) ranging between 2200 and 3000 milliseconds, and echo time (TE) between 27 and 34 milliseconds. 4.3 Brain k-space Data System Number of scans Skyra 3T 663 Prisma 3T 83 Biograph mMR 3T 153 Aera 1.5T 695 Table 2: Number of scans of knee raw data per scanner Field Strength 1.5T T1 375 407 T1 POST 849 641 T2 1651 2515 FLAIR 126 406 Total 3001 3969 Table 3: Number of scans for the different contrasts and scanner ﬁeld strengths of the brain raw dataset. includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent (labelled T1 POST) (see Figure 4). Not all imaging volumes included all pulse sequences. The exact distribution of contrasts and ﬁeld strengths is given in table 3. To ensure data de-identiﬁcation, we used only axial 2-D images in this dataset. We used zero matrices to replace the k-space slices ⪆5mm below the orbital rim. All processed k-spaces were then reconstructed to images in DICOM format, loaded into a picture archival communication system (PACS) and all images were visually checked by certiﬁed MR technologists to conﬁrm exclusion of identifying facial features. 4.4 Knee emulated Single-coil k-Space Data We used an emulated single-coil (ESC) methodology to simulate single-coil data from a multi-coil acquisition [43]. ESC computes a complex-valued linear combination of the responses from multiple coils, with the linear combination ﬁtted to the ground-truth root-sum-of-squares reconstruction in the least-squares sense. 4.5 Knee DICOM Data In addition to the scanner raw data described above, the fastMRI dataset includes DICOM data from 10,000 clinical knee MRI scans. These images represent a wider variety of scanners and pulse sequences than those represented in the collection of raw data. Each MR exam for which DICOM images are included typically consisted of ﬁve clinical pulse sequences:\n\n1. Coronal proton-density weighting without fat suppression,\n\n4. Sagittal T2 weighting with fat suppression, and\n\n5. Axial T2 weighting with fat suppression.\nThe two coronal sequences have the same basic speciﬁcations (matrix size, etc) as the sequences associated with raw data. The sagittal and axial sequences have different matrix sizes and have no The Fourier transformation of an image from a DICOM ﬁle does not directly correspond to the originally measured raw data, due to the inclusion of additional post-processing steps in the vendorspeciﬁc reconstruction pipeline. Most of the DICOM images are also derived from accelerated acquisitions and are reconstructed with parallel imaging algorithms, since this baseline acceleration represents the current clinical standard. The image quality of DICOM images, therefore, is not equivalent to that of the ground truth images directly associated with fully sampled raw data. The DICOM images are distinct from the validation, test, or challenge sets. 4.6 Brain DICOM 10,000 brain MRI DICOM studies are also included. Axial 2D image volumes are included with the following pulse sequences: T1, T2, and T2 FLAIR. All studies represent unique individuals and there is no subject overlap with the brain rawdata. In addition to the deidentiﬁcation procedures detailed above, the brain image volumes were cropped to exclude identiﬁable facial features, following which each image was visually inspected to conﬁrm appropriate deidentiﬁcation. Finally, we present 10,000 brain MRI DICOM studies from 10,000 unique subjects, each one including axial 2D DICOM image volumes through the majority of the brain representing a broad range of neurological pathologies. Not all studies include all pulse sequences. 4.7 Ground Truth The root-sum-of-squares reconstruction method applied to the fully sampled k-space data [28] provides the ground truth for the multi-coil dataset. The single-coil dataset includes two ground truth reconstructions, which we denote ESC and RSS. The ESC ground truth is given by the inverse Fourier transform of the single-coil data, and the RSS ground truth is given by the root-sum-ofsquares reconstruction computed on the multi-coil data that were used to generate the virtual single-coil k-space data. All ground truth images are cropped to the central 320 × 320 pixel region to compensate for readout-direction oversampling that is standard in clinical MR examinations. The root-sum-of-squares approach [28] is one of the most commonly-used coil combination methods in clinical imaging. It ﬁrst applies the inverse Fourier Transform to the k-space data from each coil: ˜mi = F−1(yi), (5) where yi is the k-space data from the ith coil and ˜mi is the ith coil image. Then, the individual coil images are combined voxel by voxel as follows: i=0 | ˜mi|2 !1/2 , (6) nc X ˜mrss = (a) (b) Figure 3: A proton-density weighted image (a) with fat suppression (PDFS) and (b) without fat suppression (PD). Fat has a high signal response in MR imaging, which can make details in other regions diﬃcult to see. Fat-suppressed scans typically have higher noise. Volumes Slices Multi-coil Single-coil Multi-coil Single-coil training 973 973 34,742 34,742 validation 199 199 7,135 7,135 test 118 108 4,092 3,903 challenge 104 92 3,810 3,305 Table 4: Volumes and slices in each set (a) (b) (c) (d) Figure 4: Axial brain MRI images with different contrasts: (a) FLAIR, (b) T1 weighted (c) T1 weighted with contrast agent (T1 POST), and (d) T2 weighted. 4.8 Dataset Split Each volume is randomly assigned to one of the following six component datasets: training, validation, multi-coil test, single-coil test, multi-coil challenge, or single-coil challenge. Table 4 shows the number of volumes assigned to each dataset. The training and validation datasets may be used to ﬁt model parameters or to determine hyperparameter values. The test dataset is used to compare the results across different approaches. To ensure that models do not overﬁt to the test set, the ground truth reconstructions are not publicly released for this set. Evaluation on the test set is accomplished by uploading results to the public leaderboard at http://fastmri.org/. The challenge portion of the dataset will be forthcoming. A volume from the train or validation dataset is used in both the single-coil and multi-coil tracks, whereas a volume from the test or challenge dataset is only used in either the single-coil or the multi-coil track. Volumes were only included in a single test or challenge set to ensure information from one could not be used to help the result in another. 4.9 Cartesian Undersampling Volumes in the test and challenge datasets contain undersampled k-space data. The undersampling is performed by retrospectively masking k-space lines from a fully-sampled acquisition. k-space lines are omitted only in the phase encoding direction, so as to simulate physically realizable accelerations in 2D data acquisitions. The same undersampling mask is applied to all slices in a volume, with each case consisting of a single volume. The overall acceleration factor is set randomly to either four or eight (representing a four-fold or an eight-fold acceleration, respectively), with equal probability for each. All undersampling masks are generated by ﬁrst including some number of adjacent lowestfrequency k-space lines to provide a fully-sampled k-space region. When the acceleration factor equals four, the fully-sampled central region includes 8% of all k-space lines; when it equals eight, 4% of all k-space lines are included. The remaining k-space lines are included differently for both knee and brain cases. For knee, the remaining k-space lines are included uniformly at random, with the probability set so that, on average, the undersampling mask achieves the desired acceleration factor. Random undersampling is chosen in order to meet the general conditions for compressed sensing [2, 23], for a fair comparison of learned reconstruction algorithms with traditional sparsitybased regularizers. For brain, after a random offset from the start, the remaining lines are sampled equidistant from each other with a spacing that achieves the desired acceleration factor. Equidistant was chosen because of ease of implementation on existing MRI machines. Figure 5 depicts the kspace trajectories for random and equidistant undersampling at four and eight acceleration factors.\n\n5 Metrics The assessment of MRI reconstruction quality is of paramount relevance to develop and compare machine learning and medical imaging systems [51, 53, 3, 58]. The most commonly used evaluation metrics in the MRI reconstruction literature [3] include (normalized) mean squared error, which measures pixel-wise intensity differences between reconstructed and reference images, and signalto-noise ratio, which measures the degree to which image information rises above background noise. These metrics are appealing because they are easy to understand and eﬃcient to compute. However, they both evaluate pixels independently, ignoring the overall image structure. Additional metrics have been introduced in the literature to capture structural distortion [41, 6, 58]. For example, the structural similarity index [53] and its extended version, multiscale structural (a) Random mask with 4-fold acceleration (c) Equispaced mask with 4-fold acceleration Figure 5: Examples of undersampled k-space trajectories similarity [52], provide a mechanism to assess the perceived quality of an image using local image patches. The most recent developments in the computer vision literature leverage pretrained deep neural networks to measure the perceptual quality of an image by computing differences at the representation level [19], or by means of a downstream task such as classiﬁcation [32]. In the remainder of this section, we review the deﬁnitions of the commonly-used metrics of normalized mean square error, peak signal-to-noise ratio, and structural similarity. As is discussed later, while we expect these metrics to serve as a familiar starting point, we also hope that the fastMRI dataset will enable robust investigations into improved evaluation metrics as well as improved reconstruction algorithms. (b) Random mask with 8-fold acceleration (d) Equispaced mask with 8-fold acceleration 5.1 Normalized Mean Square Error The normalized mean square error (NMSE) between a reconstructed image or image volume represented as a vector ˆv and a reference image or volume v is deﬁned as NMSE(ˆv, v) = ∥ˆv −v∥2 2 ∥v∥2 2 , (7) where ∥·∥2 2 is the squared Euclidean norm, and the subtraction is performed entry-wise. In this work we report NMSE values computed and normalized over full image volumes rather than individual slices, since image-wise normalization can result in strong variations across a volume. NMSE is widely used, and we recommend that it be reported as the primary measure of reconstruction quality for experiments on the fastMRI dataset. However, due to the many downsides of NMSE, such as a tendency to favor smoothness rather than sharpness, we recommend also reporting additional metrics such as those described below. 5.2 Peak Signal-to-Noise Ratio The peak signal-to-noise ratio (PSNR) represents the ratio between the power of the maximum possible image intensity across a volume and the power of distorting noise and other errors: PSNR(ˆv, v) = 10 log10 max(v)2 Here ˆv is the reconstructed volume, v is the target volume, max(v) is the largest entry in the target volume v, MSE(ˆv, v) is the mean square error between ˆv and v deﬁned as 1 n∥ˆv −v∥2 2 and n is the number of entries in the target volume v. Higher values of PSNR (as opposed to lower values of NMSE) indicate a better reconstruction. 5.3 Structural Similarity The structural similarity (SSIM) index measures the similarity between two images by exploiting the inter-dependencies among nearby pixels. SSIM is inherently able to evaluate structural properties of the objects in an image and is computed at different image locations by using a sliding window. The resulting similarity between two image patches ˆm and m is deﬁned as SSIM( ˆm, m) = (2µ ˆmµm + c1)(2σ ˆmm + c2) (µ2 ˆm + µ2m + c1)(σ2 ˆm + σ2m + c2), (9) where µ ˆm and µm are the average pixel intensities in ˆm and m, σ2 ˆm and σ2 m are their variances, σ ˆmm is the covariance between ˆm and m and c1 and c2 are two variables to stabilize the division; c1 = (k1L)2 and c2 = (k2L)2. For SSIM values reported in this paper, we choose a window size of 7 × 7, we set k1 = 0.01, k2 = 0.03, and deﬁne L as the maximum value of the target volume, L = max(v). 5.4 L1 Error It is sometimes advantageous to use the L1 loss L1(ˆv, v) = ∥ˆv −v∥1, (10) MSE(ˆv, v). (8) (a) Cropped and vertically ﬂipped reconstruction from fully sampled kspace data (b) Rectangular masked kspace (e) Multiscale Daubechies discrete wavelet transform (f) L1 Wavelet penalty reconstruction Figure 6: Single-coil reconstruction 6 Baseline Models Along with releasing the fastMRI data, we detail two reference approaches to be used as reconstruction baselines: a classical non-machine learning approach, and a deep-learning approach. Each of these baselines has versions tailored for single-coil or multi-coil data. The “classical” baselines are comprised of reconstruction methods developed by the MRI community over the last 30+ years. These methods have been extensively tested and validated, and many have demonstrated robustness suﬃcient for inclusion in the clinical workﬂow. By comparison, machine learning reconstruction methods are relatively new in MRI, and deep-learning reconstruction techniques in particular have emerged only in the past few years. We include some deliberately rudimentary deep-learning models as starting points, with the expectation that future learning algorithms will provide markedly improved performance. 6.1 Single-coil Classical Baselines (knee only) In the single-coil imaging setting, the task is to reconstruct an image, m, from k-space observations, y. In the presence of undersampling, the vector y has a length smaller than that of m. Therefore there are, in principle, inﬁnitely many possibilities for m that can be mapped onto a single y. The advent of compressed sensing [2, 23] provided a framework for reconstruction of images from undersampled data that closely approximate images derived from fully-sampled data, subject to (c) Reconstruction via zero-ﬁlled IFFT (d) Deep-learning baseline UNET reconstruction (g) Image gradients as given by a Sobel ﬁlter (h) Regularized totalvariation reconstruction sparsity constraints. Compressed sensing theory requires the images in question to be sparse in some transform domain. Two common examples are to assume sparsity in the wavelet domain, or to assume sparsity of the spatial gradients of the image. The particular assumption impacts the mathematical formulation of the reconstruction problem, either in the cost function or through a regularization term. More concretely, the sparse reconstruction approach consists of ﬁnding an image m whose Fourier space representation is close to the measured k-space matrix y at all measured spatial frequencies, yet at the same time minimizes a sparsity-inducing objective R(m) that penalizes unnatural reconstructions: minimize m R (m) s.t. ∥P (F (m)) −y∥2 2 ≤ϵ. (11) Here, P is a projection function that zeros out entries that are masked, and ϵ is a speciﬁed small threshold value. In most applications it is easier to work with a soft penalty instead of a constraint, so the Lagrangian dual form of Equation 11 is used instead, with penalty parameter λ: minimize m 1 2 ∥P (F (m)) −y∥2 2 + λR (m) . (12) For a convex regularizer R, there exists, for any choice ϵ > 0, a value λ such that these two formulations have equivalent solutions. The most common regularizers used for MRI are: RL1 (m) = ∥m∥1 , Rwavelet (m) = ∥Ψ (m)∥1 (Ψ is a discrete wavelet transform) , q |mi+1,j −mi,j|2 + |mi,j+1 −mi,j|2. RTV (m) = X i,j The L1 penalty works best when the MR images are sparse in image space, for instance in vascular imaging (e.g., Yamamoto et al. [54]). This is not the case for most MRI applications. The totalvariation (TV) penalty encourages sparsity in the spatial gradients of the reconstructed image, as given by a local ﬁnite-difference approximation [30] (Figure 6g). The TV regularizer can be very effective for some imaging protocols, but it also has a tendency to remove detail (Figure 6h). The Rwavelet penalty encourages sparsity in the discrete wavelet transform of the image. Most natural images exhibit signiﬁcant sparsity when expressed in a wavelet basis. The most commonly used transform is the Multiscale Daubechies (DB2) transform (Figure 6e). To date, due to their computational complexity as well as their tendency to introduce compression artifacts or oversmoothing, compressed sensing approaches have taken some time to gain acceptance in the clinic, though commercial implementations of compressed sensing are currently beginning to appear. The single-coil classical baseline provided with the fastMRI dataset was adopted from the widely-used open-source BART toolkit (Appendix B), using total variation as the regularizer. We ran the optimization algorithm for 200 iterations on each slice independently. Table 5 summarizes the results of applying this method to the single-coil validation data with different regularization strengths and different acceleration factors. These results indicate that NMSE and PSNR metrics are highly (inversely) correlated and generally favor models with stronger regularization than SSIM does. Stronger regularization generally results in smoother images that lack the ﬁne texture of the ground truth images. A regularization parameter of 0.01 yields the best results for 4-fold acceleration in most cases, whereas the higher 8-fold acceleration gets slightly better results with a regularization parameter of 0.1. Single-coil classical baseline (TV model) applied to knee validation data Acceleration Regularization Weight NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 10−4 0.0355 0.0919 30.2 27.6 0.637 0.506 10−3 0.0342 0.0916 30.4 27.6 0.641 0.505 10−2 0.0287 0.09 31.4 27.7 0.645 0.494 10−1 0.0313 0.0993 30.9 27.3 0.575 0.399 1 0.0522 0.124 28.5 26.2 0.526 0.327 8-fold 10−4 0.0708 0.118 27.1 26.4 0.551 0.417 10−3 0.0699 0.118 27.1 26.4 0.553 0.416 10−2 0.063 0.117 27.7 26.4 0.564 0.408 10−1 0.0537 0.117 28.4 26.5 0.55 0.357 1 0.0742 0.132 26.9 25.9 0.538 0.333 Table 5: Validation set results for the classical baseline model with Total Variation regularization for the single-coil task. Bold-faced numbers indicate the best performance for each image quality metric. 6.2 Multi-coil Classical Baselines When multiple receiver coils are used, the reconstruction process must combine information from multiple channels into one image. Multi-coil acquisitions currently represent the norm in clinical practice, for two principal reasons: they provide increased SNR, as compared with single-coil acquisitions, over extended ﬁelds of view, and they enable acceleration via parallel imaging. Equation 2 in Section 2.1 describes the forward model for parallel imaging. The SENSE formulation [26] of parallel image reconstruction involves direct inversion of this forward model, via a suitable pseudoinverse. Leveraging the convolution property of the Fourier Transform reveals the following convolution relationship: yi = gi ⊛F (m) + noise. (13) Here gi is the Fourier Transform of the coil sensitivity pattern Si and ⊛denotes the convolution operation. The GRAPPA/SMASH formulation of parallel image reconstruction [39, 9] involves ﬁlling in missing k-space data via combinations of acquired k-space data within a deﬁned convolution kernel, prior to inverse Fourier transformation. Either formulation requires estimates of the coil sensitivity information in Si or gi, which may be derived either from a separate reference scan or directly from the acquired undersampled k-space data itself. Reference scan methods are often used in the SENSE formulation, whereas GRAPPA formulations are typically self-calibrating, relying on subsets of fully-sampled data generally in central k-space regions. The parallel imaging techniques described above may be combined productively with compressed sensing, via the use of sparsity-based regularizers. For example, one may extend Equation 12 in Section 6.1 above to include multi-coil data as follows: nc X minimize m 1 2 the ESPIRiT approach [45], which harmonizes parallel imaging and compressed sensing in a uniﬁed framework. As was the case for the classical single-coil baseline, the classical multi-coil baseline provided with the fastMRI dataset was adopted from the BART toolkit (Appendix B). In the multi-coil case, the ESPIRiT algorithm was used to estimate coil sensitivities, and to perform parallel image reconstruction in combination with compressed sensing using a total-variation regularizer. Multi-coil classical baseline (TV model) applied to knee validation data Acceleration Regularization NMSE PSNR SSIM 4-fold 10−4 0.0246 0.0972 31.6 27.4 0.677 0.53 10−3 0.0222 0.0951 32.1 27.5 0.693 0.554 10−2 0.0198 0.0971 32.6 27.5 0.675 0.588 10−1 0.0251 0.109 31.3 27 0.633 0.538 8-fold 10−4 0.0494 0.114 28.2 26.5 0.61 0.505 10−3 0.0447 0.112 28.6 26.6 0.626 0.524 10−2 0.0352 0.109 29.6 26.8 0.642 0.551 10−1 0.0389 0.114 29.2 26.7 0.632 0.527 Table 6: Validation set results for the classical baseline model with Total Variation regularization for the knee multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Results using this baseline model are summarized in Table 6 and 7 . The experimental setup is identical to the single-coil scenario, except that we compare the reconstructions with the rootsum-of-squares ground truth instead of the ESC ground truth. 6.3 Single-coil Deep-Learning Baselines (knee only) Various deep-learning techniques based on Convolutional Neural Networks have recently been proposed to tackle the problem of reconstructing MR images from undersampled k-space data [10, 48, 11, 35, 60, 17, 12]. Many of these proposed methods are based on the U-Net architecture introduced in [29]. U-Net models and their variants have successfully been used for many image-to-image prediction tasks including MRI reconstruction [17, 12] and image segmentation [29]. The U-Net single-coil baseline model included with the fastMRI data release (Figure 7) consists of two deep convolutional networks, a down-sampling path followed by an up-sampling path. The down-sampling path consists of blocks of two 3×3 convolutions each followed by instance normalization [46] and Rectiﬁed Linear Unit (ReLU) activation functions. The blocks are interleaved by down-sampling operations consisting of max-pooling layers with stride 2 which halve each spatial dimension. The up-sampling path consists of blocks with a similar structure to the down-sampling path, interleaved with bilinear up-sampling layers which double the resolution between blocks. Each block consists of two 3×3 convolutions with instance normalization [46] and ReLU activation layers. In contrast to the down-sampling path, the up-sampling path concatenates two inputs to the ﬁrst convolution in each block: the up-sampled activations from the previous block, together with the activations that follow the skip connection from the block in the down-sampling path with the same resolution (horizontal arrows in Figure 7). At the end of the up-sampling path, we PD PDFS PD PDFS PD PDFS 1 32 32 * * 320 32 64 64 * * 160 64 128 128 * * 80 128 256 256 * * 512 128 128 * * 40 40 256 20 * Figure 7: Single-coil baseline U-Net architecture include a series of 1×1 convolutions that reduce the number of channels to one without changing the spatial resolution. For the single-coil MRI reconstruction case, the zero-ﬁlled image is used as the input to the model. The zero-ﬁlled image is obtained by ﬁrst inserting zeros at the location of all unobserved k-space values, applying a two-dimensional Inverse Fourier Transform (IFT) to the result, and ﬁnally computing the absolute value. The result is center cropped to remove any readout and phase oversampling. Using the notation from section 6.1, the zero-ﬁlled image is given by ˜m = C( F−1(P(y)) ), where C is the linear operator corresponding to the center cropping and F−1 is the two-dimensional IFT. The entire network is trained on the training data in an end-to-end manner to minimize the mean absolute error with respect to corresponding ground truth images. Let Bθ(m) be the function computed by the U-Net model, where θ represents the parameters of the model. Then the training process corresponds to the following optimization problem: ndata X minimize θ 1 2 Bθ( ˜m(i)) −m(i) 1 , (15) i=0 where the ground truths m(i) are obtained using the ESC method described in Section 4.4. Our particular single-coil U-Net baseline model was trained on 973 image volumes in the training set, using the RMSProp algorithm [42]. We used an initial learning rate of 0.001, which was multiplied by 0.1 after 40 epochs, after which the model was trained for an additional 10 epochs. During training, we randomly sampled a different mask for each training example in each epoch independently using the protocol described in Section 4.9 for the test data. At the end of each epoch, we recorded the NMSE on the validation data. After training, we picked the model that achieved the lowest validation NMSE. Table 8 presents the results from running trained U-Net models of different capacities on the single-coil validation data. These results indicate that the trained U-Net models perform signiﬁcantly better than the classical baseline method. The best U-Net models obtain 40-50% relative improvement over the classical methods (see Table 5) in terms of NMSE. The performance of the U-Net models continues to increase with increasing model capacity, and 64 32 32 16 1 1x1 1x1 * * * * 320 128 32 32 * * 160 256 64 64 * * 80 * 3x3 Convolution + ReLU + InstanceNorm 2x2 Max pooling 2x Bilinear  upsampling 1x1 256 * 1x1 Convolution even the largest model with over 200 million parameters is unable to overﬁt the training data. These improvements begin to saturate after 50 million parameters for the simpler 4-fold acceleration case. However, for the more challenging 8-fold acceleration task, the largest model performs signiﬁcantly better than the smaller models. This suggests that models with very large capacities trained on large amounts of data can enable high acceleration factors. Table 9 compares the performance of the classical and the U-Net baseline models for the singlecoil task, as applied to the test dataset. For the classical baseline model, we chose the best regularization weights for each modality and for each acceleration factor based on the validation data results, resulting in a regularization weight of 0.1 for 8-fold acceleration on Proton Density without fat suppression and 0.01 for every other case. For the U-Net baseline model, we chose the model with the largest capacity. 6.4 Multi-coil Deep-Learning Baselines In the multi-coil MRI reconstruction task, we have one set of undersampled k-space measurements from each coil, and a different zero-ﬁlled image can be computed from each coil. These coil images can be combined using the root-sum-of-squares algorithm. Let ˜mi be the zero-ﬁlled image from coil i. With ˜mrss deﬁned as in Equation 6, the U-Net model described in Section 6.3 can be used for the multi-coil reconstruction task by simply feeding this combined image in as input: Bθ( ˜mrss). The model is trained to minimize the mean absolute error loss similarly to the single-coil task. The training procedure is also identical to the single-coil case except that the root-sum-of-squares image is used as the ground truth as described in Section 4.7. As is the case for the single-coil task, the multi-coil U-Net baselines substantially outperform the classical baseline models (compare Table 10 and 11 with Table 6 and 7). Note that this is true despite the fact that the multi-coil U-Net baseline deﬁned above does not take coil sensitivity information into account, and therefore neither includes a direct parallel image reconstruction nor accounts for sparsity or other correlations among coils. Models that incorporate coil sensitivity information are expected to perform better than the current multi-coil U-Net baselines. Table 10 and Table 11 shows, once again, that the performance of the U-Net models improves with model size, with the largest U-Net baseline model providing the best performance. Table 12 compares the performance of the classical and the U-Net baseline models for the multi-coil task, as applied to the test dataset. For the classical baseline model, we chose the best regularization weights for each modality and for each acceleration factor based on the validation data results. For knees this resulted in a regularization weight of 0.001 for 4-fold undersampling for Proton Density with Fat Suppression and 0.01 for every other acquisition type. For brain this resulted in a regularization weight of 0.001 for 8-fold AXFLAIR and 4-fold AXT1, and 0.01 for every other acquisition type. For the U-Net baseline model, we chose the model with the largest capacity. To appreciate the value of the dataset size, we study how model performance scales with the amount of data used to train a model. To this end, we trained several U-Net models with varying model capacities on different sized subsets of the training data. Figure 8 shows the SSIM metric computed on the validation data for the multi-coil task. It is evident from these results that training with larger amounts of data yields substantial improvements in the quality of reconstructions, which highlights the need for the release of large datasets like fastMRI. As mentioned in Section 4.5, the fastMRI dataset also includes a large set of DICOM images that can be used as additional training data. It is possible that the baseline U-Net models could be improved further by making use of this additional data. 32 64 128 256 0.895 0.890 SSIM 0.885 0.880 0.875 100 200 500 973 Number of training cases Figure 8: Results from training the U-Net on different amounts of training data for the multi-coil knee challenge with 4-fold acceleration (left) and 8-fold acceleration (right). Each line represents a model with a different number of channels. 7\n\nDiscussion\nMR image reconstruction is an inverse problem, and thus it has many connections to inverse problems in the computer vision literature [40, 7, 4, 47], such as super-resolution, denoising and in-painting. In all of these inverse problems, the goal is to recover a high-dimensional ground truth image from a lower-dimensional measurement. Such ill-posed problems are very diﬃcult to solve since there exists an inﬁnite number of high-dimensional images that can result in the same-low dimensional measurement. In order to simplify the problem, an assumption is often made that only a small number of high-resolution images would correspond to natural images [4]. Given that MRI reconstruction is a similar inverse problem, we hope that the computer vision community, as well as the medical imaging community, will ﬁnd our dataset beneﬁcial. In the clinical setting, radiologists use MRI to search for abnormalities, make diagnoses, and recommend treatment options. Thus, contrary to many computer vision problems where small texture changes might not necessarily alter the overall satisfaction of the observer, in MRI reconstruction, extra care should be taken to ensure that the human interpreter is not misled by a very plausible but not necessarily correct reconstruction. This is especially important as image generation techniques increase in their ability to generate photo-realistic results [49]. Therefore some research effort should be devoted to look for solutions that, by design, ensure correct diagnosis, and we hope that our dataset will provide a testbed for new ideas in these directions as well. An important question in MRI reconstruction is the choice of the evaluation metric. The current consensus in the MRI community is that global metrics, such as NMSE, SSIM and PSNR, do not necessarily capture the level of detail required for proper evaluation of MRI reconstruction algorithms [25, 16]. A natural question arises: what would the optimal metric be? An ideal MRI reconstruction algorithm should produce sharp, trustworthy images, that ultimately ensure the proper radiologic interpretation. While our dataset will help ensure consistent evaluation, we hope that it will also trigger research on MRI reconstruction metrics. This goal will be impossible to achieve without clinical studies involving radiologists evaluating fully-sampled and undersampled MRI reconstructions to make sure that both images lead to the same diagnosis. Although this dataset provides an excellent entry point for machine learning methods for MR reconstruction, there are some aspects of MR imaging that we have not yet considered here. Phys- 32 64 128 256 0.860 0.855 0.850 0.845 SSIM 0.840 0.835 0.830 0.825 0.820 100 200 500 973 Number of training cases Ground Truth Total Variation U-Net Figure 9: Example knee reconstructions Multi-Coil 4-Fold Multi-Coil 8-Fold Single-Coil 4-Fold Single-Coil 8-Fold Figure 10: Example brain reconstructions ical effects such as spin relaxation, eddy currents and ﬁeld distortions are not at present explicitly accounted for in our retrospective undersampling approaches or our baseline models. The manifestation of these effects depends upon the object being imaged, the MRI scanner used, and even the sampling pattern selected. Extending the results from methods developed for this challenge to the clinic remains an open problem, but we believe the provision of this dataset is an important ﬁrst step on the path to this goal. 8\n\nConclusion\nIn this work we detailed the fastMRI dataset: the largest raw MRI dataset to be made publicly available to date. Previous public datasets have focused on post-processed magnitude images for speciﬁc biologic and pathologic questions. Although our dataset was originally acquired for a focused task, the inclusion of raw k-space data allows methods to be developed for the imaging pipeline itself, in principle allowing them to be applied on any MRI scanner for any imaging task. In addition to the data, we provide evaluation metrics and baseline algorithms to aid the research community in assessing new approaches. Consistent evaluation of MRI reconstruction techniques is provided by a leaderboard using held-out test data. We hope that the availability of this dataset will accelerate research in MR image reconstruction, and will serve as a benchmark during training and validation of new algorithms.\n\n9 Acknowledgements EB024532 and NIH P41 EB017183. We would also like to thank Michela Paganini and Mark Tygert.\n\nReferences\n[1] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson, Maschenka Balkenhol, et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. Journal of the American Medical Association, 318(22), 2017. [2] Emmanuel J Cand`es, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2), 2006. [3] Damon M Chandler. Seven challenges in image quality assessment: past, present, and future research. ISRN Signal Processing, 2013. [4] Jen-Hao Rick Chang, Chun-Liang Li, Barnab´as P´oczos, B. V. K. Vijaya Kumar, and Aswin C. Sankaranarayanan. One network to solve them all - solving linear inverse problems using deep projection models. IEEE International Conference on Computer Vision (ICCV), 2017. [5] Salman Ul Hassan Dar and Tolga C¸ukur. A transfer-learning approach for accelerated MRI using deep neural networks. arXiv preprint, 2017. [6] Michael P Eckert and Andrew P Bradley. Perceptual quality metrics applied to still image compression. Signal Processing, 70(3), 1998. [7] Kai Fan, Qi Wei, Wenlin Wang, Amit Chakraborty, and Katherine A. Heller. InverseNet: Solving inverse problems with splitting networks. arXiv preprint, 2017. [8] Bram Van Ginneken, Tobias Heimann, and Martin Styner. 3d segmentation in the clinic: A grand challenge. In MICCAI Workshop on 3D Segmentation in the Clinic: A Grand Challenge, 2007. [9] Mark A Griswold, Peter M Jakob, Robin M Heidemann, Mathias Nittka, Vladimir Jellus, Jianmin Wang, Berthold Kiefer, and Axel Haase. Generalized autocalibrating partially parallel acquisitions (GRAPPA). Magnetic Resonance in Medicine, 47(6), 2002. [10] Kerstin Hammernik, Florian Knoll, Daniel K Sodickson, and Thomas Pock. Learning a Variational Model for Compressed Sensing MRI Reconstruction. In Magnetic Resonance in Medicine (ISMRM), 2016. [11] Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P. Recht, Daniel K. Sodickson, Thomas Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI data. Magnetic Resonance in Medicine, 2018. [12] Yoseob Han and Jong Chul Ye. Framing U-Net via deep convolutional framelets: Application to sparseview CT. IEEE Transactions on Medical Imaging, 37(6), 2018. [13] Michael Heath, Kevin Bowyer, Daniel Kopans, P Kegelmeyer, Richard Moore, Kyong Chang, and S Munishkumaran. Current status of the digital database for screening mammography. In Digital Mammography, 1998. [14] Tobias Heimann, Bram Van Ginneken, Martin A Styner, Yulia Arzhaeva, Volker Aurich, Christian Bauer, Andreas Beck, Christoph Becker, Reinhard Beichel, Gy¨orgy Bekes, et al. Comparison and evaluation of methods for liver segmentation from CT datasets. IEEE Transactions on Medical Imaging, 28(8), 2009. [15] Yue Huang, John Paisley, Xianbo Chen, Xinghao Ding, Feng Huang, and Xiao-Ping Zhang. MR image reconstruction from undersampled k-space with bayesian dictionary learning. arXiv preprint, 2013. [17] Chang Min Hyun, Hwa Pyung Kim, Sung Min Lee, Sungchul Lee, and Jin Keun Seo. Deep learning for undersampled MRI reconstruction. Physics in medicine and biology, 63(13), 2018. [18] Souheil J Inati, Joseph D Naegele, Nicholas R Zwart, Vinai Roopchansingh, Martin J Lizak, David C Hansen, Chia-Ying Liu, David Atkinson, Peter Kellman, Sebastian Kozerke, et al. ISMRM raw data format: a proposed standard for MRI raw datasets. Magnetic resonance in medicine, 77(1), 2017. [19] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, 2016. [20] Erik G Larsson, Deniz Erdogmus, Rui Yan, Jose C Principe, and Jeffrey R Fitzsimmons. SNR-optimality of sum-of-squares reconstruction for phased-array magnetic resonance imaging. Journal of Magnetic Resonance, 163(1), 2003. [21] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep Learning. Nature, 521(7553), 2015. [22] Kai Lonning, Patrick Putzky, Matthan W. A. Caan, and Max Welling. Recurrent inference machines for accelerated MRI reconstruction, 2018. [23] Michael Lustig, David Donoho, and John M Pauly. Sparse MRI: The Application of Compressed Sensing for Rapid MR Imaging. Magnetic Resonance in Medicine, 58(6), 2007. [24] Bjoern H. Menze, Andrs Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, Levente Lanczi, Elizabeth R. Gerstner, Marc-Andr Weber, Tal Arbel, Brian B. Avants, Nicholas Ayache, Patricia Buendia, D. Louis Collins, Nicolas Cordier, Jason J. Corso, Antonio Criminisi, Tilak Das, Herve Delingette, agatay Demiralp, Christopher R. Durst, Michel Dojat, Senan Doyle, Joana Festa, Florence Forbes, Ezequiel Geremia, Ben Glocker, Polina Golland, Xiaotao Guo, Andac Hamamci, Khan M. Iftekharuddin, Raj Jena, Nigel M. John, Ender Konukoglu, Danial Lashkari, Jos Antonio Mariz, Raphael Meier, Srgio Pereira, Doina Precup, Stephen J. Price, Tammy Riklin Raviv, Syed M. S. Reza, Michael T. Ryan, Duygu Sarikaya, Lawrence H. Schwartz, Hoo-Chang Shin, Jamie Shotton, Carlos A. Silva, Nuno Sousa, Nagesh K. Subbanna, Gbor Szkely, Thomas J. Taylor, Owen M. Thomas, Nicholas J. Tustison, Gzde B. nal, Flor Vasseur, Max Wintermark, Dong Hye Ye, Liang Zhao, Binsheng Zhao, Darko Zikic, Marcel Prastawa, Mauricio Reyes, and Koen Van Leemput. The multimodal brain tumor image segmentation benchmark (BRATS). IEEE Transactions on Medical Imaging, 34(10), 2015. [25] Jun Miao, Feng Huang, Sreenath Narayan, and David L. Wilson. A new perceptual difference model for diagnostically relevant quantitative image quality evaluation: A preliminary study. Magnetic Resonance Imaging, 31(4), 2013. [26] Klaas P Pruessmann, Markus Weiger, Markus B Scheidegger, and Peter Boesiger. SENSE: sensitivity encoding for fast MRI. Magnetic resonance in medicine, 42(5), 1999. [27] Tran Minh Quan and Won-Ki Jeong. Compressed sensing dynamic MRI reconstruction using GPUaccelerated 3d convolutional sparse coding. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2016. [28] Peter B Roemer, William A Edelstein, Cecil E Hayes, Steven P Souza, and Otward M Mueller. The NMR phased array. Magnetic resonance in medicine, 16(2), 1990. [29] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. Medical Image Computing and Computer-Assisted Intervention, 2015. [30] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: nonlinear phenomena, 60(1-4), 1992. [32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, 2016. [33] Christopher M. Sandino, Neerav Dixit, Joseph Y. Cheng, and Shreyas S Vasanawala. Deep convolutional neural networks for accelerated dynamic magnetic resonance imaging. Technical report, Stanford University, 2017. [34] Anne Marie Sawyer, Michael Lustig, Marcus Alley, Phdmartin Uecker, Patrick Virtue, Peng Lai, Shreyas Vasanawala, and Ge Healthcare. Creation of fully sampled MR data repository for compressed sensing of the knee, 2013. [35] Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony N. Price, and Daniel Rueckert. A deep cascade of convolutional neural networks for MR image reconstruction. Information Processing in Medical Imaging, 2017. [36] Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony N. Price, and Daniel Rueckert. A deep cascade of convolutional neural networks for dynamic MR image reconstruction. IEEE Transactions on medical imaging, 37(2), 2018. [37] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas de Bel, Moira S. N. Berens, Cas van den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, Robbert van der Gugten, Pheng-Ann Heng, Bart Jansen, Michael M. J. de Kaste, Valentin Kotov, Jack Yu-Hung Lin, Jeroen T. M. C. Manders, Alexander S´onora-Mengana, Juan Carlos Garc´ıa-Naranjo, Mathias Prokop, Marco Saletta, Cornelia Schaefer-Prokop, Ernst Th. Scholten, Luuk Scholten, Miranda M. Snoeren, Ernesto Lopez Torres, Jef Vandemeulebroucke, Nicole Walasek, Guido C. A. Zuidhof, Bram van Ginneken, and Colin Jacobs. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge. Medical Image Analysis, 2017. [38] Ohad Shitrit and Tammy Riklin Raviv. Accelerated magnetic resonance imaging by adversarial neural network. In M. Jorge Cardoso, Tal Arbel, Gustavo Carneiro, Tanveer F. Syeda-Mahmood, Jo˜ao Manuel R. S. Tavares, Mehdi Moradi, Andrew P. Bradley, Hayit Greenspan, Jo˜ao Paulo Papa, Anant Madabhushi, Jacinto C. Nascimento, Jaime S. Cardoso, Vasileios Belagiannis, and Zhi Lu, editors, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, 2017. [39] Daniel K Sodickson and Warren J Manning. Simultaneous acquisition of spatial harmonics (SMASH): fast imaging with radiofrequency coil arrays. Magnetic resonance in medicine, 38(4), 1997. [40] Richard Szeliski. Computer vision algorithms and applications. Springer, 2011. [41] Patrick C Teo and David J Heeger. Perceptual image distortion. In IEEE International Conference on Image Processing (ICIP), volume 2, 1994. [42] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012. [43] Mark Tygert and Jure Zbontar. Simulating single-coil MRI from the responses of multiple coils. arXiv preprint, 2018. [44] Martin Uecker, Patrick Virtue, Frank Ong, Mark J. Murphy, Marcus T. Alley, Shreyas S. Vasanawala, and Michael Lustig. Software toolbox and programming library for compressed sensing and parallel imaging. In ISMRM Workshop on Data Sampling and Image Reconstruction, 2013. [45] Martin Uecker, Peng Lai, Mark J Murphy, Patrick Virtue, Michael Elad, John M Pauly, Shreyas S Vasanawala, and Michael Lustig. ESPIRiT -an eigenvalue approach to autocalibrating parallel MRI: where SENSE meets GRAPPA. Magnetic resonance in medicine, 71(3), 2014. [46] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint, 2016. [47] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Deep image prior. arXiv preprint, 2017. [48] Shanshan Wang, Zhenghang Su, Leslie Ying, Xi Peng, Shun Zhu, Feng Liang, Dagan Feng, and Dong Liang. Accelerating magnetic resonance imaging via deep learning. In IEEE International Symposium on Biomedical Imaging (ISBI), 2016. [49] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional GANs. Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [50] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classiﬁcation and localization of common thorax diseases. In 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2017. [51] Zhou Wang and Alan C Bovik. Mean squared error: Love it or leave it? a new look at signal ﬁdelity measures. IEEE signal processing magazine, 26(1), 2009. [52] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assessment. In Asilomar Conference on Signals, Systems & Computers, 2003. [53] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004. [54] Takayuki Yamamoto, T Okada, Yasutaka Fushimi, Akira Yamamoto, Koji Fujimoto, Sachi Okuchi, Hikaru Fukutomi, Jun C. Takahashi, Takeshi Funaki, Susumu Miyamoto, Aur´elien F. Stalder, Yutaka Natsuaki, Peter Speier, and Kaori Togashi. Magnetic resonance angiography with compressed sensing: An evaluation of moyamoya disease. In PloS one, 2018. [55] Ke Yan, Xiaosong Wang, Le Lu, and Ronald Summers. Deeplesion: Automated mining of large-scale lesion annotations and universal lesion detection with deep learning. Journal of Medical Imaging, 5, 2018. [56] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep ADMM-Net for compressive sensing MRI. Advances in Neural Information Processing Systems 29, 2016. [57] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. ADMM-Net: A deep learning approach for compressive sensing MRI. arXiv preprint, 2017. [58] Lin Zhang, Lei Zhang, Xuanqin Mou, David Zhang, et al. FSIM: a feature similarity index for image quality assessment. IEEE transactions on Image Processing, 20(8), 2011. [59] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with neural networks. IEEE Transactions on Computational Imaging, 3(1), 2017. [60] Bo Zhu, Jeremiah Z. Liu, Stephen F. Cauley, Bruce R. Rosen, and Matthew S. Rosen. Image reconstruction by domain-transform manifold learning. Nature, 555(7697), 2018.\n\n10 Changelog • V1 (Nov 2018): Arxiv preprint describing the fastMRI knee dataset. • V2 (Dec 2019): Added neuro dataset. Multi-coil classical baseline (TV model) applied to brain validation data Acceleration Regularization Weight Sequence NMSE PSNR SSIM 10−4 AXT1 0.03971 31.63 0.5677 AXT1POST 0.02581 32.39 0.5814 AXT2 0.03624 30.66 0.528 AXFLAIR 0.189 26.85 0.4512 10−3 AXT1 0.03818 31.82 0.5724 AXT1POST 0.02353 32.81 0.5919 AXT2 0.03457 30.86 0.5312 4-fold AXFLAIR 0.1869 26.96 0.4651 10−2 AXT1 0.03888 31.7 0.5376 AXT1POST 0.02199 33.17 0.5522 AXT2 0.03419 30.9 0.4923 AXFLAIR 0.1886 26.75 0.4435 10−1 AXT1 0.04916 30.54 0.5193 AXT1POST 0.02956 31.84 0.5284 AXT2 0.04708 29.39 0.4651 AXFLAIR 0.1934 26.14 0.4048 10−4 AXT1 0.06911 29.01 0.4823 AXT1POST 0.05457 29.09 0.498 AXT2 0.07904 27.05 0.4426 AXFLAIR 0.4421 23.93 0.3549 10−3 AXT1 0.06721 29.13 0.488 AXT1POST 0.05287 29.24 0.5039 AXT2 0.078 27.11 0.4405 8-fold AXFLAIR 0.1869 26.96 0.4627 10−2 AXT1 0.05935 29.68 0.5145 AXT1POST 0.04514 29.92 0.5325 AXT2 0.07486 27.29 0.4336 AXFLAIR 0.3893 24.15 0.3678 10−1 AXT1 0.06322 29.35 0.5928 AXT1POST 0.04904 29.54 0.6187 AXT2 0.0874 26.6 0.495 AXFLAIR 0.2773 24.66 0.4726 Table 7: Validation set results for the classical baseline model with Total Variation regularization for the brain multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Single-coil U-Net baseline applied to knee validation data Acceleration Channels #Params NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 32 3.35M 0.0161 0.0531 33.78 29.90 0.81 0.631 64 13.39M 0.0157 0.0528 33.90 29.9 0.813 0.633 128 53.54M 0.0154 0.0525 34.01 29.95 0.815 0.634 256 214.16M 0.0154 0.0525 34.00 29.95 0.815 0.636 8-fold 32 3.35M 0.0283 0.0698 31.13 28.6 0.754 0.555 64 13.39M 0.0272 0.0693 31.30 28.63 0.758 0.558 128 53.54M 0.0265 0.0686 31.44 28.68 0.761 0.558 256 214.16M 0.0261 0.0682 31.5 28.71 0.762 0.559 Table 8: Validation set results for the U-Net baseline model trained for the single-coil task. The channels column denotes the number of output channels of the ﬁrst convolution in the model. Doubling this number of channels roughly quadruples the total number of parameters in the model. Bold-faced numbers indicate the best performance for each image quality metric. Single-coil classical and U-Net baselines applied to test data Model Acceleration NMSE PSNR SSIM Classical Model (Total Variation) 4-fold 0.0479 30.69 0.603 8-fold 0.0795 27.12 0.469 Aggregate 0.0648 28.77 0.531 U-Net 4-fold 0.0320 32.22 0.754 8-fold 0.0480 29.45 0.651 Aggregate 0.0406 30.7 0.699 Table 9: Comparison of classical and U-Net baseline performance for the single-coil task with test data Multi-coil U-Net baseline applied to knee validation data Acceleration Channels #Params NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 32 3.35M 0.0066 0.0122 36.7 35.97 0.9192 0.8595 64 13.39M 0.0063 0.0120 36.95 36.11 0.9224 0.8615 128 53.54M 0.0057 0.0113 37.38 36.33 0.9266 0.8641 256 214.16M 0.0054 0.0112 37.58 36.39 0.9287 0.8655 8-fold 32 3.35M 0.0144 0.0197 33.31 33.82 0.8778 0.8213 64 13.39M 0.0136 0.0198 33.56 33.93 0.8825 0.8238 128 53.54M 0.0123 0.0179 34.01 34.25 0.8892 0.8277 256 214.16M 0.0120 0.0181 34.12 34.23 0.8915 0.8286 Multi-coil U-Net baseline applied to brain validation data Acceleration Channels #Params Sequence NMSE PSNR SSIM 32 3.35M AXT1 0.01498 35.67 0.9215 AXT1POST 0.013 35.43 0.9298 AXT2 0.02249 32.51 0.9112 AXFLAIR 0.1572 30.73 0.7869 64 13.39M AXT1 0.01571 35.57 0.922 AXT1POST 0.01313 35.41 0.9307 AXT2 0.02014 32.98 0.9151 4-fold AXFLAIR 0.1579 30.96 0.7917 128 53.54M AXT1 0.0142 35.92 0.9243 AXT1POST 0.01231 35.69 0.9332 AXT2 0.01855 33.34 0.9175 AXFLAIR 0.1566 30.98 0.7932 256 214.16M AXT1 0.01317 36.24 0.9275 AXT1POST 0.0111 36.11 0.9361 AXT2 0.01733 33.63 0.9207 AXFLAIR 0.1532 31.52 0.7985 32 3.35M AXT1 0.04289 31.5 0.8885 AXT1POST 0.04186 31.71 0.8816 AXT2 0.04357 30.86 0.8759 AXFLAIR 0.1594 32.86 0.8188 64 13.39M AXT1 0.04205 32.56 0.8876 AXT1POST 0.04034 31.89 0.883 AXT2 0.04248 31.1 0.8753 8-fold AXFLAIR 0.1818 30.49 0.7843 128 53.54M AXT1 0.04706 31.82 0.8804 AXT1POST 0.04005 31.47 0.8828 AXT2 0.04311 30.13 0.8806 AXFLAIR 0.2 28.97 0.7779 256 214.16M AXT1 0.0443 32.02 0.8837 AXT1POST 0.04028 31.95 0.8845 AXT2 0.04167 31.29 0.8811 AXFLAIR 0.1565 30.49 0.7805 Table 11: Validation set results for the U-Net baseline model trained for the brain multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Multi-coil classical and U-Net baselines applied to test data Dataset Model Acceleration NMSE PSNR SSIM Knee Classical Model (Total Variation) 4-fold 0.0503 30.88 0.628 8-fold 0.0760 28.25 0.593 Aggregate 0.0633 29.54 0.610 U-Net 4-fold 0.0106 35.91 0.904 8-fold 0.0171 33.57 0.858 Aggregate 0.0139 34.7 0.881 Brain Classical Model (Total Variation) 4-fold 0.1388 27.53 0.4439 8-fold 0.03753 31.32 0.5135 Aggregate 0.0882 29.42 0.4787 U-Net 4-fold 0.0107 38.13 0.9446 8-fold 0.0233 34.52 0.9146 Aggregate 0.017 36.325 0.9296 Table 12: Comparison of classical and U-Net baseline performance for the multi-coil task with knee test data.A Raw k-space File Descriptions acquisition Acquisition protocol. For knee images this is either CORPD or CORPDF, indicating coronal proton density with or without fat saturation, respectively (see Figure 3). For Brain images this is AXFLAIR, AXT1, AXT1POST or AXT2 (see Figure 4). ismrmrd header The XML header copied verbatim from the ISMRMRD ﬁle that was used to generate the HDF5 ﬁle. It contains information about the scanner, ﬁeld of view, dimensions of k-space, and sequence parameters. patient id A unique string identifying the examination, and substituting anonymously for the patient identiﬁcation. norm, max The Euclidean norm and the largest entry of the target volume. For the multi-coil track the target volume is stored in reconstruction rss. For the single-coil track the target volume is stored in reconstruction esc. These two attributes are only available in the training and validation datasets. acceleration Acceleration factor of the undersampled k-space trajectory (either 4 or 8). This attribute is only available in the test dataset. num low frequency The number of low-frequency k-space lines in the undersampled k-space trajectory. This attribute is only available in the test dataset. The rest of this section describes the format of the HDF5 ﬁles for the multi-coil and single-coil tracks. A.1 Multi-coil Track {knee,brain} multicoil train.tar.gz Training dataset for the multi-coil track. The HDF5 ﬁles contain the following tensors: kspace Multi-coil k-space data. The shape of the kspace tensor is (number of slices, number of coils, height, width). reconstruction rss root-sum-of-squares reconstruction of the multi-coil k-space data. The shape of the reconstruction rss tensor is (number of slices, r height, r width). For knee images, height and width have been cropped to 320 x 320. {knee,brain} multicoil val.tar.gz Validation dataset for the multi-coil track. The HDF5 ﬁles have the same structure as the HDF5 ﬁles in multicoil train.tar.gz. {knee,brain} multicoil test.tar.gz Test dataset for the multi-coil track. The HDF5 ﬁles contain the following tensors: kspace Undersampled multi-coil k-space. The shape of the kspace tensor is (number of slices, number of coils, height, width). mask Deﬁnes the undersampled Cartesian k-space trajectory. The number of elements in the mask tensor is the same as the width of k-space. A.2 Single-coil Track (knee only) knee singlecoil train.tar.gz Training dataset for the single-coil track. Note that only the knee dataset has a single-coil track. The HDF5 ﬁles contain the following tensors: kspace Emulated single-coil k-space data. The shape of the kspace tensor is (number of slices, height, width). reconstruction rss root-sum-of-squares reconstruction of the multi-coil k-space that was used to derive the emulated single-coil k-space cropped to the center 320 × 320 region. The shape of the reconstruction rss tensor is (number of slices, 320, 320). reconstruction esc The inverse Fourier transform of the single-coil k-space data cropped to the center 320 × 320 region. The shape of the reconstruction esc tensor is (number of slices, 320, 320). knee singlecoil val.tar.gz Validation dataset for the single-coil track. The HDF5 ﬁles have the same structure as the HDF5 ﬁles in singlecoil train.tar.gz. knee singlecoil test.tar.gz Test dataset for the single-coil track. Note that only the knee dataset has a single-coil track. The HDF5 ﬁles contain the following tensors: kspace Undersampled emulated single-coil k-space. The shape of the kspace tensor is (number of slices, height, width). mask Deﬁnes the undersampled Cartesian k-space trajectory. The number of elements in the mask tensor is the same as the width of k-space. B Classical Reconstruction with BART The Berkeley Advanced Reconstruction Toolbox (BART) [44] 5 contains implementations of standard methods for coil sensitivity estimation and undersampled MR image reconstruction incorporating parallel imaging and compressed sensing. We used this tool to produce the classical baseline MSE estimates, as well as the illustrations in Figure 2. In this section we provide a brief introduction to the tool suﬃcient for reproducing our baseline results. We will use as an example a 640x368 undersampled MRI scan with 15 coils. The target region is a 320 × 320 central region which will be cropped to after reconstruction. BART provides a command line interface which acts on ﬁles in a simple storage format. Each multidimensional array is stored in a pair of ﬁles, a header ﬁle .hdr and a data ﬁle .cfl. The header ﬁle contains the dimensions of the array given in ASCII. In our running example, this should be input.hdr: 1 640 368 15 The CFL ﬁle contains the raw data in column-major order, stored as complex ﬂoat values. Missing k-space values are indicated by 0 entries. BART provides Python and MATLAB interfaces for reading and writing this format. When working with k-space data with BART, it is simplest to use data in ”centered” form, where the low frequency values are in the center of the image, and the high frequency values are at the edges. Most FFT libraries output the data in uncentered form. BART provides a tool for conversion: bart fftshift 7 input output The input and output are speciﬁed without ﬁle extensions. The value 7 above is a bitmask indicating the image is stored in axis 0,1,2 (1+2+4) of the input array. This bitmask is used in the commands that follow also. Uncentered k-space data is easily identiﬁed by comparing the magnitude of the corners versus the center of the array. Centered FFTs of natural data will have the largest magnitudes near the center of the array when plotted. Parallel MR imaging is often performed as a two-step process consisting of coil-sensitivity estimation, then reconstruction assuming the estimated sensitivity maps are exact. BART implements this approach through the ecalib and pics commands. The coil-sensitivity maps can be estimated using the ESPIRiT approach using the command bart ecalib -m1 Produce a single set of sensitivity maps -r26 Number of fully sampled reference lines input output_sens The central reference region is used by BART to estimate the coil sensitivities. This area is also known as the auto-calibration region. The number of lines used in our masking procedure is a percentage of the k-space width, as described in Section 4.2. Given the estimated coil sensitivities, a reconstruction using TV regularization can be performed with bart pics -d4 Debug log level, use 0 for no stdout output -i200 Optimization iterations -R T:7:0:0.05 Use TV (T) with regularizer strength 0.05, with bitmask 7 input output_sens output The output of this command is in CFL format. It can be converted to a PNG using bart toimg. When using L1 wavelet regularization, the character ”W” should be used in the R option, with the additional -m argument to ensure that ADMM is used.\n--- FOOTNOTES 1 Facebook AI Research 2 NYU School of Medicine 3 NYU Center for Data Science 4 University of Florida 2D TSE protocol employed clinically at NYU School of Medicine. The dataset includes data from 3T direct correspondence to the sequences represented in raw data. We acknowledge grant support from the National Institutes of Health under grants NIH R01", "persona": "Student", "response": "### 1. Overview\nFacebook and NYU School of Medicine’s Department of Radiology are excited to announce fastMRI, a new collaborative research project that will investigate the use of artificial intelligence (AI) to make magnetic resonance imaging (MRI) scans up to 10 times faster. If this effort is successful, it will make MRI technology available to more people, expanding access to this key diagnostic tool. Around the same time, the Facebook Artificial Intelligence Research (FAIR) group, which focuses on open and foundational research that advances the state of AI, was looking for projects in which AI could have significant real-world impact. CAI²R’s image reconstruction work fit those criteria and provided FAIR with an opportunity to combine its deep learning expertise — particularly in the field of computer vision — and its ability to train models at large scale with the medical school’s leading imaging science expertise. The magnetic resonance images (which generally represent small targeted regions of anatomy) used for this project have been scrubbed of any potential distinguishing features. Comparisons of the performance between AI-based reconstructions and traditional reconstructions will, likewise, be devoid of any identifying information. No Facebook data of any kind will be used in the project.\n\n### 2. Background\nUnlike other AI-related projects, which use medical images as a starting point and then attempt to derive anatomical or diagnostic information from them (in emulation of human observers), this collaboration focuses on applying the strengths of machine learning to reconstruct the most high-value images in entirely new ways. With the goal of radically changing the way medical images are acquired in the first place, our aim is not simply enhanced data mining with AI, but rather the generation of fundamentally new capabilities for medical visualization to benefit human health. We believe the fastMRI project will demonstrate how domain-specific experts from different fields and industries can work together to produce the kind of open research that will make a far-reaching and lasting positive impact in the world. MRI scanners provide doctors and patients with images that typically show a greater level of detail related to soft tissues — such as organs and blood vessels — than is captured by other forms of medical imaging. But they are relatively slow, taking anywhere from 15 minutes to over an hour, compared with less than a second or up to a minute, respectively, for X-ray and CT scans.\n\n### 3. Methods\nLarry Zitnick and Nafissa Yakubova from the Facebook Artificial Intelligence Research (FAIR) group collaborated with Daniel Sodickson, M.D., Ph.D, Michael Recht, M.D., and Yvonne Lui, M.D., from NYU School of Medicine on this work. Project data details The imaging data set used in the project, collected exclusively by NYU School of Medicine, consists of 10,000 clinical cases and comprises approximately 3 million magnetic resonance images of the knee, brain, and liver. All data, including both images and raw scanner data, are fully stripped of patient names and all other protected health information. The work is fully HIPAA-compliant and approved under NYU Langone’s Institutional Review Board, which oversees all human subject research at the medical center. The project is governed by strict human subject data protection protocols and supported by the world-class information technology team at NYU Langone.\n\n### 4. Results\nAdvancing the AI and medical communities In the interest of advancing the state of the art in medical imaging as quickly as possible, we plan to open-source this work to allow the wider research community to build on our developments. As the project progresses, Facebook will share the AI models, baselines, and evaluation metrics associated with this research, and NYU School of Medicine will open-source the image data set. This will help ensure the work’s reproducibility and accelerate adoption of resulting methods in clinical practice. This project will initially focus on changing how MRI machines operate. Currently, scanners work by gathering raw numerical data in a series of sequential views and turning the data into cross-sectional images of internal body structures that doctors then use to evaluate a patient’s health.\n\n### 5. Takeaways\nThe larger the data set to be gathered, the longer the scan will take. NYU School of Medicine’s Department of Radiology chair Michael Recht, M.D., Daniel Sodickson, M.D., Ph.D. vice chair for research and director of the Center for Advanced Imaging Innovation and Research and Yvonne Lui, M.D., director of artificial intelligence, watch an MRI exam take place with a technician at NYU Langone Health in August, 2018. Recht, Sodickson, and Lui examine MRI scans of a knee at NYU Langone Health in August 2018. Using AI, it may be possible to capture less data and therefore scan faster, while preserving or even enhancing the rich information content of magnetic resonance images. The key is to train artificial neural networks to recognize the underlying structure of the images in order to fill in views omitted from the accelerated scan. This approach is similar to how humans process sensory information. When we experience the world, our brains often receive an incomplete picture — as in the case of obscured or dimly lit objects — that we need to turn into actionable information. Early work performed at NYU School of Medicine shows that artificial neural networks can accomplish a similar task, generating high-quality images from far less data than was previously thought to be necessary. In practice, reconstructing images from partial information poses an exceedingly hard problem.", "paper_id": "48", "story_id": "48-01"}
{"paper": "Abstract\nAccelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background. 1\n\nIntroduction\nThe excellent soft tissue contrast and ﬂexibility of magnetic resonance imaging (MRI) makes it a very powerful diagnostic tool for a wide range of disorders, including neurological, musculoskeletal, and oncological diseases. However, the long acquisition time in MRI, which can easily exceed 30 minutes, leads to low patient throughput, problems with patient comfort and compliance, artifacts from patient motion, and high exam costs. As a consequence, increasing imaging speed has been a major ongoing research goal since the advent of MRI in the 1970s. Increases in imaging speed have been achieved through both hardware developments (such as improved magnetic ﬁeld gradients) and software advances (such as new pulse sequences). One noteworthy development in this context is parallel imaging, introduced in the 1990s, which allows multiple data points to be sampled simultaneously, rather than in a traditional sequential order [39, 26, 9]. The introduction of compressed sensing (CS) in 2006 [2, 23] promised another breakthrough in the reduction of MR scan time. At their core, CS techniques speed up the MR acquisition by acquiring less measurement data than has previously been required to reconstruct diagnostic quality images. Since undersampling of this kind violates the Nyquist-Shannon sampling theorem, aliasing artifacts are introduced which must be eliminated in the course of image reconstruction. This can be achieved by incorporating additional a priori knowledge during the image reconstruction process. The last two years have seen the rapid development of machine learning approaches for MR image reconstruction, which hold great promise for further acceleration of MR image acquisition [10, 48, 11, 35, 60]. Some of the ﬁrst work on this subject was presented at the 2016 annual meeting of the International Society for Magnetic Resonance in Medicine (ISMRM). The 2017 ISMRM annual meeting included, for the ﬁrst time, a dedicated session on machine learning for image reconstruction, and presentations on the subject at the 2018 annual meeting spanned multiple focused sessions, including a dedicated category for abstracts. Despite this substantial increase in research activity, the ﬁeld of MR image reconstruction still lacks large-scale, public datasets with consistent evaluation metrics and baselines. Many MR image reconstruction studies use datasets that are not openly available to the research community. This makes it challenging to reproduce and validate comparisons of different approaches, and it restricts access to work on this important problem to researchers associated with or cooperating with large academic medical centers where such data is available. In contrast, research in computer vision applications such as object classiﬁcation has greatly beneﬁted from the availability of large-scale datasets associated with challenges such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [31]. Such challenges have served as a catalyst for the recent explosion in research activity on deep learning [21]. The goal of the fastMRI dataset is to provide a ﬁrst step towards enabling similar breakthroughs in the machine-learning-based reconstruction of accelerated MR images. In this work we describe the ﬁrst large-scale release of raw MRI data that includes 8344 volumes, consisting of 167,375 slices1, associated with in vivo examinations from a range of MRI systems. In addition, we are releasing processed MR images in DICOM format from 20,000 knee and brain examinations from a representative clinical patient population, consisting of more than 1.57 million slices. Prior to providing details about the dataset and about target reconstruction tasks with associated benchmarks, we begin with a brief primer on MR image acquisition and reconstruction, in order to enable non-MRI-experts to get up to speed quickly on the information content of the dataset. In general, both the fastMRI dataset and this paper aim to connect the data science and the MRI research communities, with the overall goal of advancing the state of the art in accelerated MRI.\n\n2 Introduction to MR Image Acquisition and Reconstruction MR imaging is an indirect process, whereby cross-sectional images of the subject’s anatomy are produced from frequency and phase measurements instead of direct, spatially-resolved measurements. A measuring instrument, known as a receiver coil, is placed in proximity to the area to be imaged (Figure 1). During imaging, a sequence of spatially- and temporally-varying magnetic ﬁelds, called a “pulse sequence,” is applied by the MRI machine. This induces the body to emit resonant electromagnetic response ﬁelds which are measured by the receiver coil. The measurements typically correspond to points along a prescribed path through the multidimensional Fourier-space representation of an imaged body. This Fourier space is known as k-space in the medical imaging community. In the most basic usage of MR imaging, the full Fourier-space representation of a region is captured by a sequence of samples that tile the space up to a speciﬁed maximum frequency. Figure 1: The receiver coil housing and its positioning on a patient for a knee MR examination. The spatially-resolved image m can be estimated from the full k-space y by performing an inverse multidimensional Fourier transform: where ˆm is a noise-corrupted estimate of the true image m. The number of samples captured in k-space is a limiting factor for the speed of MR imaging. Fewer samples can be captured by sampling up to a lower maximum frequency, however this produces images of lower spatial resolution. An alternative undersampling approach involves omitting some number of k-space samples within a given maximum frequency range, which then results in aliasing artifacts. In order to remove these artifacts and infer the true underlying spatial structure of the imaged subject, one may apply a number of possible reconstruction strategies. 2.1 Parallel MR Imaging In parallel MR imaging, multiple receiver coils are used, each of which produces a separate k-space measurement matrix. Each of these matrices is different, since the view each coil provides of the imaged volume is modulated by the differential sensitivity that coil exhibits to MR signal arising from different regions. In other words, each coil measures Fourier components of the imaged volume multiplied by a complex-valued position-dependent coil sensitivity map Si. The measured k-space signal yi for coil i in an array of nc coils is given by where the multiplication is entry-wise. This is illustrated in Figure 2b, which shows the absolute value of the inverse discrete Fourier transform (DFT) of fully-sampled complex-valued k-space signals for each coil in a 15-element coil array. Each coil is typically highly sensitive in one region, and its sensitivity falls offsigniﬁcantly in other regions. If the sensitivity maps are known, and the k-space sampling is full (i.e., satisfying the Nyquist sampling condition), then the set of linear relations between m and each yi deﬁnes a linear system that is overdetermined by a factor of nc. It may be inverted using a pseudoinverse operation to produce a reconstruction of m, as long as the linear system is full rank. The quality of this reconstruction will depend on the measurement noise, since the signal-to-noise ratio is poor in parts of the volume where the coil sensitivity is low. In accelerated parallel imaging, each coil’s k-space signal is undersampled. As long as the total number of measurements across all coils exceeds the number of image voxels to be reconstructed, an unregularized least squares solution can still be used, leading to a theoretical nc-fold ˆm = F−1(y), (1) yi = F (Sim) + noise, (2) (a) k-space data from 15 coils (b) Individual coil spatial images from fully sampled data (d) Ground truth, cropped to central region and vertically ﬂipped (e) Unregularized reconstruction Figure 2: Multi-coil MRI reconstruction (c) Coil sensitivity map magnitudes given by ESPIRiT (f) Total variation penalty reconstruction (g) Baseline model reconstruction speedup over fully-sampled single-coil imaging. Each extra coil effectively produces an additional “sensitivity-encoded” measurement of the volume [26], which augments the frequency and phase encoded measurements obtained from the sequential application of magnetic ﬁeld gradients in the MR pulse sequence. Estimates of coil sensitivity patterns, required for inversion of the undersampled multi-coil linear system, may be generated from separate low-resolution calibration scans. They may also be derived directly from the k-space measurements by fully sampling a comparatively small central region of k-space, which corresponds to low spatial frequencies. In practice, the use of sub-sampling results in signiﬁcant ampliﬁcation of noise, and regularization is usually needed. In cases where a tight imaging ﬁeld of view is used, or at imaging depths exceeding the dimensions of the individual coils, the sensitivity patterns of different coils spread out, thereby lowering the effective rank of the linear system, increasing noise ampliﬁcation associated with the inverse operation, and limiting the maximum practical acceleration. As a result, in the clinic, parallel imaging acceleration factors are typically on the order of two to three. 2.2 Machine Learning Reconstruction of Undersampled MRI Data Classical approaches to MRI reconstruction solve a regularized inverse optimization problem to ﬁnd the spatially-resolved image from the sub-sampled k-space data, in both the single-coil and the multi-coil case. We describe the classical approach in more detail in Section 6. In the machine learning approach, a reconstruction function is learned from input and output pair tuples (y, m) drawn from a population. The goal is to ﬁnd a function B that minimizes the risk (i.e., expected loss) over the population distribution: We discuss error metrics that may be used as loss functions L in Section 5. In practice this optimization problem must be approximated with the empirical risk using a sample {(m(i), y(i))}ndata i=1 from the population, with respect to a loss function L:\n\n3 Prior Public Datasets The availability of public datasets has played an important role in advancing research in medical imaging, providing benchmarks to compare different approaches and leading to more impactful contributions. Early works such as DDSM [13], SLIVER07 [14] and CAUSE07 [8] triggered increasing efforts to collect new larger-scale biomedical datasets, which resulted in over one hundred public releases (counting the entries on https://grand-challenge.org/) to advance medical image analysis research. The vast majority of these datasets, which include a range of medical imaging modalities, are designed to test the limits of current methods in the tasks of segmentation, classiﬁcation, and detection. Datasets such as BraTS [24], LUNA [37], ChestX-ray [50], DeepLesion [55], and Camelyon [1], UK biobank2, ADNI (Alzheimers Disease Neuroimaging Initiative)3 and ˆm = B(y) (3) B∗= arg min B R(B), where R(B) = E(y,m) [L (B (y) , m)] . ndata X Rempirical(B) = 1 ndata i=1 L \u0010 B \u0010 y(i)\u0011 , m(i)\u0011 . (4) Dataset Volumes Body part MR scan type NYU dataset [11] 100 knee PD, T2 Stanford dataset 2D FSE 89 knee PD Stanford dataset 3D FSE [34] 20 knee PD Stanford undersampled dataset 38 knee PD fastMRI dataset 1594 knee PD Table 1: Publicly available MRI datasets containing k-space data TCIA (The Cancer Imaging Archive)4. However, none of the most prominent public MRI datasets include k-space imaging data. However, the current lack of large-scale reference standards for MR image reconstruction hinders progress in this important area. Most research uses synthetic k-space data that is not directly acquired but rather obtained from post-processing of already-reconstructed images [5, 38, 57, 56, 27]. Research using small-scale proprietary raw k-space datasets is also common [15, 36, 35, 33, 22]. In order to address the above-mentioned shortcomings, recent efforts have been devoted to collecting and publicly releasing datasets containing raw (unprocessed) k-space data; see, e.g., [34, 11]. However, the size of these existing datasets remains small. As an example, Table 1 lists publicly available knee MR datasets containing raw k-space data. Although datasets such as these provide a valuable test bed for signal processing methods, larger datasets encompassing different anatomy are required to fully realize the potential of deep learning. 4 The fastMRI Dataset and Associated Tasks The fastMRI dataset (http://fastmri.med.nyu.edu/) contains four types of data from MRI acquisitions of knees and brains Raw multi-coil k-space data: unprocessed complex-valued multi-coil MR measurements. Emulated single-coil k-space data: combined k-space data derived from multi-coil k-space data in such as way as to approximate single-coil acquisitions, for evaluation of single-coil reconstruction algorithms. Ground-truth images: real-valued images reconstructed from fully-sampled multi-coil acquisitions using the simple root-sum-of-squares method detailed below. These may be used as references to evaluate the quality of reconstructions. DICOM images: spatially-resolved images for which the raw data was discarded during the acquisition process. These images are provided to represent a larger variety of machines and settings than are present in the raw data. This data was designed to enable two distinct types of tasks: 1. Single-coil reconstruction task: reconstruct images approximating the ground-truth from\nundersampled single-coil data. 2. Multi-coil reconstruction task: reconstruct images approximating the ground-truth from undersampled multi-coil data. For each task we provide an oﬃcial split of the k-space data and ground-truth images into training and validation subsets that contain fully-sampled acquisitions, as well as test and challenge subsets which contain k-space data that have been subjected to undersampling masks as described below. Ground-truth images are not being released for the test and challenge datasets. During training of a machine-learning model, the training k-space data should be programmatically masked following the same procedure. The challenge subsets are not being released at the time of writing and are reserved for future challenges associated with the fastMRI dataset. The rationale for having a single-coil reconstruction task (and for providing simulated single-coil data), even though reconstruction from multi-coil data is expected to be more precise, is twofold: (i) to lower the barrier of entry for researchers who may not be familiar with MRI data, since the use of a single coil removes a layer of complexity, and (ii) to include a task that is relevant for the single-coil MRI machines still in use throughout the world. The DICOM images may be useful as additional data for training. Their distribution is different from that of the ground-truth images, since they were acquired with a larger diversity of scanners, manners of acquisition, reconstruction methods, and post-processing algorithms, so the application of transfer-learning techniques may be necessary. Most DICOM images are the result of accelerated parallel imaging acquisitions and corresponding reconstructions, with image quality that differs from that of putative fully-sampled acquisitions and reconstructions. The ground-truth images may, in many cases, represent a higher standard of image quality than the clinical gold standard, for which full sampling is not routine or even practical. 4.1 Anonymization Curation of the datasets described here was part of a study approved by the NYU School of Medicine Institutional Review Board. Raw data was anonymized via conversion to the vendorneutral ISMRMRD format [18]. DICOM data was anonymized using the RSNA clinical trial processor. We performed manual inspection of each DICOM image for the presence of unexpected protected health information (PHI), manual checking of metadata in raw data ﬁles, as well as spot checking of all metadata and image content. 4.2 Knee k-space Data Multi-coil raw data was stored for 1,594 scans acquired for the purpose of diagnostic knee MRI. For each scan, a single fully sampled MRI volume was acquired on one of three clinical 3T systems (Siemens Magnetom Skyra, Prisma and Biograph mMR) or one clinical 1.5T system (Siemens Magnetom Aera). Data acquisition used a 15 channel knee coil array and conventional Cartesian two pulse sequences, yielding coronal proton-density weighting with (PDFS, 798 scans) and without (PD, 796 scans) fat suppression (see Figure 3). Sequence parameters are, as per standard clinical protocol, matched as closely as possible between the systems. The following sequence parameters were used: Echo train length 4, matrix size 320 × 320, in-plane resolution 0.5mm×0.5mm, slice thickness 3mm, no gap between slices. Timing varied between systems, with repetition time (TR) ranging between 2200 and 3000 milliseconds, and echo time (TE) between 27 and 34 milliseconds. 4.3 Brain k-space Data System Number of scans Skyra 3T 663 Prisma 3T 83 Biograph mMR 3T 153 Aera 1.5T 695 Table 2: Number of scans of knee raw data per scanner Field Strength 1.5T T1 375 407 T1 POST 849 641 T2 1651 2515 FLAIR 126 406 Total 3001 3969 Table 3: Number of scans for the different contrasts and scanner ﬁeld strengths of the brain raw dataset. includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent (labelled T1 POST) (see Figure 4). Not all imaging volumes included all pulse sequences. The exact distribution of contrasts and ﬁeld strengths is given in table 3. To ensure data de-identiﬁcation, we used only axial 2-D images in this dataset. We used zero matrices to replace the k-space slices ⪆5mm below the orbital rim. All processed k-spaces were then reconstructed to images in DICOM format, loaded into a picture archival communication system (PACS) and all images were visually checked by certiﬁed MR technologists to conﬁrm exclusion of identifying facial features. 4.4 Knee emulated Single-coil k-Space Data We used an emulated single-coil (ESC) methodology to simulate single-coil data from a multi-coil acquisition [43]. ESC computes a complex-valued linear combination of the responses from multiple coils, with the linear combination ﬁtted to the ground-truth root-sum-of-squares reconstruction in the least-squares sense. 4.5 Knee DICOM Data In addition to the scanner raw data described above, the fastMRI dataset includes DICOM data from 10,000 clinical knee MRI scans. These images represent a wider variety of scanners and pulse sequences than those represented in the collection of raw data. Each MR exam for which DICOM images are included typically consisted of ﬁve clinical pulse sequences:\n\n1. Coronal proton-density weighting without fat suppression,\n\n4. Sagittal T2 weighting with fat suppression, and\n\n5. Axial T2 weighting with fat suppression.\nThe two coronal sequences have the same basic speciﬁcations (matrix size, etc) as the sequences associated with raw data. The sagittal and axial sequences have different matrix sizes and have no The Fourier transformation of an image from a DICOM ﬁle does not directly correspond to the originally measured raw data, due to the inclusion of additional post-processing steps in the vendorspeciﬁc reconstruction pipeline. Most of the DICOM images are also derived from accelerated acquisitions and are reconstructed with parallel imaging algorithms, since this baseline acceleration represents the current clinical standard. The image quality of DICOM images, therefore, is not equivalent to that of the ground truth images directly associated with fully sampled raw data. The DICOM images are distinct from the validation, test, or challenge sets. 4.6 Brain DICOM 10,000 brain MRI DICOM studies are also included. Axial 2D image volumes are included with the following pulse sequences: T1, T2, and T2 FLAIR. All studies represent unique individuals and there is no subject overlap with the brain rawdata. In addition to the deidentiﬁcation procedures detailed above, the brain image volumes were cropped to exclude identiﬁable facial features, following which each image was visually inspected to conﬁrm appropriate deidentiﬁcation. Finally, we present 10,000 brain MRI DICOM studies from 10,000 unique subjects, each one including axial 2D DICOM image volumes through the majority of the brain representing a broad range of neurological pathologies. Not all studies include all pulse sequences. 4.7 Ground Truth The root-sum-of-squares reconstruction method applied to the fully sampled k-space data [28] provides the ground truth for the multi-coil dataset. The single-coil dataset includes two ground truth reconstructions, which we denote ESC and RSS. The ESC ground truth is given by the inverse Fourier transform of the single-coil data, and the RSS ground truth is given by the root-sum-ofsquares reconstruction computed on the multi-coil data that were used to generate the virtual single-coil k-space data. All ground truth images are cropped to the central 320 × 320 pixel region to compensate for readout-direction oversampling that is standard in clinical MR examinations. The root-sum-of-squares approach [28] is one of the most commonly-used coil combination methods in clinical imaging. It ﬁrst applies the inverse Fourier Transform to the k-space data from each coil: ˜mi = F−1(yi), (5) where yi is the k-space data from the ith coil and ˜mi is the ith coil image. Then, the individual coil images are combined voxel by voxel as follows: i=0 | ˜mi|2 !1/2 , (6) nc X ˜mrss = (a) (b) Figure 3: A proton-density weighted image (a) with fat suppression (PDFS) and (b) without fat suppression (PD). Fat has a high signal response in MR imaging, which can make details in other regions diﬃcult to see. Fat-suppressed scans typically have higher noise. Volumes Slices Multi-coil Single-coil Multi-coil Single-coil training 973 973 34,742 34,742 validation 199 199 7,135 7,135 test 118 108 4,092 3,903 challenge 104 92 3,810 3,305 Table 4: Volumes and slices in each set (a) (b) (c) (d) Figure 4: Axial brain MRI images with different contrasts: (a) FLAIR, (b) T1 weighted (c) T1 weighted with contrast agent (T1 POST), and (d) T2 weighted. 4.8 Dataset Split Each volume is randomly assigned to one of the following six component datasets: training, validation, multi-coil test, single-coil test, multi-coil challenge, or single-coil challenge. Table 4 shows the number of volumes assigned to each dataset. The training and validation datasets may be used to ﬁt model parameters or to determine hyperparameter values. The test dataset is used to compare the results across different approaches. To ensure that models do not overﬁt to the test set, the ground truth reconstructions are not publicly released for this set. Evaluation on the test set is accomplished by uploading results to the public leaderboard at http://fastmri.org/. The challenge portion of the dataset will be forthcoming. A volume from the train or validation dataset is used in both the single-coil and multi-coil tracks, whereas a volume from the test or challenge dataset is only used in either the single-coil or the multi-coil track. Volumes were only included in a single test or challenge set to ensure information from one could not be used to help the result in another. 4.9 Cartesian Undersampling Volumes in the test and challenge datasets contain undersampled k-space data. The undersampling is performed by retrospectively masking k-space lines from a fully-sampled acquisition. k-space lines are omitted only in the phase encoding direction, so as to simulate physically realizable accelerations in 2D data acquisitions. The same undersampling mask is applied to all slices in a volume, with each case consisting of a single volume. The overall acceleration factor is set randomly to either four or eight (representing a four-fold or an eight-fold acceleration, respectively), with equal probability for each. All undersampling masks are generated by ﬁrst including some number of adjacent lowestfrequency k-space lines to provide a fully-sampled k-space region. When the acceleration factor equals four, the fully-sampled central region includes 8% of all k-space lines; when it equals eight, 4% of all k-space lines are included. The remaining k-space lines are included differently for both knee and brain cases. For knee, the remaining k-space lines are included uniformly at random, with the probability set so that, on average, the undersampling mask achieves the desired acceleration factor. Random undersampling is chosen in order to meet the general conditions for compressed sensing [2, 23], for a fair comparison of learned reconstruction algorithms with traditional sparsitybased regularizers. For brain, after a random offset from the start, the remaining lines are sampled equidistant from each other with a spacing that achieves the desired acceleration factor. Equidistant was chosen because of ease of implementation on existing MRI machines. Figure 5 depicts the kspace trajectories for random and equidistant undersampling at four and eight acceleration factors.\n\n5 Metrics The assessment of MRI reconstruction quality is of paramount relevance to develop and compare machine learning and medical imaging systems [51, 53, 3, 58]. The most commonly used evaluation metrics in the MRI reconstruction literature [3] include (normalized) mean squared error, which measures pixel-wise intensity differences between reconstructed and reference images, and signalto-noise ratio, which measures the degree to which image information rises above background noise. These metrics are appealing because they are easy to understand and eﬃcient to compute. However, they both evaluate pixels independently, ignoring the overall image structure. Additional metrics have been introduced in the literature to capture structural distortion [41, 6, 58]. For example, the structural similarity index [53] and its extended version, multiscale structural (a) Random mask with 4-fold acceleration (c) Equispaced mask with 4-fold acceleration Figure 5: Examples of undersampled k-space trajectories similarity [52], provide a mechanism to assess the perceived quality of an image using local image patches. The most recent developments in the computer vision literature leverage pretrained deep neural networks to measure the perceptual quality of an image by computing differences at the representation level [19], or by means of a downstream task such as classiﬁcation [32]. In the remainder of this section, we review the deﬁnitions of the commonly-used metrics of normalized mean square error, peak signal-to-noise ratio, and structural similarity. As is discussed later, while we expect these metrics to serve as a familiar starting point, we also hope that the fastMRI dataset will enable robust investigations into improved evaluation metrics as well as improved reconstruction algorithms. (b) Random mask with 8-fold acceleration (d) Equispaced mask with 8-fold acceleration 5.1 Normalized Mean Square Error The normalized mean square error (NMSE) between a reconstructed image or image volume represented as a vector ˆv and a reference image or volume v is deﬁned as NMSE(ˆv, v) = ∥ˆv −v∥2 2 ∥v∥2 2 , (7) where ∥·∥2 2 is the squared Euclidean norm, and the subtraction is performed entry-wise. In this work we report NMSE values computed and normalized over full image volumes rather than individual slices, since image-wise normalization can result in strong variations across a volume. NMSE is widely used, and we recommend that it be reported as the primary measure of reconstruction quality for experiments on the fastMRI dataset. However, due to the many downsides of NMSE, such as a tendency to favor smoothness rather than sharpness, we recommend also reporting additional metrics such as those described below. 5.2 Peak Signal-to-Noise Ratio The peak signal-to-noise ratio (PSNR) represents the ratio between the power of the maximum possible image intensity across a volume and the power of distorting noise and other errors: PSNR(ˆv, v) = 10 log10 max(v)2 Here ˆv is the reconstructed volume, v is the target volume, max(v) is the largest entry in the target volume v, MSE(ˆv, v) is the mean square error between ˆv and v deﬁned as 1 n∥ˆv −v∥2 2 and n is the number of entries in the target volume v. Higher values of PSNR (as opposed to lower values of NMSE) indicate a better reconstruction. 5.3 Structural Similarity The structural similarity (SSIM) index measures the similarity between two images by exploiting the inter-dependencies among nearby pixels. SSIM is inherently able to evaluate structural properties of the objects in an image and is computed at different image locations by using a sliding window. The resulting similarity between two image patches ˆm and m is deﬁned as SSIM( ˆm, m) = (2µ ˆmµm + c1)(2σ ˆmm + c2) (µ2 ˆm + µ2m + c1)(σ2 ˆm + σ2m + c2), (9) where µ ˆm and µm are the average pixel intensities in ˆm and m, σ2 ˆm and σ2 m are their variances, σ ˆmm is the covariance between ˆm and m and c1 and c2 are two variables to stabilize the division; c1 = (k1L)2 and c2 = (k2L)2. For SSIM values reported in this paper, we choose a window size of 7 × 7, we set k1 = 0.01, k2 = 0.03, and deﬁne L as the maximum value of the target volume, L = max(v). 5.4 L1 Error It is sometimes advantageous to use the L1 loss L1(ˆv, v) = ∥ˆv −v∥1, (10) MSE(ˆv, v). (8) (a) Cropped and vertically ﬂipped reconstruction from fully sampled kspace data (b) Rectangular masked kspace (e) Multiscale Daubechies discrete wavelet transform (f) L1 Wavelet penalty reconstruction Figure 6: Single-coil reconstruction 6 Baseline Models Along with releasing the fastMRI data, we detail two reference approaches to be used as reconstruction baselines: a classical non-machine learning approach, and a deep-learning approach. Each of these baselines has versions tailored for single-coil or multi-coil data. The “classical” baselines are comprised of reconstruction methods developed by the MRI community over the last 30+ years. These methods have been extensively tested and validated, and many have demonstrated robustness suﬃcient for inclusion in the clinical workﬂow. By comparison, machine learning reconstruction methods are relatively new in MRI, and deep-learning reconstruction techniques in particular have emerged only in the past few years. We include some deliberately rudimentary deep-learning models as starting points, with the expectation that future learning algorithms will provide markedly improved performance. 6.1 Single-coil Classical Baselines (knee only) In the single-coil imaging setting, the task is to reconstruct an image, m, from k-space observations, y. In the presence of undersampling, the vector y has a length smaller than that of m. Therefore there are, in principle, inﬁnitely many possibilities for m that can be mapped onto a single y. The advent of compressed sensing [2, 23] provided a framework for reconstruction of images from undersampled data that closely approximate images derived from fully-sampled data, subject to (c) Reconstruction via zero-ﬁlled IFFT (d) Deep-learning baseline UNET reconstruction (g) Image gradients as given by a Sobel ﬁlter (h) Regularized totalvariation reconstruction sparsity constraints. Compressed sensing theory requires the images in question to be sparse in some transform domain. Two common examples are to assume sparsity in the wavelet domain, or to assume sparsity of the spatial gradients of the image. The particular assumption impacts the mathematical formulation of the reconstruction problem, either in the cost function or through a regularization term. More concretely, the sparse reconstruction approach consists of ﬁnding an image m whose Fourier space representation is close to the measured k-space matrix y at all measured spatial frequencies, yet at the same time minimizes a sparsity-inducing objective R(m) that penalizes unnatural reconstructions: minimize m R (m) s.t. ∥P (F (m)) −y∥2 2 ≤ϵ. (11) Here, P is a projection function that zeros out entries that are masked, and ϵ is a speciﬁed small threshold value. In most applications it is easier to work with a soft penalty instead of a constraint, so the Lagrangian dual form of Equation 11 is used instead, with penalty parameter λ: minimize m 1 2 ∥P (F (m)) −y∥2 2 + λR (m) . (12) For a convex regularizer R, there exists, for any choice ϵ > 0, a value λ such that these two formulations have equivalent solutions. The most common regularizers used for MRI are: RL1 (m) = ∥m∥1 , Rwavelet (m) = ∥Ψ (m)∥1 (Ψ is a discrete wavelet transform) , q |mi+1,j −mi,j|2 + |mi,j+1 −mi,j|2. RTV (m) = X i,j The L1 penalty works best when the MR images are sparse in image space, for instance in vascular imaging (e.g., Yamamoto et al. [54]). This is not the case for most MRI applications. The totalvariation (TV) penalty encourages sparsity in the spatial gradients of the reconstructed image, as given by a local ﬁnite-difference approximation [30] (Figure 6g). The TV regularizer can be very effective for some imaging protocols, but it also has a tendency to remove detail (Figure 6h). The Rwavelet penalty encourages sparsity in the discrete wavelet transform of the image. Most natural images exhibit signiﬁcant sparsity when expressed in a wavelet basis. The most commonly used transform is the Multiscale Daubechies (DB2) transform (Figure 6e). To date, due to their computational complexity as well as their tendency to introduce compression artifacts or oversmoothing, compressed sensing approaches have taken some time to gain acceptance in the clinic, though commercial implementations of compressed sensing are currently beginning to appear. The single-coil classical baseline provided with the fastMRI dataset was adopted from the widely-used open-source BART toolkit (Appendix B), using total variation as the regularizer. We ran the optimization algorithm for 200 iterations on each slice independently. Table 5 summarizes the results of applying this method to the single-coil validation data with different regularization strengths and different acceleration factors. These results indicate that NMSE and PSNR metrics are highly (inversely) correlated and generally favor models with stronger regularization than SSIM does. Stronger regularization generally results in smoother images that lack the ﬁne texture of the ground truth images. A regularization parameter of 0.01 yields the best results for 4-fold acceleration in most cases, whereas the higher 8-fold acceleration gets slightly better results with a regularization parameter of 0.1. Single-coil classical baseline (TV model) applied to knee validation data Acceleration Regularization Weight NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 10−4 0.0355 0.0919 30.2 27.6 0.637 0.506 10−3 0.0342 0.0916 30.4 27.6 0.641 0.505 10−2 0.0287 0.09 31.4 27.7 0.645 0.494 10−1 0.0313 0.0993 30.9 27.3 0.575 0.399 1 0.0522 0.124 28.5 26.2 0.526 0.327 8-fold 10−4 0.0708 0.118 27.1 26.4 0.551 0.417 10−3 0.0699 0.118 27.1 26.4 0.553 0.416 10−2 0.063 0.117 27.7 26.4 0.564 0.408 10−1 0.0537 0.117 28.4 26.5 0.55 0.357 1 0.0742 0.132 26.9 25.9 0.538 0.333 Table 5: Validation set results for the classical baseline model with Total Variation regularization for the single-coil task. Bold-faced numbers indicate the best performance for each image quality metric. 6.2 Multi-coil Classical Baselines When multiple receiver coils are used, the reconstruction process must combine information from multiple channels into one image. Multi-coil acquisitions currently represent the norm in clinical practice, for two principal reasons: they provide increased SNR, as compared with single-coil acquisitions, over extended ﬁelds of view, and they enable acceleration via parallel imaging. Equation 2 in Section 2.1 describes the forward model for parallel imaging. The SENSE formulation [26] of parallel image reconstruction involves direct inversion of this forward model, via a suitable pseudoinverse. Leveraging the convolution property of the Fourier Transform reveals the following convolution relationship: yi = gi ⊛F (m) + noise. (13) Here gi is the Fourier Transform of the coil sensitivity pattern Si and ⊛denotes the convolution operation. The GRAPPA/SMASH formulation of parallel image reconstruction [39, 9] involves ﬁlling in missing k-space data via combinations of acquired k-space data within a deﬁned convolution kernel, prior to inverse Fourier transformation. Either formulation requires estimates of the coil sensitivity information in Si or gi, which may be derived either from a separate reference scan or directly from the acquired undersampled k-space data itself. Reference scan methods are often used in the SENSE formulation, whereas GRAPPA formulations are typically self-calibrating, relying on subsets of fully-sampled data generally in central k-space regions. The parallel imaging techniques described above may be combined productively with compressed sensing, via the use of sparsity-based regularizers. For example, one may extend Equation 12 in Section 6.1 above to include multi-coil data as follows: nc X minimize m 1 2 the ESPIRiT approach [45], which harmonizes parallel imaging and compressed sensing in a uniﬁed framework. As was the case for the classical single-coil baseline, the classical multi-coil baseline provided with the fastMRI dataset was adopted from the BART toolkit (Appendix B). In the multi-coil case, the ESPIRiT algorithm was used to estimate coil sensitivities, and to perform parallel image reconstruction in combination with compressed sensing using a total-variation regularizer. Multi-coil classical baseline (TV model) applied to knee validation data Acceleration Regularization NMSE PSNR SSIM 4-fold 10−4 0.0246 0.0972 31.6 27.4 0.677 0.53 10−3 0.0222 0.0951 32.1 27.5 0.693 0.554 10−2 0.0198 0.0971 32.6 27.5 0.675 0.588 10−1 0.0251 0.109 31.3 27 0.633 0.538 8-fold 10−4 0.0494 0.114 28.2 26.5 0.61 0.505 10−3 0.0447 0.112 28.6 26.6 0.626 0.524 10−2 0.0352 0.109 29.6 26.8 0.642 0.551 10−1 0.0389 0.114 29.2 26.7 0.632 0.527 Table 6: Validation set results for the classical baseline model with Total Variation regularization for the knee multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Results using this baseline model are summarized in Table 6 and 7 . The experimental setup is identical to the single-coil scenario, except that we compare the reconstructions with the rootsum-of-squares ground truth instead of the ESC ground truth. 6.3 Single-coil Deep-Learning Baselines (knee only) Various deep-learning techniques based on Convolutional Neural Networks have recently been proposed to tackle the problem of reconstructing MR images from undersampled k-space data [10, 48, 11, 35, 60, 17, 12]. Many of these proposed methods are based on the U-Net architecture introduced in [29]. U-Net models and their variants have successfully been used for many image-to-image prediction tasks including MRI reconstruction [17, 12] and image segmentation [29]. The U-Net single-coil baseline model included with the fastMRI data release (Figure 7) consists of two deep convolutional networks, a down-sampling path followed by an up-sampling path. The down-sampling path consists of blocks of two 3×3 convolutions each followed by instance normalization [46] and Rectiﬁed Linear Unit (ReLU) activation functions. The blocks are interleaved by down-sampling operations consisting of max-pooling layers with stride 2 which halve each spatial dimension. The up-sampling path consists of blocks with a similar structure to the down-sampling path, interleaved with bilinear up-sampling layers which double the resolution between blocks. Each block consists of two 3×3 convolutions with instance normalization [46] and ReLU activation layers. In contrast to the down-sampling path, the up-sampling path concatenates two inputs to the ﬁrst convolution in each block: the up-sampled activations from the previous block, together with the activations that follow the skip connection from the block in the down-sampling path with the same resolution (horizontal arrows in Figure 7). At the end of the up-sampling path, we PD PDFS PD PDFS PD PDFS 1 32 32 * * 320 32 64 64 * * 160 64 128 128 * * 80 128 256 256 * * 512 128 128 * * 40 40 256 20 * Figure 7: Single-coil baseline U-Net architecture include a series of 1×1 convolutions that reduce the number of channels to one without changing the spatial resolution. For the single-coil MRI reconstruction case, the zero-ﬁlled image is used as the input to the model. The zero-ﬁlled image is obtained by ﬁrst inserting zeros at the location of all unobserved k-space values, applying a two-dimensional Inverse Fourier Transform (IFT) to the result, and ﬁnally computing the absolute value. The result is center cropped to remove any readout and phase oversampling. Using the notation from section 6.1, the zero-ﬁlled image is given by ˜m = C( F−1(P(y)) ), where C is the linear operator corresponding to the center cropping and F−1 is the two-dimensional IFT. The entire network is trained on the training data in an end-to-end manner to minimize the mean absolute error with respect to corresponding ground truth images. Let Bθ(m) be the function computed by the U-Net model, where θ represents the parameters of the model. Then the training process corresponds to the following optimization problem: ndata X minimize θ 1 2 Bθ( ˜m(i)) −m(i) 1 , (15) i=0 where the ground truths m(i) are obtained using the ESC method described in Section 4.4. Our particular single-coil U-Net baseline model was trained on 973 image volumes in the training set, using the RMSProp algorithm [42]. We used an initial learning rate of 0.001, which was multiplied by 0.1 after 40 epochs, after which the model was trained for an additional 10 epochs. During training, we randomly sampled a different mask for each training example in each epoch independently using the protocol described in Section 4.9 for the test data. At the end of each epoch, we recorded the NMSE on the validation data. After training, we picked the model that achieved the lowest validation NMSE. Table 8 presents the results from running trained U-Net models of different capacities on the single-coil validation data. These results indicate that the trained U-Net models perform signiﬁcantly better than the classical baseline method. The best U-Net models obtain 40-50% relative improvement over the classical methods (see Table 5) in terms of NMSE. The performance of the U-Net models continues to increase with increasing model capacity, and 64 32 32 16 1 1x1 1x1 * * * * 320 128 32 32 * * 160 256 64 64 * * 80 * 3x3 Convolution + ReLU + InstanceNorm 2x2 Max pooling 2x Bilinear  upsampling 1x1 256 * 1x1 Convolution even the largest model with over 200 million parameters is unable to overﬁt the training data. These improvements begin to saturate after 50 million parameters for the simpler 4-fold acceleration case. However, for the more challenging 8-fold acceleration task, the largest model performs signiﬁcantly better than the smaller models. This suggests that models with very large capacities trained on large amounts of data can enable high acceleration factors. Table 9 compares the performance of the classical and the U-Net baseline models for the singlecoil task, as applied to the test dataset. For the classical baseline model, we chose the best regularization weights for each modality and for each acceleration factor based on the validation data results, resulting in a regularization weight of 0.1 for 8-fold acceleration on Proton Density without fat suppression and 0.01 for every other case. For the U-Net baseline model, we chose the model with the largest capacity. 6.4 Multi-coil Deep-Learning Baselines In the multi-coil MRI reconstruction task, we have one set of undersampled k-space measurements from each coil, and a different zero-ﬁlled image can be computed from each coil. These coil images can be combined using the root-sum-of-squares algorithm. Let ˜mi be the zero-ﬁlled image from coil i. With ˜mrss deﬁned as in Equation 6, the U-Net model described in Section 6.3 can be used for the multi-coil reconstruction task by simply feeding this combined image in as input: Bθ( ˜mrss). The model is trained to minimize the mean absolute error loss similarly to the single-coil task. The training procedure is also identical to the single-coil case except that the root-sum-of-squares image is used as the ground truth as described in Section 4.7. As is the case for the single-coil task, the multi-coil U-Net baselines substantially outperform the classical baseline models (compare Table 10 and 11 with Table 6 and 7). Note that this is true despite the fact that the multi-coil U-Net baseline deﬁned above does not take coil sensitivity information into account, and therefore neither includes a direct parallel image reconstruction nor accounts for sparsity or other correlations among coils. Models that incorporate coil sensitivity information are expected to perform better than the current multi-coil U-Net baselines. Table 10 and Table 11 shows, once again, that the performance of the U-Net models improves with model size, with the largest U-Net baseline model providing the best performance. Table 12 compares the performance of the classical and the U-Net baseline models for the multi-coil task, as applied to the test dataset. For the classical baseline model, we chose the best regularization weights for each modality and for each acceleration factor based on the validation data results. For knees this resulted in a regularization weight of 0.001 for 4-fold undersampling for Proton Density with Fat Suppression and 0.01 for every other acquisition type. For brain this resulted in a regularization weight of 0.001 for 8-fold AXFLAIR and 4-fold AXT1, and 0.01 for every other acquisition type. For the U-Net baseline model, we chose the model with the largest capacity. To appreciate the value of the dataset size, we study how model performance scales with the amount of data used to train a model. To this end, we trained several U-Net models with varying model capacities on different sized subsets of the training data. Figure 8 shows the SSIM metric computed on the validation data for the multi-coil task. It is evident from these results that training with larger amounts of data yields substantial improvements in the quality of reconstructions, which highlights the need for the release of large datasets like fastMRI. As mentioned in Section 4.5, the fastMRI dataset also includes a large set of DICOM images that can be used as additional training data. It is possible that the baseline U-Net models could be improved further by making use of this additional data. 32 64 128 256 0.895 0.890 SSIM 0.885 0.880 0.875 100 200 500 973 Number of training cases Figure 8: Results from training the U-Net on different amounts of training data for the multi-coil knee challenge with 4-fold acceleration (left) and 8-fold acceleration (right). Each line represents a model with a different number of channels. 7\n\nDiscussion\nMR image reconstruction is an inverse problem, and thus it has many connections to inverse problems in the computer vision literature [40, 7, 4, 47], such as super-resolution, denoising and in-painting. In all of these inverse problems, the goal is to recover a high-dimensional ground truth image from a lower-dimensional measurement. Such ill-posed problems are very diﬃcult to solve since there exists an inﬁnite number of high-dimensional images that can result in the same-low dimensional measurement. In order to simplify the problem, an assumption is often made that only a small number of high-resolution images would correspond to natural images [4]. Given that MRI reconstruction is a similar inverse problem, we hope that the computer vision community, as well as the medical imaging community, will ﬁnd our dataset beneﬁcial. In the clinical setting, radiologists use MRI to search for abnormalities, make diagnoses, and recommend treatment options. Thus, contrary to many computer vision problems where small texture changes might not necessarily alter the overall satisfaction of the observer, in MRI reconstruction, extra care should be taken to ensure that the human interpreter is not misled by a very plausible but not necessarily correct reconstruction. This is especially important as image generation techniques increase in their ability to generate photo-realistic results [49]. Therefore some research effort should be devoted to look for solutions that, by design, ensure correct diagnosis, and we hope that our dataset will provide a testbed for new ideas in these directions as well. An important question in MRI reconstruction is the choice of the evaluation metric. The current consensus in the MRI community is that global metrics, such as NMSE, SSIM and PSNR, do not necessarily capture the level of detail required for proper evaluation of MRI reconstruction algorithms [25, 16]. A natural question arises: what would the optimal metric be? An ideal MRI reconstruction algorithm should produce sharp, trustworthy images, that ultimately ensure the proper radiologic interpretation. While our dataset will help ensure consistent evaluation, we hope that it will also trigger research on MRI reconstruction metrics. This goal will be impossible to achieve without clinical studies involving radiologists evaluating fully-sampled and undersampled MRI reconstructions to make sure that both images lead to the same diagnosis. Although this dataset provides an excellent entry point for machine learning methods for MR reconstruction, there are some aspects of MR imaging that we have not yet considered here. Phys- 32 64 128 256 0.860 0.855 0.850 0.845 SSIM 0.840 0.835 0.830 0.825 0.820 100 200 500 973 Number of training cases Ground Truth Total Variation U-Net Figure 9: Example knee reconstructions Multi-Coil 4-Fold Multi-Coil 8-Fold Single-Coil 4-Fold Single-Coil 8-Fold Figure 10: Example brain reconstructions ical effects such as spin relaxation, eddy currents and ﬁeld distortions are not at present explicitly accounted for in our retrospective undersampling approaches or our baseline models. The manifestation of these effects depends upon the object being imaged, the MRI scanner used, and even the sampling pattern selected. Extending the results from methods developed for this challenge to the clinic remains an open problem, but we believe the provision of this dataset is an important ﬁrst step on the path to this goal. 8\n\nConclusion\nIn this work we detailed the fastMRI dataset: the largest raw MRI dataset to be made publicly available to date. Previous public datasets have focused on post-processed magnitude images for speciﬁc biologic and pathologic questions. Although our dataset was originally acquired for a focused task, the inclusion of raw k-space data allows methods to be developed for the imaging pipeline itself, in principle allowing them to be applied on any MRI scanner for any imaging task. In addition to the data, we provide evaluation metrics and baseline algorithms to aid the research community in assessing new approaches. Consistent evaluation of MRI reconstruction techniques is provided by a leaderboard using held-out test data. We hope that the availability of this dataset will accelerate research in MR image reconstruction, and will serve as a benchmark during training and validation of new algorithms.\n\n9 Acknowledgements EB024532 and NIH P41 EB017183. We would also like to thank Michela Paganini and Mark Tygert.\n\nReferences\n[1] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson, Maschenka Balkenhol, et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. Journal of the American Medical Association, 318(22), 2017. [2] Emmanuel J Cand`es, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2), 2006. [3] Damon M Chandler. Seven challenges in image quality assessment: past, present, and future research. ISRN Signal Processing, 2013. [4] Jen-Hao Rick Chang, Chun-Liang Li, Barnab´as P´oczos, B. V. K. Vijaya Kumar, and Aswin C. Sankaranarayanan. One network to solve them all - solving linear inverse problems using deep projection models. IEEE International Conference on Computer Vision (ICCV), 2017. [5] Salman Ul Hassan Dar and Tolga C¸ukur. A transfer-learning approach for accelerated MRI using deep neural networks. arXiv preprint, 2017. [6] Michael P Eckert and Andrew P Bradley. Perceptual quality metrics applied to still image compression. Signal Processing, 70(3), 1998. [7] Kai Fan, Qi Wei, Wenlin Wang, Amit Chakraborty, and Katherine A. Heller. InverseNet: Solving inverse problems with splitting networks. arXiv preprint, 2017. [8] Bram Van Ginneken, Tobias Heimann, and Martin Styner. 3d segmentation in the clinic: A grand challenge. In MICCAI Workshop on 3D Segmentation in the Clinic: A Grand Challenge, 2007. [9] Mark A Griswold, Peter M Jakob, Robin M Heidemann, Mathias Nittka, Vladimir Jellus, Jianmin Wang, Berthold Kiefer, and Axel Haase. Generalized autocalibrating partially parallel acquisitions (GRAPPA). Magnetic Resonance in Medicine, 47(6), 2002. [10] Kerstin Hammernik, Florian Knoll, Daniel K Sodickson, and Thomas Pock. Learning a Variational Model for Compressed Sensing MRI Reconstruction. In Magnetic Resonance in Medicine (ISMRM), 2016. [11] Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P. Recht, Daniel K. Sodickson, Thomas Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI data. Magnetic Resonance in Medicine, 2018. [12] Yoseob Han and Jong Chul Ye. Framing U-Net via deep convolutional framelets: Application to sparseview CT. IEEE Transactions on Medical Imaging, 37(6), 2018. [13] Michael Heath, Kevin Bowyer, Daniel Kopans, P Kegelmeyer, Richard Moore, Kyong Chang, and S Munishkumaran. Current status of the digital database for screening mammography. In Digital Mammography, 1998. [14] Tobias Heimann, Bram Van Ginneken, Martin A Styner, Yulia Arzhaeva, Volker Aurich, Christian Bauer, Andreas Beck, Christoph Becker, Reinhard Beichel, Gy¨orgy Bekes, et al. Comparison and evaluation of methods for liver segmentation from CT datasets. IEEE Transactions on Medical Imaging, 28(8), 2009. [15] Yue Huang, John Paisley, Xianbo Chen, Xinghao Ding, Feng Huang, and Xiao-Ping Zhang. MR image reconstruction from undersampled k-space with bayesian dictionary learning. arXiv preprint, 2013. [17] Chang Min Hyun, Hwa Pyung Kim, Sung Min Lee, Sungchul Lee, and Jin Keun Seo. Deep learning for undersampled MRI reconstruction. Physics in medicine and biology, 63(13), 2018. [18] Souheil J Inati, Joseph D Naegele, Nicholas R Zwart, Vinai Roopchansingh, Martin J Lizak, David C Hansen, Chia-Ying Liu, David Atkinson, Peter Kellman, Sebastian Kozerke, et al. ISMRM raw data format: a proposed standard for MRI raw datasets. Magnetic resonance in medicine, 77(1), 2017. [19] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, 2016. [20] Erik G Larsson, Deniz Erdogmus, Rui Yan, Jose C Principe, and Jeffrey R Fitzsimmons. SNR-optimality of sum-of-squares reconstruction for phased-array magnetic resonance imaging. Journal of Magnetic Resonance, 163(1), 2003. [21] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep Learning. Nature, 521(7553), 2015. [22] Kai Lonning, Patrick Putzky, Matthan W. A. Caan, and Max Welling. Recurrent inference machines for accelerated MRI reconstruction, 2018. [23] Michael Lustig, David Donoho, and John M Pauly. Sparse MRI: The Application of Compressed Sensing for Rapid MR Imaging. Magnetic Resonance in Medicine, 58(6), 2007. [24] Bjoern H. Menze, Andrs Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, Levente Lanczi, Elizabeth R. Gerstner, Marc-Andr Weber, Tal Arbel, Brian B. Avants, Nicholas Ayache, Patricia Buendia, D. Louis Collins, Nicolas Cordier, Jason J. Corso, Antonio Criminisi, Tilak Das, Herve Delingette, agatay Demiralp, Christopher R. Durst, Michel Dojat, Senan Doyle, Joana Festa, Florence Forbes, Ezequiel Geremia, Ben Glocker, Polina Golland, Xiaotao Guo, Andac Hamamci, Khan M. Iftekharuddin, Raj Jena, Nigel M. John, Ender Konukoglu, Danial Lashkari, Jos Antonio Mariz, Raphael Meier, Srgio Pereira, Doina Precup, Stephen J. Price, Tammy Riklin Raviv, Syed M. S. Reza, Michael T. Ryan, Duygu Sarikaya, Lawrence H. Schwartz, Hoo-Chang Shin, Jamie Shotton, Carlos A. Silva, Nuno Sousa, Nagesh K. Subbanna, Gbor Szkely, Thomas J. Taylor, Owen M. Thomas, Nicholas J. Tustison, Gzde B. nal, Flor Vasseur, Max Wintermark, Dong Hye Ye, Liang Zhao, Binsheng Zhao, Darko Zikic, Marcel Prastawa, Mauricio Reyes, and Koen Van Leemput. The multimodal brain tumor image segmentation benchmark (BRATS). IEEE Transactions on Medical Imaging, 34(10), 2015. [25] Jun Miao, Feng Huang, Sreenath Narayan, and David L. Wilson. A new perceptual difference model for diagnostically relevant quantitative image quality evaluation: A preliminary study. Magnetic Resonance Imaging, 31(4), 2013. [26] Klaas P Pruessmann, Markus Weiger, Markus B Scheidegger, and Peter Boesiger. SENSE: sensitivity encoding for fast MRI. Magnetic resonance in medicine, 42(5), 1999. [27] Tran Minh Quan and Won-Ki Jeong. Compressed sensing dynamic MRI reconstruction using GPUaccelerated 3d convolutional sparse coding. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2016. [28] Peter B Roemer, William A Edelstein, Cecil E Hayes, Steven P Souza, and Otward M Mueller. The NMR phased array. Magnetic resonance in medicine, 16(2), 1990. [29] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. Medical Image Computing and Computer-Assisted Intervention, 2015. [30] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: nonlinear phenomena, 60(1-4), 1992. [32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, 2016. [33] Christopher M. Sandino, Neerav Dixit, Joseph Y. Cheng, and Shreyas S Vasanawala. Deep convolutional neural networks for accelerated dynamic magnetic resonance imaging. Technical report, Stanford University, 2017. [34] Anne Marie Sawyer, Michael Lustig, Marcus Alley, Phdmartin Uecker, Patrick Virtue, Peng Lai, Shreyas Vasanawala, and Ge Healthcare. Creation of fully sampled MR data repository for compressed sensing of the knee, 2013. [35] Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony N. Price, and Daniel Rueckert. A deep cascade of convolutional neural networks for MR image reconstruction. Information Processing in Medical Imaging, 2017. [36] Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony N. Price, and Daniel Rueckert. A deep cascade of convolutional neural networks for dynamic MR image reconstruction. IEEE Transactions on medical imaging, 37(2), 2018. [37] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas de Bel, Moira S. N. Berens, Cas van den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, Robbert van der Gugten, Pheng-Ann Heng, Bart Jansen, Michael M. J. de Kaste, Valentin Kotov, Jack Yu-Hung Lin, Jeroen T. M. C. Manders, Alexander S´onora-Mengana, Juan Carlos Garc´ıa-Naranjo, Mathias Prokop, Marco Saletta, Cornelia Schaefer-Prokop, Ernst Th. Scholten, Luuk Scholten, Miranda M. Snoeren, Ernesto Lopez Torres, Jef Vandemeulebroucke, Nicole Walasek, Guido C. A. Zuidhof, Bram van Ginneken, and Colin Jacobs. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge. Medical Image Analysis, 2017. [38] Ohad Shitrit and Tammy Riklin Raviv. Accelerated magnetic resonance imaging by adversarial neural network. In M. Jorge Cardoso, Tal Arbel, Gustavo Carneiro, Tanveer F. Syeda-Mahmood, Jo˜ao Manuel R. S. Tavares, Mehdi Moradi, Andrew P. Bradley, Hayit Greenspan, Jo˜ao Paulo Papa, Anant Madabhushi, Jacinto C. Nascimento, Jaime S. Cardoso, Vasileios Belagiannis, and Zhi Lu, editors, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, 2017. [39] Daniel K Sodickson and Warren J Manning. Simultaneous acquisition of spatial harmonics (SMASH): fast imaging with radiofrequency coil arrays. Magnetic resonance in medicine, 38(4), 1997. [40] Richard Szeliski. Computer vision algorithms and applications. Springer, 2011. [41] Patrick C Teo and David J Heeger. Perceptual image distortion. In IEEE International Conference on Image Processing (ICIP), volume 2, 1994. [42] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012. [43] Mark Tygert and Jure Zbontar. Simulating single-coil MRI from the responses of multiple coils. arXiv preprint, 2018. [44] Martin Uecker, Patrick Virtue, Frank Ong, Mark J. Murphy, Marcus T. Alley, Shreyas S. Vasanawala, and Michael Lustig. Software toolbox and programming library for compressed sensing and parallel imaging. In ISMRM Workshop on Data Sampling and Image Reconstruction, 2013. [45] Martin Uecker, Peng Lai, Mark J Murphy, Patrick Virtue, Michael Elad, John M Pauly, Shreyas S Vasanawala, and Michael Lustig. ESPIRiT -an eigenvalue approach to autocalibrating parallel MRI: where SENSE meets GRAPPA. Magnetic resonance in medicine, 71(3), 2014. [46] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint, 2016. [47] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Deep image prior. arXiv preprint, 2017. [48] Shanshan Wang, Zhenghang Su, Leslie Ying, Xi Peng, Shun Zhu, Feng Liang, Dagan Feng, and Dong Liang. Accelerating magnetic resonance imaging via deep learning. In IEEE International Symposium on Biomedical Imaging (ISBI), 2016. [49] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional GANs. Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [50] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classiﬁcation and localization of common thorax diseases. In 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2017. [51] Zhou Wang and Alan C Bovik. Mean squared error: Love it or leave it? a new look at signal ﬁdelity measures. IEEE signal processing magazine, 26(1), 2009. [52] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assessment. In Asilomar Conference on Signals, Systems & Computers, 2003. [53] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004. [54] Takayuki Yamamoto, T Okada, Yasutaka Fushimi, Akira Yamamoto, Koji Fujimoto, Sachi Okuchi, Hikaru Fukutomi, Jun C. Takahashi, Takeshi Funaki, Susumu Miyamoto, Aur´elien F. Stalder, Yutaka Natsuaki, Peter Speier, and Kaori Togashi. Magnetic resonance angiography with compressed sensing: An evaluation of moyamoya disease. In PloS one, 2018. [55] Ke Yan, Xiaosong Wang, Le Lu, and Ronald Summers. Deeplesion: Automated mining of large-scale lesion annotations and universal lesion detection with deep learning. Journal of Medical Imaging, 5, 2018. [56] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep ADMM-Net for compressive sensing MRI. Advances in Neural Information Processing Systems 29, 2016. [57] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. ADMM-Net: A deep learning approach for compressive sensing MRI. arXiv preprint, 2017. [58] Lin Zhang, Lei Zhang, Xuanqin Mou, David Zhang, et al. FSIM: a feature similarity index for image quality assessment. IEEE transactions on Image Processing, 20(8), 2011. [59] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with neural networks. IEEE Transactions on Computational Imaging, 3(1), 2017. [60] Bo Zhu, Jeremiah Z. Liu, Stephen F. Cauley, Bruce R. Rosen, and Matthew S. Rosen. Image reconstruction by domain-transform manifold learning. Nature, 555(7697), 2018.\n\n10 Changelog • V1 (Nov 2018): Arxiv preprint describing the fastMRI knee dataset. • V2 (Dec 2019): Added neuro dataset. Multi-coil classical baseline (TV model) applied to brain validation data Acceleration Regularization Weight Sequence NMSE PSNR SSIM 10−4 AXT1 0.03971 31.63 0.5677 AXT1POST 0.02581 32.39 0.5814 AXT2 0.03624 30.66 0.528 AXFLAIR 0.189 26.85 0.4512 10−3 AXT1 0.03818 31.82 0.5724 AXT1POST 0.02353 32.81 0.5919 AXT2 0.03457 30.86 0.5312 4-fold AXFLAIR 0.1869 26.96 0.4651 10−2 AXT1 0.03888 31.7 0.5376 AXT1POST 0.02199 33.17 0.5522 AXT2 0.03419 30.9 0.4923 AXFLAIR 0.1886 26.75 0.4435 10−1 AXT1 0.04916 30.54 0.5193 AXT1POST 0.02956 31.84 0.5284 AXT2 0.04708 29.39 0.4651 AXFLAIR 0.1934 26.14 0.4048 10−4 AXT1 0.06911 29.01 0.4823 AXT1POST 0.05457 29.09 0.498 AXT2 0.07904 27.05 0.4426 AXFLAIR 0.4421 23.93 0.3549 10−3 AXT1 0.06721 29.13 0.488 AXT1POST 0.05287 29.24 0.5039 AXT2 0.078 27.11 0.4405 8-fold AXFLAIR 0.1869 26.96 0.4627 10−2 AXT1 0.05935 29.68 0.5145 AXT1POST 0.04514 29.92 0.5325 AXT2 0.07486 27.29 0.4336 AXFLAIR 0.3893 24.15 0.3678 10−1 AXT1 0.06322 29.35 0.5928 AXT1POST 0.04904 29.54 0.6187 AXT2 0.0874 26.6 0.495 AXFLAIR 0.2773 24.66 0.4726 Table 7: Validation set results for the classical baseline model with Total Variation regularization for the brain multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Single-coil U-Net baseline applied to knee validation data Acceleration Channels #Params NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 32 3.35M 0.0161 0.0531 33.78 29.90 0.81 0.631 64 13.39M 0.0157 0.0528 33.90 29.9 0.813 0.633 128 53.54M 0.0154 0.0525 34.01 29.95 0.815 0.634 256 214.16M 0.0154 0.0525 34.00 29.95 0.815 0.636 8-fold 32 3.35M 0.0283 0.0698 31.13 28.6 0.754 0.555 64 13.39M 0.0272 0.0693 31.30 28.63 0.758 0.558 128 53.54M 0.0265 0.0686 31.44 28.68 0.761 0.558 256 214.16M 0.0261 0.0682 31.5 28.71 0.762 0.559 Table 8: Validation set results for the U-Net baseline model trained for the single-coil task. The channels column denotes the number of output channels of the ﬁrst convolution in the model. Doubling this number of channels roughly quadruples the total number of parameters in the model. Bold-faced numbers indicate the best performance for each image quality metric. Single-coil classical and U-Net baselines applied to test data Model Acceleration NMSE PSNR SSIM Classical Model (Total Variation) 4-fold 0.0479 30.69 0.603 8-fold 0.0795 27.12 0.469 Aggregate 0.0648 28.77 0.531 U-Net 4-fold 0.0320 32.22 0.754 8-fold 0.0480 29.45 0.651 Aggregate 0.0406 30.7 0.699 Table 9: Comparison of classical and U-Net baseline performance for the single-coil task with test data Multi-coil U-Net baseline applied to knee validation data Acceleration Channels #Params NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 32 3.35M 0.0066 0.0122 36.7 35.97 0.9192 0.8595 64 13.39M 0.0063 0.0120 36.95 36.11 0.9224 0.8615 128 53.54M 0.0057 0.0113 37.38 36.33 0.9266 0.8641 256 214.16M 0.0054 0.0112 37.58 36.39 0.9287 0.8655 8-fold 32 3.35M 0.0144 0.0197 33.31 33.82 0.8778 0.8213 64 13.39M 0.0136 0.0198 33.56 33.93 0.8825 0.8238 128 53.54M 0.0123 0.0179 34.01 34.25 0.8892 0.8277 256 214.16M 0.0120 0.0181 34.12 34.23 0.8915 0.8286 Multi-coil U-Net baseline applied to brain validation data Acceleration Channels #Params Sequence NMSE PSNR SSIM 32 3.35M AXT1 0.01498 35.67 0.9215 AXT1POST 0.013 35.43 0.9298 AXT2 0.02249 32.51 0.9112 AXFLAIR 0.1572 30.73 0.7869 64 13.39M AXT1 0.01571 35.57 0.922 AXT1POST 0.01313 35.41 0.9307 AXT2 0.02014 32.98 0.9151 4-fold AXFLAIR 0.1579 30.96 0.7917 128 53.54M AXT1 0.0142 35.92 0.9243 AXT1POST 0.01231 35.69 0.9332 AXT2 0.01855 33.34 0.9175 AXFLAIR 0.1566 30.98 0.7932 256 214.16M AXT1 0.01317 36.24 0.9275 AXT1POST 0.0111 36.11 0.9361 AXT2 0.01733 33.63 0.9207 AXFLAIR 0.1532 31.52 0.7985 32 3.35M AXT1 0.04289 31.5 0.8885 AXT1POST 0.04186 31.71 0.8816 AXT2 0.04357 30.86 0.8759 AXFLAIR 0.1594 32.86 0.8188 64 13.39M AXT1 0.04205 32.56 0.8876 AXT1POST 0.04034 31.89 0.883 AXT2 0.04248 31.1 0.8753 8-fold AXFLAIR 0.1818 30.49 0.7843 128 53.54M AXT1 0.04706 31.82 0.8804 AXT1POST 0.04005 31.47 0.8828 AXT2 0.04311 30.13 0.8806 AXFLAIR 0.2 28.97 0.7779 256 214.16M AXT1 0.0443 32.02 0.8837 AXT1POST 0.04028 31.95 0.8845 AXT2 0.04167 31.29 0.8811 AXFLAIR 0.1565 30.49 0.7805 Table 11: Validation set results for the U-Net baseline model trained for the brain multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Multi-coil classical and U-Net baselines applied to test data Dataset Model Acceleration NMSE PSNR SSIM Knee Classical Model (Total Variation) 4-fold 0.0503 30.88 0.628 8-fold 0.0760 28.25 0.593 Aggregate 0.0633 29.54 0.610 U-Net 4-fold 0.0106 35.91 0.904 8-fold 0.0171 33.57 0.858 Aggregate 0.0139 34.7 0.881 Brain Classical Model (Total Variation) 4-fold 0.1388 27.53 0.4439 8-fold 0.03753 31.32 0.5135 Aggregate 0.0882 29.42 0.4787 U-Net 4-fold 0.0107 38.13 0.9446 8-fold 0.0233 34.52 0.9146 Aggregate 0.017 36.325 0.9296 Table 12: Comparison of classical and U-Net baseline performance for the multi-coil task with knee test data.A Raw k-space File Descriptions acquisition Acquisition protocol. For knee images this is either CORPD or CORPDF, indicating coronal proton density with or without fat saturation, respectively (see Figure 3). For Brain images this is AXFLAIR, AXT1, AXT1POST or AXT2 (see Figure 4). ismrmrd header The XML header copied verbatim from the ISMRMRD ﬁle that was used to generate the HDF5 ﬁle. It contains information about the scanner, ﬁeld of view, dimensions of k-space, and sequence parameters. patient id A unique string identifying the examination, and substituting anonymously for the patient identiﬁcation. norm, max The Euclidean norm and the largest entry of the target volume. For the multi-coil track the target volume is stored in reconstruction rss. For the single-coil track the target volume is stored in reconstruction esc. These two attributes are only available in the training and validation datasets. acceleration Acceleration factor of the undersampled k-space trajectory (either 4 or 8). This attribute is only available in the test dataset. num low frequency The number of low-frequency k-space lines in the undersampled k-space trajectory. This attribute is only available in the test dataset. The rest of this section describes the format of the HDF5 ﬁles for the multi-coil and single-coil tracks. A.1 Multi-coil Track {knee,brain} multicoil train.tar.gz Training dataset for the multi-coil track. The HDF5 ﬁles contain the following tensors: kspace Multi-coil k-space data. The shape of the kspace tensor is (number of slices, number of coils, height, width). reconstruction rss root-sum-of-squares reconstruction of the multi-coil k-space data. The shape of the reconstruction rss tensor is (number of slices, r height, r width). For knee images, height and width have been cropped to 320 x 320. {knee,brain} multicoil val.tar.gz Validation dataset for the multi-coil track. The HDF5 ﬁles have the same structure as the HDF5 ﬁles in multicoil train.tar.gz. {knee,brain} multicoil test.tar.gz Test dataset for the multi-coil track. The HDF5 ﬁles contain the following tensors: kspace Undersampled multi-coil k-space. The shape of the kspace tensor is (number of slices, number of coils, height, width). mask Deﬁnes the undersampled Cartesian k-space trajectory. The number of elements in the mask tensor is the same as the width of k-space. A.2 Single-coil Track (knee only) knee singlecoil train.tar.gz Training dataset for the single-coil track. Note that only the knee dataset has a single-coil track. The HDF5 ﬁles contain the following tensors: kspace Emulated single-coil k-space data. The shape of the kspace tensor is (number of slices, height, width). reconstruction rss root-sum-of-squares reconstruction of the multi-coil k-space that was used to derive the emulated single-coil k-space cropped to the center 320 × 320 region. The shape of the reconstruction rss tensor is (number of slices, 320, 320). reconstruction esc The inverse Fourier transform of the single-coil k-space data cropped to the center 320 × 320 region. The shape of the reconstruction esc tensor is (number of slices, 320, 320). knee singlecoil val.tar.gz Validation dataset for the single-coil track. The HDF5 ﬁles have the same structure as the HDF5 ﬁles in singlecoil train.tar.gz. knee singlecoil test.tar.gz Test dataset for the single-coil track. Note that only the knee dataset has a single-coil track. The HDF5 ﬁles contain the following tensors: kspace Undersampled emulated single-coil k-space. The shape of the kspace tensor is (number of slices, height, width). mask Deﬁnes the undersampled Cartesian k-space trajectory. The number of elements in the mask tensor is the same as the width of k-space. B Classical Reconstruction with BART The Berkeley Advanced Reconstruction Toolbox (BART) [44] 5 contains implementations of standard methods for coil sensitivity estimation and undersampled MR image reconstruction incorporating parallel imaging and compressed sensing. We used this tool to produce the classical baseline MSE estimates, as well as the illustrations in Figure 2. In this section we provide a brief introduction to the tool suﬃcient for reproducing our baseline results. We will use as an example a 640x368 undersampled MRI scan with 15 coils. The target region is a 320 × 320 central region which will be cropped to after reconstruction. BART provides a command line interface which acts on ﬁles in a simple storage format. Each multidimensional array is stored in a pair of ﬁles, a header ﬁle .hdr and a data ﬁle .cfl. The header ﬁle contains the dimensions of the array given in ASCII. In our running example, this should be input.hdr: 1 640 368 15 The CFL ﬁle contains the raw data in column-major order, stored as complex ﬂoat values. Missing k-space values are indicated by 0 entries. BART provides Python and MATLAB interfaces for reading and writing this format. When working with k-space data with BART, it is simplest to use data in ”centered” form, where the low frequency values are in the center of the image, and the high frequency values are at the edges. Most FFT libraries output the data in uncentered form. BART provides a tool for conversion: bart fftshift 7 input output The input and output are speciﬁed without ﬁle extensions. The value 7 above is a bitmask indicating the image is stored in axis 0,1,2 (1+2+4) of the input array. This bitmask is used in the commands that follow also. Uncentered k-space data is easily identiﬁed by comparing the magnitude of the corners versus the center of the array. Centered FFTs of natural data will have the largest magnitudes near the center of the array when plotted. Parallel MR imaging is often performed as a two-step process consisting of coil-sensitivity estimation, then reconstruction assuming the estimated sensitivity maps are exact. BART implements this approach through the ecalib and pics commands. The coil-sensitivity maps can be estimated using the ESPIRiT approach using the command bart ecalib -m1 Produce a single set of sensitivity maps -r26 Number of fully sampled reference lines input output_sens The central reference region is used by BART to estimate the coil sensitivities. This area is also known as the auto-calibration region. The number of lines used in our masking procedure is a percentage of the k-space width, as described in Section 4.2. Given the estimated coil sensitivities, a reconstruction using TV regularization can be performed with bart pics -d4 Debug log level, use 0 for no stdout output -i200 Optimization iterations -R T:7:0:0.05 Use TV (T) with regularizer strength 0.05, with bitmask 7 input output_sens output The output of this command is in CFL format. It can be converted to a PNG using bart toimg. When using L1 wavelet regularization, the character ”W” should be used in the R option, with the additional -m argument to ensure that ADMM is used.\n--- FOOTNOTES 1 Facebook AI Research 2 NYU School of Medicine 3 NYU Center for Data Science 4 University of Florida 2D TSE protocol employed clinically at NYU School of Medicine. The dataset includes data from 3T direct correspondence to the sequences represented in raw data. We acknowledge grant support from the National Institutes of Health under grants NIH R01", "persona": "Student", "response": "### 1. Overview\nHere at the Center for Advanced Imaging Innovation and Research (CAI2R), in the Department of Radiology at NYU School of Medicine and NYU Langone Health, we bring people together to create new ways of seeing. We are committed to the translation of new imaging techniques and technologies into clinical practice, for the improvement of human health. In particular, we are pushing the boundaries of rapid image acquisition and advanced image reconstruction, with the aim of providing uniquely valuable biomedical information to advance the understanding of disease and improve the care of patients. We are partnering with Facebook AI Research (FAIR) on fastMRI – a collaborative research project to investigate the use of AI to make MRI scans up to 10X faster. NYU Langone and FAIR are providing open-source AI models, baselines, and evaluation metrics.\n\n### 2. Background\nApply for Access The Dataset The deidentified imaging dataset provided by NYU Langone comprises raw k-space data in several sub-dataset groups. Curation of these data are part of an IRB approved study. Raw and DICOM data have been deidentified via conversion to the vendor-neutral ISMRMD format and the RSNA clinical trial processor, respectively. We also performed manual inspection of each DICOM image for the presence of any unexpected protected health information (PHI), with spot checking of both metadata and image content.\n\n### 3. Methods\nTable 2: Number of scans for the different contrasts and scanner field strengths of the brain raw dataset. Read and agree to the data use agreement below to apply for access. Note: Each dataset has its own Data Sharing Agreement. Obtaining access to and using NYU fastMRI data requires adherence to the NYU fastMRI Data Sharing Agreement and the publication policies outlined in the documents listed above. Use of this Dataset Interested scientists may apply for access to fastMRI data for the purposes of internal research or education only.* Access is contingent on adherence to the fastMRI Dataset Sharing Agreement shown below, which also outlines policies for publication and citation.\n\n### 4. Results\nNote: This agreement is subject to updates Knee MRI: Data from more than 1,500 fully sampled knee MRIs obtained on 3 and 1.5 Tesla magnets and DICOM images from 10,000 clinical knee MRIs also obtained at 3 or 1.5 Tesla. The raw dataset includes coronal proton density-weighted images with and without fat suppression. The DICOM dataset contains coronal proton density-weighted with and without fat suppression, axial proton density-weighted with fat suppression, sagittal proton density, and sagittal T2-weighted with fat suppression. The exact distribution of contrasts is given in table 1.\n\n### 5. Takeaways\nPlease note that this table does not include stats about the data that was originally held back for the fastMRI reconstruction challenge. Brain MRI: Data from 6,970 fully sampled brain MRIs obtained on 3 and 1.5 Tesla magnets. The raw dataset includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent. The exact distribution of contrasts and field strengths is given in table 2. Prostate MRI: Data from 312 prostate MRI exams obtained on 3 Tesla magnets. The raw dataset includes axial T2-weighted and axial diffusion-weighted images for each of the 312 exams. For more information about the prostate dataset, please refer to our article https://www.nature.com/articles/s41597-024-03252-w and code repository https://github.com/cai2r/fastMRI_prostate Breast MRI: Data from 300 clinical breast MRI exams obtained on 3 Tesla magnets.", "paper_id": "48", "story_id": "48-05"}
{"paper": "Abstract\nAccelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background. 1\n\nIntroduction\nThe excellent soft tissue contrast and ﬂexibility of magnetic resonance imaging (MRI) makes it a very powerful diagnostic tool for a wide range of disorders, including neurological, musculoskeletal, and oncological diseases. However, the long acquisition time in MRI, which can easily exceed 30 minutes, leads to low patient throughput, problems with patient comfort and compliance, artifacts from patient motion, and high exam costs. As a consequence, increasing imaging speed has been a major ongoing research goal since the advent of MRI in the 1970s. Increases in imaging speed have been achieved through both hardware developments (such as improved magnetic ﬁeld gradients) and software advances (such as new pulse sequences). One noteworthy development in this context is parallel imaging, introduced in the 1990s, which allows multiple data points to be sampled simultaneously, rather than in a traditional sequential order [39, 26, 9]. The introduction of compressed sensing (CS) in 2006 [2, 23] promised another breakthrough in the reduction of MR scan time. At their core, CS techniques speed up the MR acquisition by acquiring less measurement data than has previously been required to reconstruct diagnostic quality images. Since undersampling of this kind violates the Nyquist-Shannon sampling theorem, aliasing artifacts are introduced which must be eliminated in the course of image reconstruction. This can be achieved by incorporating additional a priori knowledge during the image reconstruction process. The last two years have seen the rapid development of machine learning approaches for MR image reconstruction, which hold great promise for further acceleration of MR image acquisition [10, 48, 11, 35, 60]. Some of the ﬁrst work on this subject was presented at the 2016 annual meeting of the International Society for Magnetic Resonance in Medicine (ISMRM). The 2017 ISMRM annual meeting included, for the ﬁrst time, a dedicated session on machine learning for image reconstruction, and presentations on the subject at the 2018 annual meeting spanned multiple focused sessions, including a dedicated category for abstracts. Despite this substantial increase in research activity, the ﬁeld of MR image reconstruction still lacks large-scale, public datasets with consistent evaluation metrics and baselines. Many MR image reconstruction studies use datasets that are not openly available to the research community. This makes it challenging to reproduce and validate comparisons of different approaches, and it restricts access to work on this important problem to researchers associated with or cooperating with large academic medical centers where such data is available. In contrast, research in computer vision applications such as object classiﬁcation has greatly beneﬁted from the availability of large-scale datasets associated with challenges such as the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) [31]. Such challenges have served as a catalyst for the recent explosion in research activity on deep learning [21]. The goal of the fastMRI dataset is to provide a ﬁrst step towards enabling similar breakthroughs in the machine-learning-based reconstruction of accelerated MR images. In this work we describe the ﬁrst large-scale release of raw MRI data that includes 8344 volumes, consisting of 167,375 slices1, associated with in vivo examinations from a range of MRI systems. In addition, we are releasing processed MR images in DICOM format from 20,000 knee and brain examinations from a representative clinical patient population, consisting of more than 1.57 million slices. Prior to providing details about the dataset and about target reconstruction tasks with associated benchmarks, we begin with a brief primer on MR image acquisition and reconstruction, in order to enable non-MRI-experts to get up to speed quickly on the information content of the dataset. In general, both the fastMRI dataset and this paper aim to connect the data science and the MRI research communities, with the overall goal of advancing the state of the art in accelerated MRI.\n\n2 Introduction to MR Image Acquisition and Reconstruction MR imaging is an indirect process, whereby cross-sectional images of the subject’s anatomy are produced from frequency and phase measurements instead of direct, spatially-resolved measurements. A measuring instrument, known as a receiver coil, is placed in proximity to the area to be imaged (Figure 1). During imaging, a sequence of spatially- and temporally-varying magnetic ﬁelds, called a “pulse sequence,” is applied by the MRI machine. This induces the body to emit resonant electromagnetic response ﬁelds which are measured by the receiver coil. The measurements typically correspond to points along a prescribed path through the multidimensional Fourier-space representation of an imaged body. This Fourier space is known as k-space in the medical imaging community. In the most basic usage of MR imaging, the full Fourier-space representation of a region is captured by a sequence of samples that tile the space up to a speciﬁed maximum frequency. Figure 1: The receiver coil housing and its positioning on a patient for a knee MR examination. The spatially-resolved image m can be estimated from the full k-space y by performing an inverse multidimensional Fourier transform: where ˆm is a noise-corrupted estimate of the true image m. The number of samples captured in k-space is a limiting factor for the speed of MR imaging. Fewer samples can be captured by sampling up to a lower maximum frequency, however this produces images of lower spatial resolution. An alternative undersampling approach involves omitting some number of k-space samples within a given maximum frequency range, which then results in aliasing artifacts. In order to remove these artifacts and infer the true underlying spatial structure of the imaged subject, one may apply a number of possible reconstruction strategies. 2.1 Parallel MR Imaging In parallel MR imaging, multiple receiver coils are used, each of which produces a separate k-space measurement matrix. Each of these matrices is different, since the view each coil provides of the imaged volume is modulated by the differential sensitivity that coil exhibits to MR signal arising from different regions. In other words, each coil measures Fourier components of the imaged volume multiplied by a complex-valued position-dependent coil sensitivity map Si. The measured k-space signal yi for coil i in an array of nc coils is given by where the multiplication is entry-wise. This is illustrated in Figure 2b, which shows the absolute value of the inverse discrete Fourier transform (DFT) of fully-sampled complex-valued k-space signals for each coil in a 15-element coil array. Each coil is typically highly sensitive in one region, and its sensitivity falls offsigniﬁcantly in other regions. If the sensitivity maps are known, and the k-space sampling is full (i.e., satisfying the Nyquist sampling condition), then the set of linear relations between m and each yi deﬁnes a linear system that is overdetermined by a factor of nc. It may be inverted using a pseudoinverse operation to produce a reconstruction of m, as long as the linear system is full rank. The quality of this reconstruction will depend on the measurement noise, since the signal-to-noise ratio is poor in parts of the volume where the coil sensitivity is low. In accelerated parallel imaging, each coil’s k-space signal is undersampled. As long as the total number of measurements across all coils exceeds the number of image voxels to be reconstructed, an unregularized least squares solution can still be used, leading to a theoretical nc-fold ˆm = F−1(y), (1) yi = F (Sim) + noise, (2) (a) k-space data from 15 coils (b) Individual coil spatial images from fully sampled data (d) Ground truth, cropped to central region and vertically ﬂipped (e) Unregularized reconstruction Figure 2: Multi-coil MRI reconstruction (c) Coil sensitivity map magnitudes given by ESPIRiT (f) Total variation penalty reconstruction (g) Baseline model reconstruction speedup over fully-sampled single-coil imaging. Each extra coil effectively produces an additional “sensitivity-encoded” measurement of the volume [26], which augments the frequency and phase encoded measurements obtained from the sequential application of magnetic ﬁeld gradients in the MR pulse sequence. Estimates of coil sensitivity patterns, required for inversion of the undersampled multi-coil linear system, may be generated from separate low-resolution calibration scans. They may also be derived directly from the k-space measurements by fully sampling a comparatively small central region of k-space, which corresponds to low spatial frequencies. In practice, the use of sub-sampling results in signiﬁcant ampliﬁcation of noise, and regularization is usually needed. In cases where a tight imaging ﬁeld of view is used, or at imaging depths exceeding the dimensions of the individual coils, the sensitivity patterns of different coils spread out, thereby lowering the effective rank of the linear system, increasing noise ampliﬁcation associated with the inverse operation, and limiting the maximum practical acceleration. As a result, in the clinic, parallel imaging acceleration factors are typically on the order of two to three. 2.2 Machine Learning Reconstruction of Undersampled MRI Data Classical approaches to MRI reconstruction solve a regularized inverse optimization problem to ﬁnd the spatially-resolved image from the sub-sampled k-space data, in both the single-coil and the multi-coil case. We describe the classical approach in more detail in Section 6. In the machine learning approach, a reconstruction function is learned from input and output pair tuples (y, m) drawn from a population. The goal is to ﬁnd a function B that minimizes the risk (i.e., expected loss) over the population distribution: We discuss error metrics that may be used as loss functions L in Section 5. In practice this optimization problem must be approximated with the empirical risk using a sample {(m(i), y(i))}ndata i=1 from the population, with respect to a loss function L:\n\n3 Prior Public Datasets The availability of public datasets has played an important role in advancing research in medical imaging, providing benchmarks to compare different approaches and leading to more impactful contributions. Early works such as DDSM [13], SLIVER07 [14] and CAUSE07 [8] triggered increasing efforts to collect new larger-scale biomedical datasets, which resulted in over one hundred public releases (counting the entries on https://grand-challenge.org/) to advance medical image analysis research. The vast majority of these datasets, which include a range of medical imaging modalities, are designed to test the limits of current methods in the tasks of segmentation, classiﬁcation, and detection. Datasets such as BraTS [24], LUNA [37], ChestX-ray [50], DeepLesion [55], and Camelyon [1], UK biobank2, ADNI (Alzheimers Disease Neuroimaging Initiative)3 and ˆm = B(y) (3) B∗= arg min B R(B), where R(B) = E(y,m) [L (B (y) , m)] . ndata X Rempirical(B) = 1 ndata i=1 L \u0010 B \u0010 y(i)\u0011 , m(i)\u0011 . (4) Dataset Volumes Body part MR scan type NYU dataset [11] 100 knee PD, T2 Stanford dataset 2D FSE 89 knee PD Stanford dataset 3D FSE [34] 20 knee PD Stanford undersampled dataset 38 knee PD fastMRI dataset 1594 knee PD Table 1: Publicly available MRI datasets containing k-space data TCIA (The Cancer Imaging Archive)4. However, none of the most prominent public MRI datasets include k-space imaging data. However, the current lack of large-scale reference standards for MR image reconstruction hinders progress in this important area. Most research uses synthetic k-space data that is not directly acquired but rather obtained from post-processing of already-reconstructed images [5, 38, 57, 56, 27]. Research using small-scale proprietary raw k-space datasets is also common [15, 36, 35, 33, 22]. In order to address the above-mentioned shortcomings, recent efforts have been devoted to collecting and publicly releasing datasets containing raw (unprocessed) k-space data; see, e.g., [34, 11]. However, the size of these existing datasets remains small. As an example, Table 1 lists publicly available knee MR datasets containing raw k-space data. Although datasets such as these provide a valuable test bed for signal processing methods, larger datasets encompassing different anatomy are required to fully realize the potential of deep learning. 4 The fastMRI Dataset and Associated Tasks The fastMRI dataset (http://fastmri.med.nyu.edu/) contains four types of data from MRI acquisitions of knees and brains Raw multi-coil k-space data: unprocessed complex-valued multi-coil MR measurements. Emulated single-coil k-space data: combined k-space data derived from multi-coil k-space data in such as way as to approximate single-coil acquisitions, for evaluation of single-coil reconstruction algorithms. Ground-truth images: real-valued images reconstructed from fully-sampled multi-coil acquisitions using the simple root-sum-of-squares method detailed below. These may be used as references to evaluate the quality of reconstructions. DICOM images: spatially-resolved images for which the raw data was discarded during the acquisition process. These images are provided to represent a larger variety of machines and settings than are present in the raw data. This data was designed to enable two distinct types of tasks: 1. Single-coil reconstruction task: reconstruct images approximating the ground-truth from\nundersampled single-coil data. 2. Multi-coil reconstruction task: reconstruct images approximating the ground-truth from undersampled multi-coil data. For each task we provide an oﬃcial split of the k-space data and ground-truth images into training and validation subsets that contain fully-sampled acquisitions, as well as test and challenge subsets which contain k-space data that have been subjected to undersampling masks as described below. Ground-truth images are not being released for the test and challenge datasets. During training of a machine-learning model, the training k-space data should be programmatically masked following the same procedure. The challenge subsets are not being released at the time of writing and are reserved for future challenges associated with the fastMRI dataset. The rationale for having a single-coil reconstruction task (and for providing simulated single-coil data), even though reconstruction from multi-coil data is expected to be more precise, is twofold: (i) to lower the barrier of entry for researchers who may not be familiar with MRI data, since the use of a single coil removes a layer of complexity, and (ii) to include a task that is relevant for the single-coil MRI machines still in use throughout the world. The DICOM images may be useful as additional data for training. Their distribution is different from that of the ground-truth images, since they were acquired with a larger diversity of scanners, manners of acquisition, reconstruction methods, and post-processing algorithms, so the application of transfer-learning techniques may be necessary. Most DICOM images are the result of accelerated parallel imaging acquisitions and corresponding reconstructions, with image quality that differs from that of putative fully-sampled acquisitions and reconstructions. The ground-truth images may, in many cases, represent a higher standard of image quality than the clinical gold standard, for which full sampling is not routine or even practical. 4.1 Anonymization Curation of the datasets described here was part of a study approved by the NYU School of Medicine Institutional Review Board. Raw data was anonymized via conversion to the vendorneutral ISMRMRD format [18]. DICOM data was anonymized using the RSNA clinical trial processor. We performed manual inspection of each DICOM image for the presence of unexpected protected health information (PHI), manual checking of metadata in raw data ﬁles, as well as spot checking of all metadata and image content. 4.2 Knee k-space Data Multi-coil raw data was stored for 1,594 scans acquired for the purpose of diagnostic knee MRI. For each scan, a single fully sampled MRI volume was acquired on one of three clinical 3T systems (Siemens Magnetom Skyra, Prisma and Biograph mMR) or one clinical 1.5T system (Siemens Magnetom Aera). Data acquisition used a 15 channel knee coil array and conventional Cartesian two pulse sequences, yielding coronal proton-density weighting with (PDFS, 798 scans) and without (PD, 796 scans) fat suppression (see Figure 3). Sequence parameters are, as per standard clinical protocol, matched as closely as possible between the systems. The following sequence parameters were used: Echo train length 4, matrix size 320 × 320, in-plane resolution 0.5mm×0.5mm, slice thickness 3mm, no gap between slices. Timing varied between systems, with repetition time (TR) ranging between 2200 and 3000 milliseconds, and echo time (TE) between 27 and 34 milliseconds. 4.3 Brain k-space Data System Number of scans Skyra 3T 663 Prisma 3T 83 Biograph mMR 3T 153 Aera 1.5T 695 Table 2: Number of scans of knee raw data per scanner Field Strength 1.5T T1 375 407 T1 POST 849 641 T2 1651 2515 FLAIR 126 406 Total 3001 3969 Table 3: Number of scans for the different contrasts and scanner ﬁeld strengths of the brain raw dataset. includes axial T1 weighted, T2 weighted and FLAIR images. Some of the T1 weighted acquisitions included admissions of contrast agent (labelled T1 POST) (see Figure 4). Not all imaging volumes included all pulse sequences. The exact distribution of contrasts and ﬁeld strengths is given in table 3. To ensure data de-identiﬁcation, we used only axial 2-D images in this dataset. We used zero matrices to replace the k-space slices ⪆5mm below the orbital rim. All processed k-spaces were then reconstructed to images in DICOM format, loaded into a picture archival communication system (PACS) and all images were visually checked by certiﬁed MR technologists to conﬁrm exclusion of identifying facial features. 4.4 Knee emulated Single-coil k-Space Data We used an emulated single-coil (ESC) methodology to simulate single-coil data from a multi-coil acquisition [43]. ESC computes a complex-valued linear combination of the responses from multiple coils, with the linear combination ﬁtted to the ground-truth root-sum-of-squares reconstruction in the least-squares sense. 4.5 Knee DICOM Data In addition to the scanner raw data described above, the fastMRI dataset includes DICOM data from 10,000 clinical knee MRI scans. These images represent a wider variety of scanners and pulse sequences than those represented in the collection of raw data. Each MR exam for which DICOM images are included typically consisted of ﬁve clinical pulse sequences:\n\n1. Coronal proton-density weighting without fat suppression,\n\n4. Sagittal T2 weighting with fat suppression, and\n\n5. Axial T2 weighting with fat suppression.\nThe two coronal sequences have the same basic speciﬁcations (matrix size, etc) as the sequences associated with raw data. The sagittal and axial sequences have different matrix sizes and have no The Fourier transformation of an image from a DICOM ﬁle does not directly correspond to the originally measured raw data, due to the inclusion of additional post-processing steps in the vendorspeciﬁc reconstruction pipeline. Most of the DICOM images are also derived from accelerated acquisitions and are reconstructed with parallel imaging algorithms, since this baseline acceleration represents the current clinical standard. The image quality of DICOM images, therefore, is not equivalent to that of the ground truth images directly associated with fully sampled raw data. The DICOM images are distinct from the validation, test, or challenge sets. 4.6 Brain DICOM 10,000 brain MRI DICOM studies are also included. Axial 2D image volumes are included with the following pulse sequences: T1, T2, and T2 FLAIR. All studies represent unique individuals and there is no subject overlap with the brain rawdata. In addition to the deidentiﬁcation procedures detailed above, the brain image volumes were cropped to exclude identiﬁable facial features, following which each image was visually inspected to conﬁrm appropriate deidentiﬁcation. Finally, we present 10,000 brain MRI DICOM studies from 10,000 unique subjects, each one including axial 2D DICOM image volumes through the majority of the brain representing a broad range of neurological pathologies. Not all studies include all pulse sequences. 4.7 Ground Truth The root-sum-of-squares reconstruction method applied to the fully sampled k-space data [28] provides the ground truth for the multi-coil dataset. The single-coil dataset includes two ground truth reconstructions, which we denote ESC and RSS. The ESC ground truth is given by the inverse Fourier transform of the single-coil data, and the RSS ground truth is given by the root-sum-ofsquares reconstruction computed on the multi-coil data that were used to generate the virtual single-coil k-space data. All ground truth images are cropped to the central 320 × 320 pixel region to compensate for readout-direction oversampling that is standard in clinical MR examinations. The root-sum-of-squares approach [28] is one of the most commonly-used coil combination methods in clinical imaging. It ﬁrst applies the inverse Fourier Transform to the k-space data from each coil: ˜mi = F−1(yi), (5) where yi is the k-space data from the ith coil and ˜mi is the ith coil image. Then, the individual coil images are combined voxel by voxel as follows: i=0 | ˜mi|2 !1/2 , (6) nc X ˜mrss = (a) (b) Figure 3: A proton-density weighted image (a) with fat suppression (PDFS) and (b) without fat suppression (PD). Fat has a high signal response in MR imaging, which can make details in other regions diﬃcult to see. Fat-suppressed scans typically have higher noise. Volumes Slices Multi-coil Single-coil Multi-coil Single-coil training 973 973 34,742 34,742 validation 199 199 7,135 7,135 test 118 108 4,092 3,903 challenge 104 92 3,810 3,305 Table 4: Volumes and slices in each set (a) (b) (c) (d) Figure 4: Axial brain MRI images with different contrasts: (a) FLAIR, (b) T1 weighted (c) T1 weighted with contrast agent (T1 POST), and (d) T2 weighted. 4.8 Dataset Split Each volume is randomly assigned to one of the following six component datasets: training, validation, multi-coil test, single-coil test, multi-coil challenge, or single-coil challenge. Table 4 shows the number of volumes assigned to each dataset. The training and validation datasets may be used to ﬁt model parameters or to determine hyperparameter values. The test dataset is used to compare the results across different approaches. To ensure that models do not overﬁt to the test set, the ground truth reconstructions are not publicly released for this set. Evaluation on the test set is accomplished by uploading results to the public leaderboard at http://fastmri.org/. The challenge portion of the dataset will be forthcoming. A volume from the train or validation dataset is used in both the single-coil and multi-coil tracks, whereas a volume from the test or challenge dataset is only used in either the single-coil or the multi-coil track. Volumes were only included in a single test or challenge set to ensure information from one could not be used to help the result in another. 4.9 Cartesian Undersampling Volumes in the test and challenge datasets contain undersampled k-space data. The undersampling is performed by retrospectively masking k-space lines from a fully-sampled acquisition. k-space lines are omitted only in the phase encoding direction, so as to simulate physically realizable accelerations in 2D data acquisitions. The same undersampling mask is applied to all slices in a volume, with each case consisting of a single volume. The overall acceleration factor is set randomly to either four or eight (representing a four-fold or an eight-fold acceleration, respectively), with equal probability for each. All undersampling masks are generated by ﬁrst including some number of adjacent lowestfrequency k-space lines to provide a fully-sampled k-space region. When the acceleration factor equals four, the fully-sampled central region includes 8% of all k-space lines; when it equals eight, 4% of all k-space lines are included. The remaining k-space lines are included differently for both knee and brain cases. For knee, the remaining k-space lines are included uniformly at random, with the probability set so that, on average, the undersampling mask achieves the desired acceleration factor. Random undersampling is chosen in order to meet the general conditions for compressed sensing [2, 23], for a fair comparison of learned reconstruction algorithms with traditional sparsitybased regularizers. For brain, after a random offset from the start, the remaining lines are sampled equidistant from each other with a spacing that achieves the desired acceleration factor. Equidistant was chosen because of ease of implementation on existing MRI machines. Figure 5 depicts the kspace trajectories for random and equidistant undersampling at four and eight acceleration factors.\n\n5 Metrics The assessment of MRI reconstruction quality is of paramount relevance to develop and compare machine learning and medical imaging systems [51, 53, 3, 58]. The most commonly used evaluation metrics in the MRI reconstruction literature [3] include (normalized) mean squared error, which measures pixel-wise intensity differences between reconstructed and reference images, and signalto-noise ratio, which measures the degree to which image information rises above background noise. These metrics are appealing because they are easy to understand and eﬃcient to compute. However, they both evaluate pixels independently, ignoring the overall image structure. Additional metrics have been introduced in the literature to capture structural distortion [41, 6, 58]. For example, the structural similarity index [53] and its extended version, multiscale structural (a) Random mask with 4-fold acceleration (c) Equispaced mask with 4-fold acceleration Figure 5: Examples of undersampled k-space trajectories similarity [52], provide a mechanism to assess the perceived quality of an image using local image patches. The most recent developments in the computer vision literature leverage pretrained deep neural networks to measure the perceptual quality of an image by computing differences at the representation level [19], or by means of a downstream task such as classiﬁcation [32]. In the remainder of this section, we review the deﬁnitions of the commonly-used metrics of normalized mean square error, peak signal-to-noise ratio, and structural similarity. As is discussed later, while we expect these metrics to serve as a familiar starting point, we also hope that the fastMRI dataset will enable robust investigations into improved evaluation metrics as well as improved reconstruction algorithms. (b) Random mask with 8-fold acceleration (d) Equispaced mask with 8-fold acceleration 5.1 Normalized Mean Square Error The normalized mean square error (NMSE) between a reconstructed image or image volume represented as a vector ˆv and a reference image or volume v is deﬁned as NMSE(ˆv, v) = ∥ˆv −v∥2 2 ∥v∥2 2 , (7) where ∥·∥2 2 is the squared Euclidean norm, and the subtraction is performed entry-wise. In this work we report NMSE values computed and normalized over full image volumes rather than individual slices, since image-wise normalization can result in strong variations across a volume. NMSE is widely used, and we recommend that it be reported as the primary measure of reconstruction quality for experiments on the fastMRI dataset. However, due to the many downsides of NMSE, such as a tendency to favor smoothness rather than sharpness, we recommend also reporting additional metrics such as those described below. 5.2 Peak Signal-to-Noise Ratio The peak signal-to-noise ratio (PSNR) represents the ratio between the power of the maximum possible image intensity across a volume and the power of distorting noise and other errors: PSNR(ˆv, v) = 10 log10 max(v)2 Here ˆv is the reconstructed volume, v is the target volume, max(v) is the largest entry in the target volume v, MSE(ˆv, v) is the mean square error between ˆv and v deﬁned as 1 n∥ˆv −v∥2 2 and n is the number of entries in the target volume v. Higher values of PSNR (as opposed to lower values of NMSE) indicate a better reconstruction. 5.3 Structural Similarity The structural similarity (SSIM) index measures the similarity between two images by exploiting the inter-dependencies among nearby pixels. SSIM is inherently able to evaluate structural properties of the objects in an image and is computed at different image locations by using a sliding window. The resulting similarity between two image patches ˆm and m is deﬁned as SSIM( ˆm, m) = (2µ ˆmµm + c1)(2σ ˆmm + c2) (µ2 ˆm + µ2m + c1)(σ2 ˆm + σ2m + c2), (9) where µ ˆm and µm are the average pixel intensities in ˆm and m, σ2 ˆm and σ2 m are their variances, σ ˆmm is the covariance between ˆm and m and c1 and c2 are two variables to stabilize the division; c1 = (k1L)2 and c2 = (k2L)2. For SSIM values reported in this paper, we choose a window size of 7 × 7, we set k1 = 0.01, k2 = 0.03, and deﬁne L as the maximum value of the target volume, L = max(v). 5.4 L1 Error It is sometimes advantageous to use the L1 loss L1(ˆv, v) = ∥ˆv −v∥1, (10) MSE(ˆv, v). (8) (a) Cropped and vertically ﬂipped reconstruction from fully sampled kspace data (b) Rectangular masked kspace (e) Multiscale Daubechies discrete wavelet transform (f) L1 Wavelet penalty reconstruction Figure 6: Single-coil reconstruction 6 Baseline Models Along with releasing the fastMRI data, we detail two reference approaches to be used as reconstruction baselines: a classical non-machine learning approach, and a deep-learning approach. Each of these baselines has versions tailored for single-coil or multi-coil data. The “classical” baselines are comprised of reconstruction methods developed by the MRI community over the last 30+ years. These methods have been extensively tested and validated, and many have demonstrated robustness suﬃcient for inclusion in the clinical workﬂow. By comparison, machine learning reconstruction methods are relatively new in MRI, and deep-learning reconstruction techniques in particular have emerged only in the past few years. We include some deliberately rudimentary deep-learning models as starting points, with the expectation that future learning algorithms will provide markedly improved performance. 6.1 Single-coil Classical Baselines (knee only) In the single-coil imaging setting, the task is to reconstruct an image, m, from k-space observations, y. In the presence of undersampling, the vector y has a length smaller than that of m. Therefore there are, in principle, inﬁnitely many possibilities for m that can be mapped onto a single y. The advent of compressed sensing [2, 23] provided a framework for reconstruction of images from undersampled data that closely approximate images derived from fully-sampled data, subject to (c) Reconstruction via zero-ﬁlled IFFT (d) Deep-learning baseline UNET reconstruction (g) Image gradients as given by a Sobel ﬁlter (h) Regularized totalvariation reconstruction sparsity constraints. Compressed sensing theory requires the images in question to be sparse in some transform domain. Two common examples are to assume sparsity in the wavelet domain, or to assume sparsity of the spatial gradients of the image. The particular assumption impacts the mathematical formulation of the reconstruction problem, either in the cost function or through a regularization term. More concretely, the sparse reconstruction approach consists of ﬁnding an image m whose Fourier space representation is close to the measured k-space matrix y at all measured spatial frequencies, yet at the same time minimizes a sparsity-inducing objective R(m) that penalizes unnatural reconstructions: minimize m R (m) s.t. ∥P (F (m)) −y∥2 2 ≤ϵ. (11) Here, P is a projection function that zeros out entries that are masked, and ϵ is a speciﬁed small threshold value. In most applications it is easier to work with a soft penalty instead of a constraint, so the Lagrangian dual form of Equation 11 is used instead, with penalty parameter λ: minimize m 1 2 ∥P (F (m)) −y∥2 2 + λR (m) . (12) For a convex regularizer R, there exists, for any choice ϵ > 0, a value λ such that these two formulations have equivalent solutions. The most common regularizers used for MRI are: RL1 (m) = ∥m∥1 , Rwavelet (m) = ∥Ψ (m)∥1 (Ψ is a discrete wavelet transform) , q |mi+1,j −mi,j|2 + |mi,j+1 −mi,j|2. RTV (m) = X i,j The L1 penalty works best when the MR images are sparse in image space, for instance in vascular imaging (e.g., Yamamoto et al. [54]). This is not the case for most MRI applications. The totalvariation (TV) penalty encourages sparsity in the spatial gradients of the reconstructed image, as given by a local ﬁnite-difference approximation [30] (Figure 6g). The TV regularizer can be very effective for some imaging protocols, but it also has a tendency to remove detail (Figure 6h). The Rwavelet penalty encourages sparsity in the discrete wavelet transform of the image. Most natural images exhibit signiﬁcant sparsity when expressed in a wavelet basis. The most commonly used transform is the Multiscale Daubechies (DB2) transform (Figure 6e). To date, due to their computational complexity as well as their tendency to introduce compression artifacts or oversmoothing, compressed sensing approaches have taken some time to gain acceptance in the clinic, though commercial implementations of compressed sensing are currently beginning to appear. The single-coil classical baseline provided with the fastMRI dataset was adopted from the widely-used open-source BART toolkit (Appendix B), using total variation as the regularizer. We ran the optimization algorithm for 200 iterations on each slice independently. Table 5 summarizes the results of applying this method to the single-coil validation data with different regularization strengths and different acceleration factors. These results indicate that NMSE and PSNR metrics are highly (inversely) correlated and generally favor models with stronger regularization than SSIM does. Stronger regularization generally results in smoother images that lack the ﬁne texture of the ground truth images. A regularization parameter of 0.01 yields the best results for 4-fold acceleration in most cases, whereas the higher 8-fold acceleration gets slightly better results with a regularization parameter of 0.1. Single-coil classical baseline (TV model) applied to knee validation data Acceleration Regularization Weight NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 10−4 0.0355 0.0919 30.2 27.6 0.637 0.506 10−3 0.0342 0.0916 30.4 27.6 0.641 0.505 10−2 0.0287 0.09 31.4 27.7 0.645 0.494 10−1 0.0313 0.0993 30.9 27.3 0.575 0.399 1 0.0522 0.124 28.5 26.2 0.526 0.327 8-fold 10−4 0.0708 0.118 27.1 26.4 0.551 0.417 10−3 0.0699 0.118 27.1 26.4 0.553 0.416 10−2 0.063 0.117 27.7 26.4 0.564 0.408 10−1 0.0537 0.117 28.4 26.5 0.55 0.357 1 0.0742 0.132 26.9 25.9 0.538 0.333 Table 5: Validation set results for the classical baseline model with Total Variation regularization for the single-coil task. Bold-faced numbers indicate the best performance for each image quality metric. 6.2 Multi-coil Classical Baselines When multiple receiver coils are used, the reconstruction process must combine information from multiple channels into one image. Multi-coil acquisitions currently represent the norm in clinical practice, for two principal reasons: they provide increased SNR, as compared with single-coil acquisitions, over extended ﬁelds of view, and they enable acceleration via parallel imaging. Equation 2 in Section 2.1 describes the forward model for parallel imaging. The SENSE formulation [26] of parallel image reconstruction involves direct inversion of this forward model, via a suitable pseudoinverse. Leveraging the convolution property of the Fourier Transform reveals the following convolution relationship: yi = gi ⊛F (m) + noise. (13) Here gi is the Fourier Transform of the coil sensitivity pattern Si and ⊛denotes the convolution operation. The GRAPPA/SMASH formulation of parallel image reconstruction [39, 9] involves ﬁlling in missing k-space data via combinations of acquired k-space data within a deﬁned convolution kernel, prior to inverse Fourier transformation. Either formulation requires estimates of the coil sensitivity information in Si or gi, which may be derived either from a separate reference scan or directly from the acquired undersampled k-space data itself. Reference scan methods are often used in the SENSE formulation, whereas GRAPPA formulations are typically self-calibrating, relying on subsets of fully-sampled data generally in central k-space regions. The parallel imaging techniques described above may be combined productively with compressed sensing, via the use of sparsity-based regularizers. For example, one may extend Equation 12 in Section 6.1 above to include multi-coil data as follows: nc X minimize m 1 2 the ESPIRiT approach [45], which harmonizes parallel imaging and compressed sensing in a uniﬁed framework. As was the case for the classical single-coil baseline, the classical multi-coil baseline provided with the fastMRI dataset was adopted from the BART toolkit (Appendix B). In the multi-coil case, the ESPIRiT algorithm was used to estimate coil sensitivities, and to perform parallel image reconstruction in combination with compressed sensing using a total-variation regularizer. Multi-coil classical baseline (TV model) applied to knee validation data Acceleration Regularization NMSE PSNR SSIM 4-fold 10−4 0.0246 0.0972 31.6 27.4 0.677 0.53 10−3 0.0222 0.0951 32.1 27.5 0.693 0.554 10−2 0.0198 0.0971 32.6 27.5 0.675 0.588 10−1 0.0251 0.109 31.3 27 0.633 0.538 8-fold 10−4 0.0494 0.114 28.2 26.5 0.61 0.505 10−3 0.0447 0.112 28.6 26.6 0.626 0.524 10−2 0.0352 0.109 29.6 26.8 0.642 0.551 10−1 0.0389 0.114 29.2 26.7 0.632 0.527 Table 6: Validation set results for the classical baseline model with Total Variation regularization for the knee multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Results using this baseline model are summarized in Table 6 and 7 . The experimental setup is identical to the single-coil scenario, except that we compare the reconstructions with the rootsum-of-squares ground truth instead of the ESC ground truth. 6.3 Single-coil Deep-Learning Baselines (knee only) Various deep-learning techniques based on Convolutional Neural Networks have recently been proposed to tackle the problem of reconstructing MR images from undersampled k-space data [10, 48, 11, 35, 60, 17, 12]. Many of these proposed methods are based on the U-Net architecture introduced in [29]. U-Net models and their variants have successfully been used for many image-to-image prediction tasks including MRI reconstruction [17, 12] and image segmentation [29]. The U-Net single-coil baseline model included with the fastMRI data release (Figure 7) consists of two deep convolutional networks, a down-sampling path followed by an up-sampling path. The down-sampling path consists of blocks of two 3×3 convolutions each followed by instance normalization [46] and Rectiﬁed Linear Unit (ReLU) activation functions. The blocks are interleaved by down-sampling operations consisting of max-pooling layers with stride 2 which halve each spatial dimension. The up-sampling path consists of blocks with a similar structure to the down-sampling path, interleaved with bilinear up-sampling layers which double the resolution between blocks. Each block consists of two 3×3 convolutions with instance normalization [46] and ReLU activation layers. In contrast to the down-sampling path, the up-sampling path concatenates two inputs to the ﬁrst convolution in each block: the up-sampled activations from the previous block, together with the activations that follow the skip connection from the block in the down-sampling path with the same resolution (horizontal arrows in Figure 7). At the end of the up-sampling path, we PD PDFS PD PDFS PD PDFS 1 32 32 * * 320 32 64 64 * * 160 64 128 128 * * 80 128 256 256 * * 512 128 128 * * 40 40 256 20 * Figure 7: Single-coil baseline U-Net architecture include a series of 1×1 convolutions that reduce the number of channels to one without changing the spatial resolution. For the single-coil MRI reconstruction case, the zero-ﬁlled image is used as the input to the model. The zero-ﬁlled image is obtained by ﬁrst inserting zeros at the location of all unobserved k-space values, applying a two-dimensional Inverse Fourier Transform (IFT) to the result, and ﬁnally computing the absolute value. The result is center cropped to remove any readout and phase oversampling. Using the notation from section 6.1, the zero-ﬁlled image is given by ˜m = C( F−1(P(y)) ), where C is the linear operator corresponding to the center cropping and F−1 is the two-dimensional IFT. The entire network is trained on the training data in an end-to-end manner to minimize the mean absolute error with respect to corresponding ground truth images. Let Bθ(m) be the function computed by the U-Net model, where θ represents the parameters of the model. Then the training process corresponds to the following optimization problem: ndata X minimize θ 1 2 Bθ( ˜m(i)) −m(i) 1 , (15) i=0 where the ground truths m(i) are obtained using the ESC method described in Section 4.4. Our particular single-coil U-Net baseline model was trained on 973 image volumes in the training set, using the RMSProp algorithm [42]. We used an initial learning rate of 0.001, which was multiplied by 0.1 after 40 epochs, after which the model was trained for an additional 10 epochs. During training, we randomly sampled a different mask for each training example in each epoch independently using the protocol described in Section 4.9 for the test data. At the end of each epoch, we recorded the NMSE on the validation data. After training, we picked the model that achieved the lowest validation NMSE. Table 8 presents the results from running trained U-Net models of different capacities on the single-coil validation data. These results indicate that the trained U-Net models perform signiﬁcantly better than the classical baseline method. The best U-Net models obtain 40-50% relative improvement over the classical methods (see Table 5) in terms of NMSE. The performance of the U-Net models continues to increase with increasing model capacity, and 64 32 32 16 1 1x1 1x1 * * * * 320 128 32 32 * * 160 256 64 64 * * 80 * 3x3 Convolution + ReLU + InstanceNorm 2x2 Max pooling 2x Bilinear  upsampling 1x1 256 * 1x1 Convolution even the largest model with over 200 million parameters is unable to overﬁt the training data. These improvements begin to saturate after 50 million parameters for the simpler 4-fold acceleration case. However, for the more challenging 8-fold acceleration task, the largest model performs signiﬁcantly better than the smaller models. This suggests that models with very large capacities trained on large amounts of data can enable high acceleration factors. Table 9 compares the performance of the classical and the U-Net baseline models for the singlecoil task, as applied to the test dataset. For the classical baseline model, we chose the best regularization weights for each modality and for each acceleration factor based on the validation data results, resulting in a regularization weight of 0.1 for 8-fold acceleration on Proton Density without fat suppression and 0.01 for every other case. For the U-Net baseline model, we chose the model with the largest capacity. 6.4 Multi-coil Deep-Learning Baselines In the multi-coil MRI reconstruction task, we have one set of undersampled k-space measurements from each coil, and a different zero-ﬁlled image can be computed from each coil. These coil images can be combined using the root-sum-of-squares algorithm. Let ˜mi be the zero-ﬁlled image from coil i. With ˜mrss deﬁned as in Equation 6, the U-Net model described in Section 6.3 can be used for the multi-coil reconstruction task by simply feeding this combined image in as input: Bθ( ˜mrss). The model is trained to minimize the mean absolute error loss similarly to the single-coil task. The training procedure is also identical to the single-coil case except that the root-sum-of-squares image is used as the ground truth as described in Section 4.7. As is the case for the single-coil task, the multi-coil U-Net baselines substantially outperform the classical baseline models (compare Table 10 and 11 with Table 6 and 7). Note that this is true despite the fact that the multi-coil U-Net baseline deﬁned above does not take coil sensitivity information into account, and therefore neither includes a direct parallel image reconstruction nor accounts for sparsity or other correlations among coils. Models that incorporate coil sensitivity information are expected to perform better than the current multi-coil U-Net baselines. Table 10 and Table 11 shows, once again, that the performance of the U-Net models improves with model size, with the largest U-Net baseline model providing the best performance. Table 12 compares the performance of the classical and the U-Net baseline models for the multi-coil task, as applied to the test dataset. For the classical baseline model, we chose the best regularization weights for each modality and for each acceleration factor based on the validation data results. For knees this resulted in a regularization weight of 0.001 for 4-fold undersampling for Proton Density with Fat Suppression and 0.01 for every other acquisition type. For brain this resulted in a regularization weight of 0.001 for 8-fold AXFLAIR and 4-fold AXT1, and 0.01 for every other acquisition type. For the U-Net baseline model, we chose the model with the largest capacity. To appreciate the value of the dataset size, we study how model performance scales with the amount of data used to train a model. To this end, we trained several U-Net models with varying model capacities on different sized subsets of the training data. Figure 8 shows the SSIM metric computed on the validation data for the multi-coil task. It is evident from these results that training with larger amounts of data yields substantial improvements in the quality of reconstructions, which highlights the need for the release of large datasets like fastMRI. As mentioned in Section 4.5, the fastMRI dataset also includes a large set of DICOM images that can be used as additional training data. It is possible that the baseline U-Net models could be improved further by making use of this additional data. 32 64 128 256 0.895 0.890 SSIM 0.885 0.880 0.875 100 200 500 973 Number of training cases Figure 8: Results from training the U-Net on different amounts of training data for the multi-coil knee challenge with 4-fold acceleration (left) and 8-fold acceleration (right). Each line represents a model with a different number of channels. 7\n\nDiscussion\nMR image reconstruction is an inverse problem, and thus it has many connections to inverse problems in the computer vision literature [40, 7, 4, 47], such as super-resolution, denoising and in-painting. In all of these inverse problems, the goal is to recover a high-dimensional ground truth image from a lower-dimensional measurement. Such ill-posed problems are very diﬃcult to solve since there exists an inﬁnite number of high-dimensional images that can result in the same-low dimensional measurement. In order to simplify the problem, an assumption is often made that only a small number of high-resolution images would correspond to natural images [4]. Given that MRI reconstruction is a similar inverse problem, we hope that the computer vision community, as well as the medical imaging community, will ﬁnd our dataset beneﬁcial. In the clinical setting, radiologists use MRI to search for abnormalities, make diagnoses, and recommend treatment options. Thus, contrary to many computer vision problems where small texture changes might not necessarily alter the overall satisfaction of the observer, in MRI reconstruction, extra care should be taken to ensure that the human interpreter is not misled by a very plausible but not necessarily correct reconstruction. This is especially important as image generation techniques increase in their ability to generate photo-realistic results [49]. Therefore some research effort should be devoted to look for solutions that, by design, ensure correct diagnosis, and we hope that our dataset will provide a testbed for new ideas in these directions as well. An important question in MRI reconstruction is the choice of the evaluation metric. The current consensus in the MRI community is that global metrics, such as NMSE, SSIM and PSNR, do not necessarily capture the level of detail required for proper evaluation of MRI reconstruction algorithms [25, 16]. A natural question arises: what would the optimal metric be? An ideal MRI reconstruction algorithm should produce sharp, trustworthy images, that ultimately ensure the proper radiologic interpretation. While our dataset will help ensure consistent evaluation, we hope that it will also trigger research on MRI reconstruction metrics. This goal will be impossible to achieve without clinical studies involving radiologists evaluating fully-sampled and undersampled MRI reconstructions to make sure that both images lead to the same diagnosis. Although this dataset provides an excellent entry point for machine learning methods for MR reconstruction, there are some aspects of MR imaging that we have not yet considered here. Phys- 32 64 128 256 0.860 0.855 0.850 0.845 SSIM 0.840 0.835 0.830 0.825 0.820 100 200 500 973 Number of training cases Ground Truth Total Variation U-Net Figure 9: Example knee reconstructions Multi-Coil 4-Fold Multi-Coil 8-Fold Single-Coil 4-Fold Single-Coil 8-Fold Figure 10: Example brain reconstructions ical effects such as spin relaxation, eddy currents and ﬁeld distortions are not at present explicitly accounted for in our retrospective undersampling approaches or our baseline models. The manifestation of these effects depends upon the object being imaged, the MRI scanner used, and even the sampling pattern selected. Extending the results from methods developed for this challenge to the clinic remains an open problem, but we believe the provision of this dataset is an important ﬁrst step on the path to this goal. 8\n\nConclusion\nIn this work we detailed the fastMRI dataset: the largest raw MRI dataset to be made publicly available to date. Previous public datasets have focused on post-processed magnitude images for speciﬁc biologic and pathologic questions. Although our dataset was originally acquired for a focused task, the inclusion of raw k-space data allows methods to be developed for the imaging pipeline itself, in principle allowing them to be applied on any MRI scanner for any imaging task. In addition to the data, we provide evaluation metrics and baseline algorithms to aid the research community in assessing new approaches. Consistent evaluation of MRI reconstruction techniques is provided by a leaderboard using held-out test data. We hope that the availability of this dataset will accelerate research in MR image reconstruction, and will serve as a benchmark during training and validation of new algorithms.\n\n9 Acknowledgements EB024532 and NIH P41 EB017183. We would also like to thank Michela Paganini and Mark Tygert.\n\nReferences\n[1] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson, Maschenka Balkenhol, et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. Journal of the American Medical Association, 318(22), 2017. [2] Emmanuel J Cand`es, Justin Romberg, and Terence Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE Transactions on Information Theory, 52(2), 2006. [3] Damon M Chandler. Seven challenges in image quality assessment: past, present, and future research. ISRN Signal Processing, 2013. [4] Jen-Hao Rick Chang, Chun-Liang Li, Barnab´as P´oczos, B. V. K. Vijaya Kumar, and Aswin C. Sankaranarayanan. One network to solve them all - solving linear inverse problems using deep projection models. IEEE International Conference on Computer Vision (ICCV), 2017. [5] Salman Ul Hassan Dar and Tolga C¸ukur. A transfer-learning approach for accelerated MRI using deep neural networks. arXiv preprint, 2017. [6] Michael P Eckert and Andrew P Bradley. Perceptual quality metrics applied to still image compression. Signal Processing, 70(3), 1998. [7] Kai Fan, Qi Wei, Wenlin Wang, Amit Chakraborty, and Katherine A. Heller. InverseNet: Solving inverse problems with splitting networks. arXiv preprint, 2017. [8] Bram Van Ginneken, Tobias Heimann, and Martin Styner. 3d segmentation in the clinic: A grand challenge. In MICCAI Workshop on 3D Segmentation in the Clinic: A Grand Challenge, 2007. [9] Mark A Griswold, Peter M Jakob, Robin M Heidemann, Mathias Nittka, Vladimir Jellus, Jianmin Wang, Berthold Kiefer, and Axel Haase. Generalized autocalibrating partially parallel acquisitions (GRAPPA). Magnetic Resonance in Medicine, 47(6), 2002. [10] Kerstin Hammernik, Florian Knoll, Daniel K Sodickson, and Thomas Pock. Learning a Variational Model for Compressed Sensing MRI Reconstruction. In Magnetic Resonance in Medicine (ISMRM), 2016. [11] Kerstin Hammernik, Teresa Klatzer, Erich Kobler, Michael P. Recht, Daniel K. Sodickson, Thomas Pock, and Florian Knoll. Learning a variational network for reconstruction of accelerated MRI data. Magnetic Resonance in Medicine, 2018. [12] Yoseob Han and Jong Chul Ye. Framing U-Net via deep convolutional framelets: Application to sparseview CT. IEEE Transactions on Medical Imaging, 37(6), 2018. [13] Michael Heath, Kevin Bowyer, Daniel Kopans, P Kegelmeyer, Richard Moore, Kyong Chang, and S Munishkumaran. Current status of the digital database for screening mammography. In Digital Mammography, 1998. [14] Tobias Heimann, Bram Van Ginneken, Martin A Styner, Yulia Arzhaeva, Volker Aurich, Christian Bauer, Andreas Beck, Christoph Becker, Reinhard Beichel, Gy¨orgy Bekes, et al. Comparison and evaluation of methods for liver segmentation from CT datasets. IEEE Transactions on Medical Imaging, 28(8), 2009. [15] Yue Huang, John Paisley, Xianbo Chen, Xinghao Ding, Feng Huang, and Xiao-Ping Zhang. MR image reconstruction from undersampled k-space with bayesian dictionary learning. arXiv preprint, 2013. [17] Chang Min Hyun, Hwa Pyung Kim, Sung Min Lee, Sungchul Lee, and Jin Keun Seo. Deep learning for undersampled MRI reconstruction. Physics in medicine and biology, 63(13), 2018. [18] Souheil J Inati, Joseph D Naegele, Nicholas R Zwart, Vinai Roopchansingh, Martin J Lizak, David C Hansen, Chia-Ying Liu, David Atkinson, Peter Kellman, Sebastian Kozerke, et al. ISMRM raw data format: a proposed standard for MRI raw datasets. Magnetic resonance in medicine, 77(1), 2017. [19] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, 2016. [20] Erik G Larsson, Deniz Erdogmus, Rui Yan, Jose C Principe, and Jeffrey R Fitzsimmons. SNR-optimality of sum-of-squares reconstruction for phased-array magnetic resonance imaging. Journal of Magnetic Resonance, 163(1), 2003. [21] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep Learning. Nature, 521(7553), 2015. [22] Kai Lonning, Patrick Putzky, Matthan W. A. Caan, and Max Welling. Recurrent inference machines for accelerated MRI reconstruction, 2018. [23] Michael Lustig, David Donoho, and John M Pauly. Sparse MRI: The Application of Compressed Sensing for Rapid MR Imaging. Magnetic Resonance in Medicine, 58(6), 2007. [24] Bjoern H. Menze, Andrs Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, Levente Lanczi, Elizabeth R. Gerstner, Marc-Andr Weber, Tal Arbel, Brian B. Avants, Nicholas Ayache, Patricia Buendia, D. Louis Collins, Nicolas Cordier, Jason J. Corso, Antonio Criminisi, Tilak Das, Herve Delingette, agatay Demiralp, Christopher R. Durst, Michel Dojat, Senan Doyle, Joana Festa, Florence Forbes, Ezequiel Geremia, Ben Glocker, Polina Golland, Xiaotao Guo, Andac Hamamci, Khan M. Iftekharuddin, Raj Jena, Nigel M. John, Ender Konukoglu, Danial Lashkari, Jos Antonio Mariz, Raphael Meier, Srgio Pereira, Doina Precup, Stephen J. Price, Tammy Riklin Raviv, Syed M. S. Reza, Michael T. Ryan, Duygu Sarikaya, Lawrence H. Schwartz, Hoo-Chang Shin, Jamie Shotton, Carlos A. Silva, Nuno Sousa, Nagesh K. Subbanna, Gbor Szkely, Thomas J. Taylor, Owen M. Thomas, Nicholas J. Tustison, Gzde B. nal, Flor Vasseur, Max Wintermark, Dong Hye Ye, Liang Zhao, Binsheng Zhao, Darko Zikic, Marcel Prastawa, Mauricio Reyes, and Koen Van Leemput. The multimodal brain tumor image segmentation benchmark (BRATS). IEEE Transactions on Medical Imaging, 34(10), 2015. [25] Jun Miao, Feng Huang, Sreenath Narayan, and David L. Wilson. A new perceptual difference model for diagnostically relevant quantitative image quality evaluation: A preliminary study. Magnetic Resonance Imaging, 31(4), 2013. [26] Klaas P Pruessmann, Markus Weiger, Markus B Scheidegger, and Peter Boesiger. SENSE: sensitivity encoding for fast MRI. Magnetic resonance in medicine, 42(5), 1999. [27] Tran Minh Quan and Won-Ki Jeong. Compressed sensing dynamic MRI reconstruction using GPUaccelerated 3d convolutional sparse coding. In Medical Image Computing and Computer-Assisted Intervention (MICCAI), 2016. [28] Peter B Roemer, William A Edelstein, Cecil E Hayes, Steven P Souza, and Otward M Mueller. The NMR phased array. Magnetic resonance in medicine, 16(2), 1990. [29] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. Medical Image Computing and Computer-Assisted Intervention, 2015. [30] Leonid I Rudin, Stanley Osher, and Emad Fatemi. Nonlinear total variation based noise removal algorithms. Physica D: nonlinear phenomena, 60(1-4), 1992. [32] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training GANs. In Advances in Neural Information Processing Systems, 2016. [33] Christopher M. Sandino, Neerav Dixit, Joseph Y. Cheng, and Shreyas S Vasanawala. Deep convolutional neural networks for accelerated dynamic magnetic resonance imaging. Technical report, Stanford University, 2017. [34] Anne Marie Sawyer, Michael Lustig, Marcus Alley, Phdmartin Uecker, Patrick Virtue, Peng Lai, Shreyas Vasanawala, and Ge Healthcare. Creation of fully sampled MR data repository for compressed sensing of the knee, 2013. [35] Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony N. Price, and Daniel Rueckert. A deep cascade of convolutional neural networks for MR image reconstruction. Information Processing in Medical Imaging, 2017. [36] Jo Schlemper, Jose Caballero, Joseph V. Hajnal, Anthony N. Price, and Daniel Rueckert. A deep cascade of convolutional neural networks for dynamic MR image reconstruction. IEEE Transactions on medical imaging, 37(2), 2018. [37] Arnaud Arindra Adiyoso Setio, Alberto Traverso, Thomas de Bel, Moira S. N. Berens, Cas van den Bogaard, Piergiorgio Cerello, Hao Chen, Qi Dou, Maria Evelina Fantacci, Bram Geurts, Robbert van der Gugten, Pheng-Ann Heng, Bart Jansen, Michael M. J. de Kaste, Valentin Kotov, Jack Yu-Hung Lin, Jeroen T. M. C. Manders, Alexander S´onora-Mengana, Juan Carlos Garc´ıa-Naranjo, Mathias Prokop, Marco Saletta, Cornelia Schaefer-Prokop, Ernst Th. Scholten, Luuk Scholten, Miranda M. Snoeren, Ernesto Lopez Torres, Jef Vandemeulebroucke, Nicole Walasek, Guido C. A. Zuidhof, Bram van Ginneken, and Colin Jacobs. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the LUNA16 challenge. Medical Image Analysis, 2017. [38] Ohad Shitrit and Tammy Riklin Raviv. Accelerated magnetic resonance imaging by adversarial neural network. In M. Jorge Cardoso, Tal Arbel, Gustavo Carneiro, Tanveer F. Syeda-Mahmood, Jo˜ao Manuel R. S. Tavares, Mehdi Moradi, Andrew P. Bradley, Hayit Greenspan, Jo˜ao Paulo Papa, Anant Madabhushi, Jacinto C. Nascimento, Jaime S. Cardoso, Vasileios Belagiannis, and Zhi Lu, editors, Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support, 2017. [39] Daniel K Sodickson and Warren J Manning. Simultaneous acquisition of spatial harmonics (SMASH): fast imaging with radiofrequency coil arrays. Magnetic resonance in medicine, 38(4), 1997. [40] Richard Szeliski. Computer vision algorithms and applications. Springer, 2011. [41] Patrick C Teo and David J Heeger. Perceptual image distortion. In IEEE International Conference on Image Processing (ICIP), volume 2, 1994. [42] Tijmen Tieleman and Geoffrey Hinton. Lecture 6.5 - rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning, 2012. [43] Mark Tygert and Jure Zbontar. Simulating single-coil MRI from the responses of multiple coils. arXiv preprint, 2018. [44] Martin Uecker, Patrick Virtue, Frank Ong, Mark J. Murphy, Marcus T. Alley, Shreyas S. Vasanawala, and Michael Lustig. Software toolbox and programming library for compressed sensing and parallel imaging. In ISMRM Workshop on Data Sampling and Image Reconstruction, 2013. [45] Martin Uecker, Peng Lai, Mark J Murphy, Patrick Virtue, Michael Elad, John M Pauly, Shreyas S Vasanawala, and Michael Lustig. ESPIRiT -an eigenvalue approach to autocalibrating parallel MRI: where SENSE meets GRAPPA. Magnetic resonance in medicine, 71(3), 2014. [46] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint, 2016. [47] Dmitry Ulyanov, Andrea Vedaldi, and Victor S. Lempitsky. Deep image prior. arXiv preprint, 2017. [48] Shanshan Wang, Zhenghang Su, Leslie Ying, Xi Peng, Shun Zhu, Feng Liang, Dagan Feng, and Dong Liang. Accelerating magnetic resonance imaging via deep learning. In IEEE International Symposium on Biomedical Imaging (ISBI), 2016. [49] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. Highresolution image synthesis and semantic manipulation with conditional GANs. Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [50] Xiaosong Wang, Yifan Peng, Le Lu, Zhiyong Lu, Mohammadhadi Bagheri, and Ronald Summers. ChestX-ray8: Hospital-scale chest X-ray database and benchmarks on weakly-supervised classiﬁcation and localization of common thorax diseases. In 2017 IEEE Conference on Computer Vision and Pattern Recognition(CVPR), 2017. [51] Zhou Wang and Alan C Bovik. Mean squared error: Love it or leave it? a new look at signal ﬁdelity measures. IEEE signal processing magazine, 26(1), 2009. [52] Zhou Wang, Eero P Simoncelli, and Alan C Bovik. Multiscale structural similarity for image quality assessment. In Asilomar Conference on Signals, Systems & Computers, 2003. [53] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600–612, 2004. [54] Takayuki Yamamoto, T Okada, Yasutaka Fushimi, Akira Yamamoto, Koji Fujimoto, Sachi Okuchi, Hikaru Fukutomi, Jun C. Takahashi, Takeshi Funaki, Susumu Miyamoto, Aur´elien F. Stalder, Yutaka Natsuaki, Peter Speier, and Kaori Togashi. Magnetic resonance angiography with compressed sensing: An evaluation of moyamoya disease. In PloS one, 2018. [55] Ke Yan, Xiaosong Wang, Le Lu, and Ronald Summers. Deeplesion: Automated mining of large-scale lesion annotations and universal lesion detection with deep learning. Journal of Medical Imaging, 5, 2018. [56] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. Deep ADMM-Net for compressive sensing MRI. Advances in Neural Information Processing Systems 29, 2016. [57] Yan Yang, Jian Sun, Huibin Li, and Zongben Xu. ADMM-Net: A deep learning approach for compressive sensing MRI. arXiv preprint, 2017. [58] Lin Zhang, Lei Zhang, Xuanqin Mou, David Zhang, et al. FSIM: a feature similarity index for image quality assessment. IEEE transactions on Image Processing, 20(8), 2011. [59] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for image restoration with neural networks. IEEE Transactions on Computational Imaging, 3(1), 2017. [60] Bo Zhu, Jeremiah Z. Liu, Stephen F. Cauley, Bruce R. Rosen, and Matthew S. Rosen. Image reconstruction by domain-transform manifold learning. Nature, 555(7697), 2018.\n\n10 Changelog • V1 (Nov 2018): Arxiv preprint describing the fastMRI knee dataset. • V2 (Dec 2019): Added neuro dataset. Multi-coil classical baseline (TV model) applied to brain validation data Acceleration Regularization Weight Sequence NMSE PSNR SSIM 10−4 AXT1 0.03971 31.63 0.5677 AXT1POST 0.02581 32.39 0.5814 AXT2 0.03624 30.66 0.528 AXFLAIR 0.189 26.85 0.4512 10−3 AXT1 0.03818 31.82 0.5724 AXT1POST 0.02353 32.81 0.5919 AXT2 0.03457 30.86 0.5312 4-fold AXFLAIR 0.1869 26.96 0.4651 10−2 AXT1 0.03888 31.7 0.5376 AXT1POST 0.02199 33.17 0.5522 AXT2 0.03419 30.9 0.4923 AXFLAIR 0.1886 26.75 0.4435 10−1 AXT1 0.04916 30.54 0.5193 AXT1POST 0.02956 31.84 0.5284 AXT2 0.04708 29.39 0.4651 AXFLAIR 0.1934 26.14 0.4048 10−4 AXT1 0.06911 29.01 0.4823 AXT1POST 0.05457 29.09 0.498 AXT2 0.07904 27.05 0.4426 AXFLAIR 0.4421 23.93 0.3549 10−3 AXT1 0.06721 29.13 0.488 AXT1POST 0.05287 29.24 0.5039 AXT2 0.078 27.11 0.4405 8-fold AXFLAIR 0.1869 26.96 0.4627 10−2 AXT1 0.05935 29.68 0.5145 AXT1POST 0.04514 29.92 0.5325 AXT2 0.07486 27.29 0.4336 AXFLAIR 0.3893 24.15 0.3678 10−1 AXT1 0.06322 29.35 0.5928 AXT1POST 0.04904 29.54 0.6187 AXT2 0.0874 26.6 0.495 AXFLAIR 0.2773 24.66 0.4726 Table 7: Validation set results for the classical baseline model with Total Variation regularization for the brain multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Single-coil U-Net baseline applied to knee validation data Acceleration Channels #Params NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 32 3.35M 0.0161 0.0531 33.78 29.90 0.81 0.631 64 13.39M 0.0157 0.0528 33.90 29.9 0.813 0.633 128 53.54M 0.0154 0.0525 34.01 29.95 0.815 0.634 256 214.16M 0.0154 0.0525 34.00 29.95 0.815 0.636 8-fold 32 3.35M 0.0283 0.0698 31.13 28.6 0.754 0.555 64 13.39M 0.0272 0.0693 31.30 28.63 0.758 0.558 128 53.54M 0.0265 0.0686 31.44 28.68 0.761 0.558 256 214.16M 0.0261 0.0682 31.5 28.71 0.762 0.559 Table 8: Validation set results for the U-Net baseline model trained for the single-coil task. The channels column denotes the number of output channels of the ﬁrst convolution in the model. Doubling this number of channels roughly quadruples the total number of parameters in the model. Bold-faced numbers indicate the best performance for each image quality metric. Single-coil classical and U-Net baselines applied to test data Model Acceleration NMSE PSNR SSIM Classical Model (Total Variation) 4-fold 0.0479 30.69 0.603 8-fold 0.0795 27.12 0.469 Aggregate 0.0648 28.77 0.531 U-Net 4-fold 0.0320 32.22 0.754 8-fold 0.0480 29.45 0.651 Aggregate 0.0406 30.7 0.699 Table 9: Comparison of classical and U-Net baseline performance for the single-coil task with test data Multi-coil U-Net baseline applied to knee validation data Acceleration Channels #Params NMSE PSNR SSIM PD PDFS PD PDFS PD PDFS 4-fold 32 3.35M 0.0066 0.0122 36.7 35.97 0.9192 0.8595 64 13.39M 0.0063 0.0120 36.95 36.11 0.9224 0.8615 128 53.54M 0.0057 0.0113 37.38 36.33 0.9266 0.8641 256 214.16M 0.0054 0.0112 37.58 36.39 0.9287 0.8655 8-fold 32 3.35M 0.0144 0.0197 33.31 33.82 0.8778 0.8213 64 13.39M 0.0136 0.0198 33.56 33.93 0.8825 0.8238 128 53.54M 0.0123 0.0179 34.01 34.25 0.8892 0.8277 256 214.16M 0.0120 0.0181 34.12 34.23 0.8915 0.8286 Multi-coil U-Net baseline applied to brain validation data Acceleration Channels #Params Sequence NMSE PSNR SSIM 32 3.35M AXT1 0.01498 35.67 0.9215 AXT1POST 0.013 35.43 0.9298 AXT2 0.02249 32.51 0.9112 AXFLAIR 0.1572 30.73 0.7869 64 13.39M AXT1 0.01571 35.57 0.922 AXT1POST 0.01313 35.41 0.9307 AXT2 0.02014 32.98 0.9151 4-fold AXFLAIR 0.1579 30.96 0.7917 128 53.54M AXT1 0.0142 35.92 0.9243 AXT1POST 0.01231 35.69 0.9332 AXT2 0.01855 33.34 0.9175 AXFLAIR 0.1566 30.98 0.7932 256 214.16M AXT1 0.01317 36.24 0.9275 AXT1POST 0.0111 36.11 0.9361 AXT2 0.01733 33.63 0.9207 AXFLAIR 0.1532 31.52 0.7985 32 3.35M AXT1 0.04289 31.5 0.8885 AXT1POST 0.04186 31.71 0.8816 AXT2 0.04357 30.86 0.8759 AXFLAIR 0.1594 32.86 0.8188 64 13.39M AXT1 0.04205 32.56 0.8876 AXT1POST 0.04034 31.89 0.883 AXT2 0.04248 31.1 0.8753 8-fold AXFLAIR 0.1818 30.49 0.7843 128 53.54M AXT1 0.04706 31.82 0.8804 AXT1POST 0.04005 31.47 0.8828 AXT2 0.04311 30.13 0.8806 AXFLAIR 0.2 28.97 0.7779 256 214.16M AXT1 0.0443 32.02 0.8837 AXT1POST 0.04028 31.95 0.8845 AXT2 0.04167 31.29 0.8811 AXFLAIR 0.1565 30.49 0.7805 Table 11: Validation set results for the U-Net baseline model trained for the brain multi-coil task. Bold-faced numbers indicate the best performance for each image quality metric. Multi-coil classical and U-Net baselines applied to test data Dataset Model Acceleration NMSE PSNR SSIM Knee Classical Model (Total Variation) 4-fold 0.0503 30.88 0.628 8-fold 0.0760 28.25 0.593 Aggregate 0.0633 29.54 0.610 U-Net 4-fold 0.0106 35.91 0.904 8-fold 0.0171 33.57 0.858 Aggregate 0.0139 34.7 0.881 Brain Classical Model (Total Variation) 4-fold 0.1388 27.53 0.4439 8-fold 0.03753 31.32 0.5135 Aggregate 0.0882 29.42 0.4787 U-Net 4-fold 0.0107 38.13 0.9446 8-fold 0.0233 34.52 0.9146 Aggregate 0.017 36.325 0.9296 Table 12: Comparison of classical and U-Net baseline performance for the multi-coil task with knee test data.A Raw k-space File Descriptions acquisition Acquisition protocol. For knee images this is either CORPD or CORPDF, indicating coronal proton density with or without fat saturation, respectively (see Figure 3). For Brain images this is AXFLAIR, AXT1, AXT1POST or AXT2 (see Figure 4). ismrmrd header The XML header copied verbatim from the ISMRMRD ﬁle that was used to generate the HDF5 ﬁle. It contains information about the scanner, ﬁeld of view, dimensions of k-space, and sequence parameters. patient id A unique string identifying the examination, and substituting anonymously for the patient identiﬁcation. norm, max The Euclidean norm and the largest entry of the target volume. For the multi-coil track the target volume is stored in reconstruction rss. For the single-coil track the target volume is stored in reconstruction esc. These two attributes are only available in the training and validation datasets. acceleration Acceleration factor of the undersampled k-space trajectory (either 4 or 8). This attribute is only available in the test dataset. num low frequency The number of low-frequency k-space lines in the undersampled k-space trajectory. This attribute is only available in the test dataset. The rest of this section describes the format of the HDF5 ﬁles for the multi-coil and single-coil tracks. A.1 Multi-coil Track {knee,brain} multicoil train.tar.gz Training dataset for the multi-coil track. The HDF5 ﬁles contain the following tensors: kspace Multi-coil k-space data. The shape of the kspace tensor is (number of slices, number of coils, height, width). reconstruction rss root-sum-of-squares reconstruction of the multi-coil k-space data. The shape of the reconstruction rss tensor is (number of slices, r height, r width). For knee images, height and width have been cropped to 320 x 320. {knee,brain} multicoil val.tar.gz Validation dataset for the multi-coil track. The HDF5 ﬁles have the same structure as the HDF5 ﬁles in multicoil train.tar.gz. {knee,brain} multicoil test.tar.gz Test dataset for the multi-coil track. The HDF5 ﬁles contain the following tensors: kspace Undersampled multi-coil k-space. The shape of the kspace tensor is (number of slices, number of coils, height, width). mask Deﬁnes the undersampled Cartesian k-space trajectory. The number of elements in the mask tensor is the same as the width of k-space. A.2 Single-coil Track (knee only) knee singlecoil train.tar.gz Training dataset for the single-coil track. Note that only the knee dataset has a single-coil track. The HDF5 ﬁles contain the following tensors: kspace Emulated single-coil k-space data. The shape of the kspace tensor is (number of slices, height, width). reconstruction rss root-sum-of-squares reconstruction of the multi-coil k-space that was used to derive the emulated single-coil k-space cropped to the center 320 × 320 region. The shape of the reconstruction rss tensor is (number of slices, 320, 320). reconstruction esc The inverse Fourier transform of the single-coil k-space data cropped to the center 320 × 320 region. The shape of the reconstruction esc tensor is (number of slices, 320, 320). knee singlecoil val.tar.gz Validation dataset for the single-coil track. The HDF5 ﬁles have the same structure as the HDF5 ﬁles in singlecoil train.tar.gz. knee singlecoil test.tar.gz Test dataset for the single-coil track. Note that only the knee dataset has a single-coil track. The HDF5 ﬁles contain the following tensors: kspace Undersampled emulated single-coil k-space. The shape of the kspace tensor is (number of slices, height, width). mask Deﬁnes the undersampled Cartesian k-space trajectory. The number of elements in the mask tensor is the same as the width of k-space. B Classical Reconstruction with BART The Berkeley Advanced Reconstruction Toolbox (BART) [44] 5 contains implementations of standard methods for coil sensitivity estimation and undersampled MR image reconstruction incorporating parallel imaging and compressed sensing. We used this tool to produce the classical baseline MSE estimates, as well as the illustrations in Figure 2. In this section we provide a brief introduction to the tool suﬃcient for reproducing our baseline results. We will use as an example a 640x368 undersampled MRI scan with 15 coils. The target region is a 320 × 320 central region which will be cropped to after reconstruction. BART provides a command line interface which acts on ﬁles in a simple storage format. Each multidimensional array is stored in a pair of ﬁles, a header ﬁle .hdr and a data ﬁle .cfl. The header ﬁle contains the dimensions of the array given in ASCII. In our running example, this should be input.hdr: 1 640 368 15 The CFL ﬁle contains the raw data in column-major order, stored as complex ﬂoat values. Missing k-space values are indicated by 0 entries. BART provides Python and MATLAB interfaces for reading and writing this format. When working with k-space data with BART, it is simplest to use data in ”centered” form, where the low frequency values are in the center of the image, and the high frequency values are at the edges. Most FFT libraries output the data in uncentered form. BART provides a tool for conversion: bart fftshift 7 input output The input and output are speciﬁed without ﬁle extensions. The value 7 above is a bitmask indicating the image is stored in axis 0,1,2 (1+2+4) of the input array. This bitmask is used in the commands that follow also. Uncentered k-space data is easily identiﬁed by comparing the magnitude of the corners versus the center of the array. Centered FFTs of natural data will have the largest magnitudes near the center of the array when plotted. Parallel MR imaging is often performed as a two-step process consisting of coil-sensitivity estimation, then reconstruction assuming the estimated sensitivity maps are exact. BART implements this approach through the ecalib and pics commands. The coil-sensitivity maps can be estimated using the ESPIRiT approach using the command bart ecalib -m1 Produce a single set of sensitivity maps -r26 Number of fully sampled reference lines input output_sens The central reference region is used by BART to estimate the coil sensitivities. This area is also known as the auto-calibration region. The number of lines used in our masking procedure is a percentage of the k-space width, as described in Section 4.2. Given the estimated coil sensitivities, a reconstruction using TV regularization can be performed with bart pics -d4 Debug log level, use 0 for no stdout output -i200 Optimization iterations -R T:7:0:0.05 Use TV (T) with regularizer strength 0.05, with bitmask 7 input output_sens output The output of this command is in CFL format. It can be converted to a PNG using bart toimg. When using L1 wavelet regularization, the character ”W” should be used in the R option, with the additional -m argument to ensure that ADMM is used.\n--- FOOTNOTES 1 Facebook AI Research 2 NYU School of Medicine 3 NYU Center for Data Science 4 University of Florida 2D TSE protocol employed clinically at NYU School of Medicine. The dataset includes data from 3T direct correspondence to the sequences represented in raw data. We acknowledge grant support from the National Institutes of Health under grants NIH R01", "persona": "Student", "response": "### 1. Overview\nThe imaging dataset used in the project was collected exclusively by NYU School of Medicine researchers. In addition to 10,000 knee MRIs, the dataset also includes raw imaging data from nearly 1,600 cases. The fastMRI project is governed by strict human subject data protection protocols, overseen by NYU School of Medicine’s Institutional Review Boards. The work is fully HIPAA compliant and is supported by the world-class information technology team at NYU Langone. All data are fully anonymized, stripped of all protected health information including any patient identifiers and potentially distinguishing features. No Facebook data of any kind are used in the project. “This collaboration focuses on applying the strengths of machine learning to reconstruct high-value images in new ways.\n\n### 2. Background\nRather than using existing images to train AI algorithms, we will radically change the way medical images are acquired in the first place,” says Daniel K. Sodickson, MD, PhD, professor of radiology and neuroscience and physiology and director of CAI2R. “Our aim is not merely enhanced data mining with AI, but rather creating new capabilities for medical visualization, to benefit human health.” Pursuing a Faster MRI Scan with FAIR While MRIs provide doctors and patients with a wealth of critical information, capturing unprecedented detail in soft tissues such as organs and blood vessels, they are notoriously slow tests. An MRI scan can take 30 to 45 minutes, and sometimes more than an hour depending on the reason for the scan.\n\n### 3. Methods\nThis collaboration will promote research reproducibility, provide consistent evaluation methods, and empower the larger community of AI and medical imaging scientists. The joint team will also provide a suite of tools, including baseline metrics to compare results, and a leaderboard to keep track of progress as part of an organized challenge to be announced in the near future. About the Dataset While other sets of radiological images have been released previously, this dataset represents the largest public release of raw MRI data to date. The first phase of the project will involve data from knee MRI scans, but future releases will include data from liver and brain scans.\n\n### 4. Results\nNYU Langone’s Department of Radiology, through the NYU School of Medicine, is releasing the first large-scale MRI dataset of its kind as part of fastMRI, a collaborative effort with Facebook AI Research (FAIR) to speed up MRI scans with artificial intelligence (AI). This initial dataset release includes more than 1.5 million anonymous MR images of the knee, drawn from 10,000 scans, in addition to raw measurement data from nearly 1,600 scans. The data release is the latest phase of a collaboration, called fastMRI, between the Department of Radiology’s Center for Advanced Imaging Innovation and Research (CAI2R) and Facebook AI Research (FAIR), aimed at sharing open source tools and spurring the development of AI systems to make MRI scans 10 times faster. “We hope that the release of this landmark dataset, the largest-ever collection of fully-sampled MRI raw data, will provide researchers with the tools necessary to overcome the challenges inherent in accelerating MR imaging. This work has the potential to not only help increase access to MR imaging, but also improve patient care worldwide,” says Michael P.\n\n### 5. Takeaways\nRecht, MD, chair and the Louis Marx Professor of Radiology at NYU Langone. Dr. Recht announced the release of the dataset on Sunday, November 25, 2018 during a plenary address at the 2018 Annual Meeting of the Radiological Society of North America (RSNA). While reiterating the goal of even greater acceleration, Recht cited baseline results from FAIR and NYU Langone showing that acceleration of MRI by a factor of four is already possible. He emphasized, though, that broad validation of such early results is essential. “fastMRI not only could have an important impact in the medical field, it’s also an interesting research challenge that will help to advance the field of AI,” said Larry Zitnick, research manager, Facebook AI Research. “To be medically useful, our AI-reconstructed images need to be more than just good looking, they must also be accurate representations of the ground truth, even though they're created from significantly less data.", "paper_id": "48", "story_id": "48-03"}
{"paper": "Medical artifcial intelligence (AI) ofers great potential for recognizing signs of health conditions in retinal images and expediting the diagnosis of eye diseases and systemic disorders1 . However, the development of AI models requires substantial annotation and models are usually task-specifc with limited generalizability to diferent clinical applications2 . Here, we present RETFound, a foundation model for retinal images that learns generalizable representations from unlabelled retinal images and provides a basis for label-efcient model adaptation in several applications. Specifcally, RETFound is trained on 1.6 million unlabelled retinal images by means of self-supervised learning and then adapted to disease detection tasks with explicit labels. We show that adapted RETFound consistently outperforms several comparison models in the diagnosis and prognosis of sight-threatening eye diseases, as well as incident prediction of complex systemic disorders such as heart failure and myocardial infarction with fewer labelled data. RETFound provides a generalizable solution to improve model performance and alleviate the annotation workload of experts to enable broad clinical AI applications from retinal imaging.\n\nMedical artificial intelligence (AI) has achieved significant progress in recent years with the notable evolution of deep learning techniques1,3,4 . For instance, deep neural networks have matched or surpassed the accuracy of clinical experts in various applications5 , such as referral recommendations for sight-threatening retinal diseases6  and pathology detection in chest X-ray images7 . These models are typically developed using large volumes of high-quality labels, which requires expert assessment and laborious workload1,2 . However, the scarcity of experts with domain knowledge cannot meet such an exhaustive requirement, leaving vast amounts of medical data unlabelled and unexploited. Self-supervised learning (SSL) aims to alleviate data inefficiency by deriving supervisory signals directly from data, instead of resorting to expert knowledge by means of labels8–11. SSL trains models to perform ‘pretext tasks’ for which labels are not required or can be generated automatically. This process leverages formidable amounts of unlabelled data to learn general-purpose feature representations that adapt easily to more specific tasks. Following this pretraining phase, models are fine-tuned to specific downstream tasks, such as classification or segmentation. The SSL model has outperformed supervised learning-based transfer learning (for example, pretraining the models with ImageNet12 and categorical labels) in various computer vision tasks, even when the SSL models are fine-tuned with smaller amounts of data13,14. Besides this label efficiency, SSL-based models perform better than supervised models when tested on new data from different domains15,16. The combined qualities of strong generalization capacity of representations, and high performance achieved by fine-tuned models in many downstream tasks, indicate the great potential of SSL in medical AI in which data are abundant and healthcare tasks are diverse but labels are scarce1,8 . Colour fundus photography (CFP) and optical coherence tomography (OCT) are the most common imaging modalities in ophthalmology and such retinal images accumulate quickly in routine clinical practice. In addition to illustrating clinical features associated with ocular diseases, these images also provide valuable insights into systemic diseases, a field that has recently been termed ‘oculomics’17,18. For example, the optic nerve and inner retinal layers provide a non-invasive view of central nervous system tissue19–21, and thus a window into neurodegeneration. Similarly, retinal vascular geometry provides insights into other vascular organ systems22–25, such as the heart and kidneys. Although several studies have shown that SSL can increase performance for individual ocular disease detection tasks, such as the diagnosis of diabetic macular oedema26, age-related macular degeneration (AMD)27 and referable diabetic retinopathy28–30, there has been limited work demonstrating the ability of a single SSL pretrained model to generalize to a diverse range of complex tasks. Progress has probably been hampered by the challenges involved with curating a large repository of retinal images with extensive linkage to several relevant disease outcomes. Moreover, the capabilities of different SSL approaches (contrastive SSL versus generative SSL) and the interpretability of SSL models in retinal imaging, remain relatively under-explored. Developing an understanding of the specific features that SSL models learn during training is an important step for safe and reliable translation to clinical practice. In this work, we present a new SSL-based foundation model for retinal images (RETFound) and systematically evaluate its performance and generalizability in adapting to many disease detection tasks. A foundation model is defined as a large AI model trained on a vast quantity of unlabelled data at scale resulting in a model that can be adapted to a wide range of downstream tasks31,32. Here we construct RETFound from large-scale unlabelled retinal images by means of SSL and use it to promote the detection of many diseases. Specifically, we develop two separate RETFound models, one using CFP and the other using OCT, by means of an advanced SSL technique (masked autoencoder15) successively on natural images (ImageNet-1k) followed by retinal images from the Moorfields diabetic image dataset (MEH-MIDAS) and public data (totalling 904,170 CFPs and 736,442 OCTs). We adapt RETFound to a series of challenging detection and prediction tasks by fine-tuning RETFound with specific task labels, and then validate its performance. We consider first the diagnostic classification of ocular diseases, including diabetic retinopathy and glaucoma; second, ocular disease prognosis, specifically conversion of contralateral (‘fellow’) eyes to neovascular (‘wet’) AMD in a 1-year time period and, finally, oculomic challenges, specifically the 3-year prediction of cardiovascular diseases (ischaemic stroke, myocardial infarction and heart failure) and a neurodegenerative disease (Parkinson’s disease). RETFound achieves consistently superior performance and label efficiency in adapting to these tasks, compared to state-of-the-art competing models, including that pretrained on ImageNet-21k with traditional transfer learning. We also probe the interpretation of disease detection performance of RETFound with qualitative results and variable-controlling experiments, showing that salient image regions reflect established knowledge from ocular and oculomic literature. Finally, we make RETFound publicly available so others can use it as the basis for their own downstream tasks, facilitating diverse ocular and oculomic research. Figure 1 gives an overview of the construction and application of RETFound. For construction of RETFound, we curated 904,170 CFP in which 90.2% of images came from MEH-MIDAS and 9.8% from Kaggle EyePACS33, and 736,442 OCT in which 85.2% of them came from MEH-MIDAS and 14.8% from ref. 34. MEH-MIDAS is a retrospective dataset that includes the complete ocular imaging records of 37,401 patients with diabetes who were seen at Moorfields Eye Hospital between January 2000 and March 2022. After self-supervised pretraining on these retinal images, we evaluated the performance and generalizability of RETFound in adapting to diverse ocular and oculomic tasks. We selected publicly available datasets for the tasks of ocular disease diagnosis. Details are listed in Supplementary Table 1. For the tasks of ocular disease prognosis and systemic disease prediction, we used a cohort from the Moorfields AlzEye study (MEH-AlzEye) that links ophthalmic data of 353,157 patients, who attended Moorfields Eye Hospital between 2008 and 2018, with systemic disease data from hospital admissions across the whole of England35. We also used UK Biobank36 for external evaluation in predicting systemic diseases. The validation datasets used for ocular disease diagnosis are sourced from several countries, whereas systemic disease prediction was solely validated on UK datasets due to limited availability of this type of longitudinal data. Our assessment of generalizability for systemic disease prediction was therefore based on many tasks and datasets, but did not extend to vastly different geographical settings. Details of the clinical datasets are listed in Supplementary Table 2 (data selection is introduced in the Methods section). We compared the performance and label efficiency of RETFound against three pretrained comparison models: SL-ImageNet, SSL-ImageNet and SSL-Retinal. All models use differing pretraining strategies but have the same model architecture as well as fine-tuning processes for downstream tasks (architecture details are introduced in the Methods section). SL-ImageNet uses traditional transfer learning, that is, pretraining the model by means of supervised learning on ImageNet-21k (about 14 million natural images with categorical labels); SSL-ImageNet pretrains the model by means of SSL on ImageNet-1k (about 1.4 million natural images) and SSL-Retinal pretrains the model using SSL on retinal images from scratch. RETFound uses the weights of SSL-ImageNet as a baseline before extending to retinal images (equivalent to pretraining the model by means of SSL successively on natural images followed by retinal images). The pretraining schematics are shown in Extended Data Fig. 1. Furthermore, we explored the performance of using different SSL strategies, that is, generative SSL versus contrastive SSL approaches, by substituting the primary SSL technique (that is, masked autoencoder) for SimCLR16, SwAV37, DINO38 and MoCo-v3 (ref. 14) within the RETFound framework, respectively. We reported internal and external evaluation results for these models. The models were adapted to each task with labelled training data, and evaluated on both held-out internal test sets, as well as external datasets completely distinct from the training data (details are listed in the Methods section). Model performance was reported using the area under the receiver operating curve (AUROC) and area under the precision-recall curve (AUPR). We calculated P values with the two-sided t-test between RETFound and the most competitive comparison model for each task to check for significance.\n\nOcular disease diagnosis We included eight publicly available datasets to verify the model’s performance on several ocular diseases and imaging conditions (Fig. 2). RETFound generally achieved the best performance in most datasets and SL-ImageNet ranked second, as shown in Fig. 2a. For instance, on diabetic retinopathy classification, RETFound achieved AUROC of 0.943 (95% confidence interval (CI) 0.941, 0.944), 0.822 (95% CI 0.815, 0.829) and 0.884 (95% CI 0.88, 0.887), respectively, on Kaggle APTOS2019, IDRID39 and MESSIDOR-2 (refs. 40,41) datasets, significantly outperforming SL-ImageNet (all P < 0.001). The superior performance can also be observed for glaucoma and the classification of many diseases. The AUPR results of RETFound were also significantly higher than the compared groups (Extended Data Fig. 2a). For external evaluation, we evaluated the performance of RETFound on diabetic retinopathy datasets (Kaggle APTOS-2019, IDRID and MESSIDOR-2), which were both labelled on the basis of the five-stage International Clinical Diabetic Retinopathy Severity scale. We conducted cross evaluation among the three datasets, that is, fine-tuned models on one dataset and evaluated them on the others. RETFound achieved the best performance in all cross evaluations, as shown in Fig. 2b. For instance, when fine-tuned on Kaggle APTOS-2019, RETFound achieved AUROC of 0.822 (95% CI 0.815, 0.829) and 0.738 (95% CI 0.729, 0.747), respectively, on IDRID and MESSIDOR-2 datasets, statistically significantly higher than SL-ImageNet (P < 0.001) on IDRID and SSL-ImageNet (P < 0.001) on MESSIDOR-2. The AUPR results of all groups were low but RETFound achieved significantly higher performance (Extended Data Fig. 2b). All quantitative results are listed in Supplementary Table 3. Ocular disease prognosis For 1-year prognosis of fellow eye converting to wet-AMD, we evaluated the internal performance on data from AlzEye (Fig. 2c). With CFP as the input modality, RETFound showed the best performance with an AUROC of 0.862 (95% CI 0.86, 0.865), significantly outperforming the comparison groups (P < 0.001). The runner-up SL-ImageNet achieved an AUROC of 0.83 (95% CI 0.825, 0.836). With OCT, RETFound scored the highest AUROC of 0.799 (95% CI 0.796, 0.802), showing a statistically significantly higher AUROC (P < 0.001) than SSL-Retinal. The AUPR results of RETFound are highest with CFP and comparable to SSL-Retinal with OCT (Extended Data Fig. 2c). Systemic diseases prediction We organized four oculomic tasks to evaluate the model performance in predicting the incidence of systemic diseases with retinal images (Fig. 3). Although the overall performance was limited in these challenging tasks, RETFound has shown significant improvement in internal evaluation for both CFP and OCT, as shown in Fig. 3a. For the prediction of myocardial infarction with CFP, RETFound achieved AUROC of 0.737 (95% CI 0.731, 0.743). SSL-Retinal scored the second-best performance but was significantly worse than RETFound (P < 0.001). The confusion matrix (Extended Data Table 1) shows that RETFound achieved the highest sensitivity of 0.7 and specificity of 0.67. Likewise, RETFound also ranked first for prediction of heart failure, ischaemic stroke and Parkinson’s disease with AUROCs of 0.794 (95% CI 0.792, 0.797), 0.754 (95% CI 0.752, 0.756) and 0.669 (0.65, 0.688), respectively. RETFound also performed significantly better than the other models when using OCT as the input modality. It achieved significantly higher AUPR results in all tasks (Extended Data Fig. 3a). External evaluation on the UK Biobank (Fig. 3b) showed that RETFound and SSL-Retinal performed similarly in prediction of ischaemic stroke. For tasks of myocardial infarction, heart failure and Parkinson’s disease, RETFound achieved the best performance both with CFP and OCT. RETFound also showed significantly higher AUPR in most tasks when it was externally evaluated on UK Biobank (Extended Data Fig. 3b). Label efficiency for disease detection Label efficiency refers to the amount of training data and labels required to achieve a target performance level for a given downstream task, which indicates the annotation workload for medical experts. RETFound showed superior label efficiency across various tasks (Fig. 4). For heart failure prediction, RETFound outperformed the other pretraining strategies using only 10% of labelled training data, demonstrating the potential of this approach in alleviating data shortages. RETFound similarly showed superior label efficiency for diabetic retinopathy classification and myocardial infarction prediction. Furthermore, RETFound showed consistently high adaptation efficiency (Extended Data Fig. 4), suggesting that RETFound required less time in adapting to downstream tasks. For example, RETFound can potentially save about 80% of the training time required to achieve convergence for the task of predicting myocardial infarction, leading to significant reductions in computational costs (for example, credits on Google Cloud Platform) when appropriate mechanisms such as early stopping are used. SSL strategies for RETFound We explored the performance of different SSL strategies, that is, generative SSL (for example, masked autoencoder) and contrastive SSL (for example, SimCLR, SwAV, DINO and MoCo-v3), in the RETFound framework. As shown in Fig. 5, RETFound with different contrastive SSL strategies showed decent performance in downstream tasks. For instance, RETFound with DINO achieved AUROC of 0.866 (95% CI 0.864, 0.869) and 0.728 (95% CI 0.725, 0.731), respectively, on wet-AMD prognosis (Extended Data Fig. 5) and ischaemic stroke prediction (Fig. 5), outperforming the baseline SL-ImageNet (Supplementary Tables 3 and 4). This demonstrates the effectiveness of RETFound framework with diverse SSL strategies. Among these SSL strategies, the masked autoencoder (primary SSL strategy for RETFound) performed significantly better than the contrastive learning approaches in most disease detection tasks (Fig. 5 and Extended Data Fig. 5). All quantitative results are listed in Supplementary Table 4.\n\nModel interpretation To gain insights into the inner-workings of RETFound leading to its superior performance and label efficiency in downstream tasks, we performed qualitative analyses of the pretext task used for self-supervised pretraining and task-specific decisions of RETFound (Extended Data Fig. 6). The pretext task of RETFound allows models to learn retina-specific context, including anatomical structures and disease lesions. As shown in Extended Data Fig. 6a, RETFound was able to reconstruct major anatomical structures, including the optic nerve and large vessels on CFP, and the nerve fibre layer and retinal pigment epithelium on OCT, despite 75% of the retinal image being masked. This demonstrates that RETFound has learned to identify and infer the representation of disease-related areas by means of SSL, which contributes to performance and label efficiency in downstream tasks. On top of the reconstruction-based interpretation, we further used an advanced explanation tool (RELPROP42) to visualize the salient regions of images conducive to classifications made by fine-tuned models in downstream tasks (Extended Data Fig. 6b). For ocular disease diagnosis, well-defined pathologies were identified and used for classification, such as hard exudates and haemorrhage for diabetic retinopathy and parapapillary atrophy for glaucoma. For oculomic tasks, we observed that anatomical structures associated with systemic conditions, such as the optic nerve on CFP and nerve fibre layer and ganglion cell layer on OCT, were highlighted as areas that contributed to the incidence prediction of systemic diseases (Extended Data Fig. 6b). Robustness to age distribution shifts For ageing-associated systemic diseases, clinically relevant anatomical structures alter with both ageing43,44 and disease progression19,20,22. RETFound was trained to identify general structure alterations for detection of systemic diseases (Extended Data Fig. 6b). To further verify the extent to which models can learn anatomical structure changes, respectively, relating to ageing and disease progression, we evaluated performance of the models when using four different control groups with varying ages (mean ages 66.8, 68.5, 70.4 and 71.9 years) versus a fixed disease group (mean age 72.1 years) in the task of myocardial infarction. As shown in Extended Data Fig. 7, the models showed better performance when the age difference is larger, indicating that age is indeed a confounder for studying ageing-associated diseases. The contribution of age can be demonstrated by the extreme case in which the age difference between cohorts is maximal (5.3 years in our scenario), at which point a simple logistic regression with the input of age achieved an AUROC of 0.63, surpassing SSL-ImageNet and SL-ImageNet. When the age difference decreased, the models clearly outperformed the logistic regression. We observed that RETFound kept stable performance even when the age difference decreased, which suggested that RETFound well identified the disease-related anatomical structure alteration and used the information for predicting systemic diseases.\n\nDiscussion This work introduces a new SSL-based foundation model, RETFound, and evaluates its generalizability in adapting to diverse downstream tasks. After training on large-scale unlabelled retinal images using an advanced SSL technique (masked autoencoder), RETFound can be efficiently adapted to a broad range of disease detection tasks, resulting in significant performance improvements for detecting ocular diseases and predicting cardiovascular and neurodegenerative diseases. It is a medical foundation model that has been developed and assessed, and shows considerable promise for leveraging such multidimensional data without constraints of enormous high-quality labels. RETFound enhances the performance of detecting ocular diseases by learning to identify disease-related lesions. Ocular diseases are diagnosed by the presence of well-defined pathological patterns, such as hard exudates and haemorrhages for diabetic retinopathy. These features involve abnormal variations in colour or structure, showing visible differences from the surrounding retina. RETFound can identify disease-related patterns and correctly diagnose ocular diseases (for example, myopia and diabetic retinopathy cases in Extended Data Fig. 6b). In Fig. 2, we observe that RETFound ranks first in various tasks, followed by SL-ImageNet. SL-ImageNet pretrains the model using supervised learning on ImageNet-21k, which contains 14 million images with 21,000 categories of natural objects with diverse shapes and textures, such as zebras and oranges. Such diverse characteristics allow models to learn abundant low-level features (for example, lines, curves and edges) to identify the boundary of abnormal patterns, thus improving disease diagnosis when the model adapts to medical tasks. In this paper, we demonstrate that by using SSL successively on natural images and unlabelled retinal images, a generalizable foundation model (RETFound) can be developed to further improve ocular disease diagnosis and prognosis, even outperforming the powerful SL-ImageNet. RETFound learns retina-specific context by SSL on unlabelled retinal data to improve the prediction of systemic health states. RETFound and SSL-Retinal rank top 2 in both internal and external evaluation in predicting systemic diseases by using SSL on unlabelled retinal images (Fig. 3). In pretraining RETFound learns representations by performing a pretext task involving the reconstruction of an image from its highly masked version, requiring the model to infer masked information with limited visible image patches. Solving such a pretext task in retinal images allows the model to learn retina-specific context, including anatomical structures such as the optic nerve and retinal nerve fibre layer (Extended Data Fig. 6a) that are potential markers in retinal images for neurodegenerative diseases and cardiovascular diseases17,19,21,45. The confusion matrix shows that RETFound achieves the highest sensitivity (Extended Data Table 1), indicating that more individuals with a high risk of systemic diseases are identified. The evaluation on oculomic tasks demonstrates the use of retinal images for incidence prediction and risk stratification of systemic diseases, significantly promoted by RETFound. Compared to SSL-Retinal and SSL-ImageNet, RETFound shows consistently better performance for disease detection (Figs. 2 and 3 and Supplementary Table 3), thus demonstrating SSL on retinal and natural images is complementary to developing the powerful foundation model. The strategy of combining natural images and medical data in model development has also been validated in other medical fields, such as chest X-rays6  and dermatology imaging46. We also conducted calibration analyses for prediction models in oculomic tasks, which examines the agreement between predicted probabilities and real incidence. A well-calibrated model can provide a meaningful and reliable disease prediction as the predicted probability indicates the real likelihood of disease occurrence, enabling the risk stratification of diseases47,48. We observed that RETFound was better calibrated compared to other models and showed the lowest expected calibration error in the reliability diagram (Extended Data Fig. 8). This verifies that RETFound generates reliable predicted probabilities, rather than overconfident ones. The experiments show that both modalities of CFP and OCT have unique ocular and systemic information encoded that is valuable in predicting future health states. For ocular diseases, some image modalities are commonly used for a diagnosis in which the specific lesions can be well observed, such as OCT for wet-AMD. However, such knowledge is relatively vague in oculomic tasks as (1) the markers for oculomic research on different modalities are under exploration and (2) it requires a fair comparison between many modalities with identical evaluation settings. In this work, we investigate and compare the efficacy of CFP and OCT for oculomic tasks with identical training and evaluation details (for example, train, validation and/or test data splitting is aligned by anonymous patient IDs). We notice that the models with CFP and OCT achieve unequal performances in predicting systemic diseases (Fig. 3 and Supplementary Table 3), suggesting that CFP and OCT contain different levels of information for oculomic tasks. For instance, in 3-year incidence prediction of ischaemic stroke, RETFound with CFP performs better than with OCT on both MEH-AlzEye (internal evaluation) and UK Biobank (external evaluation). For the task of Parkinson’s disease, RETFound with OCT shows significantly better performance in internal evaluation. These observations may indicate that various disorders of ageing (for example, stroke and Parkinson’s disease) manifest different early markers on retinal images. A practical implication for health service providers and imaging device manufacturers is to recognize that CFP has continuing value, and should be retained as part of the standard retinal assessment in eye health settings. This observation also encourages oculomic research to investigate the strength of association between systemic health with the information contained in several image modalities. There is a significant fall in performance when adapted models are tested against new cohorts that differ in the demographic profile, and even on the imaging devices that were used (external evaluation phase). This phenomenon is observed both in the external evaluation of ocular disease diagnosis (Fig. 2b) and systemic disease prediction (Fig. 3b). For example, the performance on ischaemic stroke drops (RETFound’s AUROC decreases by 0.16 with CFP and 0.19 with OCT). In the challenging oculomic tasks, the age and ethnicity profile of the internal and external validation cohorts (MEH-AlzEye and UK Biobank) as well as the imaging devices are significantly different (Supplementary Table 2), and this is likely to be reflected in the drop in performance when externally evaluated in the UK Biobank cohort. Compared to other models, RETFound achieves significantly higher performance in external evaluation in most tasks (Fig. 3b) as well as different ethnicities (Extended Data Figs. 9–11), showing good generalizability. We observe that RETFound maintains competitive performance for disease detection tasks, even when substituting various contrastive SSL approaches into the framework (Fig. 5 and Extended Data Fig. 5). It seems that the generative approach using the masked autoencoder generally outperforms the contrastive approaches, including SwAV, SimCLR, MoCo-v3 and DINO. However, it is worth noting that asserting the superiority of the masked autoencoder requires caution, given the presence of several variables across all models, such as network architectures (for example, ResNet-50 for SwAV and SimCLR, Transformers for the others) and hyperparameters (for example, learning rate scheduler). Our comparison demonstrates that the combination of powerful network architecture and complex pretext tasks can produce effective and general-purpose medical foundation models, aligning with the insights derived from large language models in healthcare49,50. Furthermore, the comparison further supports the notion that the retinal-specific context learned from the masked autoencoder’s pretext task, which includes anatomical structures such as the optic nerve head and retinal nerve fibre layer (as shown in Extended Data Fig. 6a), indeed provides discriminative information for the detection of ocular and systemic diseases. We believe that research on medical foundation models, such as RETFound, has the potential to democratize access to medical AI and accelerate progress towards widespread clinical implementation. To this end, foundation models must learn powerful representations from enormous volumes of medical data (1.6 million retinal images in our case), which is often only accessible to large institutions with efficient dataset curation workflows. Also, SSL pretraining of foundation models requires many computational resources to achieve training convergence. We used eight NVIDIA Tesla A100 (40 GB) graphical processing units (GPUs) on the Google Cloud Platform, requiring 2 weeks of developing time. By contrast, the data and computational requirements required to fine-tune RETFound to downstream tasks are comparatively small and therefore more achievable for most institutions. We required only one NVIDIA Tesla T4 (16 GB) GPU, requiring about 1.2 h with a dataset of 1,000 images. Moreover, foundational models offer the potential to raise the general quality of healthcare AI models. Their adoption may help avoid superficially impressive models that rarely affect clinical care. These poorly generalizable models consume significant resources and can feed scepticism about the benefits of AI in healthcare. By making RETFound publicly available, we hope to accelerate the progress of AI in medicine by enabling researchers to use our large dataset to design models for use in their own institutions or to explore alternative downstream applications. Although this work systematically evaluates RETFound in detecting and predicting diverse diseases, there are several limitations and challenges requiring exploration in future work. First, most data used to develop RETFound came from UK cohorts, therefore it is worth exploring the impact of introducing a larger dataset by incorporating retinal images worldwide, with more diverse and balanced data distribution. Second, although we study the performance with modalities of CFP and OCT, the multimodal information fusion between CFP and OCT has not been investigated, which might lead to further improvement in performance. Finally, some clinically relevant information, such as demographics and visual acuity that may work as potent covariates for ocular and oculomic research, has not been included in SSL models. Combining these, we propose to further enhance the strength of RETFound in subsequent iterations by introducing even larger quantities of images, exploring further modalities and enabling dynamic interaction across multimodal data. While we are optimistic about the broad scope of RETFound to be used for a range of AI tasks, we also acknowledge that enhanced human–AI integration is critical to achieving true diversity in healthcare AI applications. In conclusion, we have verified the efficacy and efficiency of RETFound in adapting to diverse healthcare applications, showing high performance and generalizability in detecting ocular diseases and significant improvement in predicting systemic diseases. By overcoming current barriers to clinical AI applications—notably, the extent of labelled data and limited performance and generalizability—SSL-based foundation models open the door to accelerated, data-efficient devices that may transform care for patients with ocular or systemic diseases.\n\nMethods    Datasets for developing RETFound    We curate large collections of unannotated retinal images for SSL, totalling 904,170 CFPs and 736,442 OCT scans. Of these, 815,468 (90.2%)    CFPs and 627,133 (85.2%) OCTs are from Moorfields Diabetic imAge    dataSet (MEH-MIDAS), and 88,702 (9.8%) CFPs are Kaggle EyePACS and    109,309 (14.8%) OCTs that come from ref. 34. MEH-MIDAS is a retrospective dataset that includes the complete ocular imaging records of 37,401    patients (16,429 female, 20,966 male and six unknown) with diabetes    who were seen at Moorfields Eye Hospital, London, UK between 2000    and 2022. The age distribution has a mean value of 64.5 and standard    deviation of 13.3. The ethnicity distributes diversly: British (13.7%),    Indian (14.9%), Caribbean (5.2%), African (3.9%), other ethnicity (37.9%)    and not stated (24.4%). MEH-MIDAS includes various imaging devices,    such as topcon 3DOCT-2000SA (Topcon), CLARUS (ZEISS) and Triton    (Topcon). EyePACS includes images devices of Centervue DRS    (Centervue), Optovue iCam (Optovue), Canon CR1/DGi/CR2 (Canon) and    Topcon NW (Topcon). Reference 34 contains images from SPECTRALIS    (Heidelberg).    Data for ocular disease diagnosis    We evaluate the model performance on three different categories of    disease detection tasks. The first category of tasks involves diagnostic    classification of ocular diseases with publicly available ophthalmic data.    For diabetic retinopathy diagnosis, Kaggle APTOS-2019 (India), IDRID    (India) and MESSIDOR-2 (France) are used. The labels for diabetic retinopathy are based on the International Clinical Diabetic Retinopathy    Severity scale, indicating five stages from no diabetic retinopathy to    proliferative diabetic retinopathy. For glaucoma, PAPILA51 (Spain) and    Glaucoma Fundus52 (South Korea) are included. Glaucoma Fundus and    PAPILA have three categorical labels, non-glaucoma, early glaucoma    (suspected glaucoma) and advanced glaucoma. For datasets with several diseases, JSIEC53 (China), Retina and OCTID54 (India) are included.    JSIEC includes 1,000 images with 39 categories of common referable    fundus diseases and conditions. Retina has labels of normal, glaucoma,    cataract and retina disease. OCTID includes 470 OCT scans with labels    of normal, macular hole, AMD, central serous retinopathy and diabetic    retinopathy. The grading protocols for the public datasets are summarized as: IDRiD, two medical experts provided adjudicated consensus    grades; MESSIDOR-2, adjudicated by a panel of three retina specialists    in accordance with a published protocol55; APTOS-2019, Kaggle dataset    with limited information but possibly a single clinician grader; PAPILA,    labelling and segmentation by two experts following extensive clinical    examination and testing procedure including a retrospective clinical    record review; Glaucoma Fundus, agreement of two specialists based    on visual fields and extensive imaging and JSIEC, labelled by ophthalmologists and confirmed by senior retina specialists. Disagreements    resolved by panel of five senior retina specialists were as follows: Retina,    details not available and OCTID, describes image labelling based on    the diagnosis of retinal clinical experts but does not specify duplicate    adjudication. The details of datasets, such as imaging devices, country    and label category, are listed in Supplementary Table 1.    Data for disease prognosis and prediction    For disease prognosis of fellow eye converting to wet-AMD in 1 year, we    use data from the Moorfields AlzEye study (MEH-AlzEye). MEH-AlzEye    is a retrospective cohort study linking ophthalmic data of 353,157    patients, who attended Moorfields Eye Hospital between 2008 and    2018, with systemic health data from hospital admissions across the    whole of England. Systemic health data are derived from Hospital    Episode Statistics (HES) data relating to admitted patient care, with    a focus on cardiovascular disease and all-cause dementia. Diagnostic    codes in HES admitted patient care are reported according to the tenth    revision of the ICD (International Statistical Classification of Diseases)56.    In line with previous reports, we selected the study cohort using ICD    code: stroke (I23-I24), myocardial infarction (I21-I22), heart failure    (I50) and Parkinson’s disease (G20). Among 186,651 patients with HES,    6,504 patients are diagnosed with wet-AMD in at least one eye, 819    patients have retinal imaging within 1 year before their fellow eyes convert to wet-AMD and 747 patients with their fellow eyes not converting    wet-AMD, after excluding other eye diseases. The final category of    tasks studies the 3-year prediction of systemic diseases, with a focus    on cardiovascular and neurodegenerative dysfunctions, using the    MEH-AlzEye and UK Biobank. The UK Biobank includes 502,665 UK    residents aged between 40 and 69 years who are registered with the    National Health Service. Among all participants, 82,885 get CFP and    OCT examinations and a total of 171,500 retinal images are collected.    For each patient, we only include the retinal image from the left eye in    one visit, to avoid potential bias by inconsistent individual visits. For    internal evaluation, we split the patient groups into training, validation    and test sets at a ratio of 55:15:30%. The training set is used to revise    model parameters to achieve objective function. The validation set is    for monitoring training converge and checkpoint selection. The test    set is used to test the saved model checkpoint and evaluate the internal    performance. For external validation, all patient data are used for    evaluating the saved model checkpoint. The detailed data flowcharts    are listed in Supplementary Figs. 1–5.    Data processing and augmentation for SSL    For CFP image preprocessing, we use AutoMorph57, an automated retinal image analysis tool, to exclude the background and keep the retinal    area. All images are resized to 256 × 256 with cubic interpolation. For    OCT, we extract the middle slices and resize them to 256 × 256. We follow the same data augmentation as the masked autoencoder in model    training, including random crop (lower bounds 20% of the whole image    and upper bounds 100%) and resizing the cropped patches to 224 × 224,    random horizontal flipping and image normalization.    RETFound architecture and implementation    We use a specific configuration of the masked autoencoder15, which    consists of an encoder and a decoder. The architecture detail is shown    in Supplementary Fig. 6. The encoder uses a large vision Transformer58    (ViT-large) with 24 Transformer blocks and an embedding vector size    of 1,024, whereas the decoder is a small vision Transformer (Vit-small)    with eight Transformer blocks and an embedding vector size of 512. The    encoder takes unmasked patches (patch size of 16 × 16) as input and    projects it into a feature vector with a size of 1,024. The 24 Transformer    blocks, comprising multiheaded self-attention and multilayer perceptron, take feature vectors as input and generate high-level features.    The decoder inserts masked dummy patches into extracted high-level    features as the model input and then reconstructs the image patch    after a linear projection. In model training, the objective is to reconstruct retinal images from the highly masked version, with a mask    ratio of 0.75 for CFP and 0.85 for OCT. The batch size is 1,792 (8 GPUs ×     224 per GPU). The total training epoch is 800 and the first 15 epochs    are for learning rate warming up (from 0 to a learning rate of 1 × 10−3).    The model weights at the final epoch are saved as the checkpoint for    adapting to downstream tasks.    Adaptation to downstream tasks    In adapting to downstream tasks, we only need the encoder (ViT-large)    of the foundation model and discard the decoder. The encoder    generates high-level features from retinal images. A multilayer perceptron takes the features as input and outputs the probability of    disease categories. The category with the highest probability will be    defined as the final classification. The number of categories decides    the neuron of the final layer of the multilayer perceptron. We include    label smoothing to regulate the output distribution thus preventing overfitting of the model by softening the ground-truth labels in the training data. The training objective is to generate the same    categorical output as the label. The batch size is 16. The total training    epoch is 50 and the first ten epochs are for learning rate warming up    (from 0 to a learning rate of 5 × 10−4), followed by a cosine annealing    schedule (from learning rates of 5 × 10−4 to 1 × 10−6 in the rest of the 40    epochs). After each epoch training, the model will be evaluated on    the validation set. The model weights with the highest AUROC on the    validation set will be saved as the model checkpoint for internal and    external evaluation.    Contrastive SSL implementation    We replace the primary SSL approach (that is, masked autoencoder)    with SimCLR16, SwAV37, DINO38 and MoCo-v3 (ref. 14) in the RETFound    framework to produce variants of the pretrained model for comparison.    For SSL training with each contrastive learning approach, we follow the    recommended network architectures and hyperparameter settings    from the published papers for optimal performance. We first load the    pretrained weights on ImageNet-1k to the models and further train the    models with 1.6 million retinal images with each contrastive learning    approach to obtain pretrained models. We then follow the identical    process of transferring the masked autoencoder to fine-tune those    pretrained models for the downstream disease detection tasks.    Explanations for fine-tuned models    We use RELPROP42 specified for Transformer-based networks. The    method uses layer-wise relevance propagation to compute relevancy    scores for each attention head in each layer and then integrates them    throughout the attention graph, by combining relevancy and gradient    information. As a result, it visualizes the areas of input images that lead    to a certain classification. RELPROP has been shown to outperform    other well-known explanation techniques, such as GradCam59.    Computational resources    SSL typically benefits from a large batch size for training and extracting context from data, which requires powerful GPUs for computation. We use eight NVIDIA Tesla A100 (40 GB) on the Google Cloud    Platform. It takes about 14 days to develop RETFound. We allocate an    equal computational cost to each SSL approach for pretraining. For    fine-tuning RETFound to downstream tasks, we use NVIDIA Tesla T4    (16 GB). Fine-tuning takes about 70 min for every 1,000 images.    Evaluation and statistical analysis    All task performances are evaluated by the classification metrics known    as AUROC and AUPR, computed from the receiver operating characteristics and precision-recall curves of classifiers, respectively. For    ocular prognosis and oculomic prediction tasks, the AUROC and AUPR    are calculated in a binary setting. For multiclass classification, such as    five-stage diabetic retinopathy and multicategory disease diagnosis,    we calculate the AUROC and AUPR for each disease category and then    average them to get the general AUROC and AUPR. For each task, we    train the model with five different random seeds, determining the    shuffling of training data. We calculate the mean and standard    deviation of the performance over the five iterations and calculate the    standard error by (standard deviation/ 5). We obtain the 95% CI by    means of 1.96 × standard error. We use the two-sided t-tests between    the performance of RETFound and the most competitive comparison    model to show whether significant differences exist.    Ethics statement    This study involves human participants and was approved by the    London-Central Research Ethics Committee (18/LO/1163, approved    1 August 2018), Advanced statistical modelling of multimodal data    of genetic and acquired retinal diseases (20/HRA/2158, approved    5 May 2020) and the Confidential Advisory Group for Section 251    support (18/CAG/0111, approved 13 September 2018). The National    Health Service Health Research Authority gave final approval on 13    September 2018. Moorfields Eye Hospital NHS Foundation Trust validated the de-identifications. Only de-identified retrospective data    were used for research, without the active involvement of patients.    Reporting summary    Further information on research design is available in the Nature    Portfolio Reporting Summary linked to this article.    Data availability    The MIDAS dataset consists of routinely collected healthcare data.    Owing to its sensitive nature and the risk of reidentification, the dataset    is subject to controlled access by means of a structured application    process. Data access enquiries may be made to enquiries@insight.    hdrhub.org and we will aim to respond within 2 weeks. Further details    about the data request pipeline may be found on the INSIGHT Health    Data Research Hub website https://www.insight.hdrhub.org. The AlzEye    dataset is subject to the contractual restrictions of the data sharing    agreements between National Health Service Digital, Moorfields Eye    Hospital and University College London, and is not available for access    beyond the AlzEye research team. National and international collaborations are welcomed, although restrictions on access to the cohort mean    that only the AlzEye researchers can directly analyse individual-level    systemic health data. More details can be found at https://readingcentre.org/studies/artificial_intelligence/alzeye. UK Biobank data are    available at https://www.ukbiobank.ac.uk/. Data for ocular disease    experiments are publicly available online and can be accessed through    the following links: IDRID (https://ieee-dataport.org/open-access/    indian-diabetic-retinopathy-image-dataset-idrid), MESSIDOR-2 (https://    www.adcis.net/en/third-party/messidor2/), APTOS-2019 (https://    www.kaggle.com/competitions/aptos2019-blindness-detection/data),    PAPILA (https://figshare.com/articles/dataset/PAPILA/14798004/1),    Glaucoma Fundus (https://dataverse.harvard.edu/dataset.    xhtml?persistentId=doi:10.7910/DVN/1YRRAC), JSIEC (https://zenodo.    org/record/3477553), Retina (https://www.kaggle.com/datasets/jr2ngb/    cataractdataset) and OCTID (https://borealisdata.ca/dataverse/OCTID).    Code availability    The code used to train, fine-tune and evaluate RETFound from Y.Z.    is available at https://github.com/rmaphoh/RETFound_MAE, which    is based on PyTorch. Furthermore, a Keras version implemented by    Y.K. is available at https://github.com/uw-biomedical-ml/RETFound_    MAE. Please note that the reported results are obtained from PyTorch    models. Images were processed with automated retinal image analysis    tool AutoMorph v.1.0 (https://github.com/rmaphoh/AutoMorph).    Image data were extracted from Dicom files with Pydicom v.2.3.0.    Results were further analysed and visualized with Python v.3.6, NumPy    v.1.19.5, SciPy v.1.5.4, seaborn v.0.12.0, Matplotlib v.3.6.1, pandas v.1.5.0,    Scikit-Learn v.1.1.3 and Pillow v.9.2.0. Heatmaps were generated with    RELPROP (https://github.com/hila-chefer/Transformer-Explainability).", "persona": "Student", "response": "### 1. Overview\nScientists have developed an artificial intelligence (AI) tool capable of diagnosing and predicting the risk of developing multiple health conditions — from ocular diseases to heart failure to Parkinson’s disease — all on the basis of people’s retinal images. AI tools have been trained to detect disease using retinal images before, but what makes the new tool — called RETFound — special is that it was developed using a method known as self-supervised learning. That means that the researchers did not have to analyse each of the 1.6 million retinal images used for training and label them as ‘normal’ or ‘not normal’, for instance. Such procedures are time-consuming and expensive, and are needed during the development of most standard machine-learning models. Using unlabelled data to initially train the model “unblocks a major bottleneck for researchers”, says Xiaoxuan Liu, a clinical researcher who studies responsible innovation in AI at the University of Birmingham, UK. Radiologist Curtis Langlotz, director of the Center for Artificial Intelligence in Medicine and Imaging at Stanford University in California, agrees.\n\n### 2. Background\nThe system performed well at detecting ocular diseases such as diabetic retinopathy. On a scale where 0.5 represents a model that performs no better than a random prediction and 1 represents a perfect model that makes an accurate prediction each time, it scored between 0.822 and 0.943 for diabetic retinopathy, depending on the data set used. When predicting the risk for systemic diseases — such as heart attacks, heart failure, stroke and Parkinson’s — the overall performance was limited, but still superior to that of other AI models. RETFound is so far one of the few successful applications of a foundation model to medical imaging, Liu says. “High-quality labels for medical data are extremely expensive, so label efficiency has become the coin of the realm,” he says.\n\n### 3. Methods\nInstead, the scientists used a method similar to the one used to train large language models such as ChatGPT. That AI tool harnesses myriad examples of human-generated text to learn how to predict the next word in a sentence from the context of the preceding words. In the same kind of way, RETFound uses a multitude of retinal photos to learn how to predict what missing portions of images should look like. Retinas are also an extension of the central nervous system, sharing similarities with the brain, which means that retinal images can be used to evaluate neural tissue. “The rub is that a lot of the time people don’t have the expertise to interpret these scans. This is where AI comes in,” Keane says. Once they had pre-trained RETFound on those 1.6 million unlabelled retinal images, Keane and his colleagues could then introduce a small number of labelled images — say, 100 retinal images from people who had developed Parkinson’s and 100 from people who had not — to teach the model about specific conditions.\n\n### 4. Results\nHaving learnt from all the unlabelled images what a retina should look like, Keane says, the model is able to easily learn the retinal features associated with a disease. Researchers are now looking ahead to what other types of medical imaging the techniques used to develop RETFound might be applied to. “It will be interesting to see whether these methods generalize to more complex images,” Langlotz says — for example, to magnetic resonance images or computed tomography scans, which are often three- or even four-dimensional. “This is tremendously exciting,” Liu says. But using RETFound as the basis for other models to detect diseases comes with a risk, she adds.\n\n### 5. Takeaways\n“Over the course of millions of images, the model somehow learns what a retina looks like and what all the features of a retina are,” says Pearse Keane, an ophthalmologist at Moorfields Eye Hospital NHS Foundation Trust in London who co-authored a url published today in Nature1 describing the tool. This forms the cornerstone of the model, and classifies it as what some call a foundation model, which means that it can be adapted for many tasks. A person’s retinas can offer a window into their health, because they are the only part of the human body through which the capillary network, made up of the smallest blood vessels, can be observed directly. “If you have some systemic cardiovascular disease, like hypertension, which is affecting potentially every blood vessel in your body, we can directly visualize [that] in retinal images,” Keane says. Expanding applications", "paper_id": "51", "story_id": "51-01"}
{"paper": "Asymptomatic left ventricular dysfunction (ALVD) is present in 3–6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1–4 . An inexpensive, noninvasive screening tool for ALVD in the doctor’s office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardio- gram (ECG), a routine method of measuring the heart’s elec- trical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejec- tion fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neu- ral network to identify patients with ventricular dysfunction, defined as ejection fraction ≤ 35%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG—a ubiquitous, low-cost test—permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD.\n\nALVD is present in 1.4–2.2% of the population (9% in the elderly) and is associated with reduced quality of life and increased morbidity and mortality1. Once ALVD is identi- fied, medical treatments (angiotensin-converting enzyme inhibitors, angiotensin receptor, and beta blockers) and device implantation (implantable cardioverter-defibrillators and car- diac resynchronization systems) are effective in prevention of progression to symptomatic heart failure and reduce mortal- ity2–4. While strategies for early identification of ALVD may prevent progression to symptomatic heart failure, a noninva- sive and low-cost screening tool does not currently exist. As a result, several groups have sought to identify less costly and minimally invasive or noninvasive approaches to identify- ing patients with ALVD5,6. Currently, the best-studied test for screening is B-type natriuretic peptide (BNP) levels, but stud- ies on BNP have been disappointing, and the test requires inva- sive blood draws5,6 . AI using neural networks has been applied to sophisticated recognition of subtle patterns in digital data in numerous fields, including image recognition, self-driving automobiles, lesion identification in pathological specimens, speech recognition, language translation, and automated detection of mammo- graphic lesions7–11. We hypothesized that the metabolic and structural derangements associated with the cardiomyop- athic process would result in ECG changes that could be reli- ably detected by a properly trained neural network. To test this hypothesis, we created, trained, validated, and then tested a large neural network. A total of 625,326 patients with paired ECG and transthoracic echocardiogram (TTE) were screened to identify the study cohort selected for analysis (Fig. 1). The first ECG–TTE data pair from patients with ECG and echocardiogram performed within a 2 week interval constituted the analysis data set, which consisted of 97,829 patients: 35,970 in the training set, 8,989 in the validation set, and 52,870 in the holdout testing set. No patient was in more than one group (Fig. 1). The overall patient population had a mean age of 61.8 ± 16.5 years, and 7.8% of the population had an ejection fraction (EF) of ≤ 35%. Table 1 shows patient characteristics for the training, validation, and test sets. In the testing data set, 4,131 patients (7.8%) had an EF of 35% or less, 6,740 patients (12.7%) had an EF greater than 35% and less than 50%, and 41,999 patients (79.5%) had an EF of 50% or higher. Over 89% of the TTEs were performed within 1 d of the index ECG. In the test data (that is, data not used to train the algorithm), the algorithm provided a high degree of discrimination between EF ≦ 35% and EF > 35% (area under the curve (AUC), 0.93; Fig. 2a). When selecting a threshold with no preference for sensi- tivity, the overall accuracy was 85.7%, with a specificity of 85.7% and sensitivity of 86.3%, an F1 score of 49.5%, and a negative pre- dictive value of 98.7%. Using a threshold to yield a 90% sensitiv- ity on the validation set and applying the algorithm to the testing data set, the sensitivity was 89.1%, specificity 83%, overall accu- racy 83.5%, and negative predictive value 98.9%. When patients with no known comorbidities (Table 1) were separately analyzed by the network, the AUC increased to 0.98, with a sensitivity of 95.6%, specificity of 92.4%, negative predictive value of 99.8%, and accuracy of 92.5%. The identical AUCs among the train- ing, validation, and test data sets demonstrate the robustness of the algorithm to different data sets. The network performance was strong across all age and sex groups (Fig. 2b); however, sig- nificant differences were noted in the strength of association (P < 0.001). When selecting a threshold with no preference for sensitivity (that is, a threshold that will yield an equal sensitivity and speci- ficity using validation data), 10,544 patients (19.9%) in the test set were identified by the network as having a low EF. Of these, 33.8% had an EF of 35% or less, 29.5% had an EF of 36–50%, and 36.6% had a normal EF. In the group that the network identified as normal, 1.3% had an EF of 35% or less and 8.6% had EF of 36–50%; the rest (90.1%) had a normal EF (Supplementary Fig. 1). Of the patients identified by the network as having a nor- mal EF who also had a confirmatory normal contemporaneous EF by echocardiography (‘true negative’), 11,515 had a follow-up 72 echocardiogram. Of these true negative patients, 302 developed a low EF over a median follow-up period of 3.8 years (interquar- tile range, 1.4–7.5 years) (Fig. 3, 1.8% and 4.4% for 5 and 10 year incidence, respectively). In contrast, 1,335 patients were labeled by the network as having a low EF, but the contemporaneous echo- cardiogram demonstrated a normal EF (‘false positive’). Of these patients with an initial false positive result, 147 developed left ven- tricular dysfunction during a median follow-up period of 3.4 years (interquartile range, 1.2–6.8) (Fig. 3, 9.5% and 20.8% for 5 and 10 year incidence, respectively). This represented a fourfold risk of future low EF when the AI algorithm defines the ECG as abnor- mal (age and sex adjusted hazard ratio (HR), 4.1 (95% CI, 3.3–5.0), P < 0.001), suggesting that the network identified ECG abnormali- ties before overt ventricular dysfunction became manifest. Left ventricular systolic dysfunction is associated with impaired quality of life, increased morbidity, and increased mortality1 . Furthermore, ALVD affects > 7 million people in the United States, and many more individuals globally. Major cardiovascular pro- fessional societies have endorsed evidence-based therapies that improve symptoms and survival once left ventricular dysfunction is detected12,13. However, effective population-wide screening for ventricular dysfunction is lacking1,6. We found that the application of AI using a convolutional neural network to the standard 12-lead ECG—an inexpensive, widely available, common clinical test— enabled detection of left ventricular dysfunction with an AUC of 0.93. The performance of this test compares favorably with other common screening tests, such as prostate-specific antigen for pros- tate cancer (AUC, 0.92), mammography for breast cancer (AUC, 0.85), and cervical cytology for cervical cancer (AUC, 0.70). In con- trast to BNP, accuracy was not affected by age or sex. Importantly, in addition to effectively identifying individuals with ventricular systolic dysfunction, the network also identified those patients with initially normal left ventricular function who are at risk of subse- quently developing a low EF. Such ‘false positive’ patients, with an abnormal network screen but a normal EF, had a fourfold increased risk of developing ventricular dysfunction over the next 5 years (10% risk at 5 years). This suggests the network detected early, sub- clinical, metabolic or structural abnormalities that manifest in the ECG. Whether this group would benefit from serial screening or medical therapy to prevent the development of ventricular dysfunc- tion, however, is unknown. Congestive heart failure afflicts more than 5 million people and consumes more than $30 billion in health-care expenditures annu- ally in the United States alone14,15. Early detection and prevention are a health-care imperative. We found that 6% of patients in our population had low EF, consistent with previous studies1. BNP and N-terminal pro b-type natriuretic peptide (NTproBNP) have been proposed for detection of left ventricular systolic dysfunction. Bhalla et al.16 assessed BNP to screen for systolic and diastolic dysfunction and found an AUC of 0.60 for BNP and 0.70 for NTproBNP; results improved with the addition of impedance car- diography to 0.70 and 0.73, respectively. A Mayo Clinic study from Olmsted County assessing individuals aged 45 and older found the AUC was greater for individuals with more severe systolic dys- function (0.82 to 0.92) than than for those with any systolic dys- function (0.51 to 0.74)17. Moreover, optimal discriminatory levels for BNP varied with age and sex. In contrast, we found excellent AI network performance across all age and sex strata. This capabil- ity appears to be unique to the neural network screen. The specific ECG characteristics used by our convolutional neural network to classify individuals as having or lacking a low EF are not known, owing to the nature of neural networks, although we presume it is detecting the known pathological effects of heart failure on the ECG (Supplementary Fig. 2). However, by training the network with a large cohort of approximately 45,000 ECG and EF data pairs, the network was exposed to a sufficient number of ECG variants to clas- sify with certainly individuals with a low EF, as demonstrated by the AUC of 0.93 obtained when testing in a population of 52,870 indi- viduals. In contrast to previous applications of neural networks in medicine, such as for identification of mammographic lesions9, we expand the use of AI to extend beyond the capacity of human skills. Another important characteristic of our network is that it uses an inexpensive, standardized, ubiquitous test as its input—the 12-lead ECG. In many rural areas in the United States18 and in develop- ing countries, access to cardiological care and imaging is limited. The availability of a portable, inexpensive test for ventricular sys- tolic dysfunction permits optimal utilization of limited imaging resources, while also enabling individuals to benefit from early institution of effective therapies such as beta adrenergic blockers, angiotensin receptor antagonists, and, where available, implantable devices (defibrillators and cardiac resynchronization systems)13,19 . With the emergence of smartphone-enabled electrodes20, mobile applications may permit ECG use in resource-constrained regions. The software-based nature of test ‘samples’ for our network also enables continuous feedback and refinement, with rapid distribu- tion of system improvements. Our work is best understood in the context of its limitations. The accuracy, sensitivity, and specificity of the networks were all excel- lent, but the test’s positive predictive value was only 33.8%. This is in part because we selected an EF cutoff of 35%. We selected this detection threshold owing to the well-established outcome and therapeutic implications of this value19. However, identification of an EF of < 50% is still clinically significant, as this reflects an abnor- mal EF. When considering a higher cut-off for abnormal function of < 50%, the positive predictive value of the network was 63.4%. Of the ‘false positives’, 45% had an EF of < 50% but > 35% (29.5% of all positives). While statistically patients with an EF in this range are considered false positives, medically this value is actionable, and an echocardiogram is justified. An interesting finding was that in patients with a false positive result with a network-predicted low EF and an echocardiographic normal EF at the time of screening—a group that assessed with current technology would be considered healthy—there was a fourfold increase in risk of developing a low EF in the future compared with patients with a negative AI screen.\n\nFig. 1 | Creation of the study data sets. Schematic of the data set creation and analysis strategy, which was devised to assure a robust and reliable data set for training, validating, and testing of the network. Once a patient’s data were placed in one of the data sets, that individual’s data were used only in that set, avoiding ‘cross-contamination’ among the training, validation, and test data sets. Patients for whom data at more than one time point were available were used for the follow-up analysis, to determine whether an index abnormal AI screen with a normal EF (apparent false positive) was associated with future risk of a low EF. The details of the flow chart and how each of the data sets was used are described in the Methods. The final data sets used in the analysis for training, validating, and testing of the network are depicted as green boxes. Table 1 | Patient characteristics and comorbidities at enrollment Training set (n = 35,970) Validation set (n = 8,989) Test set (n = 52,870) P value Age, years 61.6 (16.5) 61.8 (16.5) 61.8 (16.5) 0.44 Age groups, n (%) 0.86 < 40 4,046 (11%) 1,008 (11%) 5,861 (11%) 40–49 3,875 (11%) 942 (10%) 5,599 (11%) 50–59 6,376 (18%) 1,587 (18%) 9,341 (18%) 60–69 8,559 (24%) 2,110 (23%) 12,649 (24%) 70–79 8,573 (24%) 2,158 (24%) 12,550 (24%) 80+ 4,541 (13%) 1,184 (13%) 6,870 (13%) Female 15,358 (43%) Sex, n (%) 0.64 3,821 (43%) 22,704 (43%) Male 20,612 (57%) 5,168 (57%) 30,166 (57%) Mean EF 56.3 (11.9) 56.1 (12.1) 56.2 (12.0) 0.45 Heart failure, n (%) 10,365 (20%) 7,003 (19%) 1,803 (20%) 0.45 Diabetes mellitus, n (%) 8,458 (24%) 2,079 (23%) 12,433 (24%) 0.71 Hypercholesterolemia, n (%) 15,593 (43%) 3,851 (43%) 23,059 (44%) 0.35 Renal disease, n (%) 6,929 (19%) 1,685 (19%) 10,219 (19%) 0.43 Hypertension, n (%) 16,831 (47%) 4,163 (46%) 24,643 (47%) 0.69 Coronary artery disease, n (%) 13,563 (38%) 3,380 (38%) 20,040 (38%) 0.77 Myocardial infarction, n (%) 4,556 (13%) 1,111 (12%) 6,770 (13%) 0.48 Enrollment refers to the time of the initial ECG–TTE data pair acquisition. Numbers in parentheses refer to the standard deviation (for age and sex) or to the percentage of individuals randomly assigned to a given subset (all other rows). P values refer to two-tailed analysis of variance or χ 2 tests for a difference in the distribution of values between the training, validation, and test sets. a b ROC Group Sensitivity Specificity OR (95% CI) Sensitivity 1.0 0.8 0.6 0.4 0.2 0 Female <40 74 (53/72) 95 (2,895/3,053) 51.11 (29.55–88.41) Male <40 72 (86/120) 92 (2,405/2,616) 28.83 (18.92–43.93) Female 40–49 74 (63/85) 92 (2,300/2,495) 33.78 (20.35–56.07) Male 40–49 80 (178/223) 89 (2,491/2,796) 32.31 (22.81–45.75) Female 50–59 85 (150/177) 91 (3,286/3,593) 59.46 (38.83–91.06) Male 50–59 86 (390/455) 87 (4,440/5,116) 39.41 (29.94–51.87) Female 60–69 90 (235/260) 89 (4,207/4,707) 79.09 (51.83–120.69) Male 60–69 89 (722/815) 84 (5,762/6,867) 40.48 (32.32–50.71) Female 70–79 80 (228/284) 86 (4,230/4,897) 25.82 (19.06–34.98) Male 70–79 90 (843/936) 78 (5,014/6,433) 32.03 (25.65–40.00) Female ≥80 84 (156/186) 81 (2,350/2,895) 22.42 (15.00–33.50) Male ≥80 89 (463/518) 73 (2,382/3,271) 22.56 (16.88–30.14) Validation set (AUC = 0.933) Testing set (AUC = 0.932) Overall 86 (3,567/4,131) 86 (41,762/48,739) 37.86 (34.52–41.52) Initially classified as normal EF Initially classified as low EF 0 0.2 0.4 0.6 0.8 1.0 0 25 50 75 100 125 1 − specificity OR Fig. 2 | Network ROC and sensitivity and specificity across age and gender subsets. a, The ROC of the convolutional neural network used to identify patients with an EF of ≤ 35%. The ROC curve and AUC were calculated using the validation and testing (holdout) data sets. b, The convolutional neural network’s sensitivity and specificity to detect EF ≤ 35% is tabulated across a range of age and gender combinations. The diagnostic OR, which is the ratio of positive likelihood ratio (sensitivity / (1− specificity)) to the negative likelihood ratio ((1− sensitivity) / specificity), as well as the associated 95% CI, is shown for each age and gender combination and for the overall study sample. The Breslow–Day test of homogeneity showed significant variation across the age and gender combinations (P < 0.0001). 50 40 Incidence of developing EF of ≤35% 30 20 10 0 35.3% (53) 23.6% (120) 17.6% (224) 11.1% (396) 10.4% (518) 7.2% (1,048) 5.8% (714) 2.3% (3,853) 3.9% (2,129) 1.0% (6,600) 0 3 6 9 12 15 Time after index ECG–TTE (years) Fig. 3 | Long-term incidence of developing an EF of ≤35% in patients with an initially normal EF stratified by AI classification. Long-term outcome of patients with an echocardiographic EF of ≥ 50% at the time of initial classification, stratified by the initial network classification. The ordinate shows the cumulative incidence of developing a low EF (35%), and the abscissa indicates years from the time of index ECG–TTE evaluation. A fourfold risk of future low EF was present when the AI algorithm defined the ECG as abnormal (age- and sex-adjusted HR, 4.1 (95% CI, 3.3–5.0), P < 0.001), compared with patients with an echocardiographic normal EF who were classified as having a normal EF by the ECG network. The numbers reported along the cumulative incidence curves reflect the estimated cumulative incidence (and number at risk, in parentheses) for each group at the times indicated along the axis.\n\nAcknowledgements The study was conceived, funded, and executed entirely by Mayo Clinic. There was no industry support of any kind. Author contributions D.J.L. and G.S. contributed to the literature search, study coordination, data management, and data collection. Z.I.A., P.A.F., S.K., and F.L.-J. contributed to the study design. C.G.S., Z.I.A., R.E.C., and F.L.-J. contributed to the data analysis. C.G.S., Z.I.A., Additional information Supplementary information is available for this paper at https://doi.org/10.1038/ s41591-018-0240-2. Reprints and permissions information is available at www.nature.com/reprints. Correspondence and requests for materials should be addressed to P.A.F. Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. © The Author(s), under exclusive licence to Springer Nature America, Inc. 2019\n\nReferences\n1. McDonagh, T. A., McDonald, K. & Maisel, A. S. Screening for asymptomatic\nlef ventricular dysfunction using B-type natriuretic Peptide. Congest. Heart Fail. 14, 5–8 (2008).\n2. Dargie, H. J. Effect of carvedilol on outcome afer myocardial infarction in\npatients with lef-ventricular dysfunction: the CAPRICORN randomised trial. Lancet 357, 1385–1390 (2001).\n3. Pfeffer, M. A. et al. Effect of captopril on mortality and morbidity in\npatients with lef ventricular dysfunction afer myocardial infarction. Results of the survival and ventricular enlargement trial. N. Engl. J. Med. 327, 669–677 (1992).\n4. Priori, S. G. et al. 2015 ESC guidelines for the management of patients\nwith ventricular arrhythmias and the prevention of sudden cardiac death: the Task Force for the Management of Patients with Ventricular Arrhythmias and the Prevention of Sudden Cardiac Death of the European Society of Cardiology (ESC). Endorsed by: Association for European Paediatric and Congenital Cardiology (AEPC). Eur. Heart J. 36, 2793–2867 (2015).\n5. Betti, I. et al. Te role of N-terminal PRO-brain natriuretic peptide and\nechocardiography for screening asymptomatic lef ventricular dysfunction in a population at high risk for heart failure. Te PROBE-HF study. J. Card. Fail. 15, 377–384 (2009).\n6. Red$eld, M. M. et al. Plasma brain natriuretic peptide to detect preclinical\nventricular systolic or diastolic dysfunction: a community-based study. Circulation 109, 3176–3181 (2004).\n7. Kim, J. H., Kwon, H. S. & Seo, H. W. Evaluating a pivot-based approach for\nbilingual lexicon extraction. Comput. Intell. Neurosci. 2015, 434153 (2015).\n8. Pasquier, M., Quek, C. & Toh, M. Fuzzylot: a novel self-organising\nfuzzy-neural rule-based pilot system for automated vehicles. Neural Netw. 14, 1099–1112 (2001).\n9. Salazar-Licea, L. A., Pedraza-Ortega, J. C., Pastrana-Palma, A. & Aceves-\nFernandez, M. A. Location of mammograms ROI’s and reduction of false-positive. Comput. Methods Programs Biomed. 143, 97–111 (2017).\n10. Wing$eld, C. et al. Relating dynamic brain states to dynamic machine states:\nhuman and machine solutions to the speech recognition problem. PLoS Comput. Biol. 13, e1005617 (2017).\n11. Yoshida, H. et al. Automated histological classi$cation of whole-slide images\nof gastric biopsy specimens. Gastric Cancer 21, 249–257 (2018).\n12. Al-Khatib, S. M. et al. 2017 AHA/ACC/HRS guideline for management\nof patients with ventricular arrhythmias and the prevention of sudden cardiac death: executive summary. A report of the American College of Cardiology/American Heart Association Task Force on Clinical Practice Guidelines and the Heart Rhythm Society. Circulation 138, e210–e271 (2018).\n13. Yancy, C. W. et al. 2013 ACCF/AHA guideline for the management of heart\nfailure: a report of the American College of Cardiology Foundation/American Heart Association Task Force on Practice Guidelines. J. Am. Coll. Cardiol. 62, e147–e239 (2013).\n14. Heidenreich, P. A. et al. Forecasting the impact of heart failure in the United\npeptide and B-type natriuretic peptide in the general community: determinants and detection of lef ventricular dysfunction. J. Am. Coll. Cardiol. 47, 345–353 (2006).\n18. Gruca, T. S., Pyo, T. H. & Nelson, G. C. Providing cardiology care in rural\nareas through visiting consultant clinics. J. Am. Heart Assoc. 5, e002909 (2016).\n19. Yancy, C. W. et al. 2017 ACC/AHA/HFSA focused update of the 2013 ACCF/\nAHA guideline for the management of heart failure: a report of the American College of Cardiology/American Heart Association Task Force on Clinical Practice Guidelines and the Heart Failure Society of America. J. Am. Coll. Cardiol. 70, 776–803 (2017).\n20. Yasin, O. Z. et al. Noninvasive blood potassium measurement using\nsignal-processed, single-lead ECG acquired from a handheld smartphone.\nJ. Electrocardiol. 50, 620–625 (2017).\nAcknowledgements The study was conceived, funded, and executed entirely by Mayo Clinic. There was no industry support of any kind. Author contributions D.J.L. and G.S. contributed to the literature search, study coordination, data management, and data collection. Z.I.A., P.A.F., S.K., and F.L.-J. contributed to the study design. C.G.S., Z.I.A., R.E.C., and F.L.-J. contributed to the data analysis. C.G.S., Z.I.A., Competing interests Mayo Clinic has licensed the underlying technology to EKO, a maker of digital stethoscopes with embedded ECG electrodes.  Mayo Clinic may receive financial benefit from the use of this technology, but at no point will Mayo Clinic benefit financially from its use for the care of patients at Mayo Clinic.  P.A.F., F.L.-J., S.K., and Z.I.A. may also receive financial benefit from this agreement. Additional information s41591-018-0240-2. Reprints and permissions information is available at www.nature.com/reprints. Correspondence and requests for materials should be addressed to P.A.F. Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in © The Author(s), under exclusive licence to Springer Nature America, Inc. 2019 .\n\nData sources and study population. Following institutional review board approval, we obtained data from the Mayo Clinic digital data vault. We identi$ed 163,892 adult patients (18 years old or older) with at least one digital, standard 10-s 12-lead ECG acquired in the supine position between January 1994 and February 2017 and at least one TTE obtained within 14 d of the index ECG (Fig. 1). For patients with multiple ECG and TTE data sets meeting these criteria, the earliest pair was used for network creation, validation, or testing, and subsequent TTE data were used for analysis of follow-up. We performed a preliminary proof-of-concept assessment to obtain internal funding using 2,200 ECG–TTE data pairs, which were excluded from the present analysis, leaving a cohort of 97,829 patients whose $rst ECG–TTE paired data sets were used for the primary analysis. ECGs were acquired at a sampling rate of 500 Hz using a Marquette ECG machine (GE Healthcare) and stored using the MUSE data management system for later retrieval. Comprehensive 2-D or 3-D and Doppler echocardiography were available for all patients. Quantitative data were recorded at the time of the acquisition in a Mayo Clinic–developed, custom database (Echo Image Management System). EF is routinely measured or estimated using standardized methodologies, and in most reports, more than one value may be recorded. For the purpose of this study, the EF value used in the models was the first available from a standard hierarchical sequence: EF determined using 3-D echocardiography21, a biplane approach using the Simpson method, a 2-D method22, or M-mode and, in the absence of any of the preceding, the reported visually estimated EF. If the estimation was a range, we used the midpoint as a single EF value. EF was classified as low (≤ 35%), mildly depressed (35–49%), or normal (≥ 50%). Primary and secondary outcomes. The primary outcome was the ability of the AI network to identify patients with an EF of 35% or less using the ECG signal alone. This value was selected owing to its clear-cut clinical and therapeutic importance23. The secondary outcome was the ability of the AI network to identify individuals with a normal EF at the time of screening, but with an increased risk of subsequent low EF during follow-up. Overview of AI model development. We implemented a convolutional neural network using the Keras framework with a Tensorflow (Google) backend and Python24. Convolutional neural networks, which are commonly applied to images, operate such that the convolutions can be used to extract very subtle patterns in a data set. Each 12-lead ECG was considered a 12 × 5,000 (that is, 12 leads by 10-s duration sampled at 500 Hz) matrix, where the first dimension represents a spatial dimension and the second represents a temporal one24. We used the internal validation set to optimize the network architecture, and hyperparameters such as batch and step size. Multiple networks were tested, and the network that yielded the highest AUC of the receiver operator curve (ROC) for the validation data set was selected. In cases in which more than one network had similar results when tested using the validation set, the ‘simpler’ network—the one with fewer parameters or layers—was selected24. ECG analysis is mostly a visual task. While the signal is a time series, it is pseudocyclical, and its main features are morphologic25–27. To enable detection of patterns in these features, we used architectures that were based on convolutional layers for feature extraction. The main architecture tested was a convolutional neural network28,29 in which the convolutional blocks were followed by two fully connected layers. The selected network (Supplementary Fig. 3) was composed of six convolutional layers, each of which was followed by a nonlinear ‘Relu’ activation function, a batchnormalization layer30, and a max-pooling layer31. As these first six layers were designed to learn features within each lead, each convolutional layer had a K × 1 filter shape with N filters. K started at 5 and was decreased to 3 after the third layer, while N started at 16 and was increased to 32 in the third layer and to 64 in the fifth layer. After each ‘temporal’ layer, there was a Relu activation layer, a batchnormalization layer, and max-pooling layer (4 × 1 after the third and fifth layers and 2 × 1 after other temporal convolutional layers). Following the last temporal convolutional layer, a ‘spatial’ convolutional layer was used. In this layer, the filter shape was 12 × 1, allowing it to fuse data from the different leads (hence the term ‘spatial’). The reason a convolutional filter as opposed to a pooling layer was used for lead fusion is that the ECG is structured and the spatial structure of a 12-lead ECG is consistent among patients and adjusted per patient32,33. After this spatial layer, the data were fed to a fully connected network with two hidden layers (64 and 32 nodes) followed by Relu activation layers, batch-normalization layers, and dropout layers to avoid overfitting. The output layer had two classes and was activated using the ‘Softmax’ function34. Optimizing the architecture and hyperparameters. The convolutional neural network framework provides immense flexibility with structure, and as a result, turning the algorithm involves modification of both the architecture and the hyperparameters associated with each aspect of the model specification. We evaluated combinations of architecture and hyperparameters using an iterative process, including a formalized grid search for hyperparameters. In terms of architecture, we evaluated the change in model performance based on adding neurons to the fully connected layers. Increasing the number of neurons yielded lower performance (for example, 0.912 using 256 hidden nodes). Removing two of the pooling layers to allow more temporal features (by a factor of 16 overall) and replacing the fully connected layers with long short term memory (LSTM) layers to allow real temporal feature detection (128 and 32 LSTM cells in each layer) did not significantly improve the validation AUC (0.936) and increased the number of parameters (to 560,000). To exploit the pseudocyclical nature of the signal and reduce the number of parameters, we also tested the same architecture on a shorter segment of 2 s (1,000 samples zero-padded to 1,024) with an overlap of 1 s (500 samples). While the AUC per segment was lower (0.91), the average score of all nine segments (due to the overlap) per ECG yielded an AUC of 0.934. The architecture reduced the number of parameters by half, to 159,000, which helped optimize the training. All results presented in this manuscript were created using this specific architecture. Computer code is available upon request. AI-augmented ECG to identify a low EF. After selecting the optimal network using validation data, we created an ROC using the same validation set and measured its AUC as a primary assessment of network strength. We used the validation data set ROC to select two thresholds for the probability of having a low EF: the first was selected by giving an equal weight to sensitivity and specificity, and the second was selected to yield a sensitivity of 90% on the validation data set. The convolutional neural network model was then used on the test data to test its ability to predict a low EF. The two thresholds were used to calculate sensitivity, specificity, and accuracy in the test data, which had not been used for model training or threshold selection. To investigate network performance differences by age and sex, odds ratios (ORs) were calculated within groups. The Breslow–Day test was used to test for homogeneity of ORs across the groups defined on age and sex. Sensitivity and specificity were also calculated within these groups. AI-augmented ECG to predict a future low EF. We hypothesized that early in the course of any disease that impacts EF, ECG signals would show subtle abnormal patterns due to metabolic and structural derangements that had not affected a sufficient quantity of myocardium to cause a reduction in EF. We further hypothesized that the convolutional neural network would classify some of these cases as abnormal, giving the initial appearance of a false positive test (that is, an individual classified as having a low EF but reported as normal) that with time would become a true positive test. To test this hypothesis, we designed a substudy to identify patients that met the following conditions: (i) the network predicted the patient had a low EF; (ii) the individual had an echocardiogram performed within 14 d that demonstrated a normal EF (≥ 50%), indicating a false positive finding by the algorithm; and (iii) the individual had at least one additional echocardiogram (not used for training or testing) available at a later date. A control group (EF ≥ 50%) was created using the true negative cases (algorithm and clinical determinations were both consistent with not having a low EF). A Kaplan–Meier analysis was used to depict the incidence of low EF for the true negatives versus the false positives over time. Subsequently, Cox proportional hazards regression was used to estimate the hazard for low EF after adjusting for age and sex. Statistical considerations. For measures of diagnostic performance (AUC of ROC, sensitivity), the sample sizes are so large that normal CIs are expected to have a width of < 0.5%. As such, the CIs are not reported alongside the estimated values due to the values’ high precision. Continuous data are presented as mean ± s.d. Cox models are presented with two-sided P values without any correction for multiple testing. The effect size of the Cox model is represented by the HR and its associated 95% CI. Survival analyses were conducted using SAS version 9.4. The convolutional neural network was trained using Keras (version 2.0.3) and TensorFlow (version 1.0.1).\nReferences\n21. Yamani, H., Cai, Q. & Ahmad, M. Tree-dimensional echocardiography\nin evaluation of lef ventricular indices. Echocardiography 29, 66–75 (2012).\n22. Quinones, M. A. et al. A new, simpli$ed and accurate method for\ndetermining ejection fraction with two-dimensional echocardiography. Circulation 64, 744–753 (1981).\n23. Russo, A. M. et al. ACCF/HRS/AHA/ASE/HFSA/SCAI/SCCT/SCMR 2013\nappropriate use criteria for implantable cardioverter-de$brillators and cardiac resynchronization therapy: a report of the American College of Cardiology Foundation appropriate use criteria task force, Heart Rhythm Society, American Heart Association, American Society of Echocardiography, Heart Failure Society of America, Society for Cardiovascular Angiography and Interventions, Society of Cardiovascular Computed Tomography, and Society for Cardiovascular Magnetic Resonance. J. Am. Coll. Cardiol. 61, 1318–1368 (2013).\n24. van Rossum, G. Python Tutorial, Technical Report CS-R9526 (CWI,\nAmsterdam, 1995). signal parameters. In Proc. 20th Annual International Conference of the IEEE Engineering in Medicine and Biology Society 127–130 (IEEE, 1998).\n26. Sugrue, A. et al. Identi$cation of concealed and manifest long QT syndrome\nusing a novel T wave analysis program. Circ. Arrhythm. Electrophysiol. 9, e003830 (2016).\n27. Couderc, J. P. et al. T-wave morphology abnormalities in benign, potent, and\narrhythmogenic Ikr inhibition. Heart Rhythm 8, 1036–1043 (2011).\n28. Krizhevsky A., S I., Hinton G. E. ImageNet classi$cation with deep\nconvolutional neural networks. In Advances in Neural Information Processing Systems 1097–1105 (Neural Information Processing Systems, 2012).\n29. Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. Preprint\nat https://arxiv.org/abs/1412.6980 (2014).\n30. Ioffe, S. & Szegedy, C. Batch normalization: accelerating deep network\ntraining by reducing internal covariate shif. Preprint at https://arxiv.org/ abs/1502.03167 (2015).\n31. Nagi, J. et al. Max-pooling convolutional neural networks for vision-based\nhand gesture recognition. In Proc. 2011 IEEE International Conference on Signal and Image Processing Applications 342–347 (IEEE, 2011).\n32. Wilson, F. N. et al. Te precordial electrocardiogram. Am. Heart. J. 27,\n19–85 (2004).\n33. Khan, G. M. A new electrode placement method for obtaining 12-lead ECGs.\nOpen Heart 2, e000226 (2015).\n34. Cristianini, N. & Shawe-Taylor, J. An Introduction to Support Vector\nMachines and other Kernel-based Learning Methods (Cambridge University Press, New York, 2000).\n--- FOOTNOTES published maps and institutional affiliations", "persona": "Student", "response": "### 1. Overview\nAn artificial intelligence (AI)-enabled ECG algorithm integrated into routine care can increase the diagnosis of low ejection fraction (EF), according to new randomized trial data. “Because ECG is a low-cost test frequently performed for a variety of purposes, the algorithm could potentially improve early diagnosis and treatment in broad populations,” write Xiaoxi Yao, PhD (Mayo Clinic, Rochester, MN), and colleagues in their study published online last week in Nature Medicine. The algorithm uses neural networks to predict a high likelihood of low EF, an often-missed predictor of adverse events, based on standard 12-lead electrocardiogram data. Overall, 11,573 patients without prior heart failure were assessed via the intervention pathway and 11,068 served as controls between August 2019 and March 2020. Six percent of patients in each group had positive AI-ECG results, indicating a high likelihood of low EF. In addition to giving insight into the specific use and effect of this AI-algorithm, the authors say the study offers broader benefit.\n\n### 2. Background\nBut the success of the technology—developed by the same team who recently published results on identifying long QT syndrome in a similar fashion—is dependent on it actually being used by clinicians, Yao told TCTMD. Since a myriad of AI-based algorithms are created daily, and money and IT resources to be engaged, “we cannot afford to implement all the AI algorithms into the EHR [electronic health record],” she said, noting the importance of randomized trials in this space. More echocardiograms were obtained for patients with positive AI-ECG results in the intervention arm compared with patients whose clinicians did not have access to the technology (49.6% vs 38.1%; P < 0.001). However, echocardiogram use was similar for the cohorts in the overall population (19.2% vs 18.2%; P = 0.17). Additionally, clinician access to AI-ECG data increased the diagnosis of low EF compared with usual care, both overall (2.1% vs 1.6%; OR 1.32; 95% CI 1.01-1.67) and especially among patients with positive results (19.5% vs 14.5%; OR 1.43; 95% CI 1.08-1.91).\n\n### 3. Methods\n“The AI-enabled EKG facilitated the diagnosis of patients with low ejection fraction in a real-world setting by identifying people who previously would have slipped through the cracks,” senior author Peter Noseworthy, MD (Mayo Clinic), said in a press release. Yao said the most surprising study finding to her was that even with a positive AI-ECG result, “about half of the time clinicians didn't order an echo.” She said they have since surveyed clinicians to dig deeper into their rationale—data that will be published at a later date—but speculated several reasons why this might have been the case. “Some patients might not be able to afford the cost, some patients might have other competing priorities, or clinicians just don't think it's valuable in this context,” she said, adding that some clinicians simply don’t believe they need AI to tell them what to do. Randomized data show potential for a clinically integrated algorithm: the trick will be actually putting information to use. For the EAGLE study, Yao and colleagues cluster randomized 120 primary care teams in Minnesota and Wisconsin to treat patients either with access to their AI-ECG, which gives an indication of potential for low EF (n = 181 clinicians), or with usual care (n = 177 clinicians).\n\n### 4. Results\n“The trial was conducted across diverse practice settings, including primary care clinics at a tertiary medical center as well as community and rural clinics,” they write. “The clinicians involved in the study were not AI experts or specialists but were busy clinicians engaged in the care of patients presenting with a wide range of conditions. Therefore, this trial might form a template for how new technologies can be studied in ways that closely mirror their potential deployment.” Also, this type of study can save costs and time, the authors say. “This pragmatic trial was embedded in everyday practice to efficiently generate relevant and valid evidence. The study design, embedding of the technology in the EHR, and enrollment and follow-up was completed in less than 2 years at a much lower cost than a traditional RCT.\n\n### 5. Takeaways\nGiven the increasing number of AI tools being developed, this framework might facilitate rigorous and timely evaluation and implementation.” “The AI itself is the intervention to be evaluated, but the AI can also help the clinical trials to be done fairly quickly,” Yao added. “During the trial, we also evaluated implementation strategies and identified the problems so we can fix them in the subsequent implementation. So, once the trial is done, it can be quickly turned to a translation or used for implementation.”", "paper_id": "61", "story_id": "61-03"}
{"paper": "Asymptomatic left ventricular dysfunction (ALVD) is present in 3–6% of the general population, is associated with reduced quality of life and longevity, and is treatable when found1–4 . An inexpensive, noninvasive screening tool for ALVD in the doctor’s office is not available. We tested the hypothesis that application of artificial intelligence (AI) to the electrocardio- gram (ECG), a routine method of measuring the heart’s elec- trical activity, could identify ALVD. Using paired 12-lead ECG and echocardiogram data, including the left ventricular ejec- tion fraction (a measure of contractile function), from 44,959 patients at the Mayo Clinic, we trained a convolutional neu- ral network to identify patients with ventricular dysfunction, defined as ejection fraction ≤ 35%, using the ECG data alone. When tested on an independent set of 52,870 patients, the network model yielded values for the area under the curve, sensitivity, specificity, and accuracy of 0.93, 86.3%, 85.7%, and 85.7%, respectively. In patients without ventricular dysfunction, those with a positive AI screen were at 4 times the risk (hazard ratio, 4.1; 95% confidence interval, 3.3 to 5.0) of developing future ventricular dysfunction compared with those with a negative screen. Application of AI to the ECG—a ubiquitous, low-cost test—permits the ECG to serve as a powerful screening tool in asymptomatic individuals to identify ALVD.\n\nALVD is present in 1.4–2.2% of the population (9% in the elderly) and is associated with reduced quality of life and increased morbidity and mortality1. Once ALVD is identi- fied, medical treatments (angiotensin-converting enzyme inhibitors, angiotensin receptor, and beta blockers) and device implantation (implantable cardioverter-defibrillators and car- diac resynchronization systems) are effective in prevention of progression to symptomatic heart failure and reduce mortal- ity2–4. While strategies for early identification of ALVD may prevent progression to symptomatic heart failure, a noninva- sive and low-cost screening tool does not currently exist. As a result, several groups have sought to identify less costly and minimally invasive or noninvasive approaches to identify- ing patients with ALVD5,6. Currently, the best-studied test for screening is B-type natriuretic peptide (BNP) levels, but stud- ies on BNP have been disappointing, and the test requires inva- sive blood draws5,6 . AI using neural networks has been applied to sophisticated recognition of subtle patterns in digital data in numerous fields, including image recognition, self-driving automobiles, lesion identification in pathological specimens, speech recognition, language translation, and automated detection of mammo- graphic lesions7–11. We hypothesized that the metabolic and structural derangements associated with the cardiomyop- athic process would result in ECG changes that could be reli- ably detected by a properly trained neural network. To test this hypothesis, we created, trained, validated, and then tested a large neural network. A total of 625,326 patients with paired ECG and transthoracic echocardiogram (TTE) were screened to identify the study cohort selected for analysis (Fig. 1). The first ECG–TTE data pair from patients with ECG and echocardiogram performed within a 2 week interval constituted the analysis data set, which consisted of 97,829 patients: 35,970 in the training set, 8,989 in the validation set, and 52,870 in the holdout testing set. No patient was in more than one group (Fig. 1). The overall patient population had a mean age of 61.8 ± 16.5 years, and 7.8% of the population had an ejection fraction (EF) of ≤ 35%. Table 1 shows patient characteristics for the training, validation, and test sets. In the testing data set, 4,131 patients (7.8%) had an EF of 35% or less, 6,740 patients (12.7%) had an EF greater than 35% and less than 50%, and 41,999 patients (79.5%) had an EF of 50% or higher. Over 89% of the TTEs were performed within 1 d of the index ECG. In the test data (that is, data not used to train the algorithm), the algorithm provided a high degree of discrimination between EF ≦ 35% and EF > 35% (area under the curve (AUC), 0.93; Fig. 2a). When selecting a threshold with no preference for sensi- tivity, the overall accuracy was 85.7%, with a specificity of 85.7% and sensitivity of 86.3%, an F1 score of 49.5%, and a negative pre- dictive value of 98.7%. Using a threshold to yield a 90% sensitiv- ity on the validation set and applying the algorithm to the testing data set, the sensitivity was 89.1%, specificity 83%, overall accu- racy 83.5%, and negative predictive value 98.9%. When patients with no known comorbidities (Table 1) were separately analyzed by the network, the AUC increased to 0.98, with a sensitivity of 95.6%, specificity of 92.4%, negative predictive value of 99.8%, and accuracy of 92.5%. The identical AUCs among the train- ing, validation, and test data sets demonstrate the robustness of the algorithm to different data sets. The network performance was strong across all age and sex groups (Fig. 2b); however, sig- nificant differences were noted in the strength of association (P < 0.001). When selecting a threshold with no preference for sensitivity (that is, a threshold that will yield an equal sensitivity and speci- ficity using validation data), 10,544 patients (19.9%) in the test set were identified by the network as having a low EF. Of these, 33.8% had an EF of 35% or less, 29.5% had an EF of 36–50%, and 36.6% had a normal EF. In the group that the network identified as normal, 1.3% had an EF of 35% or less and 8.6% had EF of 36–50%; the rest (90.1%) had a normal EF (Supplementary Fig. 1). Of the patients identified by the network as having a nor- mal EF who also had a confirmatory normal contemporaneous EF by echocardiography (‘true negative’), 11,515 had a follow-up 72 echocardiogram. Of these true negative patients, 302 developed a low EF over a median follow-up period of 3.8 years (interquar- tile range, 1.4–7.5 years) (Fig. 3, 1.8% and 4.4% for 5 and 10 year incidence, respectively). In contrast, 1,335 patients were labeled by the network as having a low EF, but the contemporaneous echo- cardiogram demonstrated a normal EF (‘false positive’). Of these patients with an initial false positive result, 147 developed left ven- tricular dysfunction during a median follow-up period of 3.4 years (interquartile range, 1.2–6.8) (Fig. 3, 9.5% and 20.8% for 5 and 10 year incidence, respectively). This represented a fourfold risk of future low EF when the AI algorithm defines the ECG as abnor- mal (age and sex adjusted hazard ratio (HR), 4.1 (95% CI, 3.3–5.0), P < 0.001), suggesting that the network identified ECG abnormali- ties before overt ventricular dysfunction became manifest. Left ventricular systolic dysfunction is associated with impaired quality of life, increased morbidity, and increased mortality1 . Furthermore, ALVD affects > 7 million people in the United States, and many more individuals globally. Major cardiovascular pro- fessional societies have endorsed evidence-based therapies that improve symptoms and survival once left ventricular dysfunction is detected12,13. However, effective population-wide screening for ventricular dysfunction is lacking1,6. We found that the application of AI using a convolutional neural network to the standard 12-lead ECG—an inexpensive, widely available, common clinical test— enabled detection of left ventricular dysfunction with an AUC of 0.93. The performance of this test compares favorably with other common screening tests, such as prostate-specific antigen for pros- tate cancer (AUC, 0.92), mammography for breast cancer (AUC, 0.85), and cervical cytology for cervical cancer (AUC, 0.70). In con- trast to BNP, accuracy was not affected by age or sex. Importantly, in addition to effectively identifying individuals with ventricular systolic dysfunction, the network also identified those patients with initially normal left ventricular function who are at risk of subse- quently developing a low EF. Such ‘false positive’ patients, with an abnormal network screen but a normal EF, had a fourfold increased risk of developing ventricular dysfunction over the next 5 years (10% risk at 5 years). This suggests the network detected early, sub- clinical, metabolic or structural abnormalities that manifest in the ECG. Whether this group would benefit from serial screening or medical therapy to prevent the development of ventricular dysfunc- tion, however, is unknown. Congestive heart failure afflicts more than 5 million people and consumes more than $30 billion in health-care expenditures annu- ally in the United States alone14,15. Early detection and prevention are a health-care imperative. We found that 6% of patients in our population had low EF, consistent with previous studies1. BNP and N-terminal pro b-type natriuretic peptide (NTproBNP) have been proposed for detection of left ventricular systolic dysfunction. Bhalla et al.16 assessed BNP to screen for systolic and diastolic dysfunction and found an AUC of 0.60 for BNP and 0.70 for NTproBNP; results improved with the addition of impedance car- diography to 0.70 and 0.73, respectively. A Mayo Clinic study from Olmsted County assessing individuals aged 45 and older found the AUC was greater for individuals with more severe systolic dys- function (0.82 to 0.92) than than for those with any systolic dys- function (0.51 to 0.74)17. Moreover, optimal discriminatory levels for BNP varied with age and sex. In contrast, we found excellent AI network performance across all age and sex strata. This capabil- ity appears to be unique to the neural network screen. The specific ECG characteristics used by our convolutional neural network to classify individuals as having or lacking a low EF are not known, owing to the nature of neural networks, although we presume it is detecting the known pathological effects of heart failure on the ECG (Supplementary Fig. 2). However, by training the network with a large cohort of approximately 45,000 ECG and EF data pairs, the network was exposed to a sufficient number of ECG variants to clas- sify with certainly individuals with a low EF, as demonstrated by the AUC of 0.93 obtained when testing in a population of 52,870 indi- viduals. In contrast to previous applications of neural networks in medicine, such as for identification of mammographic lesions9, we expand the use of AI to extend beyond the capacity of human skills. Another important characteristic of our network is that it uses an inexpensive, standardized, ubiquitous test as its input—the 12-lead ECG. In many rural areas in the United States18 and in develop- ing countries, access to cardiological care and imaging is limited. The availability of a portable, inexpensive test for ventricular sys- tolic dysfunction permits optimal utilization of limited imaging resources, while also enabling individuals to benefit from early institution of effective therapies such as beta adrenergic blockers, angiotensin receptor antagonists, and, where available, implantable devices (defibrillators and cardiac resynchronization systems)13,19 . With the emergence of smartphone-enabled electrodes20, mobile applications may permit ECG use in resource-constrained regions. The software-based nature of test ‘samples’ for our network also enables continuous feedback and refinement, with rapid distribu- tion of system improvements. Our work is best understood in the context of its limitations. The accuracy, sensitivity, and specificity of the networks were all excel- lent, but the test’s positive predictive value was only 33.8%. This is in part because we selected an EF cutoff of 35%. We selected this detection threshold owing to the well-established outcome and therapeutic implications of this value19. However, identification of an EF of < 50% is still clinically significant, as this reflects an abnor- mal EF. When considering a higher cut-off for abnormal function of < 50%, the positive predictive value of the network was 63.4%. Of the ‘false positives’, 45% had an EF of < 50% but > 35% (29.5% of all positives). While statistically patients with an EF in this range are considered false positives, medically this value is actionable, and an echocardiogram is justified. An interesting finding was that in patients with a false positive result with a network-predicted low EF and an echocardiographic normal EF at the time of screening—a group that assessed with current technology would be considered healthy—there was a fourfold increase in risk of developing a low EF in the future compared with patients with a negative AI screen.\n\nFig. 1 | Creation of the study data sets. Schematic of the data set creation and analysis strategy, which was devised to assure a robust and reliable data set for training, validating, and testing of the network. Once a patient’s data were placed in one of the data sets, that individual’s data were used only in that set, avoiding ‘cross-contamination’ among the training, validation, and test data sets. Patients for whom data at more than one time point were available were used for the follow-up analysis, to determine whether an index abnormal AI screen with a normal EF (apparent false positive) was associated with future risk of a low EF. The details of the flow chart and how each of the data sets was used are described in the Methods. The final data sets used in the analysis for training, validating, and testing of the network are depicted as green boxes. Table 1 | Patient characteristics and comorbidities at enrollment Training set (n = 35,970) Validation set (n = 8,989) Test set (n = 52,870) P value Age, years 61.6 (16.5) 61.8 (16.5) 61.8 (16.5) 0.44 Age groups, n (%) 0.86 < 40 4,046 (11%) 1,008 (11%) 5,861 (11%) 40–49 3,875 (11%) 942 (10%) 5,599 (11%) 50–59 6,376 (18%) 1,587 (18%) 9,341 (18%) 60–69 8,559 (24%) 2,110 (23%) 12,649 (24%) 70–79 8,573 (24%) 2,158 (24%) 12,550 (24%) 80+ 4,541 (13%) 1,184 (13%) 6,870 (13%) Female 15,358 (43%) Sex, n (%) 0.64 3,821 (43%) 22,704 (43%) Male 20,612 (57%) 5,168 (57%) 30,166 (57%) Mean EF 56.3 (11.9) 56.1 (12.1) 56.2 (12.0) 0.45 Heart failure, n (%) 10,365 (20%) 7,003 (19%) 1,803 (20%) 0.45 Diabetes mellitus, n (%) 8,458 (24%) 2,079 (23%) 12,433 (24%) 0.71 Hypercholesterolemia, n (%) 15,593 (43%) 3,851 (43%) 23,059 (44%) 0.35 Renal disease, n (%) 6,929 (19%) 1,685 (19%) 10,219 (19%) 0.43 Hypertension, n (%) 16,831 (47%) 4,163 (46%) 24,643 (47%) 0.69 Coronary artery disease, n (%) 13,563 (38%) 3,380 (38%) 20,040 (38%) 0.77 Myocardial infarction, n (%) 4,556 (13%) 1,111 (12%) 6,770 (13%) 0.48 Enrollment refers to the time of the initial ECG–TTE data pair acquisition. Numbers in parentheses refer to the standard deviation (for age and sex) or to the percentage of individuals randomly assigned to a given subset (all other rows). P values refer to two-tailed analysis of variance or χ 2 tests for a difference in the distribution of values between the training, validation, and test sets. a b ROC Group Sensitivity Specificity OR (95% CI) Sensitivity 1.0 0.8 0.6 0.4 0.2 0 Female <40 74 (53/72) 95 (2,895/3,053) 51.11 (29.55–88.41) Male <40 72 (86/120) 92 (2,405/2,616) 28.83 (18.92–43.93) Female 40–49 74 (63/85) 92 (2,300/2,495) 33.78 (20.35–56.07) Male 40–49 80 (178/223) 89 (2,491/2,796) 32.31 (22.81–45.75) Female 50–59 85 (150/177) 91 (3,286/3,593) 59.46 (38.83–91.06) Male 50–59 86 (390/455) 87 (4,440/5,116) 39.41 (29.94–51.87) Female 60–69 90 (235/260) 89 (4,207/4,707) 79.09 (51.83–120.69) Male 60–69 89 (722/815) 84 (5,762/6,867) 40.48 (32.32–50.71) Female 70–79 80 (228/284) 86 (4,230/4,897) 25.82 (19.06–34.98) Male 70–79 90 (843/936) 78 (5,014/6,433) 32.03 (25.65–40.00) Female ≥80 84 (156/186) 81 (2,350/2,895) 22.42 (15.00–33.50) Male ≥80 89 (463/518) 73 (2,382/3,271) 22.56 (16.88–30.14) Validation set (AUC = 0.933) Testing set (AUC = 0.932) Overall 86 (3,567/4,131) 86 (41,762/48,739) 37.86 (34.52–41.52) Initially classified as normal EF Initially classified as low EF 0 0.2 0.4 0.6 0.8 1.0 0 25 50 75 100 125 1 − specificity OR Fig. 2 | Network ROC and sensitivity and specificity across age and gender subsets. a, The ROC of the convolutional neural network used to identify patients with an EF of ≤ 35%. The ROC curve and AUC were calculated using the validation and testing (holdout) data sets. b, The convolutional neural network’s sensitivity and specificity to detect EF ≤ 35% is tabulated across a range of age and gender combinations. The diagnostic OR, which is the ratio of positive likelihood ratio (sensitivity / (1− specificity)) to the negative likelihood ratio ((1− sensitivity) / specificity), as well as the associated 95% CI, is shown for each age and gender combination and for the overall study sample. The Breslow–Day test of homogeneity showed significant variation across the age and gender combinations (P < 0.0001). 50 40 Incidence of developing EF of ≤35% 30 20 10 0 35.3% (53) 23.6% (120) 17.6% (224) 11.1% (396) 10.4% (518) 7.2% (1,048) 5.8% (714) 2.3% (3,853) 3.9% (2,129) 1.0% (6,600) 0 3 6 9 12 15 Time after index ECG–TTE (years) Fig. 3 | Long-term incidence of developing an EF of ≤35% in patients with an initially normal EF stratified by AI classification. Long-term outcome of patients with an echocardiographic EF of ≥ 50% at the time of initial classification, stratified by the initial network classification. The ordinate shows the cumulative incidence of developing a low EF (35%), and the abscissa indicates years from the time of index ECG–TTE evaluation. A fourfold risk of future low EF was present when the AI algorithm defined the ECG as abnormal (age- and sex-adjusted HR, 4.1 (95% CI, 3.3–5.0), P < 0.001), compared with patients with an echocardiographic normal EF who were classified as having a normal EF by the ECG network. The numbers reported along the cumulative incidence curves reflect the estimated cumulative incidence (and number at risk, in parentheses) for each group at the times indicated along the axis.\n\nAcknowledgements The study was conceived, funded, and executed entirely by Mayo Clinic. There was no industry support of any kind. Author contributions D.J.L. and G.S. contributed to the literature search, study coordination, data management, and data collection. Z.I.A., P.A.F., S.K., and F.L.-J. contributed to the study design. C.G.S., Z.I.A., R.E.C., and F.L.-J. contributed to the data analysis. C.G.S., Z.I.A., Additional information Supplementary information is available for this paper at https://doi.org/10.1038/ s41591-018-0240-2. Reprints and permissions information is available at www.nature.com/reprints. Correspondence and requests for materials should be addressed to P.A.F. Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. © The Author(s), under exclusive licence to Springer Nature America, Inc. 2019\n\nReferences\n1. McDonagh, T. A., McDonald, K. & Maisel, A. S. Screening for asymptomatic\nlef ventricular dysfunction using B-type natriuretic Peptide. Congest. Heart Fail. 14, 5–8 (2008).\n2. Dargie, H. J. Effect of carvedilol on outcome afer myocardial infarction in\npatients with lef-ventricular dysfunction: the CAPRICORN randomised trial. Lancet 357, 1385–1390 (2001).\n3. Pfeffer, M. A. et al. Effect of captopril on mortality and morbidity in\npatients with lef ventricular dysfunction afer myocardial infarction. Results of the survival and ventricular enlargement trial. N. Engl. J. Med. 327, 669–677 (1992).\n4. Priori, S. G. et al. 2015 ESC guidelines for the management of patients\nwith ventricular arrhythmias and the prevention of sudden cardiac death: the Task Force for the Management of Patients with Ventricular Arrhythmias and the Prevention of Sudden Cardiac Death of the European Society of Cardiology (ESC). Endorsed by: Association for European Paediatric and Congenital Cardiology (AEPC). Eur. Heart J. 36, 2793–2867 (2015).\n5. Betti, I. et al. Te role of N-terminal PRO-brain natriuretic peptide and\nechocardiography for screening asymptomatic lef ventricular dysfunction in a population at high risk for heart failure. Te PROBE-HF study. J. Card. Fail. 15, 377–384 (2009).\n6. Red$eld, M. M. et al. Plasma brain natriuretic peptide to detect preclinical\nventricular systolic or diastolic dysfunction: a community-based study. Circulation 109, 3176–3181 (2004).\n7. Kim, J. H., Kwon, H. S. & Seo, H. W. Evaluating a pivot-based approach for\nbilingual lexicon extraction. Comput. Intell. Neurosci. 2015, 434153 (2015).\n8. Pasquier, M., Quek, C. & Toh, M. Fuzzylot: a novel self-organising\nfuzzy-neural rule-based pilot system for automated vehicles. Neural Netw. 14, 1099–1112 (2001).\n9. Salazar-Licea, L. A., Pedraza-Ortega, J. C., Pastrana-Palma, A. & Aceves-\nFernandez, M. A. Location of mammograms ROI’s and reduction of false-positive. Comput. Methods Programs Biomed. 143, 97–111 (2017).\n10. Wing$eld, C. et al. Relating dynamic brain states to dynamic machine states:\nhuman and machine solutions to the speech recognition problem. PLoS Comput. Biol. 13, e1005617 (2017).\n11. Yoshida, H. et al. Automated histological classi$cation of whole-slide images\nof gastric biopsy specimens. Gastric Cancer 21, 249–257 (2018).\n12. Al-Khatib, S. M. et al. 2017 AHA/ACC/HRS guideline for management\nof patients with ventricular arrhythmias and the prevention of sudden cardiac death: executive summary. A report of the American College of Cardiology/American Heart Association Task Force on Clinical Practice Guidelines and the Heart Rhythm Society. Circulation 138, e210–e271 (2018).\n13. Yancy, C. W. et al. 2013 ACCF/AHA guideline for the management of heart\nfailure: a report of the American College of Cardiology Foundation/American Heart Association Task Force on Practice Guidelines. J. Am. Coll. Cardiol. 62, e147–e239 (2013).\n14. Heidenreich, P. A. et al. Forecasting the impact of heart failure in the United\npeptide and B-type natriuretic peptide in the general community: determinants and detection of lef ventricular dysfunction. J. Am. Coll. Cardiol. 47, 345–353 (2006).\n18. Gruca, T. S., Pyo, T. H. & Nelson, G. C. Providing cardiology care in rural\nareas through visiting consultant clinics. J. Am. Heart Assoc. 5, e002909 (2016).\n19. Yancy, C. W. et al. 2017 ACC/AHA/HFSA focused update of the 2013 ACCF/\nAHA guideline for the management of heart failure: a report of the American College of Cardiology/American Heart Association Task Force on Clinical Practice Guidelines and the Heart Failure Society of America. J. Am. Coll. Cardiol. 70, 776–803 (2017).\n20. Yasin, O. Z. et al. Noninvasive blood potassium measurement using\nsignal-processed, single-lead ECG acquired from a handheld smartphone.\nJ. Electrocardiol. 50, 620–625 (2017).\nAcknowledgements The study was conceived, funded, and executed entirely by Mayo Clinic. There was no industry support of any kind. Author contributions D.J.L. and G.S. contributed to the literature search, study coordination, data management, and data collection. Z.I.A., P.A.F., S.K., and F.L.-J. contributed to the study design. C.G.S., Z.I.A., R.E.C., and F.L.-J. contributed to the data analysis. C.G.S., Z.I.A., Competing interests Mayo Clinic has licensed the underlying technology to EKO, a maker of digital stethoscopes with embedded ECG electrodes.  Mayo Clinic may receive financial benefit from the use of this technology, but at no point will Mayo Clinic benefit financially from its use for the care of patients at Mayo Clinic.  P.A.F., F.L.-J., S.K., and Z.I.A. may also receive financial benefit from this agreement. Additional information s41591-018-0240-2. Reprints and permissions information is available at www.nature.com/reprints. Correspondence and requests for materials should be addressed to P.A.F. Publisher’s note: Springer Nature remains neutral with regard to jurisdictional claims in © The Author(s), under exclusive licence to Springer Nature America, Inc. 2019 .\n\nData sources and study population. Following institutional review board approval, we obtained data from the Mayo Clinic digital data vault. We identi$ed 163,892 adult patients (18 years old or older) with at least one digital, standard 10-s 12-lead ECG acquired in the supine position between January 1994 and February 2017 and at least one TTE obtained within 14 d of the index ECG (Fig. 1). For patients with multiple ECG and TTE data sets meeting these criteria, the earliest pair was used for network creation, validation, or testing, and subsequent TTE data were used for analysis of follow-up. We performed a preliminary proof-of-concept assessment to obtain internal funding using 2,200 ECG–TTE data pairs, which were excluded from the present analysis, leaving a cohort of 97,829 patients whose $rst ECG–TTE paired data sets were used for the primary analysis. ECGs were acquired at a sampling rate of 500 Hz using a Marquette ECG machine (GE Healthcare) and stored using the MUSE data management system for later retrieval. Comprehensive 2-D or 3-D and Doppler echocardiography were available for all patients. Quantitative data were recorded at the time of the acquisition in a Mayo Clinic–developed, custom database (Echo Image Management System). EF is routinely measured or estimated using standardized methodologies, and in most reports, more than one value may be recorded. For the purpose of this study, the EF value used in the models was the first available from a standard hierarchical sequence: EF determined using 3-D echocardiography21, a biplane approach using the Simpson method, a 2-D method22, or M-mode and, in the absence of any of the preceding, the reported visually estimated EF. If the estimation was a range, we used the midpoint as a single EF value. EF was classified as low (≤ 35%), mildly depressed (35–49%), or normal (≥ 50%). Primary and secondary outcomes. The primary outcome was the ability of the AI network to identify patients with an EF of 35% or less using the ECG signal alone. This value was selected owing to its clear-cut clinical and therapeutic importance23. The secondary outcome was the ability of the AI network to identify individuals with a normal EF at the time of screening, but with an increased risk of subsequent low EF during follow-up. Overview of AI model development. We implemented a convolutional neural network using the Keras framework with a Tensorflow (Google) backend and Python24. Convolutional neural networks, which are commonly applied to images, operate such that the convolutions can be used to extract very subtle patterns in a data set. Each 12-lead ECG was considered a 12 × 5,000 (that is, 12 leads by 10-s duration sampled at 500 Hz) matrix, where the first dimension represents a spatial dimension and the second represents a temporal one24. We used the internal validation set to optimize the network architecture, and hyperparameters such as batch and step size. Multiple networks were tested, and the network that yielded the highest AUC of the receiver operator curve (ROC) for the validation data set was selected. In cases in which more than one network had similar results when tested using the validation set, the ‘simpler’ network—the one with fewer parameters or layers—was selected24. ECG analysis is mostly a visual task. While the signal is a time series, it is pseudocyclical, and its main features are morphologic25–27. To enable detection of patterns in these features, we used architectures that were based on convolutional layers for feature extraction. The main architecture tested was a convolutional neural network28,29 in which the convolutional blocks were followed by two fully connected layers. The selected network (Supplementary Fig. 3) was composed of six convolutional layers, each of which was followed by a nonlinear ‘Relu’ activation function, a batchnormalization layer30, and a max-pooling layer31. As these first six layers were designed to learn features within each lead, each convolutional layer had a K × 1 filter shape with N filters. K started at 5 and was decreased to 3 after the third layer, while N started at 16 and was increased to 32 in the third layer and to 64 in the fifth layer. After each ‘temporal’ layer, there was a Relu activation layer, a batchnormalization layer, and max-pooling layer (4 × 1 after the third and fifth layers and 2 × 1 after other temporal convolutional layers). Following the last temporal convolutional layer, a ‘spatial’ convolutional layer was used. In this layer, the filter shape was 12 × 1, allowing it to fuse data from the different leads (hence the term ‘spatial’). The reason a convolutional filter as opposed to a pooling layer was used for lead fusion is that the ECG is structured and the spatial structure of a 12-lead ECG is consistent among patients and adjusted per patient32,33. After this spatial layer, the data were fed to a fully connected network with two hidden layers (64 and 32 nodes) followed by Relu activation layers, batch-normalization layers, and dropout layers to avoid overfitting. The output layer had two classes and was activated using the ‘Softmax’ function34. Optimizing the architecture and hyperparameters. The convolutional neural network framework provides immense flexibility with structure, and as a result, turning the algorithm involves modification of both the architecture and the hyperparameters associated with each aspect of the model specification. We evaluated combinations of architecture and hyperparameters using an iterative process, including a formalized grid search for hyperparameters. In terms of architecture, we evaluated the change in model performance based on adding neurons to the fully connected layers. Increasing the number of neurons yielded lower performance (for example, 0.912 using 256 hidden nodes). Removing two of the pooling layers to allow more temporal features (by a factor of 16 overall) and replacing the fully connected layers with long short term memory (LSTM) layers to allow real temporal feature detection (128 and 32 LSTM cells in each layer) did not significantly improve the validation AUC (0.936) and increased the number of parameters (to 560,000). To exploit the pseudocyclical nature of the signal and reduce the number of parameters, we also tested the same architecture on a shorter segment of 2 s (1,000 samples zero-padded to 1,024) with an overlap of 1 s (500 samples). While the AUC per segment was lower (0.91), the average score of all nine segments (due to the overlap) per ECG yielded an AUC of 0.934. The architecture reduced the number of parameters by half, to 159,000, which helped optimize the training. All results presented in this manuscript were created using this specific architecture. Computer code is available upon request. AI-augmented ECG to identify a low EF. After selecting the optimal network using validation data, we created an ROC using the same validation set and measured its AUC as a primary assessment of network strength. We used the validation data set ROC to select two thresholds for the probability of having a low EF: the first was selected by giving an equal weight to sensitivity and specificity, and the second was selected to yield a sensitivity of 90% on the validation data set. The convolutional neural network model was then used on the test data to test its ability to predict a low EF. The two thresholds were used to calculate sensitivity, specificity, and accuracy in the test data, which had not been used for model training or threshold selection. To investigate network performance differences by age and sex, odds ratios (ORs) were calculated within groups. The Breslow–Day test was used to test for homogeneity of ORs across the groups defined on age and sex. Sensitivity and specificity were also calculated within these groups. AI-augmented ECG to predict a future low EF. We hypothesized that early in the course of any disease that impacts EF, ECG signals would show subtle abnormal patterns due to metabolic and structural derangements that had not affected a sufficient quantity of myocardium to cause a reduction in EF. We further hypothesized that the convolutional neural network would classify some of these cases as abnormal, giving the initial appearance of a false positive test (that is, an individual classified as having a low EF but reported as normal) that with time would become a true positive test. To test this hypothesis, we designed a substudy to identify patients that met the following conditions: (i) the network predicted the patient had a low EF; (ii) the individual had an echocardiogram performed within 14 d that demonstrated a normal EF (≥ 50%), indicating a false positive finding by the algorithm; and (iii) the individual had at least one additional echocardiogram (not used for training or testing) available at a later date. A control group (EF ≥ 50%) was created using the true negative cases (algorithm and clinical determinations were both consistent with not having a low EF). A Kaplan–Meier analysis was used to depict the incidence of low EF for the true negatives versus the false positives over time. Subsequently, Cox proportional hazards regression was used to estimate the hazard for low EF after adjusting for age and sex. Statistical considerations. For measures of diagnostic performance (AUC of ROC, sensitivity), the sample sizes are so large that normal CIs are expected to have a width of < 0.5%. As such, the CIs are not reported alongside the estimated values due to the values’ high precision. Continuous data are presented as mean ± s.d. Cox models are presented with two-sided P values without any correction for multiple testing. The effect size of the Cox model is represented by the HR and its associated 95% CI. Survival analyses were conducted using SAS version 9.4. The convolutional neural network was trained using Keras (version 2.0.3) and TensorFlow (version 1.0.1).\nReferences\n21. Yamani, H., Cai, Q. & Ahmad, M. Tree-dimensional echocardiography\nin evaluation of lef ventricular indices. Echocardiography 29, 66–75 (2012).\n22. Quinones, M. A. et al. A new, simpli$ed and accurate method for\ndetermining ejection fraction with two-dimensional echocardiography. Circulation 64, 744–753 (1981).\n23. Russo, A. M. et al. ACCF/HRS/AHA/ASE/HFSA/SCAI/SCCT/SCMR 2013\nappropriate use criteria for implantable cardioverter-de$brillators and cardiac resynchronization therapy: a report of the American College of Cardiology Foundation appropriate use criteria task force, Heart Rhythm Society, American Heart Association, American Society of Echocardiography, Heart Failure Society of America, Society for Cardiovascular Angiography and Interventions, Society of Cardiovascular Computed Tomography, and Society for Cardiovascular Magnetic Resonance. J. Am. Coll. Cardiol. 61, 1318–1368 (2013).\n24. van Rossum, G. Python Tutorial, Technical Report CS-R9526 (CWI,\nAmsterdam, 1995). signal parameters. In Proc. 20th Annual International Conference of the IEEE Engineering in Medicine and Biology Society 127–130 (IEEE, 1998).\n26. Sugrue, A. et al. Identi$cation of concealed and manifest long QT syndrome\nusing a novel T wave analysis program. Circ. Arrhythm. Electrophysiol. 9, e003830 (2016).\n27. Couderc, J. P. et al. T-wave morphology abnormalities in benign, potent, and\narrhythmogenic Ikr inhibition. Heart Rhythm 8, 1036–1043 (2011).\n28. Krizhevsky A., S I., Hinton G. E. ImageNet classi$cation with deep\nconvolutional neural networks. In Advances in Neural Information Processing Systems 1097–1105 (Neural Information Processing Systems, 2012).\n29. Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. Preprint\nat https://arxiv.org/abs/1412.6980 (2014).\n30. Ioffe, S. & Szegedy, C. Batch normalization: accelerating deep network\ntraining by reducing internal covariate shif. Preprint at https://arxiv.org/ abs/1502.03167 (2015).\n31. Nagi, J. et al. Max-pooling convolutional neural networks for vision-based\nhand gesture recognition. In Proc. 2011 IEEE International Conference on Signal and Image Processing Applications 342–347 (IEEE, 2011).\n32. Wilson, F. N. et al. Te precordial electrocardiogram. Am. Heart. J. 27,\n19–85 (2004).\n33. Khan, G. M. A new electrode placement method for obtaining 12-lead ECGs.\nOpen Heart 2, e000226 (2015).\n34. Cristianini, N. & Shawe-Taylor, J. An Introduction to Support Vector\nMachines and other Kernel-based Learning Methods (Cambridge University Press, New York, 2000).\n--- FOOTNOTES published maps and institutional affiliations", "persona": "Teacher", "response": "### 1. Overview\nArtificial intelligence (AI), defined as intelligence exhibited by machines, conjures up images of HAL, the rogue computer in Stanley Kubrick's epic 1968 science fiction movie, \"2001: A Space Odyssey.\" The idea of intelligence being incorporated into inanimate objects has been floated since antiquity. The first rudimentary AI programs were developed in 1951 to play checkers and chess. Since then, AI has expanded to almost every facet of modern life, including the medical field. The collaborative Mayo Clinic cardiovascular AI team recently published the results of their study utilizing AI electrocardiogram (ECG) analysis to predict the presence of left ventricular dysfunction in asymptomatic patients. Study results were published in Nature Medicine in 2019. Some basics Some common examples of machines that utilize versions of AI include: iRobot Roomba, which vacuums the floor and can navigate around obstacles Mars rovers Spirit and Opportunity Siri, Apple's virtual assistant These and most other devices utilize weak (narrow) AI . They operate within a restricted range of pre-defined functions to accomplish narrowly demarcated tasks. Deep learning ― strong AI ― is one of a family of machine learning methods based on learning data set representations.\n\n### 2. Background\nOccult disease detection, such as identifying atrial fibrillation's earliest, subclinical stages, through heart physiology signals transmitted by mobile ECG Advances in computing power so that large amounts of data can be quickly analyzed have made the application of AI to huge, complex data sets feasible. Deep learning architectures have been applied to diverse fields such as speech recognition, social network filtering, bioinformatics, drug design and medical image interpretation. Deep neural systems comprise a series of layers: An input layer Recurrent neural network Enlarge image Close Recurrent neural network Recurrent neural network Recurrent neural networks transform a single vector input by subjecting it to a series of hidden layers. Every layer is made up of a set of neurons (circles).\n\n### 3. Methods\nALVD is present in 3% to 6% of the general population, affecting more than 7 million Americans. Risk increases with age; in the elderly, ALVD is present in 9% of individuals. Effective therapies exist and, if administered early, can reduce hospitalizations and mortality significantly. Yet, no ALVD screening tool exists that is inexpensive, widely available and noninvasive to facilitate broad and early intervention by general clinicians. Currently, the gold standard screen for ALVD is echocardiogram, not readily available in nonspecialty clinics. B-type natriuretic peptide (BNP) levels have been studied for screening purposes, but results have been disappointing, and the test requires invasive phlebotomy. Dr. Friedman and his team envisioned screening for ALVD by subjecting the common and inexpensive ECG to a custom-designed and trained CNN . Explains Dr. Friedman: \"We hypothesized that the metabolic and structural derangements associated with the cardiomyopathic process would result in ECG changes that we could reliably detect with a properly trained neural network.\"\n\n### 4. Results\nAn output layer Pooling examples Enlarge image Close Pooling examples Pooling examples For each of the color regions represented in the original matrix input (left), the top right filter pulls the maximum value of that region and creates a new output matrix; this is the most common pooling method utilized. The lower right example illustrates applying an averaging filter to create a new output matrix. The team tested the network on 52,870 patients reserved for external validation. Of those, approximately 8% had an EF < 35%. The sensitivity, specificity and accuracy of the CNN were 85%, 86% and 86%, respectively.\n\n### 5. Takeaways\n\"The network ranked 0.93 in its ability to flag low EFs — out of a perfect score of 1.0. To put this in perspective, a mammogram is 0.85. These results show the network is very robust,\" Dr. Friedman says. Early risk prediction of conditions such as embolic stroke", "paper_id": "61", "story_id": "61-02"}
{"paper": "There are currently no effective biomarkers for diagnosing Parkinson’s disease (PD) or tracking its progression. Here, we developed an artificial intelligence (AI) model to detect PD and track its progression from nocturnal breathing signals. The model was evaluated on a large dataset comprising 7,671 individuals, using data from several hospitals in the United States, as well as multiple public datasets. The AI model can detect PD with an area-under-the-curve of 0.90 and 0.85 on held-out and external test sets, respectively. The AI model can also estimate PD severity and progression in accordance with the Movement Disorder Society Unified Parkinson’s Disease Rating Scale (R= 0.94, P= 3.6 × 10–25). The AI model uses an attention layer that allows for interpreting its predictions with respect to sleep and electroencephalogram. Moreover, the model can assess PD in the home setting in a touchless manner, by extracting breathing from radio waves that bounce off a person’s body during sleep. Our study demonstrates the feasibility of objective, noninvasive, at-home assessment of PD, and also provides initial evidence that this AI model may be useful for risk assessment before clinical diagnosis\n\nPD is the fastest-growing neurological disease in the world1 . Over 1million people in the United States are living with PD as of 2020 (ref. 2 ), resulting in an economic burden of $52 billion per year3 . Thus far, no drugs can reverse or stop the progression caused by the disease4 . A key difficulty in PD drug development and disease management is the lack of effective diagnostic biomarkers5 . The disease is typically diagnosed based on clinical symptoms, related mainly to motor functions such as tremor and rigidity6 . However, motor symptoms tend to appear several years after the onset of the disease, leading to late diagnosis4 . Thus, there is a strong need for new diagnostic biomarkers, particularly ones that can detect the disease at an early stage. There are also no effective progression biomarkers for tracking the severity of the disease over time5 . Today, assessment of PD progression relies on patient self-reporting or qualitative rating by a clinician7 . Typically, clinicians use a questionnaire called the Movement Disorder Society Unified Parkinson’s Disease Rating Scale (MDS-UPDRS)8 . The MDS-UPDRS is semisubjective and does not have enough sensitivity to capture small changes in patient status9–11. As a result, PD clinical trials need to last several years before changes in MDS-UPDRS can be reported with sufficient statistical confidence9,12, which increases cost and delays progress13. The literature has investigated a few potential PD biomarkers, among which cerebrospinal fluid14,15, blood biochemical16 and neuroimaging17 have good accuracy. However, these biomarkers are costly, invasive and require access to specialized medical centers and, as a result, are not suitable for frequent testing to provide early diagnosis or continuous tracking of disease progression. A relationship between PD and breathing was noted as early as 1817, in the work of James Parkinson18. This link was further strengthened in later work which reported degeneration in areas in the brainstem that control breathing19, weakness of respiratory muscle function20 and sleep breathing disorders21–24. Further, these respiratory symptoms often manifest years before clinical motor symptoms20,23,25, which indicates that the breathing attributes could be promising for risk assessment before clinical diagnosis. In this article, we present a new AI-based system (Fig. 1 and Extended Data Fig. 1) for detecting PD, predicting disease severity and tracking disease progression over time using nocturnal breathing. The system takes as input one night of breathing signals, which can be collected using a breathing belt worn on the person’s chest or abdomen26. Alternatively, the breathing signals can be collected without wearable devices by transmitting a low power radio signal and analyzing its reflections off the person’s body27–29. An important component of the design of this model is that it learns the auxiliary task of predicting the person’s quantitative electroencephalogram (qEEG) from nocturnal breathing, which prevents the model from overfitting and helps in interpreting the output of the model. Our system aims to deliver a diagnostic and progression digital biomarker that is objective, nonobtrusive, low-cost and can be measured repeatedly in the patient’s home.\n\nResults\nDatasets and model training. We use a large and diverse dataset created by pulling several datasets from several sources, including the Mayo Clinic, Massachusetts General Hospital (MGH) sleep lab, observational PD clinical trials sponsored by the Michael (NIH) Udall Center, an observational study conducted by the Heart Health Study (SHHS)26 and the MrOS Sleep Study (MrOS)30. The combined dataset contains 11,964 nights with over 120,000 h of nocturnal breathing signals from 757 PD subjects (mean (s.d.) age 69.1 (10.4), 27% women) and 6,914 control subjects (mean (s.d.) age 66.2 (18.3), 30% women). Table 1 summarizes the datasets and Extended Data Table 1 describes their demographics. The data were divided into two groups: the breathing belt datasets and the wireless datasets. The first group comes from polysomnography (PSG) sleep studies and uses a breathing belt to record the person’s breathing throughout the night. The second group collects nocturnal breathing in a contactless manner using a radio device27. The radio sensor is deployed in the person’s bedroom, and analyzes the radio reflections from the environment to extract the person’s breathing signal28,29. The breathing belt datasets have only one or two nights per person and lack MDS-UPDRS and Hoehn and Yahr (H&Y) scores32. In contrast, the wireless datasets include longitudinal data for up to 1 year and MDS-UPDRS and H&Y scores, allowing us to validate the model’s predictions of PD severity and its progression. Since some individuals in the wireless datasets are fairly young (for example, in their 20s or 30s), when testing on the wireless data, we limit ourselves to the PD patients and their age-matched control subjects (that is, 10 control subjects from the Udall and MJFF studies and 18 age and gender-matched subjects from the MIT and MGH studies for a total of 28 control individuals). Control subjects missing MDS-UPDRS or H&Y scores receive the mean value for the control group. Subjects used in training the neural network were not used for testing. We performed k-fold cross-validation (k = 4) for PD detection, and leave-one-out validation for severity prediction. We also Exhale Inhale (Full-night) nocturnal breathing Disease detection: has PD or not? PD severity prediction (Estimate of patient’s MDS-UPDRS) 85 assessed cross-institution prediction by training and testing the model on data from different medical centers. Furthermore, data from the Mayo Clinic was kept as external data, never seen during development or validation, and used only for a final test. Evaluation of PD diagnosis. We evaluated the accuracy of diagnosing PD from one night of nocturnal breathing. Figure 2a,b show the receiver operating characteristic (ROC) curves for data from breathing belt and data from wireless signals, respectively. The AI model detects PD with high accuracy. For nights measured using a breathing belt, the model achieves an area under the ROC curve (AUC) of 0.889 with a sensitivity of 80.22% (95% confidence interval (CI) (70.28%, 87.55%)) and specificity of 78.62% (95% CI (77.59%, 79.61%)). For nights measured using wireless signals, the model achieves an AUC of 0.906 with a sensitivity of 86.23% (95% CI (84.08%, 88.13%)) and specificity of 82.83% (95% CI (79.94%, 85.40%)). Extended Data Fig. 2 further shows the cumulative distributions of the prediction score for PD diagnosis. We further investigated whether the accuracy improves by combining several nights from the same individual. We use the wireless datasets since they have several nights per subject (mean (SD) 61.3 (42.5)), and compute the model prediction score for all nights. The PD prediction score is a continuous number between 0 and 1, where the subject is considered to have PD if the score exceeds 0.5. We use the median PD score for each subject as the final diagnosis result. As shown in Fig. 2d,e, with several nights considered for each subject, both sensitivity and specificity of PD diagnosis further increase to 100% for the PD and control subjects in this cohort. Next, we compute the number of nights needed to achieve a high test–retest reliability31. We use the wireless datasets, and compute the test–retest reliability by averaging the prediction across consecutive nights within a time window. The results show that the reliability improves when we use several nights from the same subject, and reaches 0.95 (95% CI (0.92, 0.97)) with only 12 nights (Fig. 2c). Table 1 | Characteristics of the datasets used in this study Data source Data type Source of breathing signals No. of PD patients No. of controls MDS-UPDRS H&Y stage No. of nights per subject Mayo Clinic (External test cohort) PSG sleep study (sampled from the population visiting the Mayo Clinic sleep lab) Breathing belt 644 1,276 – PD: 2.2 (1.0) Control: N/A 1 (0) SHHS Visit 2 PSG sleep study (heart disease, sleep disorders) Breathing belt 13 2,617 – – 1 (0) MrOS Sleep Study PSG sleep study (sleep disorders, vascular disease) Breathing belt 48 2,827 – – 1.4 (0.5) MGH study PSG sleep study (sampled from the population visiting the MGH sleep lab) Breathing belt 27 120 PD: 39.8 (17.4) Control: N/A PD: 2.2 (0.4) Control: N/A 1 (0) MGH study Sleep study (sampled from the population visiting the MGH sleep lab) Wireless 0 8 N/A N/A 9.5 (4.0) Udall study Observational clinical study in PD Wireless 14 6 PD: 61.1 (20.1) Control: 1.8 (2.0) MJFF Parkinson’s study Observational clinical study in PD Wireless 11 4 PD: 58.3 (19.3) Control: 7.0 (1.9) MIT study Sleep study (healthy volunteers) Wireless 0 56 N/A N/A 18.7 (24.4) Dashes, unavailable data; N/A, inapplicable data. subjects out of which 644 have PD) from an independent hospital not involved during model development (Mayo Clinic). Our model achieved an AUC of 0.851 (Fig. 2f). The performance indicates that our model can generalize to diverse data sources from institutions not encountered during training. We also examined the cross-institution prediction performance by testing the model on data from one institution, but training it on data from the other institutions excluding the test institution. For breathing belt data, and as highlighted in Fig. 2g,h, the model achieved a cross-institution AUC of 0.857 on SHHS and 0.874 on MrOS. For wireless data, the cross-institution performance was 0.892 on MJFF, 0.884 on Udall, 0.974 on MGH and 0.916 on MIT. These results show that the model is highly accurate on data from institutions it never saw during training. Hence, the accuracy is not due to leveraging institution-related information, or misattribution of institution-related information to the disease. We also studied the feasibility of predicting each of the four subparts of MDS-UPDRS (that is, predicting subparts I, II, III and IV). This was done by replacing the module for predicting the total MDS-UPDRS by a module that focuses on the subpart of interest, while keeping all the other components of the neural network unmodified. Figure 3d–g show the correlation between the model prediction and the different subparts of MDS-UPDRS. We observe a strong correlation between model prediction and Part I (R = 0.84, P = 2 × 10–15), Part II (R = 0.91, P = 2.9 × 10–21) and Part III (R = 0.93, P = 7.1 × 10–24) scores. This indicates that the model captures both nonmotor (for example, Part I), and motor symptoms (for example, Part II and III) of PD. The model’s prediction has mild correlation with Part IV (R = 0.52, P = 7.6 × 10–5). This may be caused by the large overlap between PD and control subjects in Part IV scores (that is, most of the PD patients and control subjects in the studied population have a score of 0 for Part IV). We also compared the severity prediction of our model with H&Y stage32—another standard for PD severity estimation. The H&Y stage uses a categorical scale, where a higher stage indicates worse severity. Again, we used the Udall and the MJFF datasets since they report the H&Y scores and have several nights per subject. Figure 3b shows that, even though it is not trained using H&Y, the model can differentiate patients reliably in terms of their H&Y stages (P = 5.6 × 10–8, Kruskal–Wallis test). Finally, we computed the test–retest reliability of PD severity prediction on the same datasets in Fig. 3c. Our model provides consistent and reliable predictions for assessing PD severity with its reliability reaching 0.97 (95% CI (0.95, 0.98)) with 12 nights per subject. Evaluation of PD severity prediction. Today the MDS-UPDRS is the most common method for evaluating PD severity, with higher scores indicating more severe impairment. Evaluating MDS-UPDRS requires effort from both patients and clinicians: patients are asked to visit the clinic in person and evaluations are performed by trained clinicians who categorize symptoms based on quasi-subjective criteria9. We evaluate the ability of our model to produce a PD severity score that correlates well with the MDS-UPDRS simply by analyzing the patients’ nocturnal breathing at home. We use the wireless dataset where MDS-UPDRS assessment is available, and each subject has several nights of measurements (n = 53 subjects, 25 PD subjects with a total of 1,263 nights and 28 controls with a total of 1,338 nights). We compare the MDS-UPDRS at baseline with the model’s median prediction computed over the nights from the 1-month period following the subject’s baseline visit. Figure 3a shows strong correlation between the model’s severity prediction and the MDS-UPDRS (R = 0.94, P = 3.6 × 10–25), providing evidence that the AI model can capture PD disease severity. PD: 2.3 (0.6) Control: 0.2 (0.4) 86.7 (67.2) PD: 2.2 (0.7) Control: 0 (0) 35.1 (19.1) 1.0 1.0 0.8 0.8 Sensitivity Sensitivity 0.6 0.6 0.4 0.4 Breathing belt AUC: 0.889 Wireless signal AUC: 0.906 0.2 0.2 0 0 0 0.2 0.4 0.6 0.8 1.0 0 0.2 0.4 0.6 0.8 1.0 Specificity Threshold d 1.00 PD pred. score 0.75 0.50 0.25 Non-PD 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 Control subject ID e 1.00 PD pred. score 0.75 0.50 0.25 Non-PD 0 threshold 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 PD subject ID f g 1.0 1.0 0.8 0.8 Sensitivity Sensitivity 0.6 0.6 (External test) (Cross-institution test) SHHS AUC: 0.857 Mayo clinic 0.4 0.4 AUC: 0.851 0.2 0.2 0 0 0 0.2 0.4 0.6 0.8 1.0 0 0.2 0.4 0.6 0.8 1.0 Specificity Fig. 2 | PD diagnosis from nocturnal breathing signals. a, ROC curves for detecting PD from breathing belt (n = 6,660 nights from 5,652 subjects). b, ROC curves for detecting PD from wireless data (n = 2,601 nights from 53 subjects). c, Test–retest reliability of PD diagnosis as a function of the number of nights used by the AI model. The test was performed on 1 month of data from each subject in the wireless dataset (n = 53 subjects). The dots and the shadow denote the mean and 95% CI, respectively. The model achieved a reliability of 0.95 (95% CI (0.92, 0.97)) with 12 nights of data. d,e, Distribution of PD prediction (pred.) scores for subjects with several nights (n1 = 1,263 nights from 25 PD subjects and n2 = 1,338 nights from 28 age- and gender-matched controls). The graphs show a boxplot of the prediction scores as a function of the subject ids. On each box, the central line indicates the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively. The whiskers extend to 1.5 times the interquartile range. Points beyond the whiskers are plotted individually using the + symbol. f, ROC curves for detecting PD on an external test set from Mayo Clinic (n = 1,920 nights from 1,920 subjects). The model has an AUC of 0.851 with a sensitivity of 80.12% and specificity of 72.65%. g, Cross-institution PD prediction performance on SHHS (n = 2,630 nights from 2,630 subjects). h, Cross-institution PD prediction performance on MrOS (n = 3,883 nights from 2,875 subjects). In this analysis, all data from one institution was held back as test data, and the AI model was retrained excluding all data from that institution. Cross-institution prediction achieved an AUC of 0.857 with a sensitivity of 76.92% and specificity of 83.45% on SHHS, and an AUC of 0.874 with a sensitivity of 82.69% and specificity of 75.72% on MrOS. who were diagnosed with PD by their second visit, but had no such diagnosis by their first visit, and refer to them as the ‘prodromal PD group’ (n = 12). To select the ‘control group’, we sample subjects from the MrOS dataset who did not have a PD diagnosis in the first visit or in the second visit, occurring 6 years later. For each of the subjects in the prodromal group, we sample up to 40 control subjects that are age- and gender-matched, resulting in 476 qualified control subjects. We evaluated our model on breathing data from the first visit, when neither the prodromal group nor the control group had a PD diagnosis. Figure 4a shows that the model gives the prodromal group (that is, subjects eventually diagnosed with PD) much higher PD scores than the control group (P = 4.27 × 10–6, one-tailed 1.00 Test–retest reliability 0.95 0.75 0.50 0.25 95% CI 0 0 2 4 6 8 10 12 Specificity Number of nights PD PD h 1.0 0.8 Sensitivity 0.6 (Cross-institution test) MrOS AUC: 0.874 0.4 0.2 0 0 0.2 0.4 0.6 0.8 1.0 Specificity Specificity Wilcoxon rank-sum test). Indeed, the model predicts 75% of them as individuals with PD before their reported PD diagnosis. PD disease progression. Today, assessment of PD progression relies on MDS-UPDRS, which is semisubjective and does not have enough sensitivity to capture small, progressive changes in patient status9,10. As a result, PD clinical trials need to last for several years before changes in MDS-UPDRS can be reported with sufficient statistical confidence9,12, which creates a great challenge for drug development. A progression marker that captures statistically significant changes in disease status over short intervals could shorten PD clinical trials. 100 Kruskal–Wallis, P = 5.6 × 10–8 R = 0.94, P = 3.6 × 10–25 80 80 Model prediction Model prediction 60 60 40 n = 19 40 20 20 95% CI Control (n = 28) PD (n = 25) n = 27 n = 2 0 0 0 20 40 60 80 100 0 1 2 3 4 MDS–UPDRS H&Y stage d e R = 0.84, P = 2 × 10 –15 R = 0.91, P = 2.9 × 10 –21 20 30 Model prediction Model prediction 15 20 10 10 5 95% CI Control (n = 28) PD (n = 25) 95% CI Control (n = 28) PD (n = 25) 0 0 0 5 10 15 20 0 10 20 30 MDS–UPDRS Part I MDS–UPDRS Part II Fig. 3 | PD severity prediction from nocturnal breathing signals. a, Severity prediction of the model with respect to MDS-UPDRS (two-sided t-test). The center line and the shadow denote the mean and 95% CI, respectively. b, Severity prediction distribution of the model with respect to the H&Y stage; a higher H&Y stage indicates increased PD severity (Kruskal–Wallis test). On each box, the central line indicates the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively. The whiskers extend to 1.5 times the interquartile range. c, Test–retest reliability of PD severity prediction as a function of the number of nights per subject. The dots and the shadow denote the mean and 95% CI, respectively. The model achieved a reliability of 0.97 (95% CI (0.95, 0.98)) with 12 nights of data. d, Correlation of the AI model predictions with MDS-UPDRS subpart I (two-sided t-test). e, Correlation of the AI model predictions with MDS-UPDRS subpart II (two-sided t-test). f, Correlation of the AI model predictions with MDS-UPDRS subpart III (two-sided t-test). g, Correlation of the AI model predictions MDS-UPDRS subpart IV (two-sided t-test). The center line and the shadow denote the mean and 95% CI, respectively. Data in all panels are from the wireless dataset (n = 53 subjects). We evaluated disease progression tracking on data from the Udall study, which includes longitudinal data from participants with PD 6 months (n = 13) and 12 months (n = 12) into the study. For those individuals, we assess their disease progression using two methods. In the first method, we use the difference in the clinician-scored MDS-UPDRS at baseline and at month 6, or month 12. In the second method, we use the change in their predicted MDS-UPDRS over 6 months or 12 months. To compute the change in the predicted MDS-UPDRS, we took the data from the 1 month following baseline and computed its median MDS-UPDRS prediction, and the month following the month-6 visit and computed its median MDS-UPDRS prediction. We then subtracted the median at month 6 from the median at baseline. We repeated the same procedure for computing the prediction difference between month 12 and baseline. We plotted the results in Fig. 4b,c. The results show both the 6-month and 1-year changes in MDS-UPDRS as scored by a clinician are not statistically significant (6-month P = 0.983, 12-month P = 0.748, one-tailed one-sample Wilcoxon signed-rank test), which is consistent with previous observations9,10,12. In contrast, the model’s estimates of changes in MDS-UPDRS over the same periods are statistically significant (6-month P = 0.024, 12-month P = 0.006, one-tailed one-sample Wilcoxon signed-rank test). The key reason why our model can achieve statistical significance for progression analysis while the clinician-scored MDS-UPDRS cannot stems from being able to aggregate measurements from several nights. Any measurement, whether the clinician-scored MDS-UPDRS or the model-predicted MDS-UPDRS, has some noise. By aggregating a large number of measurements, one can 1.00 0.97 Test–retest reliability n = 2 0.75 n = 3 0.50 0.25 95% CI 0 0 2 4 6 8 10 12 Number of nights f g R = 0.52, P = 7.6 × 10 –5 R = 0.93, P = 7.1 × 10 –24 40 20 Model prediction Model prediction 30 20 10 10 95% CI Control (n = 28) PD (n = 25) 95% CI Control (n = 28) PD (n = 25) 0 0 0 10 20 30 40 50 0 5 10 15 20 MDS–UPDRS Part III MDS–UPDRS Part IV reduce the noise and improve sensitivity to disease progression over a short period. This is feasible for the model-predicted MDS-UPDRS because the measurements can be repeated every night with no overhead to patients. In contrast, one cannot do the same for the clinician-scored MDS-UPDRS as it is infeasible to ask the patient to come to the clinic every day to repeat the MDS-UPDRS test. This point is illustrated in Extended Data Fig. 3, which shows that, if the model-predicted MDS-UPDRS used a single night for tracking progression, then, similarly to clinician-scored MDS-UPDRS, it would be unable to achieve statistical significance. To provide more insight, we examined continuous severity tracking over 1 year for the patient in our cohort who exhibited the maximum increase in MDS-UPDRS over this period (Fig. 4d). The results show that the AI model can achieve statistical significance in tracking disease progression in this patient from one month to the next (P = 2.9 × 10–6, Kruskal–Wallis test). The figure also shows that the clinician-scored MDS-UPDRS is noisy; the MDS-UPDRS at month 6 is lower than at baseline, although PD is a progressive disease and the severity should be increasing monotonically. Finally, we note that the above results persist if one controls for changes in symptomatic therapy. Specifically, we repeated the above analysis, limiting it to patients who had no change in symptomatic therapy. The changes in the model-predicted MDS-UPDRS are statistically significant (6-month P = 0.049, 12-month P = 0.032, one-tailed one-sample Wilcoxon signed-rank test), whereas the changes in the clinician-scored MDS-UPDRS are statistically insignificant (6-month P = 0.894, 12-month P = 0.819, one-tailed one-sample Wilcoxon signed-rank test). 4.27 × 10–6 15 P = 0.983 P = 0.024 6–month MDS–UPDRS change 10 0.75 PD Pd prediction score 5 0.50 0 n = 12 –5 0.25 Non-PD n = 13 n = 476 –10 Threshold 0 –15 Control group Prodromal PD group Clinician assessment Model prediction d Kruskal–Wallis, P = 2.9 × 10–6 80 70 Model prediction Baseline: 60 60 50 Month–6: 47 40 September December November March February January October 2019 2020 Fig. 4 | Model evaluation for PD risk assessment before actual diagnosis, and disease progression tracking using longitudinal data. a, Model prediction scores for the prodromal PD group (that is, undiagnosed individuals who were eventually diagnosed with PD) and the age- and gender-matched control group (one-tailed Wilcoxon rank-sum test). b, The AI model assessment of the change in MDS-UPDRS over 6 months (one-tailed one-sample Wilcoxon signed-rank test) and the clinician assessment of the change in MDS-UPDRS over the same period (one-tailed one-sample Wilcoxon signed-rank test). c, The AI model assessment of the change in MDS-UPDRS over 12 months (one-tailed one-sample Wilcoxon signed-rank test) and the clinician assessment of the change in MDS-UPDRS over the same period (one-tailed one-sample Wilcoxon signed-rank test). d, Continuous severity prediction across 1 year for the patient with maximum MDS-UPDRS increase (Kruskal–Wallis test; n = 365 nights from 1 September 2019 to 31 October 2020). For each box in a–d, the central line indicates the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively. The whiskers extend to 1.5 times the interquartile range. Distinguishing PD from Alzheimer’s disease. We additionally tested the ability of the model to distinguish between PD and Alzheimer’s disease (AD)—the two most common neurodegenerative diseases. To evaluate this capability, we leveraged the SHHS26 and MrOS30 datasets, which contain subjects identified with AD (Methods). In total, 99 subjects were identified with AD, with 9 of these also reported to have PD. We excluded subjects with both AD and PD, and evaluate the ability of our model to distinguish the PD group (n = 57) from the AD group (n = 91). Extended Data Fig. 4 shows that the model achieves an AUC of 0.895 with a sensitivity of 80.70% and specificity of 78.02% in differentiating PD from AD, and reliably distinguished PD from AD subjects (P = 3.52 × 10–16, one-tailed Wilcoxon rank-sum test). Model interpretability. Our AI model employs a self-attention module33, which scores each interval of data according to its contribution to making a PD or non-PD prediction (model details in Methods). Since the SHHS and MrOS datasets include EEG signals and sleep stages throughout the night, we can analyze the breathing periods with high attention scores, and the corresponding sleep stages and EEG bands. Such analysis allows for interpreting and explaining the results of the model. 15 P = 0.748 P = 0.006 12–month MDS–UPDRS change 10 5 0 n = 12 n = 13 –5 n = 12 –10 No increase No increase –15 Clinician assessment Model prediction Month–12: 68 September June August May October April July The analysis shows that the attention of the model focuses on periods with relatively high qEEG Delta activity for control individuals, while focusing on periods with high activities in β and other bands for PD patients (Fig. 5a). Interestingly, these differences are aligned with previous work that observed that PD patients have reduced power in Delta band and increased power in β and other EEG bands during non-REM (rapid eye movement) sleep34,35. Further, comparing the model’s attention to the person’s sleep stages shows that the model recognizes control subjects by focusing on their light/deep sleep periods, while attending more to sleep onset and awakenings in PD patients (Fig. 5b). This is consistent with the medical literature, which reports that PD patients have substantially less light and deep sleep, and more interruptions and wakeups during sleep34,36, and the EEG in PD patients during sleep onset and awake periods show abnormalities in comparison with non-PD individuals37–39. (For more insight, Extended Data Fig. 5 shows a visualization of the attention score for one night, and the corresponding sleep stages and qEEG for a PD and a control individual.) P = 3.57 × 10–273 0.25 P = 1.28 × 10–49 0.75 0.20 Attention score 0.15 0.50 0.10 0.25 0.05 0 0 Control PD Control PD Δ band θ band b P = 1.39 × 10–234 P = 4.89 × 10–282 1.0 1.0 0.8 0.8 Attention score 0.6 0.6 0.4 0.4 0.2 0.2 0 0 Control PD Control PD Sleep onset Awake Fig. 5 | Interpretation of the output of the AI model with respect to EEG and sleep status. a,b, Attention scores were aggregated according to sleep status and EEG bands for PD patients (n = 736 nights from 732 subjects) and controls (n = 7,844 nights from 6,840 subjects). Attention scores were normalized across EEG bands or sleep status. Attention scores for different EEG bands between PD patients and control individuals (one-tailed Wilcoxon rank-sum test) (a). Attention scores for different sleep status between PD patients and control individuals (one-tailed Wilcoxon rank-sum test) (b). On each box in a and b, the central line indicates the median, and the bottom and top edges of the box indicate the 25th and 75th percentiles, respectively. The whiskers extend to 1.5 times the interquartile range. (that is, uses an auxiliary task of predicting qEEG) and transfer learning (Methods). We conducted ablation experiments to assess the benefits of (1) the qEEG auxiliary task, and (2) the use of transfer learning. To do so, we assessed the AUC of the model with and without each of those components. The results show that the qEEG auxiliary task is essential for good AUC, and transfer learning further improves the performance (Extended Data Fig. 6). Comparison with machine learning baselines. We compared the performance of our model with that of two machine learning models: a support vector machine (SVM)40 model, and a basic neural network that employs ResNet and LSTM but lacks our transfer learning module and the qEEG auxiliary task41. (More details about these baselines are provided in Methods.) The results show that both SVM and the ResNet+LSTM network substantially underperform our model (Extended Data Fig. 7). Performance for different disease durations. We examined the accuracy of the model for different disease durations. We considered the Udall dataset, where disease duration was collected for each PD patient. We divided the patients into three groups based on their disease duration: less than 5 years (n = 6), 5 to 10 years (n = 4) and over 10 years (n = 4). The PD detection accuracy using one night per patient for these groups is: 86.5%, 89.4% and 93.9%, respectively. The PD detection accuracy increases to 100% for all three groups when taking the median prediction over 1 month. The errors in predicting the MDS-UPDRS for the three groups were 9.7%, 7.6% and 13.8%, respectively. These results show that the model has good performance across a wide range of disease durations. P = 1.84 × 10–287 P = 2.79 × 10–275 0.3 0.2 0.2 0.1 0.1 0 0 Control PD Control PD α band β band P = 2.74 × 10–160 P = 7.04 × 10–163 1.0 1.0 0.8 0.8 0.6 0.6 0.4 0.4 0.2 0.2 0 0 Control PD Control PD Light sleep Deep sleep Evaluation of qEEG prediction from nocturnal breathing. Finally, since our model predicts qEEG from nocturnal breathing as an auxiliary task, we evaluate the accuracy of qEEG prediction. We use the SHHS, MrOS and MGH datasets, which include nocturnal EEG. The results show that our prediction can track the ground-truth power in different qEEG bands with good accuracy (Extended Data Fig. 8 and Supplementary Fig. 1).\n\nDiscussion\nThis work provides evidence that AI can identify people who have PD from their nocturnal breathing and can accurately assess their disease severity and progression. Importantly, we were able to validate our findings in an independent external PD cohort. The results show the potential of a new digital biomarker for PD. This biomarker has several desirable properties. It operates both as a diagnostic (Figs. 2 and 3) and a progression (Fig. 4d) biomarker. It is objective and does not suffer from the subjectivity of either patient or clinician (Fig. 4b,c). It is noninvasive and easy to measure in the person’s own home. Further, by using wireless signals to monitor breathing, the measurements can be collected every night in a touchless manner. Our results have several implications. First, our approach has the potential of reducing the cost and duration of PD clinical trials, and hence facilitating drug development. The average cost and time of PD drug development are approximately $1.3 billion and 13 years, respectively, which limits the interest of many pharmaceutical companies in pursuing new therapies for PD13. PD is a disease that progresses slowly and current methods for tracking disease progression are insensitive and cannot capture small changes9,10,12,13; hence, they require several years to detect progression9,10,12,13. In contrast, our AI-based biomarker has shown potential evidence of increased sensitivity to progressive changes in PD (Fig. 4d). This can help shorten clinical trials, reduce cost and speed up progress. Our approach can also improve patient recruitment and reduce churn because measurements can be collected at home with no overhead to patients. Second, about 40% of individuals with PD currently do not receive care from a PD specialist42. This is because PD specialists are concentrated in medical centers in urban areas, while patients are spread geographically, and have problems traveling to such centers due to old age and limited mobility. By providing an easy and passive approach for assessing disease severity at home and tracking changes in patient status, our system can reduce the need for clinic visits and help extend care to patients in underserved communities. Third, our system could also help in early detection of PD. Currently, diagnosis of PD is based on the presence of clinical motor symptoms6 , which are estimated to develop after 50–80% of dopaminergic neurons have already degenerated43. Our system shows initial evidence that it could potentially provide risk assessment before clinical motor symptoms (Fig. 4a). We envision that the system could eventually be deployed in the homes of PD patients and individuals at high risk for PD (for example, those with LRRK2 gene mutation) to passively monitor their status and provide feedback to their provider. If the model detects severity escalation in PD patients, or conversion to PD in high-risk individuals, the clinician could follow up with the patient to confirm the results either via telehealth or a visit to the clinic. Future research is required to establish the feasibility of such use pattern, and the potential impact on clinical practice. Our study also has some limitations. PD is a nonhomogeneous disease with many subtypes44. We did not explore subtypes of PD and whether our system works equally well with all subtypes. Another limitation of the paper is that both the progression analysis and preclinical diagnosis were validated in a small number of participants. Future studies with larger populations are required to further confirm those results. Also, while we have confirmed that our system could separate PD from AD, we did not investigate the ability of our model to separate PD from broader neurological diseases. Further, while we have tested the model across institutions and using independent datasets, further studies can expand the diversity of datasets and institutions. Additionally, our empirical results highlight a strong connection between PD and breathing and confirm past work on the topic; however, the mechanisms that lead to the development and progression of respiratory symptoms in PD are only partially understood and require further study. Finally, our work shows that advances in AI can support medicine by addressing important unsolved challenges in neuroscience research and allowing for the development of new biomarkers. While the medical literature has reported several PD respiratory symptoms, such as weakness of respiratory muscles20, sleep breathing disorders21–24 and degeneration in the brain areas that control breathing19, without our AI-based model, no physician today can detect PD or assess its severity from breathing. This shows that AI can provide new clinical insights that otherwise may be inaccessible.\n\nOnline content Any methods, additional references, Nature Research reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/ s41591-022-01932-x. Received: 17 November 2021; Accepted: 5 July 2022; Published online: 22 August 2022 References 1. Dorsey, E. R., Sherer, T., Okun, M. S. & Bloem, B. R. Te emerging evidence of the Parkinson pandemic. J. Parkinsons Dis. 8, S3–S8 (2018). 2. Marras, C. et al. Prevalence of Parkinson’s disease across North America. NPJ Parkinson’s Dis. 4, 21 (2018). 3. Yang, W. et al. Current and projected future economic burden of Parkinson’s disease in the U.S. NPJ Parkinsons Dis. 6, 15 (2020). 4. Armstrong, M. J. & Okun, M. S. Diagnosis and treatment of Parkinson disease: a review. JAMA 323, 548–560 (2020). 5. Delenclos, M., Jones, D. R., McLean, P. J. & Uitti, R. J. Biomarkers in Parkinson’s disease: advances and strategies. Parkinsonism Relat. Disord. 22, S106–S110 (2016). 6. Jankovic, J. Parkinson’s disease: clinical features and diagnosis. J. Neurol. Neurosurg. Psychiatry 79, 368–376 (2008). 7. Hauser, R. A. et al. A home diary to assess functional status in patients with Parkinson’s disease with motor fuctuations and dyskinesia. Clin. Neuropharmacol. 23, 75–81 (2000). 8. Goetz, C. G. et al. Movement Disorder Society-sponsored revision of the Unifed Parkinson’s Disease Rating Scale (MDS-UPDRS): scale presentation and clinimetric testing results. Mov. Disord. 23, 2129–2170 (2008). 9. Evers, L. J. W., Krijthe, J. H., Meinders, M. J., Bloem, B. R. & Heskes, T. M. Measuring Parkinson’s disease over time: the real-world within-subject reliability of the MDS-UPDRS. Mov. Disord. 34, 1480–1487 (2019). 10. Regnault, A. et al. Does the MDS-UPDRS provide the precision to assess progression in early Parkinson’s disease? Learnings from the Parkinson’s progression marker initiative cohort. J. Neurol. 266, 1927–1936 (2019). 11. Ellis, T. D. et al. Identifying clinical measures that most accurately refect the progression of disability in Parkinson disease. Parkinsonism Relat. Disord. 25, 65–71 (2016). 12. Athauda, D. & Foltynie, T. Te ongoing pursuit of neuroprotective therapies in Parkinson disease. Nat. Rev. Neurol. 11, 25–40 (2015). 13. Kieburtz, K., Katz, R. & Olanow, C. W. New drugs for Parkinson’s disease: the regulatory and clinical development pathways in the United States. Mov. Disord. 33, 920–927 (2018). 14. Zhang, J. et al. Longitudinal assessment of tau and amyloid beta in cerebrospinal fuid of Parkinson disease. Acta Neuropathol. 126, 671–682 (2013). 15. Parnetti, L. et al. Cerebrospinal fuid β-glucocerebrosidase activity is reduced in Parkinson’s disease patients. Mov. Disord. 32, 1423–1431 (2017). 16. Parnetti, L. et al. CSF and blood biomarkers for Parkinson’s disease. Lancet Neurol. 18, 573–586 (2019). 17. Tang, Y. et al. Identifying the presence of Parkinson’s disease using low-frequency fuctuations in BOLD signals. Neurosci. Lett. 645, 1–6 (2017). 18. Parkinson, J. An essay on the shaking palsy. 1817. J. Neuropsychiatry Clin. Neurosci. 14, 223–226, discussion 222 (2002). 19. Benarroch, E. E., Schmeichel, A. M., Low, P. A. & Parisi, J. E. Depletion of ventromedullary NK-1 receptor-immunoreactive neurons in multiple system atrophy. Brain 126, 2183–2190 (2003). 20. Baille, G. et al. Early occurrence of inspiratory muscle weakness in Parkinson’s disease. PLoS ONE 13, e0190400 (2018). 21. Wang, Y. et al. Abnormal pulmonary function and respiratory muscle strength fndings in Chinese patients with Parkinson’s disease and multiple system atrophy–comparison with normal elderly. PLoS ONE 9, e116123 (2014). 22. Torsney, K. M. & Forsyth, D. Respiratory dysfunction in Parkinson’s disease. J. R. Coll. Physicians Edinb. 47, 35–39 (2017). 23. Pokusa, M., Hajduchova, D., Buday, T. & Kralova Trancikova, A. Respiratory function and dysfunction in Parkinson-type neurodegeneration. Physiol. Res. 69, S69–S79 (2020). 24. Baille, G. et al. Ventilatory dysfunction in Parkinson’s disease. J. Parkinsons Dis. 6, 463–471 (2016). 25. Seccombe, L. M. et al. Abnormal ventilatory control in Parkinson’s disease–further evidence for non-motor dysfunction. Respir. Physiol. Neurobiol. 179, 300–304 (2011). 26. Quan, S. F. et al. Te Sleep Heart Health Study: design, rationale, and methods. Sleep 20, 1077–1085 (1997). 27. Adib, F., Mao, H., Kabelac, Z., Katabi, D. & Miller, R. C. Smart homes that monitor breathing and heart rate. In Proc. of the 33rd Annual ACM Conference on Human Factors in Computing Systems (eds Begole, B. et al.) 837–846 (ACM, 2015). 28. Yue, S., He, H., Wang, H., Rahul, H. & Katabi, D. Extracting multi-person respiration from entangled RF signals. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 2, 86 (2018). 29. Yue, S., Yang, Y., Wang, H., Rahul, H. & Katabi, D. BodyCompass: monitoring sleep posture with wireless signals. Proc. ACM Interact. Mob. Wearable Ubiquitous Technol. 4, 66 (2020). 30. Blackwell, T. et al. Associations of sleep architecture and sleep-disordered breathing and cognition in older community-dwelling men: the Osteoporotic Fractures in Men Sleep Study. J. Am. Geriatr. Soc. 59, 2217–2225 (2011). 31. Guttman, L. A basis for analyzing test-retest reliability. Psychometrika 10, 255–282 (1945). 32. Hoehn, M. M. & Yahr, M. D. Parkinsonism: onset, progression and mortality. Neurology 17, 427–442 (1967). 33. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition. Computer Vision and Pattern Recognition (CVPR) (Eds. Bajcsy, R. et al.) 770–778 (IEEE, 2016). 34. Zahed, H. et al. Te neurophysiology of sleep in Parkinson’s disease. Mov. Disord. 36, 1526–1542 (2021). 35. Brunner, H. et al. Microstructure of the non-rapid eye movement sleep electroencephalogram in patients with newly diagnosed Parkinson’s disease: efects of dopaminergic treatment. Mov. Disord. 17, 928–933 (2002). 36. González-Naranjo, J. E. et al. Analysis of sleep macrostructure in patients diagnosed with Parkinson’s disease. Behav. Sci. (Basel). 9, 6 (2019). 37. Soikkeli, R., Partanen, J., Soininen, H., Pääkkönen, A. & Riekkinen, P. Sr. Slowing of EEG in Parkinson’s disease. Electroencephalogr. Clin. Neurophysiol. 79, 159–165 (1991). 38. Klassen, B. T. et al. Quantitative EEG as a predictive biomarker for Parkinson disease dementia. Neurology 77, 118–124 (2011). 39. Railo, H. et al. Resting state EEG as a biomarker of Parkinson’s disease: infuence of measurement conditions. Preprint at https://doi. org/10.1101/2020.05.08.084343 (2020). 40. Boser, B. E., Vapnik, V. N. & Guyon, I. M. Training algorithm margin for optimal classifers. In COLT '92: Proc. Fifh Annual Workshop on Computational learning theory (ed. Haussler, D) 144–152 (Association for Computing Machinery, 1992). 41. Yang, Y., Zha, K., Chen, Y-C., Wang, H. & Katabi, D. Delving into deep imbalanced regression. In Proc. 38th International Conference on Machine Learning, Vol. 139 (Eds. Meila, M. & Zhang, T.) 11842–11851 (PMLR, 2021). 42. Willis, A. W., Schootman, M., Evanof, B. A., Perlmutter, J. S. & Racette, B. A. neurologist care in Parkinson disease: a utilization, outcomes, and survival study. Neurology 77, 851–857 (2011). 43. Braak, H. et al. Staging of brain pathology related to sporadic Parkinson’s disease. Neurobiol. Aging 24, 197–211 (2003). 44. Mestre, T. A. et al. Parkinson’s disease subtypes: critical appraisal and recommendations. J. Parkinsons. Dis. 11, 395–404 (2021).\n\nMethods Dataset descriptions. Additional information about the datasets used in this study are summarized in Table 1 and their demographics are provided in Extended Data Table 1. SHHS dataset. The SHHS26 dataset (visit 2) is a multicenter cohort PSG study of cardiovascular diseases and sleep-related breathing. Dataset description and ethical oversight are available at the National Sleep Research Resource (https://sleepdata. org/datasets/shhs). MrOS dataset. The MrOS30 dataset is a multicenter cohort PSG study for understanding the relationship between sleep disorders and falls, fractures, mortality and vascular disease. The dataset description and ethical oversight are available at the National Sleep Research Resource (https://sleepdata.org/ datasets/mros). Udall dataset. The Udall dataset is comprised of PD and normal participants who underwent an in-home observational study between 1 June 2019 and 1 January 2021. Inclusion criteria require participants to be at least 30 years of age, able and willing to provide informed consent, have Wi-Fi in their residence, have English fluency and be resident in the United States. Exclusion criteria include nonambulatory status, pregnancy, having more than one ambulatory pet in the household, the inability to complete study activities as determined by the study team and any medical or psychiatric condition that, in the study team’s judgment, would preclude participation. Additionally, the PD participants were required to have a diagnosis of PD according to the UK PD Brain Bank criteria. Control participants are required to be in generally good health with no disorder that causes involuntary movements or gait disturbances. The study protocol was reviewed and approved by the University of Rochester Research Subjects Review Boards (RSRB00001787); MIT Institutional Review Board (IRB) ceded to the Rochester IRB. The participants provided written informed consent to participate in this study. MJFF dataset. The MJFF dataset is comprised of PD and normal participants who underwent an in-home observational study between 1 July 2018 and 1 January 2020. The inclusion criteria require participants to be able to speak and understand English, have capacity to provide informed consent, be ambulatory, have Wi-Fi in their residence, agree to allow for coded clinical research and for data to be shared with study collaborators, be willing and able to complete study activities, live with no more than one additional individual in the same residence and have no more than one ambulatory pet. The PD participants are required to have a diagnosis of PD according to the UK PD Brain Bank Criteria. Control participants are required to have no clinical evidence of PD, to not live with individuals with PD or other disorder that affects ambulatory status and to be age-matched to PD participants. The study protocol was reviewed and approved by the University of Rochester Research Subjects Review Boards (RSRB00072169); MIT Institutional Review Board and Boston University Charles River Campus IRB ceded to the Rochester IRB. The participants provided written informed consent to participate in this study. MIT dataset. The MIT control dataset is comprised of participants who underwent an in-home observational study between 1 June 2020 and 1 June 2021. The study investigates the use of wireless signals to monitor movements and vital signs. The inclusion criteria require participants to be above 18 years old, have home Wi-Fi, be able to give informed consent or have a legally authorized representative to provide consent, agree to confidential use and storage of all data and the use of all anonymized data for publication including scientific publication. The study protocol was reviewed and approved by Massachusetts Institute of Technology Committee on the Use of Humans as Experimental Subjects (COUHES) (IRB no.: 1910000024). The participants provided written informed consent to participate in this study. MGH dataset. The MGH control dataset is comprised of adult male and female subjects who have undergone in-lab PSG in the MGH for Sleep Medicine between 1 January 2019 and 1 July 2021. The MGH PD dataset is comprised of PD participants recruited from the Parkinson’s Disease and Movement Disorders Centers at Northwestern University and the Parkinson’s Disease and Movement Disorders Program at Rush University between 1 March 2007 and 31 October 2012. PD patients were enrolled in the study if they (1) had a diagnosis of idiopathic PD, as defined by the UK Parkinson’s Disease Society Brain Bank Criteria; (2) were classified as H&Y stages 2–4; (3) had EDS, as defined by an ESS score of 12 or greater; (4) had a stable PD medication regimen for at least 4weeks before study screening; and (5) were willing and able to give written informed consent. Patients were excluded from participation if they (1) had atypical parkinsonian syndrome; (2) had substantial sleep-disordered breathing, defined as an apnea-hypopnea index of more than 15 events per hour of sleep on screening PSG; (3) had substantial periodic limb-movement disorder, defined as a periodic limb-movement arousal index of more than ten events per hour of sleep on screening PSG; (4) had REM sleep behavior disorder based on the presence of both clinical symptomatology and intermittent loss of REM atonia on screening PSG; (5) had cognitive impairment, as indicated by a Mini-Mental State Examination score less than 24; (6) had untreated hallucinations or psychosis; (7) used hypnosedative or stimulant drugs; (8) used antidepressants, unless the patient had been receiving a stable dose for at least 3months; (9) had visual abnormalities that may interfere with light therapy (LT), such as significant cataracts, narrow-angle glaucoma or blindness; or (10) traveled across two or more time zones within 90days before study screening. The study protocols involving PD participants were reviewed and approved by the IRBs of Northwestern University, Rush University, and MGH. All study participants provided written informed consent. The protocol involving control participants and the sharing of deidentified data with MIT were reviewed by the Mass General Brigham IRB (IRB no. 2018P000337). Mayo Clinic dataset. The Mayo Clinic PD dataset is comprised of adult subjects who underwent in-lab PSG between 1 January 2020 and 22 July 2021 and carried a diagnosis code for PD (ICD-10 CM G20 or ICD-9 CM 332.0) at the time of PSG. The control dataset consists of adult male and female subjects who have undergone in-lab PSG in the Mayo Clinic Center for Sleep Medicine between 1 January 2020 and 22 July 2021. The use of the Mayo Clinic dataset and sharing of deidentified data with MIT was reviewed by the Mayo Clinic IRB, and the study was conducted in accordance with Institutional regulations and appropriate ethical oversight. Waiver of informed consent and waiver of HIPAA authorization were granted as the Mayo Clinic portion of the study involves only use of deidentified retrospective records and does not involve any direct contact with study participants. Data preprocessing. The datasets were divided into two groups. The first group comes from PSG sleep studies. Such studies use a breathing belt to record the subject’s breathing signals throughout the night. They also include EEG and sleep data. The PSG datasets are the SHHS26 (n=2,630 nights from 2,630 subjects), MrOS30 (n=3,883 nights from 2,875 subjects) and MGH (n=223 nights from 155 subjects) sleep datasets. Further, an external PSG dataset from the Mayo Clinic (n=1,920 nights from 1,920 subjects) was held back during the AI model development and serves as an independent test set. The second group of datasets collects nocturnal breathing in a contactless manner using a radio device developed by our team at MIT27. The data were collected by installing a low-power radio sensor in the subject’s bedroom, and analyzing the radio reflections from the environment to extract the subject’s breathing signal as described in our previous work28,29. This group includes the MJFF dataset (n=526 nights from 15 subjects), the Udall dataset (n=1,734 nights from 20 subjects) and the MIT dataset (n=1,048 nights from 56 subjects). The wireless datasets have several nights per subject and information about PD severity such as MDS-UPDRS and/or H&Y stage32. We processed the data to filter out nights shorter than 2h. We also filter out nights where the breathing signal is distorted or nonexistent, which occurs when the person does not wear the breathing belt properly for breathing belt data, and when a source of interference (for example, fans or pets) exists near the subject for wireless data. We normalized the breathing signal from each night by clipping values larger than a particular range (we used [−6, +6]), subtracting the mean of the signal and dividing by the s.d. The resulting breathing signal is a one-dimensional (1D) time series x ∈ R1×fbT, with a sampling frequency fb of 10Hz, and a length of Ts. We use the following variables to determine whether a participant has PD: ‘Drugs used to treat Parkinson’s’ for SHHS and ‘Has a doctor or other healthcare provider ever told you that you had Parkinson’s disease?’ for MrOS. The other datasets explicitly report whether the person has PD and, for those who do have PD, they provided their MDS-UPDRS and H&Y stage. In the experiments involving distinguishing PD from AD, we use the following variables to identify AD patients: ‘Acetylcholine Esterase Inhibitors For Alzheimer’s’ for SHHS, and ‘Has a doctor or other healthcare provider ever told you that you had dementia or Alzheimer’s disease?’ for MrOS. Photos of the radio device and breathing belt are in Extended Data Fig. 1. Consent was obtained from all individuals whose images are shown in Extended Data Fig. 1 for publication of these images. Sensing breathing using radio signals. By capturing breathing signals using radio signals, our system can run in a completely contactless manner. We leveraged past work on extracting breathing signals from radio frequency (RF) signals that bounce off people’s bodies. The RF data were collected using a multi-antenna frequency-modulated continuous waves (FMCW) radio, used commonly in passive health monitoring28,29. The radio sweeps the frequencies from 5.4GHz to 7.2GHz, transmits at submilliwatt power in accordance with Federal Communications Commission regulations and captures reflections from the environment. The radio reflections are processed to infer the subject’s breathing signals. Past work shows that respiration signals extracted in this manner are highly accurate, even when several people sleep in the same bed27,28,45. In this paper, we extract the participant’s breathing signal from the RF signal using the method developed by Yue et al.28, which has been shown to work well even in the presence of bed partners, producing an average correlation 0.914 with a United States Food and", "persona": "Student", "response": "### 1. Overview\nIn a recent Nature Medicine journal study, researchers develop an artificial intelligence (AI)-based model to detect Parkinson’s disease (PD) and track its progression from nocturnal breathing signals. Study: Artificial intelligence-enabled detection and assessment of Parkinson’s disease using nocturnal breathing signals. Image Credit: Natali _ Mis / Shutterstock.com Background In the present study, researchers evaluated a novel AI-based study model on 7,671 individuals using data from several public datasets and hospitals in the United States. The model learned the auxiliary task of predicting each subject's quantitative electroencephalogram (qEEG) from nocturnal breathing. Notably, the researchers excluded subjects from testing who helped train the neural network. The AI model diagnosed PD from one night of nocturnal breathing, which was presented as the receiver operating characteristic (ROC) curve. Although the AI model detected PD with high accuracy, the researchers were interested in assessing whether its accuracy was improved by combining data from several nights in the same test subject.\n\n### 2. Background\nWireless datasets were used to compute the model prediction score for all nights. The test-retest reliability was determined by averaging the prediction across consecutive nights within a pre-specified timeframe. Lastly, the ability of the AI model to generate a PD severity score that correlated with the MDS-UPDRS was evaluated. Since PD is the fastest-growing neurological disease worldwide, there is an urgent need for novel diagnostic biomarkers that can detect the disease at an early stage. Currently, there are no drugs capable of reversing or ceasing PD progression.\n\n### 3. Methods\nThe study model gathered data in two distinct ways. The first method required that the test subject wore a breathing belt on the chest or abdomen to track breathing signals overnight. The data acquired from this method was referred to as the breathing belt dataset. Alternatively, the second contactless method collected breathing data using a radio sensor that transmitted a low-power radio signal in the individual’s bedroom and analyzed its reflections from the environment. This dataset was referred to as wireless data.\n\n### 4. Results\nLongitudinal data of several nights for each person for up to one year were collected. Taken together, the combined study data included 11,964 nights with over 120,000 hours of nocturnal breathing signals from 757 PD patients. The breathing belt datasets lacked MDS-UPDRS and Hoehn and Yahr (H&Y) scores. Conversely, the wireless datasets had MDS-UPDRS and H&Y scores. The study model attained an area under the ROC curve (AUC) of 0.889 with a sensitivity and specificity of 80.22% and 78.62%, respectively, all with a 95% confidence interval (CI) for nights measured using a breathing belt. For nights measured using a wireless dataset, the model reached an AUC of 0.906 with a sensitivity and specificity of 86.23% and 82.83%, respectively.\n\n### 5. Takeaways\nRenowned English surgeon James Parkinson first noted a connection between PD and breathing in 1817. Subsequent studies also reported degenerated brainstem areas that control breathing among PD patients, thus making breathing a promising risk assessment attribute for the clinical diagnosis of PD. Importantly, respiratory symptoms often manifest much before clinical motor symptoms of PD. The AI-based system described in the current study served as a promising diagnostic and progression digital biomarker for PD. Moreover, the model was remarkably objective, non-obtrusive, inexpensive, and had the potential to allow nocturnal breathing measurements to be obtained repeatedly at home. About 40% of PD patients currently do not receive care from PD specialists, as these clinicians are often concentrated in medical centers in urban areas. For such cases, as well as those at high risk of developing PD, the latter of which includes those with leucine-rich repeat kinase 2 gene mutation, an AI system could be deployed at home for passive tracking. Furthermore, this type of system could provide regular feedback to the patient’s doctor who could follow up with the patient to confirm the results through telehealth or in-person visit.", "paper_id": "55", "story_id": "55-03"}
{"paper": "Acinetobacter baumannii is a nosocomial Gram-negative pathogen that often displays multidrug resistance. Discovering new antibiotics against A. baumannii has proven challenging through conventional screening approaches. Fortunately, machine learning methods allow for the rapid exploration of chemical space, increasing the probability of discovering new antibacterial molecules. Here we screened ~7,500 molecules for those that inhibited the growth of A. baumannii in vitro. We trained a neural network with this growth inhibition dataset and performed in silico predictions for structurally new molecules with activity against A. baumannii. Through this approach, we discovered abaucin, an antibacterial compound with narrow-spectrum activity against A. baumannii. Further investigations revealed that abaucin perturbs lipoprotein trafcking through a mechanism involving LolE. Moreover, abaucin could control an A. baumannii infection in a mouse wound model. This work highlights the utility of machine learning in antibiotic discovery and describes a promising lead with targeted activity against a challenging Gram-negative pathogen.\n\nAcinetobacter baumannii is a nosocomial Gram-negative pathogen that often displays multidrug resistance due to its robust outer membrane and its ability to acquire and retain extracellular DNA1  that frequently encodes antibiotic resistance genes. Moreover, it can survive for prolonged durations on surfaces and is resistant to desiccation. Discovering fundamentally new antibiotics against A. baumannii has proven challenging through conventional screening approaches. Indeed, most new antibiotics that achieve clinical use are analogs of existing classes2 . However, while structural analogs of existing antibiotic classes may satisfy short-term clinical needs, their long-term efficacy is inherently limited due to the high prevalence of existing resistance determinants3 . Ideally, new antibiotic discovery efforts should focus on identifying new chemotypes with mechanisms of action that are unique relative to existing antibiotics. Such compounds are likely to have prolonged utility, given that the probability of pre-existing clinical resistance is low. Fortunately, machine learning methods allow for the rapid exploration of vast chemical/sequence spaces in silico, increasing the probability of discovering desirable new chemotypes with antibacterial activity, particularly against challenging pathogens like A. baumannii. As a reference, typical high-throughput screening programs are limited to testing a few million molecules for antibacterial activity at the largest scales4 . Contrarily, contemporary algorithmic approaches can assess hundreds of millions to billions of molecules for antibacterial properties. For example, Stokes et al.5  applied a message-passing neural network trained on growth inhibition of lab strain Escherichia coli to discover new broad-spectrum small molecule antibacterial compounds. In a complementary application, Ma et al.6  applied multiple natural language processing neural network models to predict broad-spectrum antimicrobial peptides encoded in the human gut microbiome. These, and other important studies (detailed in ref. 7) showcase the importance of machine learning approaches toward the discovery of structurally and functionally new antibiotic candidates. Beyond simply discovering structurally and functionally new antibiotics, a largely unmet need exists for the application of narrow-spectrum therapies that target specific bacterial species. Such antibiotics are beneficial for the following two reasons8 : first, the rate at which resistance to narrow-spectrum agents would disseminate is likely lower than conventional broad-spectrum agents, because narrow-spectrum drugs do not impose a universal selective pressure that favors the wide propagation of resistance determinants; second, narrow-spectrum antibiotics would not disrupt the ecology of the microbiota during treatment. Indeed, dysbiosis has been associated with a wide array of poor health outcomes, including infectious diseases9 , inflammatory bowel diseases10, metabolic diseases11, neuropsychiatric disorders12 and cancer13. For instance, Clostridioides difficile infections are prime examples of opportunistic infections resulting from antibiotic-induced dysbiosis, causing upwards of 224,000 infections in hospitalized patients and 13,000 deaths in the US alone in 2017 (ref. 14). An opportunity exists to apply contemporary machine learning methods to discover structurally and functionally new antibiotics that specifically target challenging pathogens, with A. baumannii being a prime candidate. Here we screened ~7,500 molecules for those that inhibited the growth of A. baumannii in vitro. We trained a message-passing neural network with this growth inhibition dataset and performed predictions on the Drug Repurposing Hub15 for structurally new molecules with activity against A. baumannii. Through this approach, we discovered abaucin, an antibacterial compound with narrow-spectrum activity against A. baumannii, which could overcome intrinsic and acquired resistance mechanisms in clinical isolates. Further mechanistic investigations revealed that abaucin perturbs lipoprotein trafficking through a mechanism involving LolE, a functionally conserved protein that contributes to shuttling lipoproteins from the inner membrane to the outer membrane16,17. Moreover, abaucin was able to control an A. baumannii infection in a mouse wound model. This study highlights the utility of machine learning in discovering new antibiotics and describes a promising lead molecule with specific activity against a challenging Gram-negative pathogen.\n\nA. baumannii, emphasizing the ability of our model to generalize to\ncompounds that are highly divergent from the chemical space on which the model was trained. We note here that >80% growth inhibition is a statistically more stringent cutoff than that used for model training, which was a conventional one standard deviation below the mean growth of the dataset, or ~20% growth inhibition. This stringent >80% growth inhibition cutoff was applied to efficiently prioritize the most potent predicted molecules on which to conduct further experimentation. For reference, if we defined a hit cutoff of 20% growth inhibition for prediction selection—similar to that used for model training—we would acquire 41 molecules classified as active predictions (Supplementary Data 3). Notably, we also tested the 240 molecules 1.0 0.8 Prediction score 0.6 0.4 0.2 0 0 2,000 4,000 6,000 8,000 Molecule rank 10 10 PBS - 1.5 h PBS - 3 h PBS - 4.5 h PBS - 6 h 10 9 10 8 CFU ml−1 10 7 10 6 10 5 10 4 0 5 10 15 20 Abaucin (µg ml−1) d, A t-SNE plot showing the chemical relationship between the training dataset (blue), the prediction set (red) and abaucin (yellow). e, Growth inhibition of A. baumannii by abaucin in LB medium. Experiments were conducted in biological duplicate. Individual replicates with means connected are plotted. The structure of abaucin is shown. f, Killing of A. baumannii by abaucin in nutrient-replete conditions (LB) at varying concentrations after incubation for 1.5 h (blue), 3 h (teal), 4.5 h (green) and 6 h (purple). Experiments were conducted in biological duplicate. Individual replicates with means connected are plotted. g, Killing of A. baumannii by abaucin in nutrient-deplete conditions (PBS) at varying concentrations after incubation for 1.5 h (blue), 3 h (teal), 4.5 h (green) and 6 h (purple). Experiments were conducted in biological duplicate. Individual replicates with means connected are plotted. with the lowest prediction scores and observed that none displayed antibacterial activity as defined by >80% growth inhibition, emphasizing the discriminatory utility of our model (Extended Data Fig. 1b and Supplementary Data 3). Furthermore, testing the 240 molecules with the highest prediction scores, without considering Tanimoto nearest neighbor similarity to training set ‘actives’, resulted in 40 molecules that passed the stringent >80% growth inhibition cutoff, indicating that our model has a high predictive value over traditional chemical screening (Extended Data Fig. 1b and Supplementary Data 3). The nine priority molecules were subsequently assessed to remove (1) those with major structural features that are observed in known antibiotics; (2) those with reported antibacterial activity from the scientific or patent literature; and (3) those with possible nonspecific membrane activity as assessed by the presence of acyclic aliphatic 1.4 1.4 CRE\n\nResults Machine learning-guided discovery of abaucin Our recent work has highlighted the utility of machine learning in discovering new antibacterial molecules using E. coli K12 as a model organism5 . Building off this prior research, here we applied a message-passing deep neural network18 to discover new antibiotics against A. baumannii, a problematic nosocomial Gram-negative pathogen that commonly displays multidrug resistance and, increasingly, pan-drug resistance (Fig. 1a). We first screened a diverse collection of 7,684 small molecules at 50 µM for those that inhibited the growth of A. baumannii ATCC 17978 in Lysogeny Broth (LB) medium (Fig. 1b and Extended Data Fig. 1a). This chemical collection consisted of both off-patent drugs (2,341 molecules) and synthetic chemicals (5,343 molecules) curated from various high-throughput screening sub-libraries at the Broad Institute. Using a conventional hit cutoff of one standard deviation below the mean growth of the entire dataset resulted in 480 molecules being defined as ‘active’ and 7,204 being defined as ‘inactive’ (Supplementary Data 1). Next, this dataset was used to train a binary classifier to predict whether structurally new molecules may display activity against A. baumannii. Briefly, we leveraged a directed message-passing neural network architecture, which translates the graph structure of a molecule into a continuous vector18 (Fig. 1a). This type of model operates by iteratively exchanging information of local chemistry between adjacent atoms and bonds in a series of ‘message-passing’ steps. Each iteration of message passing propagates information about local chemistry across the molecule, thereby allowing the model to build a more holistic representation of the molecule. After a defined number of message-passing steps, the vector representations of various local chemical regions of a molecule are summed into a single continuous vector that captures the complexity of the entire compound. This learned final vector is then supplemented with fixed molecular features computed using RDKit19. A final vector containing both learned and computed features is then used as an input vector for a feed-forward neural network that predicts antibacterial properties. The model was further optimized by using an ensemble of ten classifiers, increasing its robustness. Our final model achieved an area under the precision-recall curve of 0.337 ± 0.088 and an area under the receiver-operating characteristic curve of 0.792 ± 0.042, providing confidence in leveraging the model for predictions in new chemical spaces. After model training, we applied our ensemble of ten RDKit-augmented models to identify antibacterial molecules with activity against A. baumannii from the updated Drug Repurposing Hub15 consisting of 6,680 molecules. This chemical library was selected as a proof-of-concept due to the structural diversity inherent to this collection, as well as the favorable cytotoxicity and drug-like properties that are observed for many Drug Repurposing Hub molecules. Our ensemble of ten classifiers returned a prediction score for each compound, representing the probability of growth inhibition against A. baumannii (Fig. 1c and Supplementary Data 2). Molecules that were both strongly predicted to be antibacterial and structurally unique from training set ‘actives’ were prioritized for in vitro testing (Supplementary Data 3). Notably, this process of performing predictions and prioritizing molecules for validation was completed within a couple of hours. We note here that we also trained and applied a message-passing neural network model that was not augmented with RDKit features as a baseline reference (Supplementary Data 2). This model did not perform as well as the RDKit-augmented primary model, achieving an area under the precision-recall curve of 0.266 ± 0.070 and an area under the receiver-operating characteristic curve of 0.756 ± 0.050. The reduced performance of the model omitting RDKit features highlights the importance of these computable molecular features in maximizing predictive utility in the context of our training dataset. Using a prediction score threshold of >0.2 (Fig. 1c) and subsequently filtering these predicted compounds based on a Tanimoto nearest neighbor similarity of <0.3 to molecules that were ‘active’ in the training dataset, we identified 240 priority molecules that met these highly stringent criteria (Supplementary Data 3). These molecules were acquired and tested against A. baumannii at a concentration of 50 µM in LB medium—the same conditions in which the training data were acquired (Extended Data Fig. 1b and Supplementary Data 3). Using another stringent cutoff of >80% growth inhibition, we observed that nine of the tested molecules displayed antibacterial activity against A. baumannii, emphasizing the ability of our model to generalize to compounds that are highly divergent from the chemical space on which the model was trained. We note here that >80% growth inhibition is a statistically more stringent cutoff than that used for model training, which was a conventional one standard deviation below the mean growth of the dataset, or ~20% growth inhibition. This stringent >80% growth inhibition cutoff was applied to efficiently prioritize the most potent predicted molecules on which to conduct further experimentation. For reference, if we defined a hit cutoff of 20% growth inhibition for prediction selection—similar to that used for model training—we would acquire 41 molecules classified as active predictions (Supplementary Data 3). Notably, we also tested the 240 molecules with the lowest prediction scores and observed that none displayed antibacterial activity as defined by >80% growth inhibition, emphasizing the discriminatory utility of our model (Extended Data Fig. 1b and Supplementary Data 3). Furthermore, testing the 240 molecules with the highest prediction scores, without considering Tanimoto nearest neighbor similarity to training set ‘actives’, resulted in 40 molecules that passed the stringent >80% growth inhibition cutoff, indicating that our model has a high predictive value over traditional chemical screening (Extended Data Fig. 1b and Supplementary Data 3). The nine priority molecules were subsequently assessed to remove (1) those with major structural features that are observed in known antibiotics; (2) those with reported antibacterial activity from the scientific or patent literature; and (3) those with possible nonspecific membrane activity as assessed by the presence of acyclic aliphatic moieties. This structure-based filtering resulted in the retention of two molecules—RS102895 and serdemetan. RS102895 is a well-studied CCR2−  selective chemokine receptor antagonist20,21 that displayed a minimum inhibitory concentration (MIC) of ~2 µg ml−1 against A. baumannii ATCC 17978 (Fig. 1d,e); serdemetan22,23 is an antagonist of the transcription factor HDM2 that displayed MIC of ~32 µg ml−1 (Extended Data Fig. 1c). Given that RS102895 was substantially more potent at inhibiting the growth of A. baumannii, we focused our subsequent investigations on this molecule. Upon further experimentation with RS102895 to assess A. baumannii viability after treatment, we observed modest bactericidal activity against A. baumannii in LB medium (Fig. 1f). Indeed, upon removal of RS102895 from A. baumannii cultures in vitro after 6 h of treatment, we observed A. baumannii regrowth, wherein the apparent lag period increased with increasing concentrations of abaucin (Extended Data Fig. 1d). We observed no discernable activity in nutrient-deplete PBS (Fig. 1g). Collectively, these data suggest that RS102895—renamed abaucin for its activity against A. baumannii—displayed its antibacterial efficacy through inhibition of a biological process that was maximally active during growth and division24, consistent with most known antibiotics25. Notably, these data also suggest that abaucin is not membrane active via physical disruption of the phospholipid bilayer, a mechanism of action that we deliberately attempted to avoid during our prediction filtering process. Indeed, membrane-active molecules generally retain bactericidal efficacy in nutrient-deplete conditions24. Abaucin has a narrow spectrum of activity After probing the activity of abaucin against A. baumannii ATCC 17978, we next tested for growth inhibitory activity against clinical isolates of A. baumannii. Here we acquired 41 strains of A. baumannii from the Center for Disease Control and Prevention Antibiotic Resistance Isolate Bank (ARIsolate Bank; Supplementary Table 1) and tested abaucin at a range of concentrations below and above MIC. Remarkably, we observed that abaucin could overcome all intrinsic and acquired resistance mechanisms within the A. baumannii isolates from this diverse clinical strain library (Fig. 2a and Supplementary Table 1). Next, we investigated the phylogenetic spectrum of activity displayed by abaucin by testing this compound against 24 carbapenem-resistant Enterobacteriaceae strains (Supplementary Table 2), 24 Pseudomonas aeruginosa strains (Supplementary Table 3) and 14 Staphylococcus aureus strains (Supplementary Table 4), all from the ARIsolate Bank. In stark contrast to our observations with A. baumannii, abaucin did not display any growth inhibitory activity against these pathogenic species up to 20× the MIC in A. baumannii ATCC 17978 (Fig. 2b–d). Excitingly, in the context of clinical bacterial pathogens, these data provide strong evidence that abaucin has narrow-spectrum antibacterial activity, which is advantageous for decreasing the interpathogen dissemination of resistance. We note here that a modest collection of structural analogs of abaucin displayed varying levels of activity against A. baumannii but avoided any discernable activity against lab strains of the bacterial species mentioned above (Table 1). This initial structure–activity relationship investigation provides strong support that exploration of the chemical space around abaucin toward developing a medicinal chemistry-optimized narrow-spectrum antibiotic against A. baumannii is feasible without being hampered by broad-spectrum antibacterial activity. Given these observations, we hypothesized that abaucin would display minimal growth inhibitory activity against human commensal species. Indeed, currently employed antibiotics often induce dysbiosis during treatment26, resulting in a wide array of complications, including secondary infections caused by opportunistic pathogens—C. difficile gut infections being a common example27. To test this hypothesis, we curated panels of 34 diverse human gut commensal isolates (Supplementary Table 5) and 19 diverse human skin commensal isolates (Supplementary Table 6), then tested the growth inhibitory properties of abaucin at varying concentrations, as well as ampicillin and ciprofloxacin at their respective MIC concentrations in A. baumannii ATCC 17978 (Extended Data Fig. 2a). As expected, ampicillin and ciprofloxacin displayed antibacterial activity across a wide range of commensal isolates, whereas abaucin largely avoided growth inhibition of commensal species, even up to 20× MIC (Fig. 2e). Indeed, of the 53 isolates tested, abaucin only displayed bona fide growth inhibition against the gut isolates Bifidobacterium breve and Bifidobacterium longum, and this occurred above the MIC observed in A. baumannii ATCC 17978 (Extended Data Fig. 2b–d). Given that Bifidobacterium is a Gram-positive genus that is phylogenetically divergent from Gram-negative Acinetobacter, it is likely that the lower potency activity of abaucin against Bifidobacterium is through a mechanism that is unrelated to that in Acinetobacter. This statement is elaborated below. Abaucin inhibits lipoprotein trafficking in A. baumannii Given the A. baumannii selectivity displayed by abaucin, we next ventured to elucidate the mechanism underlying its narrow-spectrum functionality. To this end, we first selected abaucin-resistant mutants using wild-type A. baumannii growing on solid media supplemented with varying concentrations of abaucin. Using 4 µg ml−1 and 5 µg ml−1, we were able to isolate abaucin-resistant clones that did not display cross-resistance to functionally diverse antibiotics (Fig. 3a and Extended Data Fig. 3a–h). Whole-genome sequencing of four independent isolates revealed mutations in or upstream of the gene encoding LolE, an essential inner membrane protein involved in lipoprotein trafficking that has become a target of strong interest for new Gram-negative antibiotic development16,17,28. Two mutants (Y394F and an upstream G to A mutation) displayed fourfold resistance to abaucin relative to wild-type A. baumannii, and two independent mutants contained an identical A362T variant that resulted in 16-fold resistance. We note that the frequency of resistance to abaucin in vitro is 10−8–10−7 (Supplementary Table 7), largely consistent with known antibiotics that target a single protein29. Interestingly, A. baumannii LolE position A362 is homologous to E. coli LolE position I365, which resides near the acyl chains of the nascent lipoprotein during transport17. We predicted the structure of A. baumannii LolE using RoseTTAFold30,31 (Extended Data Fig. 3i,j) and observed that AbLolE position A362 is near EcLolE position I365 in space, and both are adjacent to the accommodated acyl chains of the nascent lipoprotein (Fig. 3b). These mutational data, combined with in silico structural insights, provided evidence suggesting that abaucin may disrupt lipoprotein accommodation and transport facilitated by LolE. To further explore the hypothesis that abaucin was interfering with lipoprotein trafficking, we treated wild-type A. baumannii with 5× MIC of abaucin, or no compound, for varying durations and subjected cells to RNA sequencing. A transcriptomics approach would afford systems-level insight that could provide additional indirect evidence to support or negate our initial mechanistic hypothesis. After sequencing, we performed differential expression analyses between the no-drug control cultures and the abaucin-treated cultures to quantify up- and down-regulated transcripts. Differentially abundant transcripts were then clustered based on Gene Ontology (GO) term to identify the biological processes that most statistically changed due to abaucin exposure. Through this method, we observed that abaucin caused downregulation of genes involved in the aerobic electron transport chain and transmembrane ion transport (Fig. 3c, Extended Data Fig. 3k and Supplementary Data 4). Interestingly, this transcriptional response is consistent with the activation of the Cpx two-component envelope stress response that has been well-characterized in the model bacterium E. coli32,33. Indeed, CpxAR has a principal role in monitoring lipoprotein trafficking from the inner membrane to the outer membrane in Gram-negative bacteria34. When lipoprotein trafficking is perturbed, CpxA autophosphorylates upon associated membrane stress, before phosphotransfer to the transcriptional regulator CpxR and subsequent transcriptional remodeling to restore envelope homeostasis32. These transcriptomics data further strengthen the hypothesis that abaucin displays antibacterial efficacy through perturbation of Lol complex-mediated lipoprotein trafficking. We note that the A. baumannii Lol system has not yet been thoroughly explored, necessitating our interpretation of these data in the context of the better-studied E. coli model. With mutational, in silico structural and transcriptional data providing support that abaucin perturbs lipoprotein trafficking, we next investigated whether modulation of LolE expression would change abaucin potency. First, we hypothesized that the MIC of abaucin would decrease with decreased LolE expression35. To test this, we engineered A. baumannii with an inducible CRISPRi system using three distinct guide RNAs targeting AbLolE. As expected, we observed that induction of the CRISPRi construct resulted in fourfold to eightfold decreased abaucin MIC relative to an empty vector control with no guide RNA (Fig. 3d). Notably, uninduced cells with or without LolE-targeting guide RNAs all displayed identical abaucin sensitivities (Extended Data Fig. 3l). Next, we hypothesized that the abaucin-resistant mutant with the upstream intergenic mutation may be conferring resistance through increased expression of lolE—multicopy suppression-mediated resistance36. This is the inverse experiment to that described above. To test this hypothesis, we performed qPCR on all four independent abaucin-resistant mutants, as well as the wild-type parent strain of A. baumannii, and observed that the mutant carrying the upstream G to A mutation displayed ~4-fold increased expression of lolE relative to all other strains tested (Fig. 3e and Extended Data Fig. 3m). Intriguingly, this fourfold increase in lolE expression parallels the fourfold increase in abaucin MIC observed relative to wild-type A. baumannii. Collectively, these data are consistent with the mechanistic model that abaucin targets LolE-mediated lipoprotein trafficking. Prior work has shown that inhibition of lipoprotein transport in E. coli results in abnormal cell morphology characterized by significant bacterial cell swelling and loss of nucleoid condensation37. We hypothesized that abaucin treatment would result in similar morphological features in A. baumannii. Therefore, we subjected wild-type A. baumannii to increasing concentrations of abaucin and imaged these cells using fluorescence microscopy. Here we stained A. baumannii cells with DAPI and FM4-64, respectively, to visualize DNA and the cell envelope. Consistent with prior work in E. coli, abaucin-treated A. baumannii cells displayed increased swelling and a loss of intracellular nucleoid condensation as a function of concentration (Fig. 3f). These data provide compelling phenotypic support that abaucin disrupts lipoprotein trafficking. Abaucin can suppress A. baumannii in a wound infection model A. baumannii is a problematic nosocomial pathogen that survives for prolonged periods on surfaces and has the ability to accumulate extracellular DNA1 , including antibiotic resistance genes38. Indeed, A. baumannii is a major cause of multidrug-resistant infections in wounded military personnel39. Our data showing that abaucin inhibited the growth of a wide array of multidrug-resistant clinical isolates of A. baumannii—including pan-resistant strains—provided confidence that abaucin may be used to treat such problematic wound infections. To test the in vivo efficacy of abaucin, we established a wound infection on the dorsal surface of neutropenic C57BL/6 mice using A. baumannii ATCC 17978 (~6.5 × 106  CFU inoculum) and allowed the bacterial population to expand for 1 h. Mice were subsequently treated with Glaxal Base Moisturizing Cream supplemented with vehicle (1.65% DMSO) or abaucin (4% wt/vol). Application of vehicle or abaucin occurred at 2 h, 3 h, 4 h, 6 h, 10 h, 21 h and 24 h postinfection. Mice were killed at 25 h postinfection, and tissue was aseptically dissected and then plated to quantify A. baumannii viability. In vehicle-treated mice, we retrieved ~6.9 × 108  CFU g−1 at the experimental endpoint and observed that the wounded tissues displayed significant inflammation. Contrarily, abaucin-treated mice carried ~4.0 × 107  CFU g−1—nearly identical to the pretreated infection control mice—and abaucin-treated tissues displayed markedly less inflammation (Fig. 4). These data show that abaucin can effectively suppress an A. baumannii wound infection, consistent with its effect on A. baumannii viability in vitro (Fig. 1f). We note, however, that conventional clinically employed bactericidal antibiotics would likely result in enhanced A. baumannii clearance in our wound model relative to abaucin. However, due to widespread resistance to conventional antibiotics, as well as the broad-spectrum activity of these medicines, abaucin and derivatives thereof may represent a structurally and functionally new class of A. baumannii-specific antibiotics that can overcome existing resistance determinants. Indeed, given that abaucin is readily amenable to medicinal chemistry optimization (Table 1), we emphasize the existing opportunity to develop structural analogs of abaucin with enhanced in vivo activity.\n\nDiscussion Structurally and functionally new antibiotics are urgently needed for A. baumannii, which is notoriously difficult to eradicate due to its ability to uptake and retain antibiotic resistance determinants1 . Moreover, species-selective antibiotics hold promise to limit the horizontal dissemination of resistance determinants and decrease the likelihood of dysbiosis during treatment8 . The machine learning-guided discovery of abaucin highlights the utility of algorithmic approaches to discover new antibacterial molecules against A. baumannii and provides the field with a promising new narrow-spectrum molecular scaffold to address one of the world’s most challenging Gram-negative pathogens. Lipoprotein trafficking is a highly sought-after antibiotic target that has yet to be perturbed by clinically used antibacterial drugs, which is advantageous toward increasing the duration of clinical utility of molecules that disrupt this process. Notably, our observations that abaucin displays narrow-spectrum activity through perturbation of LolE-mediated lipoprotein trafficking can be explained, at least in part, due to the divergence of the A. baumannii Lol system relative to most other Gram-negative species16. Specifically, in most Gram-negative organisms, the inner membrane-associated lipoprotein transport machinery consists of LolC, LolD and LolE in an asymmetric multiprotein complex. A. baumannii, on the other hand, encodes a symmetric inner membrane complex containing LolD and two copies of LolE (also termed LolF), without LolC. The work described herein represents an advancement toward the validation of machine learning for new narrow-spectrum antibiotic discovery and adds to the growing body of literature that supports the utility of computational approaches to accelerate drug discovery more generally7 . Indeed, over the past half-decade, investigators have successfully applied classic molecular fingerprint-based machine learning methods, as well as more sophisticated neural network approaches, to predict antibiotics5 , antiviral compounds40 and anticancer therapies41. Furthermore, recent advancements in chemical generation algorithms have resulted in the design of new molecules with desired properties of interest42. Expectedly, there exist opportunities for improvement of molecular property prediction methods. For example, future endeavors can readily expand on this work by increasing the size of the A. baumannii antibiotic training dataset and performing predictions on larger in silico chemical libraries. Given that our prediction filtering method was exceptionally stringent to enrich for chemicals that were structurally divergent from our training dataset, it is likely that similarly aggressive filtering approaches could be leveraged going forward to manage the number of molecules for experimental validation, even when predictions are performed on the scale of hundreds of millions to billions of compounds. However, we emphasize that algorithmic approaches may be applied to in silico chemical libraries of any size, serving to decrease the time and costs required to identify valuable molecules by avoiding the strict necessity for successive rounds of resource-intensive screening. Moreover, investigators can apply multiproperty optimization models that simultaneously predict numerous properties of interest. In the context of antibiotic drug discovery specifically, future models could be trained on (1) growth inhibition of a pathogen of interest and (2) mammalian cell toxicity, given that a sufficiently robust training dataset exists. In this manner, forthcoming predictive models can be applied to wide regions of unexplored chemical space, with more confidence that prioritized molecules will satisfy multiple properties that are required of new clinical antibiotics. Indeed, with the increasing availability of high-quality datasets on which to train, we posit that machine learning methods are now well positioned to become widely employed tools to more efficiently identify structurally and functionally new antibacterial leads.\n\nAcknowledgements We thank S. French from McMaster University for technical assistance with fluorescence microscopy experiments. This work was supported by the David Braley Centre for Antibiotic Discovery (to J.M.S.); the Weston Family Foundation (POP and Catalyst to J.M.S.); the Audacious Project (to J.J.C. and J.M.S.); the C3.ai Digital Transformation Institute (to R.B.); the Abdul Latif Jameel Clinic for Machine Learning in Health (to R.B.); the DTRA Discovery of Medical Countermeasures Against New and Emerging (DOMANE) threats program (to R.B.); the DARPA Accelerated Molecular Discovery program (to R.B.); the Canadian Institutes of Health Research (FRN-156361 to B.K.C.); Genome Canada GAPP (OGI-146 to M.G.S.); the Canadian Institutes of Health Research (FRN-148713 to M.G.S.); the Faculty of Health Sciences of McMaster University (to J.M.); the Boris Family (to J.M.); a Marshall Scholarship (to K.S.); and the DOE BER (DE-FG02-02ER63445 to A.C-P.).\n\nOnline content Any methods, additional references, Nature Portfolio reporting summaries, source data, extended data, supplementary information, acknowledgements, peer review information; details of author contributions and competing interests; and statements of data and code availability are available at https://doi.org/10.1038/s41589-023-01349-8. References 1. Antunes, L. C. S., Visca, P. & Towner, K. J. Acinetobacter baumannii: evolution of a global pathogen. Pathog. Dis. 71, 292–301 (2014). 2. 2020 Antibacterial Agents in Clinical and Preclinical Development: An Overview and Analysis (World Health Organization, 2021); https://www.who.int/publications/i/item/9789240021303 3. Walsh, C. Where will new antibiotics come from? Nat. Rev. Microbiol. 1, 65–70 (2003). 4. Tommasi, R., Brown, D. G., Walkup, G. K., Manchester, J. I. & Miller, A. A. ESKAPEing the labyrinth of antibacterial discovery. Nat. Rev. Drug Discov. 14, 529–542 (2015). 5. Stokes, J. M. et al. A deep learning approach to antibiotic discovery. Cell 180, 688–702 (2020). 6. Ma, Y. et al. Identification of antimicrobial peptides from the human gut microbiome using deep learning. Nat. Biotechnol. 40, 921–931 (2022). 7. Lluka, T. & Stokes, J. M. Antibiotic discovery in the artificial intelligence era. Ann. N. Y. Acad. Sci. 1519, 74–93 (2023). 8. Melander, R. J., Zurawski, D. V. & Melander, C. Narrow-spectrum antibacterial agents. MedChemComm 9, 12–21 (2018). 9. Theriot, C. M. et al. Antibiotic-induced shifts in the mouse gut microbiome and metabolome increase susceptibility to Clostridium dificile infection. Nat. Commun. 5, 3114 (2014). 10. Willing, B. P. et al. A pyrosequencing study in twins shows that gastrointestinal microbial profiles vary with inflammatory bowel disease phenotypes. Gastroenterology 139, 1844–1854 (2010). 11. Turnbaugh, P. J. et al. An obesity-associated gut microbiome with increased capacity for energy harvest. Nature 444, 1027–1031 (2006). 12. Kelly, J. R. et al. Transferring the blues: depression-associated gut microbiota induces neurobehavioural changes in the rat. J. Psychiatr. Res. 82, 109–118 (2016). 13. Wu, N. et al. Dysbiosis signature of fecal microbiota in colorectal cancer patients. Microb. Ecol. 66, 462–470 (2013). 14. Lee, H. S., Plechot, K., Gohil, S. & Le, J. Clostridium dificile: diagnosis and the consequence of over diagnosis. Infect. Dis. Ther. 10, 687–697 (2021). 15. Corsello, S. M. et al. The Drug Repurposing Hub: a next-generation drug library and information resource. Nat. Med. 23, 405–408 (2017). 16. Kaplan, E., Greene, N. P., Crow, A. & Koronakis, V. Insights into bacterial lipoprotein traficking from a structure of LolA bound to the LolC periplasmic domain. Proc. Natl Acad. Sci. USA 115, E7389–E7397 (2018). 17. Tang, X. et al. Structural basis for bacterial lipoprotein relocation by the transporter LolCDE. Nat. Struct. Mol. Biol. 28, 347–355 (2021). 18. Yang, K. et al. Analyzing learned molecular representations for property prediction. J. Chem. Inf. Model. 59, 3370–3388 (2019). 19. Landrum, G. RDKit: a software suite for cheminformatics, computational chemistry, and predictive modeling. https://www. rdkit.org/RDKit_Overview.pdf 20. Seok, S. J. et al. Blockade of CCL2/CCR2 signalling ameliorates diabetic nephropathy in db/db mice. Nephrol. Dial. Transplant. 28, 1700–1710 (2013). 21. Cerri, C. et al. The chemokine CCL2 mediates the seizure-enhancing efects of systemic inflammation. J. Neurosci. 36, 3777–3788 (2016). 22. Chargari, C. et al. Preclinical assessment of JNJ-26854165 (Serdemetan), a novel tryptamine compound with radiosensitizing activity in vitro and in tumor xenografts. Cancer Lett. 312, 209–218 (2011). 23. Lehman, J. A. et al. Serdemetan antagonizes the Mdm2-HIF1α axis leading to decreased levels of glycolytic enzymes. PLoS ONE 8, e74741 (2013). 24. Stokes, J. M., Lopatkin, A. J., Lobritz, M. A. & Collins, J. J. Bacterial metabolism and antibiotic eficacy. Cell Metab. 30, 251–259 (2019). 25. Zheng, E. J., Stokes, J. M. & Collins, J. J. Eradicating bacterial persisters with combinations of strongly and weakly metabolism-dependent antibiotics. Cell Chem. Biol. 27, 1544–1552 (2020). 26. Francino, M. P. Antibiotics and the human gut microbiome: dysbioses and accumulation of resistances. Front. Microbiol. 6, 1543 (2016). 27. Smits, W. K., Lyras, D., Lacy, D. B., Wilcox, M. H. & Kuijper, E. J. Clostridium dificile infection. Nat. Rev. Dis. Prim. 2, 16020 (2016). 28. Sharma, S. et al. Mechanism of LolCDE as a molecular extruder of bacterial triacylated lipoproteins. Nat. Commun. 12, 4687 (2021). 29. Nicholson, W. L. & Maughan, H. The spectrum of spontaneous rifampin resistance mutations in the rpoB gene of Bacillus subtilis 168 spores difers from that of vegetative cells and resembles that of Mycobacterium tuberculosis. J. Bacteriol. 184, 4936–4940 (2002). 30. Baek, M. et al. Accurate prediction of protein structures and interactions using a three-track neural network. Science 373, 871–876 (2021). 31. Buel, G. R. & Walters, K. J. Can AlphaFold2 predict the impact of missense mutations on structure? Nat. Struct. Mol. Biol. 29, 1–2 (2022). 32. Raivio, T. L., Leblanc, S. K. D. & Price, N. L. The Escherichia coli Cpx envelope stress response regulates genes of diverse function that impact antibiotic resistance and membrane integrity. J. Bacteriol. 195, 2755–2767 (2013). 33. Guest, R. L., Wang, J., Wong, J. L. & Raivio, T. L. A bacterial stress response regulates respiratory protein complexes to control envelope stress adaptation. J. Bacteriol. 199, e00153-17 (2017). 34. Delhaye, A., Laloux, G. & Collet, J.-F. The lipoprotein NlpE is a Cpx sensor that serves as a sentinel for protein sorting and folding defects in the Escherichia coli envelope. J. Bacteriol. 201, e00611-18 (2019). 35. Peters, J. M. et al. A comprehensive, CRISPR-based functional analysis of essential genes in bacteria. Cell 165, 1493–1506 (2016). 36. Pathania, R. et al. Chemical genomics in Escherichia coli identifies an inhibitor of bacterial lipoprotein targeting. Nat. Chem. Biol. 5, 849–856 (2009). 37. McLeod, S. M. et al. Small-molecule inhibitors of Gram-negative lipoprotein traficking discovered by phenotypic screening. J. Bacteriol. 197, 1075–1082 (2015). 38. Manchanda, V., Sanchaita, S. & Singh, N. Multidrug resistant acinetobacter. J. Glob. Infect. Dis. 2, 291–304 (2010). 39. Davis, K. A., Moran, K. A., McAllister, C. K. & Gray, P. J. Multidrug-resistant Acinetobacter extremity infections in soldiers. Emerg. Infect. Dis. 11, 1218–1224 (2005). 40. Jin, W. et al. Deep learning identifies synergistic drug combinations for treating COVID-19. Proc. Natl Acad. Sci. USA 118, e2105070118 (2021). 41. Preuer, K. et al. DeepSynergy: predicting anti-cancer drug synergy with Deep Learning. Bioinformatics 34, 1538–1546 (2018). Nature Chemical Biology Article https://doi.org/10.1038/s41589-023-01349-8 42. Jin, W., Barzilay, R. & Jaakkola, T. Junction tree variational autoencoder for molecular graph generation. In Proc. 35th International Conference on Machine Learning (eds Dy, J. & Krause, A.) 2323–2332 (PMLR, 2018). Publisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afiliations. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. © The Author(s), under exclusive licence to Springer Nature America, Inc. 2023\n\nMethods Training data acquisition A. baumannii ATCC 17978 was grown in 2 ml LB medium (Becton, Dickinson and Company) overnight at 37 °C with shaking. Cells were diluted 1/10,000 into fresh LB and 99 µl of cells was added to each well of a 96-well flat-bottom plate (Corning). Next, 1 µl of a 5 mM stock of each molecule from a collection of 7,684 small molecules (FDA-approved drugs and molecules from screening collections from the Broad Institute) was added, in duplicate, using an Agilent Bravo liquid handling system. The final concentration was 50 µM. Plates were incubated in sealed plastic bags at 37 °C for 16 h and read at 600 nm using a SpectraMax M3 plate reader. Plate data were normalized based on the interquartile mean of each plate before binarization into ‘active’ and ‘nonactive’ categories for model training. Active molecules were defined as those that resulted in growth at least 1σ below the mean growth of the entire dataset. Model training and predictions A directed message-passing neural network (Chemprop), like other message-passing neural networks, learns to predict molecular properties directly from the graph structure of molecules, where atoms are nodes and bonds are edges. For every molecule in our training dataset, we reconstructed the molecular graph corresponding to each compound’s SMILES string and determined the set of atoms and bonds using RDKit19. Next, we initialized a feature vector for each atom and bond, as described previously5 . The model applies a series of message-passing steps where it aggregates information from neighboring atoms and bonds to build a representation of local chemistry. On each step of message passing, every bond’s featurization is updated by summing the featurization of neighboring atoms and bonds, applying a single neural network layer, adding the bond’s previous featurization, and then applying ReLU activation. After a defined number of message-passing steps, the learned featurizations across the molecule are summed to produce a single featurization for the whole molecule. Lastly, this featurization is subjected to a feed-forward neural network that outputs a prediction of the property of interest—in our case, antibiotic activity against A. baumannii. To augment the architecture described here, we employed the following two model enhancements: molecule-level features and ensembling. Molecule-level features. The message-passing approach is ideal for extracting features of local chemistry within a larger molecule. However, it can struggle to extract global molecular features for larger molecules. To address this limitation, we concatenated the molecular representation that is learned during message passing with 200 additional molecule-level features computed using RDKit. Ensembling. Ensembling was used to further improve model performance, where several copies of the same model architecture with different random initial weights are trained and their predictions are averaged. Here we used an ensemble of ten models, with each model trained on a unique split of the training dataset. After model building and training on the 7,684-molecule training dataset (with ~6.2% active examples), we applied our model to the updated Drug Repurposing Hub, consisting of 6,680 compounds, many of which occupy a unique chemical space relative to that on which the model was trained. Here we randomly split the dataset into 80% training data, 10% validation data and 10% test data. We trained our model on these data for 30 epochs and evaluated the model on the validation data at the end of each epoch. Once training was complete, we used the model parameters that performed best on the validation data and tested the model on the test data. We ran tenfold cross-validation by repeating this procedure with ten different splits by systematically dividing training, validation and test sets such that all molecules appeared at equal proportions across all sets over the course of training iterations. After we achieved model performance that was acceptable for our prediction task, we conducted predictions on the Drug Repurposing Hub. Our model had the following hyperparameters: number of message-passing steps = 3; neural network hidden size = 300; number of feed-forward layers = 2; dropout probability = 0. Structural analysis We used Tanimoto similarity to quantify the chemical relationship between molecules in our training dataset and prediction dataset. The Tanimoto similarity between two molecules is a measure of the proportion of shared chemical substructures. To compute Tanimoto similarity, we first made Morgan fingerprints for each molecule using radius = 2 and 2,048-bit fingerprint vectors using RDKit. Tanimoto similarity was then computed as the number of chemical substructures contained in both molecules divided by the total number of unique chemical substructures in either molecule. The Tanimoto similarity is a number between 0 and 1, where 0 represents no shared substructures and 1 represents all substructures that are shared. For t-distributed stochastic neighbor embedding (t-SNE) analysis, plots were created using the implementation of t-distributed stochastic neighbor embedding by scikit-learn43. Here we first used RDKit to compute Morgan fingerprints for each molecule as described above. We then used t-SNE with the Jaccard distance metric to reduce the data from 2,048 dimensions to the two dimensions that are plotted. Jaccard distance is Tanimoto distance, which is defined as follows: Tanimoto distance = 1 – Tanimoto similarity. We used the scikit-learn default values for all t-SNE parameters besides the distance metric. Pathogen growth inhibition Cells were grown overnight in 2 ml LB medium and diluted 1/10,000 into fresh LB. In 96-well flat-bottom plates (Corning), cells were then introduced to compound at a final concentration of 50 µM, or to compound at twofold serial dilutions, in final volumes of 100 µl. Plates were then incubated at 37 °C until untreated control cultures reached the stationary phase, at which time plates were read at 600 nm using a SpectraMax M3 plate reader. We note here that the incubation time required to reach the stationary phase differed slightly between strains. For abaucin analog spectrum of coverage assays, the laboratory strains were E. coli BW25113, S. aureus RN4220 and P. aeruginosa PAO1. Clinical isolate panels were curated from the CDC ARIsolate Bank and assayed in LB as described above. Bacterial cell killing Cells were grown overnight in 2 ml LB medium and diluted 1/10,000 into fresh LB. In 96-well flat-bottom plates (Corning), cells were grown to the required density (100 µl final volume) at 37 °C, at which time compound was added at the indicated concentration and cultures were incubated for the required duration. Cells were then pelleted in plates by centrifugation at 4,000g for 15 min at 4 °C and washed in ice-cold PBS. After washing, cells were tenfold serially diluted in PBS and plated on LB. In experiments where cells were incubated with antibiotic in nutrient-deplete conditions, cells were grown to the required density in LB media at 37 °C, washed in PBS and subsequently resuspended in PBS before the addition of compound (100 µl final volume). After cultures were incubated for the required duration at 37 °C, cells were pelleted in plates by centrifugation at 4,000g for 15 min at 4 °C and washed in ice-cold PBS. After washing, cells were tenfold serially diluted in PBS and plated on LB. For post-treatment regrowth experiments, cells were treated with abaucin and incubated in LB for 6 hours as described above (100 µl final volume). Cells were then pelleted in plates by centrifugation at 4,000g for 15 min at 4 °C and washed in ice-cold PBS. After washing, cells were resuspended in 100 µl fresh LB, diluted 1/1,000 in fresh 100 µl LB and grown at 37 °C in a Biotek Synergy H1 plate reader. Plates were read at 600 nm every 20 min for 16 h. Commensal species growth inhibition For analysis of commensal species from the human gut, strains were grown from frozen stock on brain heart infusion (BHI) agar supplemented with 0.5 g l−1 l-cysteine, 10 mg l−1 hemin and 1 mg l−1 vitamin K (BHI3). Single colonies were picked, transferred to 96-well plates containing liquid BHI3 and grown for 24 h at 37 °C. Antibiotics (ampicillin, Sigma-Aldrich; ciprofloxacin, Sigma-Aldrich; abaucin; Supplementary Note) were arrayed in BHI3 broth at twofold the final desired concentrations, and prereduced in an anaerobic environment overnight. Liquid bacterial cultures were standardized to twofold the desired optical density (OD600 = 0.02). The twofold concentrated cultures were then inoculated into 96-well plates (50 µl) containing twofold concentrated antibiotics (50 µl). The final antibiotic concentrations were ampicillin (128 µg ml−1), ciprofloxacin (0.25 µg ml−1) and abaucin (2 µg ml−1, 20 µg ml−1 and 40 µg ml−1). The final culture volume was 100 µl. After inoculation, plates were sealed with breathable membranes (Breathe-Easy) and incubated at 37 °C without shaking for 24 h. After incubation, the plates were read at 600 nm using Biotek Synergy H1 plate reader. All bacterial strains were grown and incubated in a Coy Laboratory Vinyl Anaerobic Chamber (5% CO2, 2% H2 and 93% N2). All media was prereduced under anaerobic conditions for at least 4 h before use. For analysis of commensal skin species in aerobic conditions, strains were grown from frozen on BHI3 agar and incubated at 37 °C for 24 h. Single colonies were picked and transferred to 96-well plates containing liquid BHI3, which were then incubated at 37 °C without shaking for 24 h. Liquid cultures were then standardized to OD600 = 0.02 (twofold the final OD). Antibiotics (ampicillin, ciprofloxacin and abaucin) were arrayed in BHI3 broth at twofold the final concentrations (the final antibiotic concentrations were ampicillin (128 µg ml−1), ciprofloxacin (0.25 µg ml−1) and abaucin (2 µg ml−1, 20 µg ml−1 and 40 µg ml−1)). The twofold concentrated cultures (50 µl) were then inoculated into 96-well plates containing the twofold concentrated antibiotics (50 µl) to achieve a final volume of 100 µl. Plates were sealed with breathable membranes (Breathe-Easy) and incubated at 37 °C without shaking for 48 h under 5% CO2. After incubation, plates were read at 600 nm using a Biotek Synergy H1 plate reader. Suppressor mutant evolution and sequencing A. baumannii ATCC 17978 was grown in 2 ml LB medium overnight at 37 °C with shaking. ~108  CFU in 100 µl liquid LB was deposited onto solid LB plates supplemented with abaucin at varying concentrations ranging from 2 µg ml−1 to 10 µg ml−1. Plates were incubated at 37 °C and monitored every 24 h for the emergence of abaucin mutants. On the emergence of colonies, these were purified by restreaking onto solid LB and solid LB supplemented with abaucin at the same concentration from which the colonies were originally collected. Four independent abaucin-resistant strains were subsequently selected for whole-genome sequencing. Chromosomal DNA from each mutant and wild-type A. baumannii ATCC 17978 was purified using a Gentra Puregene DNA isolation kit (Qiagen). DNA (chromosomal and plasmid) was sequenced on an Illumina MiSeq platform and reads were aligned to the reference A. baumannii ATCC 17978 genome (CP053098.1, CP053099.1 and CP053100.1) using Breseq44. For frequency of resistance quantification, 1.1 × 108  CFU of A. baumannii ATCC 17978 in 100 µl liquid LB was deposited onto solid LB plates supplemented with abaucin at the noted concentrations. After 24 h, 48 h and 72 h of incubation at 37 °C, colonies were counted, and these values were divided by 1.1 × 108  to quantify the frequency of resistance. Transcriptomic analysis A. baumannii ATCC 17978 was grown in 2 ml LB medium overnight at 37 °C with shaking. Cells were diluted 1/10,000 into 50 ml fresh LB and grown to mid-log at 37 °C with shaking, at which time cultures were supplemented with 5× MIC (10 µg ml−1) abaucin or no drug and grown for an additional 3 h, 4.5 h or 6 h. After the required durations of incubation post-treatment, 2 ml samples were collected and flash-frozen on liquid nitrogen. cDNA library construction and sequencing for these samples were performed by Genewiz. Paired-end sequence data in FASTQ file format were aligned to the transcripts of CP053098.1 (A. baumannii ATCC 17978 chromosome), CP053099.1 (A. baumannii ATCC 17978 plasmid 1) and CP053100.1 (A. baumannii ATCC 17978 plasmid 2). mRNA abundances were then quantified as transcripts per million using the kallisto (version 0.46.2)45 ‘quant’ function, and the parameter ‘-b 100’ with paired-end reads. All downstream analyses were performed using R (version 4.1.0)46. Expression differences between abaucin-treated samples and nontreated controls were quantified using the DESeq2 pipeline (version 1.34.0)47. We defined a cutoff of log2(FC) ≥ 1.5 and padj ≤ 0.01 to identify differentially expressed genes. For GO enrichment, differentially expressed A. baumannii transcripts for which no gene name was identified in CP053098.1, CP053099.1 or CP053100.1 were subject to blastp analysis against E. coli MG1655 (https://blast. ncbi.nlm.nih.gov/). A. baumannii proteins with E values < 10−50, percent identify >40% and query coverage >80% to annotated E. coli proteins were binned and GO-enriched with EcoCyc Pathway Tools48–50 using the Fisher exact test for statistical significance of GO enrichment. CRISPRi LolE knockdown Three independent 20 base pair sgRNAs targeting lolE (sgRNAlolE-1: 5′-TAAACGTAAGCCAAGCGAAT-3′; sgRNAlolE-2: 5′-CAAATTCTTCAC AATGTCAT-3′; sgRNAlolE-3: 5′-TTTAAGTGAGTCGAGGCTAC-3′) were designed using predictive software51 to maximize on-target activity and minimize off-target binding in A. baumannii ATCC 17978. sgRNAs were then cloned into pFD152 (Addgene plasmid 125546; provided by D. Bikard) using a single-step golden gate assembly reaction, as described previously52. sgRNA sequences were verified using Sanger sequencing and constructs were transformed into A. baumannii ATCC 17978. pFD152 was selected in A. baumannii using spectinomycin (250 µg ml−1 in liquid media and 500 µg ml−1 on solid media; Sigma-Aldrich). To determine whether LolE knockdown enhanced the growth inhibitory potency of abaucin, A. baumannii ATCC 17978 harboring pFD152, pFD152-sgRNAlolE-1, pFD152-sgRNAlolE-2 or pFD152-sgRNAlolE-3 were grown overnight in LB medium, diluted 1/10,000 into fresh LB, and grown mid-log phase (OD ~ 0.2) at 37 °C. Anhydrotetracycline (aTc; Sigma-Aldrich) was added to the subcultures at a final concentration of 1 µg ml−1 to induce knockdown of lolE expression and cultures were incubated for an additional 2.5 h at 37 °C with shaking. Following this induction period, cultures were back diluted to the equivalent OD of 1/10,000 of a dense culture and introduced to varying concentrations of abaucin and 0.5 µg ml−1 of aTc. MICs were then determined as described above. Quantitative reverse transcription (qRT)–PCR Wild-type A. baumannii ATCC 17978 and the resistant mutants were grown in 3 ml LB medium overnight at 37 °C with shaking. Cells were diluted 1/10,000 into 15 ml fresh LB and grown to mid-log at 37 °C with shaking, at which time cells were pelleted by centrifugation at 4,000g for 15 min at 4 °C and frozen for ~2 h. Cells were lysed in 100 µl of 1 mg ml−1 lysozyme and incubated at room temperature for 10 min. RNA was isolated using the RNEasy Mini Kit (Qiagen), with some modifications. Briefly, to each sample, 3.5 µl of 2-mercaptoethanol, 250 µl of RLT buffer and 250 µl of 95% ethanol was added. Seven hundred microliters of each sample was then loaded onto a spin column and incubated at room temperature for 5 min. The sample was centrifuged at 8,000g for 1 min. The column was washed with 700 µl RW1 buffer and centrifuged at 8,000g for 1 min. The column was washed with 700 µl RPE buffer and centrifuged at 8,000g for 2 min. The column was washed again with 500 µl RPE buffer and centrifuged at 8,000g for 2 min. The column was transferred to a new collection tube and incubated at room temperature for 5 min. The empty column was centrifuged at 13,000g for 2 min and incubated at room temperature for 5 min. The column was then placed into a fresh 1.5 ml microcentrifuge tube and 45 µl of RNase-free water was added to the center of the column and incubated at room temperature for 5 min. To elute RNA, the column was centrifuged at 8,000g for 2 min. RNA was subject to DNase I treatment for 10 min at room temperature. The resulting RNA was quantified using a Nanodrop. Next, the RNA integrity was assessed through agarose gel electrophoresis. For cDNA synthesis, the High-Capacity cDNA Reverse Transcription Kit (Applied Biosystems) was used as specified by the manufacturer’s instructions. The cDNA was then amplified with iTaq Universal SYBR Green Supermix (Bio-Rad) via a Bio-Rad CFX96 (Bio-Rad) following the manufacturer’s instructions. The target amplification efficiency was evaluated by generating a standard curve with dilutions of cDNA (>95% amplification efficiency for each primer pair). Primers lolE195F + lolE195R were used to amplify lolE; rpoD182F + rpoD182R, glt188F + glt188R and gyrB196F + gyrB196R were used to amplify the housekeeping genes rpoD, gltA and gyrB. Triple technical replicates were examined per biological sample, with controls omitting reverse-transcriptase to verify a lack of genomic DNA contamination, and cDNA to verify the absence of external nucleic acid contamination and primer dimer formation. Transcript level of lolE from each mutant was evaluated using the comparative 2−ΔΔCt method to the wild-type strain, normalizing to the 2−ΔΔCt value of housekeeping genes. lolE195F—5′-CAGCTGGAGATGGAGTCGCT-3′ lolE195R—5′-TGGTGACGGGTATTGACCCT-3′ rpoD182F—5′-ACATGGCCGTAATAGCCCTGA-3′ rpoD182F—5′-CCATACGGCCACGACGTACT-3′ glt188F—5′-GCGACAGCTTCATGCGAGTC-3′ glt188R—5′-TGAGCACGAACTTTCGCATCG-3′ gyrB196F—5′-ATTAGTGCTGATGCGCCTGC-3′ gyrB196R—5′-CTTGATCCGCCCATTGCTGG-3′ Structural protein prediction We aligned the wild-type A. baumannii protein sequence of interest against the UniProt/SwissProt database53 using blastp (version 2.13.0+)54,55 through the NCBI blast platform (https://blast.ncbi. nlm.nih.gov/) and obtained a multiple-sequence alignment. This alignment identified the E. coli K12 LolE protein as a homolog with E value = 6 × 10−88. Next, we aligned the E. coli LolE protein, the A. baumannii wild-type LolE protein and the A. baumannii abaucin-resistant A362T variant LolE protein using Geneious Prime software (version 2021.2.2; https://www.geneious.com); we applied a ‘global alignment with free end gaps’ as the alignment type and ‘blosum62’ as the cost matrix, with parameters gap open penalty = 12; gap extension penalty = 3 and refinement iterations = 2. We then predicted the structure of the wild-type A. baumannii LolE protein and A362T variant protein sequences with RoseTTAfold30 through the Robetta platform (https://robetta.bakerlab.org/). We did not provide a multiple-sequence alignment for this computation. We obtained the protein structure of the wild-type E. coli LolCDE in complex with a lipoprotein from ref. 17, Protein Data Bank: 7ARH. We then aligned the structures of the wild-type A. baumannii LolE protein and the LolCDE complex with Maestro (version 12.9.137) from Schrödinger and ChimeraX (version 1.3rc202112020528)56,57. In Maestro, we used the default structural alignment parameters. With ChimeraX, we used the ‘Needleman-Wunsch’ alignment algorithm with ‘blosum62’ as the similarity matrix, with parameters ‘bb’ as chain pairing; 0.3 as SS fraction; 1 as gap extend; 18/18/6 as gap open (HH/SS/other); and 2 as iteration cutoff. Final figures of structural alignments were generated with ChimeraX (version 1.3rc202112020528)56,57. Quantitative microscopy A. baumannii ATCC 17978 was grown overnight in 2 ml LB medium and diluted 1/10,000 into fresh LB. In 96-well flat-bottom plates, cells were then introduced to the compound at the indicated concentrations, in final volumes of 100 µl. Plates were grown to mid-exponential phase in a Biotek Epoch 2 plate reader with shaking at 37 °C. Cell suspensions were transferred to a poly-lysine coated 0.17 mm glass-bottom imaging plate (Brooks Scientific) and incubated with FM 4-64 (1 µg ml−1 final concentration; Invitrogen) and DAPI (0.2 µg ml−1 final concentration; Invitrogen) probes for 10 min in the dark. Samples were imaged using a Nikon Eclipse Ti inverted microscope at ×100 magnification, and cell widths were assessed using Nikon Elements software. Cell widths for each sample were summarized in R using the density function to explore population shifts in morphology as a function of drug concentration. Animal models Mouse model experiments were conducted according to the guidelines set by the Canadian Council on Animal Care, using protocols approved by the Animal Review Ethics Board and McMaster University under Animal Use Protocol 20-12-41. Before infection, mice were relocated at random from housing cages to single-occupancy control or treatment cages. No animals were excluded from the analysis, and blinding was considered unnecessary. Six- to eight-week-old female C57BL/6N mice were pretreated with 150 mg kg−1 (day T-4) and 100 mg kg−1 (day T-1) of cyclophosphamide to render mice neutropenic. On day T-0, mice were anesthetized using isoflurane and administered buprenorphine as an analgesic at 0.1 mg kg−1 intraperitoneally. A 2 cm2  abrasion on the dorsal surface of the mouse was inflicted through tape-stripping to the basal layer of the epidermis using approximately 30–35 pieces of autoclave tape. Mice were immediately infected with ~6.5 × 106  CFU A. baumannii ATCC 17978 directly pipetted onto the wound bed. The infection was left to establish for 1 h before the first treatment with Glaxal Base supplemented with vehicle (1.65% DMSO) or abaucin (4% wt/vol). Mice (n = 5 or 6) were treated 1, 2, 3, 4, 6, 10, 21 and 24 h postinfection with ~10 to 20 µl Glaxal Base with abaucin (treatment) or DMSO (control). Mice were killed at experimental endpoint (25 h postinfection) and wound tissue was collected, homogenized in phosphate-buffered saline and plated on solid LB medium supplemented with chloramphenicol to quantify bacterial load. For chemical preparation, abaucin was weighed and solubilized in 1.65% DMSO and then added to a predetermined amount of Glaxal Base to a final concentration of 4% w/v. The solution was mixed thoroughly to ensure an even distribution of the compound in the carrier. For control groups, the same amount of vehicle (DMSO) was measured and mixed through Glaxal Base. Reporting summary Further information on research design is available in the Nature Portfolio Reporting Summary linked to this article. Data Availability GenBank accession numbers for sequencing of abaucin-resistant mutants are BankIt2629921 – OP677864, OP677865, OP677866 and OP677867. GEO accession numbers for RNA sequencing datasets are GSE214305 – GSM6603484, GSM6603485, GSM6603486, GSM6603487, GSM6603488, GSM6603489 and GSM6603490. Source data are provided with this paper. Code Availability All custom code used for antibiotic prediction is open source and can be accessed without restriction at https://github.com/chemprop/ chemprop. A cloned snapshot used for this paper is available at https:// github.com/GaryLiu152/chemprop_abaucin. All commercial software used is described in Methods. Source data are provided with this paper.", "persona": "Student", "response": "### 1. Overview\nUsing an artificial intelligence algorithm, researchers at MIT and McMaster University have identified a new antibiotic that can kill a type of bacteria that is responsible for many drug-resistant infections. “After that url, when we showed that these machine-learning approaches can work well for complex antibiotic discovery tasks, we turned our attention to what I perceive to be public enemy No. 1 for multidrug-resistant bacterial infections, which is Acinetobacter,” Stokes says. The researchers also plan to use their modeling approach to identify potential antibiotics for other types of drug-resistant infections, including those caused by Staphylococcus aureus and Pseudomonas aeruginosa. The researchers identified the new drug from a library of nearly 7,000 potential drug compounds using a machine-learning model that they trained to evaluate whether a chemical compound will inhibit the growth of A. baumannii.\n\n### 2. Background\nIn studies in mice, the researchers showed that the drug, which they named abaucin, could treat wound infections caused by A. baumannii. They also showed, in lab tests, that it works against a variety of drug-resistant A. baumannii strains isolated from human patients. Stokes’ lab is now working with other researchers at McMaster to optimize the medicinal properties of the compound, in hopes of developing it for eventual use in patients.\n\n### 3. Methods\nTo obtain training data for their computational model, the researchers first exposed A. baumannii grown in a lab dish to about 7,500 different chemical compounds to see which ones could inhibit growth of the microbe. Then they fed the structure of each molecule into the model. They also told the model whether each structure could inhibit bacterial growth or not. This allowed the algorithm to learn chemical features associated with growth inhibition. Once the model was trained, the researchers used it to analyze a set of 6,680 compounds it had not seen before, which came from the Drug Repurposing Hub at the Broad Institute. This analysis, which took less than two hours, yielded a few hundred top hits. Of these, the researchers chose 240 to test experimentally in the lab, focusing on compounds with structures that were different from those of existing antibiotics or molecules from the training data.\n\n### 4. Results\n“This finding further supports the premise that AI can significantly accelerate and expand our search for novel antibiotics,” says James Collins, the Termeer Professor of Medical Engineering and Science in MIT’s Institute for Medical Engineering and Science (IMES) and Department of Biological Engineering. “I’m excited that this work shows that we can use AI to help combat problematic pathogens such as A. baumannii.” Drug discovery Several years ago, Collins, Stokes, and MIT Professor Regina Barzilay (who is also an author on the new study), set out to combat this growing problem by using machine learning, a type of artificial intelligence that can learn to recognize patterns in vast amounts of data. Collins and Barzilay, who co-direct MIT’s Abdul Latif Jameel Clinic for Machine Learning in Health, hoped this approach could be used to identify new antibiotics whose chemical structures are different from any existing drugs. In their initial demonstration, the researchers trained a machine-learning algorithm to identify chemical structures that could inhibit growth of E. coli. In a screen of more than 100 million compounds, that algorithm yielded a molecule that the researchers called halicin, after the fictional artificial intelligence system from “2001: A Space Odyssey.” This molecule, they showed, could kill not only E. coli but several other bacterial species that are resistant to treatment.\n\n### 5. Takeaways\nFurther experiments revealed that the drug kills cells by interfering with a process known as lipoprotein trafficking, which cells use to transport proteins from the interior of the cell to the cell envelope. Specifically, the drug appears to inhibit LolE, a protein involved in this process. All Gram-negative bacteria express this enzyme, so the researchers were surprised to find that abaucin is so selective in targeting A. baumannii. They hypothesize that slight differences in how A. baumannii performs this task might account for the drug’s selectivity. “We haven’t finalized the experimental data acquisition yet, but we think it’s because A. baumannii does lipoprotein trafficking a little bit differently than other Gram-negative species. We believe that’s why we’re getting this narrow spectrum activity,” Stokes says.", "paper_id": "52", "story_id": "52-01"}
