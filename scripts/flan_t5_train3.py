# -*- coding: utf-8 -*-
"""Flan-t5-vers3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1oRVF8pT0TXhqsSTOVU5Y2hqMI9gQKxIL
"""

# Aggiorna sistema
!apt-get update -y
!apt-get install -y libstdc++6

# Rimuovi pacchetti corrotti
!pip uninstall -y torch torchvision torchaudio numpy transformers bitsandbytes peft accelerate

# Installa NumPy compatibile
!pip install numpy==1.26.4

# Installa PyTorch e TorchVision compatibili
!pip install torch==2.2.0 torchvision==0.17.0

# Installa Hugging Face + BitsAndBytes
!pip install transformers==4.39.3 bitsandbytes==0.45.2 peft==0.10.0 accelerate==0.27.2 datasets sentencepiece rouge-score evaluate

import numpy, torch, bitsandbytes, accelerate
print("NumPy:", numpy.__version__)
print("Torch:", torch.__version__)
print("BitsAndBytes:", bitsandbytes.__version__)
print("Accelerate:", accelerate.__version__)

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    Seq2SeqTrainer, Seq2SeqTrainingArguments,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
import torch

# Model config
model_name = "google/flan-t5-large"
MAX_INPUT_LEN = 1024
MAX_TARGET_LEN = 1024

# Output dir
output_dir = "/content/drive/MyDrive/AIScientist/FlanT5/flan_t5large_lora_chunked"

dataset = load_dataset(
    "json",
    data_files={
        "train": "/content/drive/MyDrive/AIScientist/FlanT5/train_ready_trunc.json",
        "test": "/content/drive/MyDrive/AIScientist/FlanT5/test_ready_trunc.json"
    }
)

# 87.5% train, 12.5% val â†’ 70/10 split overall
split_dataset = dataset["train"].train_test_split(test_size=0.125, seed=42)
train_data = split_dataset["train"]
val_data = split_dataset["test"]
test_data = dataset["test"]

tokenizer = AutoTokenizer.from_pretrained(model_name)

def chunk_tokens(token_ids, max_len):
    return [token_ids[i:i+max_len] for i in range(0, len(token_ids), max_len)]

def preprocess_chunked_function(examples):
    chunked_inputs, chunked_labels, attention_masks = [], [], []

    for inp_text, tgt_text in zip(examples["input"], examples["output"]):
        # Tokenize without truncation
        inp_ids = tokenizer(inp_text, truncation=False)["input_ids"]
        tgt_ids = tokenizer(tgt_text, truncation=False)["input_ids"]

        # Split into chunks
        input_chunks = chunk_tokens(inp_ids, MAX_INPUT_LEN)
        target_chunks = chunk_tokens(tgt_ids, MAX_TARGET_LEN)

        # Pair chunks and append
        for ic, tc in zip(input_chunks, target_chunks):
            chunked_inputs.append(ic)
            chunked_labels.append(tc)
            attention_masks.append([1] * len(ic))

    # Debug: print number of chunks (first call only)
    if len(chunked_inputs) > 0:
        print(f"Created {len(chunked_inputs)} chunks for this batch.")

    return {
        "input_ids": chunked_inputs,
        "attention_mask": attention_masks,
        "labels": chunked_labels
    }

train_tokenized = train_data.map(
    preprocess_chunked_function,
    batched=True,
    remove_columns=train_data.column_names
)
val_tokenized = val_data.map(
    preprocess_chunked_function,
    batched=True,
    remove_columns=val_data.column_names
)
test_tokenized = test_data.map(
    preprocess_chunked_function,
    batched=True,
    remove_columns=test_data.column_names
)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=32,            # higher capacity
    lora_alpha=64,   # proportional
    lora_dropout=0.1,
    bias="none"
)

model = get_peft_model(model, peft_config)

model.config.use_cache = False
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

model.is_loaded_in_4bit = True
model.is_loaded_in_8bit = False

# Freeze base params, train LoRA only
for param in model.parameters():
    param.requires_grad = False
for name, param in model.named_parameters():
    if "lora" in name:
        param.requires_grad = True

training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=250,
    logging_steps=100,
    num_train_epochs=5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,
    learning_rate=2e-4,
    fp16=True,
    save_total_limit=2,
    predict_with_generate=True,
    generation_max_length=MAX_TARGET_LEN,
    warmup_steps=500,
    ddp_find_unused_parameters=False,
    report_to="none"
)

from evaluate import load
import numpy as np

rouge = load("rouge")
bleu = load("bleu")

def safe_decode(batch, tokenizer, vocab_size, max_len):
    """Clip token ids to valid vocab range and length."""
    batch = [np.clip(pred[:max_len], 0, vocab_size - 1) for pred in batch]
    return tokenizer.batch_decode(batch, skip_special_tokens=True)

def compute_metrics(eval_pred):
    preds, labels = eval_pred

    # Rimuovi -100 (usato per ignorare padding in loss)
    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)

    # Decodifica sicura
    decoded_preds = safe_decode(preds, tokenizer, tokenizer.vocab_size, MAX_TARGET_LEN)
    decoded_labels = safe_decode(labels, tokenizer, tokenizer.vocab_size, MAX_TARGET_LEN)

    # Calcolo metriche
    rouge_res = rouge.compute(predictions=decoded_preds, references=decoded_labels)
    bleu_res = bleu.compute(predictions=decoded_preds, references=[[ref] for ref in decoded_labels])

    return {
        "rouge1": round(rouge_res["rouge1"] * 100, 2),
        "rouge2": round(rouge_res["rouge2"] * 100, 2),
        "rougeL": round(rouge_res["rougeL"] * 100, 2),
        "bleu": round(bleu_res["bleu"] * 100, 2)
    }

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

import torch
torch.cuda.empty_cache()

trainer.train(resume_from_checkpoint=True)

model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)
tokenizer = AutoTokenizer.from_pretrained(output_dir)

# Move the model to the CUDA device
model.to("cuda")

def evaluate_model(model, tokenizer, dataset, max_target_len=1024):
    predictions = []
    references = []

    for example in dataset:
        inputs = tokenizer(example["input"], return_tensors="pt", truncation=False).to("cuda")

        # Generate in chunks
        output_ids = model.generate(**inputs, max_length=max_target_len)
        pred_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        predictions.append(pred_text)
        references.append(example["output"])

    rouge_res = rouge.compute(predictions=predictions, references=references)
    bleu_res = bleu.compute(predictions=predictions, references=[[ref] for ref in references])

    return rouge_res, bleu_res, predictions, references

torch.cuda.empty_cache()
rouge_res, bleu_res, preds, refs = evaluate_model(model, tokenizer, test_data)

print("=== ROUGE ===")
print(rouge_res)
print("\n=== BLEU ===")
print(bleu_res)

for i in range(5):
    print(f"\n--- Example {i+1} ---")
    print(f"Input:\n{test_data[i]['input']}")
    print(f"\nGenerated Output:\n{preds[i]}")
    print(f"\nReference Output:\n{refs[i]}")