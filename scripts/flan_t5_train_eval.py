# -*- coding: utf-8 -*-
"""T5_1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QJKCfYnYeLMLmtHJ0BucXe0x26dQAf4K
"""

# 1. Disinstalla tutto e pulisci la cache
!pip uninstall -y transformers accelerate torch torchvision torchaudio textstat
!pip cache purge

# 2. Installa versioni compatibili (CUDA 11.8)
!pip install --index-url https://download.pytorch.org/whl/cu118 \
          torch==2.7.0 torchvision==0.22.0 torchaudio==2.7.0

!pip install transformers==4.48.2 accelerate==1.9.0 \
          datasets sentencepiece evaluate rouge-score bert-score textstat

from google.colab import drive
drive.mount('/content/drive')

# Percorsi dataset
TRAIN_PATH = "/content/drive/MyDrive/AIScientist/FlanT5/train_ready_trunc.json"
TEST_PATH = "/content/drive/MyDrive/AIScientist/FlanT5/test_ready_trunc.json"

# Dove salvare il modello
OUTPUT_DIR = "/content/drive/MyDrive/AIScientist/FlanT5/flan_t5_step2_model"

"""# Prepara dataset Hugging Face"""

import json
from datasets import Dataset

# Carica JSON
with open(TRAIN_PATH, "r", encoding="utf-8") as f:
    train_data = json.load(f)
with open(TEST_PATH, "r", encoding="utf-8") as f:
    test_data = json.load(f)

# Converti in Dataset HuggingFace
train_dataset = Dataset.from_list([{"input_text": d["input"], "target_text": d["output"]} for d in train_data])
test_dataset = Dataset.from_list([{"input_text": d["input"], "target_text": d["output"]} for d in test_data])

print(train_dataset)
print(test_dataset)

"""# Tokenizzazione"""

from transformers import T5Tokenizer

MODEL_NAME = "google/flan-t5-base"
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)

MAX_SOURCE_LEN = 1024
MAX_TARGET_LEN = 1024

def preprocess_data(examples):
    model_inputs = tokenizer(
        examples["input_text"],
        max_length=MAX_SOURCE_LEN,
        truncation=True,
        padding="max_length"
    )
    labels = tokenizer(
        examples["target_text"],
        max_length=MAX_TARGET_LEN,
        truncation=True,
        padding="max_length"
    )["input_ids"]
    model_inputs["labels"] = labels
    return model_inputs

train_enc = train_dataset.map(preprocess_data, batched=True, remove_columns=["input_text", "target_text"])
test_enc = test_dataset.map(preprocess_data, batched=True, remove_columns=["input_text", "target_text"])

"""# Setup Trainer"""

from transformers import (
    T5ForConditionalGeneration,
    Seq2SeqTrainingArguments,
    Seq2SeqTrainer,
)

model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)

import os
os.environ["WANDB_DISABLED"] = "true"

training_args = Seq2SeqTrainingArguments(
    output_dir=OUTPUT_DIR,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=1000,
    logging_steps=100,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=5e-5,
    save_total_limit=2,
    predict_with_generate=True,
    generation_max_length=MAX_TARGET_LEN,  # opzionale ma consigliato
    fp16=True
)

"""# Avvia training"""

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_enc,
    eval_dataset=test_enc,
)

trainer.train()

"""# Salva modello finale"""

trainer.save_model(OUTPUT_DIR)
print(f"Model saved to {OUTPUT_DIR}")

# =======================
# 2. Import e setup
# =======================
import json
import evaluate
from transformers import T5Tokenizer, T5ForConditionalGeneration
from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction
from bert_score import score as bert_score
from textstat import flesch_kincaid_grade
from tqdm import tqdm

# Percorsi
MODEL_PATH = "/content/drive/MyDrive/AIScientist/FlanT5/flan_t5_step2_model"
TEST_FILE = "/content/drive/MyDrive/AIScientist/FlanT5/test_ready_trunc.json"

MAX_INPUT_TOKENS = 1024
MAX_OUTPUT_TOKENS = 1024
DEVICE = "cuda"  # usa "cpu" se non hai GPU

# =======================
# 3. Carica modello e tokenizer
# =======================
tokenizer = T5Tokenizer.from_pretrained("google/flan-t5-base")
model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)

# =======================
# 4. Carica dati di test
# =======================
with open(TEST_FILE, "r", encoding="utf-8") as f:
    test_data = json.load(f)

inputs = [item["input"] for item in test_data]
references = [item["output"] for item in test_data]

# =======================
# 5. Generazione predizioni
# =======================
predictions = []
for text in tqdm(inputs, desc="Generating predictions"):
    enc = tokenizer(text, return_tensors="pt", max_length=MAX_INPUT_TOKENS, truncation=True).to(DEVICE)
    output_ids = model.generate(**enc, max_length=MAX_OUTPUT_TOKENS)
    decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    predictions.append(decoded)

# =======================
# 6. Calcolo ROUGE
# =======================
rouge = evaluate.load("rouge")
rouge_results = rouge.compute(predictions=predictions, references=references)

# =======================
# 7. Calcolo BLEU
# =======================
tokenized_preds = [pred.split() for pred in predictions]
tokenized_refs = [[ref.split()] for ref in references]
smoothing = SmoothingFunction().method1
bleu_score_value = corpus_bleu(tokenized_refs, tokenized_preds, smoothing_function=smoothing)

# =======================
# 8. Calcolo BERTScore
# =======================
P, R, F1 = bert_score(predictions, references, lang="en", rescale_with_baseline=True)

# =======================
# 9. Calcolo FKGL (leggibilit√†)
# =======================
fkgl_scores = [flesch_kincaid_grade(pred) for pred in predictions]
avg_fkgl = sum(fkgl_scores) / len(fkgl_scores)

# =======================
# 10. Risultati finali
# =======================
print("\n===== Evaluation Results =====")
print(f"ROUGE-1: {rouge_results['rouge1']:.4f}")
print(f"ROUGE-2: {rouge_results['rouge2']:.4f}")
print(f"ROUGE-L: {rouge_results['rougeL']:.4f}")
print(f"BLEU: {bleu_score_value:.4f}")
print(f"BERTScore (F1): {F1.mean().item():.4f}")
print(f"Average FKGL: {avg_fkgl:.2f}")

# =======================
# 11. (Opzionale) Salva risultati su CSV
# =======================
import pandas as pd

df_results = pd.DataFrame({
    "input": inputs,
    "reference": references,
    "prediction": predictions,
    "fkgl": fkgl_scores
})
df_results.to_csv("/content/drive/MyDrive/AIScientist/FlanT5/evaluation_results.csv", index=False)
print("Detailed results saved to evaluation_results.csv")