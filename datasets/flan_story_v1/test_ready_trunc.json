[
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech",
    "output": "Generation of linguistic features (phonemes, duration) that are extracted from the text. Generation of features that represent corresponding extracted speech signals (cepstrum, frequency, linear spectrogram, Mel spectrogram).\nDeep neural networks are powerful tools that can approximate an arbitrarily complex function – bring some space of input data X into output space Y. In the context of our task, X and Y will be text and audio recordings with speech, respectively.\nWe chose the LJSpeech dataset as a training data set, which contains 13’100 audio tracks, 2-10 seconds each, and a text file corresponding to each record (aka lyrics). Based on the above-mentioned transformations, the sound is encoded into the Mel-spectrogram. The text is then tokenized and turned into a sequence of integers. Additionally we normalized all texts, transcribed all numbers into words (“5” —> “five”), and decoded abbreviations (“Mrs. Robinson” —> “Misses Robinson”). So, after preprocessing, we received sets of arrays of numerical sequences and Mel-spectrograms recorded as .npy files.\nIn order to match all the dimensions in the tensor batches at the learning stage, we’ve added paddings to short sequences. For sequences in text format, 0 was reserved for padding, and for spectrograms, frames, the values of which were slightly lower than the minimum value of the spectrograms defined by us. This is recommended for separating these paddings from noise and silence.\nThe decoder output from each step will be the predicted spectrogram. However, this is not the end. To improve the spectrogram quality, it is passed through the PostNet module, which is a stack of 5 one-dimensional convolutional layers with 512 filters in each and a filter size of 5. Each layer (except the last) is followed by batch-normalization and tangent activation. To return to the spectrogram dimension, we skip the output of the PostNet through a fully-connected layer with 80 neurons and add the obtained data to the initial result of the decoder. We then receive the Mel-spectrogram generated from the text.\nWhile working with seq2seq-models you will face the problem of different lengths of sequences in the batch. It is simply solved by adding paddings – reserved characters (in case of encoder input data), or frames with specific values (in case of a decoder). However, you should properly apply the error function to the predicted and real spectrograms. We used a mask in the error function for reading the errors excluding paddings."
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.",
    "output": "Conventional Text-to-Speech Approaches\nThe task of computer speech synthesis has long been the focus of scientists and engineers. However, classic approaches do not synthesize speech indistinguishable from that of a human. That is why, here, as in many other areas, deep learning has come to the rescue.\nThis method is based on the pre-recording of short audio sequences, which are then combined to create coherent speech. It turns out very clean and clear but is absolutely devoid of emotion and tone components, so it sounds unnatural. This happens because it is impossible to get an audio recording of all possible words uttered in all possible combinations of emotions and prosody.\nThe use of the concatenated TTS method is limited due to a large amount of data and enormous development time required. Therefore, a statistical parametric speech synthesis method, which explores the very nature of the data, was developed. It generates speech by combining certain parameters such as frequency, amplitude spectrum, etc.\nParametric synthesis consists of two stages:\nThese manually-configured parameters, along with linguistic features, are transmitted to the vocoder model, which performs many complex transformations to generate a sound wave. In this case, the vocoder evaluates speech parameters such as phase, prosody, tone, and others.\nIf we can approximate the parameters that define speech on each of its samples, then we can create a parametric model. Parametric synthesis requires less data and effort than concatenative systems.\nTheoretically, parametric synthesis is simple, but in practice, there are many artifacts that lead to the production of muffled speech with a buzzing sidetone, which sounds unnatural. This is happening because at each stage of the synthesis, we encode some features to achieve a realistic speech production. However, the selected data is based on our understanding of the speech, and in fact, human knowledge is not entirely comprehensive so the selected features will not necessarily be the best fit for any situation. Here is where deep learning takes the stage.\nF is a convolution operation,"
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM",
    "output": "The input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results.\nAnother crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data:\nAttention defines the part of the encoder data that should be used at the current decoder step.\nThen the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet.\nAt each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step. Probably this correspondence will be direct. For example, if we have an input text sequence of 200 characters and the corresponding spectrogram of 800 frames, then there will be 4 frames per character. However, speech generated on the basis of such a spectrogram would sound unnatural. We say some words faster, others slower, sometimes we make pauses, and sometimes we don’t. It is impossible to foresee all possible contexts. That is why attention is a key element of the entire system: it sets the correspondence between the decoder pitch and the information from the encoder. In this way we obtain the information necessary to generate a specific frame. The greater value of the attention weight, the more “attention” should be “paid” to a respective part of the encoder data when generating a spectrogram frame.\nAt the training stage, not only the visual assessment of the quality of spectrograms and attention is important, but also the generated audio. However, using WaveNet as a vocoder during the training phase is an impermissible luxury in terms of time. Therefore, we used the Griffin-Lim algorithm, which enables a partial restore of the signal after fast Fourier transforms. Why partial? The fact is, while converting a signal into spectrograms, we lose information about the phase. However, the quality of audio obtained in this way is enough to understand if we are moving in the right direction.\nIf the batches are formed on the basis of their length (not randomly), it would speed up the learning process and improve the quality of generated spectrograms.\nAfter a specific learning step, use a dynamic learning rate. This helps reduce the value of errors and increase the quality of the generated spectrograms.\nAfter creating a model and fail to train it in batches from the entire data set, try retraining it in one batch. If you succeed, you will get an alignment and recreated audio based on the generated spectrograms that contain speech (or at least its similarity). This will confirm that the overall architecture is correct, and only small details are missing."
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages",
    "output": "The input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results.\nAnother crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data:\nAttention defines the part of the encoder data that should be used at the current decoder step.\nThen the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet.\nAt each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step. Probably this correspondence will be direct. For example, if we have an input text sequence of 200 characters and the corresponding spectrogram of 800 frames, then there will be 4 frames per character. However, speech generated on the basis of such a spectrogram would sound unnatural. We say some words faster, others slower, sometimes we make pauses, and sometimes we don’t. It is impossible to foresee all possible contexts. That is why attention is a key element of the entire system: it sets the correspondence between the decoder pitch and the information from the encoder. In this way we obtain the information necessary to generate a specific frame. The greater value of the attention weight, the more “attention” should be “paid” to a respective part of the encoder data when generating a spectrogram frame.\nAt the training stage, not only the visual assessment of the quality of spectrograms and attention is important, but also the generated audio. However, using WaveNet as a vocoder during the training phase is an impermissible luxury in terms of time. Therefore, we used the Griffin-Lim algorithm, which enables a partial restore of the signal after fast Fourier transforms. Why partial? The fact is, while converting a signal into spectrograms, we lose information about the phase. However, the quality of audio obtained in this way is enough to understand if we are moving in the right direction.\nIf the batches are formed on the basis of their length (not randomly), it would speed up the learning process and improve the quality of generated spectrograms.\nAfter a specific learning step, use a dynamic learning rate. This helps reduce the value of errors and increase the quality of the generated spectrograms.\nAfter creating a model and fail to train it in batches from the entire data set, try retraining it in one batch. If you succeed, you will get an alignment and recreated audio based on the generated spectrograms that contain speech (or at least its similarity). This will confirm that the overall architecture is correct, and only small details are missing."
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic",
    "output": "The input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results.\nAnother crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data:\nAttention defines the part of the encoder data that should be used at the current decoder step.\nThen the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet.\nAt each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step. Probably this correspondence will be direct. For example, if we have an input text sequence of 200 characters and the corresponding spectrogram of 800 frames, then there will be 4 frames per character. However, speech generated on the basis of such a spectrogram would sound unnatural. We say some words faster, others slower, sometimes we make pauses, and sometimes we don’t. It is impossible to foresee all possible contexts. That is why attention is a key element of the entire system: it sets the correspondence between the decoder pitch and the information from the encoder. In this way we obtain the information necessary to generate a specific frame. The greater value of the attention weight, the more “attention” should be “paid” to a respective part of the encoder data when generating a spectrogram frame.\nAt the training stage, not only the visual assessment of the quality of spectrograms and attention is important, but also the generated audio. However, using WaveNet as a vocoder during the training phase is an impermissible luxury in terms of time. Therefore, we used the Griffin-Lim algorithm, which enables a partial restore of the signal after fast Fourier transforms. Why partial? The fact is, while converting a signal into spectrograms, we lose information about the phase. However, the quality of audio obtained in this way is enough to understand if we are moving in the right direction.\nIf the batches are formed on the basis of their length (not randomly), it would speed up the learning process and improve the quality of generated spectrograms.\nAfter a specific learning step, use a dynamic learning rate. This helps reduce the value of errors and increase the quality of the generated spectrograms.\nAfter creating a model and fail to train it in batches from the entire data set, try retraining it in one batch. If you succeed, you will get an alignment and recreated audio based on the generated spectrograms that contain speech (or at least its similarity). This will confirm that the overall architecture is correct, and only small details are missing."
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.",
    "output": "The input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results.\nAnother crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data:\nAttention defines the part of the encoder data that should be used at the current decoder step.\nThen the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet.\nAt each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step. Probably this correspondence will be direct. For example, if we have an input text sequence of 200 characters and the corresponding spectrogram of 800 frames, then there will be 4 frames per character. However, speech generated on the basis of such a spectrogram would sound unnatural. We say some words faster, others slower, sometimes we make pauses, and sometimes we don’t. It is impossible to foresee all possible contexts. That is why attention is a key element of the entire system: it sets the correspondence between the decoder pitch and the information from the encoder. In this way we obtain the information necessary to generate a specific frame. The greater value of the attention weight, the more “attention” should be “paid” to a respective part of the encoder data when generating a spectrogram frame.\nAt the training stage, not only the visual assessment of the quality of spectrograms and attention is important, but also the generated audio. However, using WaveNet as a vocoder during the training phase is an impermissible luxury in terms of time. Therefore, we used the Griffin-Lim algorithm, which enables a partial restore of the signal after fast Fourier transforms. Why partial? The fact is, while converting a signal into spectrograms, we lose information about the phase. However, the quality of audio obtained in this way is enough to understand if we are moving in the right direction.\nIf the batches are formed on the basis of their length (not randomly), it would speed up the learning process and improve the quality of generated spectrograms.\nAfter a specific learning step, use a dynamic learning rate. This helps reduce the value of errors and increase the quality of the generated spectrograms.\nAfter creating a model and fail to train it in batches from the entire data set, try retraining it in one batch. If you succeed, you will get an alignment and recreated audio based on the generated spectrograms that contain speech (or at least its similarity). This will confirm that the overall architecture is correct, and only small details are missing."
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side",
    "output": "Applying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, i-1 — previous attention weights, hj — j-th hidden encoder state, i-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5. A distinctive feature of this module is that the dropout is used in it not only on the stage of learning the model but also at the output stage. Why did we mention that the context vector might already be different? One possible approach is the recalculation of the context vector once the latent state of the LSTM network is obtained. However, in our experiments, this approach was not justified. In addition to the projection onto the 80-neural fully-connected layer, the concatenation of the output data of LSTM-layers with the context vector is directed into a fully-connected layer with one neuron, followed by sigmoid activation – this is known as the stop token prediction layer. It predicts the probability that the frame created at the step of the decoder is the final one. This layer is designed to generate a spectrogram not of a fixed length but of an arbitrary one at the stage of model output. At the output stage this element determines the number of decoder steps. It can be considered as a binary classifier. All convolutional modules are regularized with dropout layers with a rate of 0.50, and recurrent layers with a newer zoneout method with a rate of 0.10. It is quite simple: instead of submitting the hidden state and cell state obtained at the current step to the next step of an LSTM-network, we replaced part of the data with the values from the previous step. This is done both at the training and output stages. At the same time, only the hidden state is exposed to the zoneout method at each step, which is transmitted to the next LSTM step, while the output of the LSTM cell at the current step remains unchanged. We chose PyTorch as a deep learning framework. Although at the time of network implementation, it was in a state of pre-release. PyTorch is a very powerful tool for building and training artificial neural networks. We had experience using TensorFlow and Keras frameworks, however due to the need to implement non-standard custom structures, we’ve chosen PyTorch. However, it doesn’t mean that one framework is better than the other; the choice of framework depends on various factors. The network was learning using a backpropagation algorithm. ADAM was used as an optimizer. The model has been learning on a single GPU GeForce 1080 Ti with 11 GB of RAM. While working with such a huge model, it is important to monitor how the learning process goes. Here, TensorBoard became a convenient tool. We tracked the value of the errors in both training and validation iterations. In addition, we displayed target spectrograms, predicted spectrograms on the training and validation stages, and performed an alignment, which is additive accumulated attention weights from all the training stages. Use a version control system that clearly describes all changes. While searching for optimal architecture, changes occur constantly. Make sure you are"
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel",
    "output": "Applying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, i-1 — previous attention weights, hj — j-th hidden encoder state, i-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5. A distinctive feature of this module is that the dropout is used in it not only on the stage of learning the model but also at the output stage. Why did we mention that the context vector might already be different? One possible approach is the recalculation of the context vector once the latent state of the LSTM network is obtained. However, in our experiments, this approach was not justified. In addition to the projection onto the 80-neural fully-connected layer, the concatenation of the output data of LSTM-layers with the context vector is directed into a fully-connected layer with one neuron, followed by sigmoid activation – this is known as the stop token prediction layer. It predicts the probability that the frame created at the step of the decoder is the final one. This layer is designed to generate a spectrogram not of a fixed length but of an arbitrary one at the stage of model output. At the output stage this element determines the number of decoder steps. It can be considered as a binary classifier. All convolutional modules are regularized with dropout layers with a rate of 0.50, and recurrent layers with a newer zoneout method with a rate of 0.10. It is quite simple: instead of submitting the hidden state and cell state obtained at the current step to the next step of an LSTM-network, we replaced part of the data with the values from the previous step. This is done both at the training and output stages. At the same time, only the hidden state is exposed to the zoneout method at each step, which is transmitted to the next LSTM step, while the output of the LSTM cell at the current step remains unchanged. We chose PyTorch as a deep learning framework. Although at the time of network implementation, it was in a state of pre-release. PyTorch is a very powerful tool for building and training artificial neural networks. We had experience using TensorFlow and Keras frameworks, however due to the need to implement non-standard custom structures, we’ve chosen PyTorch. However, it doesn’t mean that one framework is better than the other; the choice of framework depends on various factors. The network was learning using a backpropagation algorithm. ADAM was used as an optimizer. The model has been learning on a single GPU GeForce 1080 Ti with 11 GB of RAM. While working with such a huge model, it is important to monitor how the learning process goes. Here, TensorBoard became a convenient tool. We tracked the value of the errors in both training and validation iterations. In addition, we displayed target spectrograms, predicted spectrograms on the training and validation stages, and performed an alignment, which is additive accumulated attention weights from all the training stages. Use a version control system that clearly describes all changes. While searching for optimal architecture, changes occur constantly. Make sure you are"
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better",
    "output": "Applying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, i-1 — previous attention weights, hj — j-th hidden encoder state, i-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5. A distinctive feature of this module is that the dropout is used in it not only on the stage of learning the model but also at the output stage. Why did we mention that the context vector might already be different? One possible approach is the recalculation of the context vector once the latent state of the LSTM network is obtained. However, in our experiments, this approach was not justified. In addition to the projection onto the 80-neural fully-connected layer, the concatenation of the output data of LSTM-layers with the context vector is directed into a fully-connected layer with one neuron, followed by sigmoid activation – this is known as the stop token prediction layer. It predicts the probability that the frame created at the step of the decoder is the final one. This layer is designed to generate a spectrogram not of a fixed length but of an arbitrary one at the stage of model output. At the output stage this element determines the number of decoder steps. It can be considered as a binary classifier. All convolutional modules are regularized with dropout layers with a rate of 0.50, and recurrent layers with a newer zoneout method with a rate of 0.10. It is quite simple: instead of submitting the hidden state and cell state obtained at the current step to the next step of an LSTM-network, we replaced part of the data with the values from the previous step. This is done both at the training and output stages. At the same time, only the hidden state is exposed to the zoneout method at each step, which is transmitted to the next LSTM step, while the output of the LSTM cell at the current step remains unchanged. We chose PyTorch as a deep learning framework. Although at the time of network implementation, it was in a state of pre-release. PyTorch is a very powerful tool for building and training artificial neural networks. We had experience using TensorFlow and Keras frameworks, however due to the need to implement non-standard custom structures, we’ve chosen PyTorch. However, it doesn’t mean that one framework is better than the other; the choice of framework depends on various factors. The network was learning using a backpropagation algorithm. ADAM was used as an optimizer. The model has been learning on a single GPU GeForce 1080 Ti with 11 GB of RAM. While working with such a huge model, it is important to monitor how the learning process goes. Here, TensorBoard became a convenient tool. We tracked the value of the errors in both training and validation iterations. In addition, we displayed target spectrograms, predicted spectrograms on the training and validation stages, and performed an alignment, which is additive accumulated attention weights from all the training stages. Use a version control system that clearly describes all changes. While searching for optimal architecture, changes occur constantly. Make sure you are"
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.",
    "output": "Applying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, i-1 — previous attention weights, hj — j-th hidden encoder state, i-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5. A distinctive feature of this module is that the dropout is used in it not only on the stage of learning the model but also at the output stage. Why did we mention that the context vector might already be different? One possible approach is the recalculation of the context vector once the latent state of the LSTM network is obtained. However, in our experiments, this approach was not justified. In addition to the projection onto the 80-neural fully-connected layer, the concatenation of the output data of LSTM-layers with the context vector is directed into a fully-connected layer with one neuron, followed by sigmoid activation – this is known as the stop token prediction layer. It predicts the probability that the frame created at the step of the decoder is the final one. This layer is designed to generate a spectrogram not of a fixed length but of an arbitrary one at the stage of model output. At the output stage this element determines the number of decoder steps. It can be considered as a binary classifier. All convolutional modules are regularized with dropout layers with a rate of 0.50, and recurrent layers with a newer zoneout method with a rate of 0.10. It is quite simple: instead of submitting the hidden state and cell state obtained at the current step to the next step of an LSTM-network, we replaced part of the data with the values from the previous step. This is done both at the training and output stages. At the same time, only the hidden state is exposed to the zoneout method at each step, which is transmitted to the next LSTM step, while the output of the LSTM cell at the current step remains unchanged. We chose PyTorch as a deep learning framework. Although at the time of network implementation, it was in a state of pre-release. PyTorch is a very powerful tool for building and training artificial neural networks. We had experience using TensorFlow and Keras frameworks, however due to the need to implement non-standard custom structures, we’ve chosen PyTorch. However, it doesn’t mean that one framework is better than the other; the choice of framework depends on various factors. The network was learning using a backpropagation algorithm. ADAM was used as an optimizer. The model has been learning on a single GPU GeForce 1080 Ti with 11 GB of RAM. While working with such a huge model, it is important to monitor how the learning process goes. Here, TensorBoard became a convenient tool. We tracked the value of the errors in both training and validation iterations. In addition, we displayed target spectrograms, predicted spectrograms on the training and validation stages, and performed an alignment, which is additive accumulated attention weights from all the training stages. Use a version control system that clearly describes all changes. While searching for optimal architecture, changes occur constantly. Make sure you are"
  },
  {
    "id_paper": "23",
    "id_story": "23-05",
    "id_chunk": "23-05_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.",
    "output": "Now we have data representing text and audio that are suitable for processing by a neural network. Let’s consider the architecture of the feature prediction net, which we will call Tacotron 2, named after the central element of the entire synthesis system.\nTacotron 2 Architecture Explained\nTacotron 2 is not one network, but two: Feature prediction net and NN-vocoder WaveNet. Feature prediction net is considered as the main network, while the WaveNet vocoder plays the role of a supplemental system.\nTacotron2 has sequence to sequence architecture. It consists of an encoder, which creates internal representation of the input signal (symbolic tokens), and a decoder, which turns this representation into a Mel-spectrogram. A very important element of the network is the PostNet, designed to improve the spectrogram generated by the decoder.\nSome of the modules use information from the previous step of the decoder. But on the first step, the information will be zero-value tensors, which is a common approach in creating recurrent structures.\nThe Workflow with Tacontron 2\nThe output of PreNet in concatenation with the context vector, obtained as a result of the attention mechanism, is directed to the entrance of the unidirectional two-layer LSTM network with 1024 neurons in each layer.\nVisualization of Tacotron 2 Processing\nHowever, after all the modules started working properly, something like this is displayed:\nWe have accumulated some thoughts on the development process with text to speech synthesis so far. Some of them are common, others are more specific and unique. Here are our tips for those who consider Tacotron 2 as a text-to-speech solution for their projects.\nGeneral Tips on the Workflow with Tacontron 2:\nIn Tacontron 2 and similar architectures, you should follow the principles of encapsulation: 1 class equals 1 Python-module. This approach is rarely found in ML tasks but helps to structure the code, speed up debugging, and the development process.\nBuilding and Training of Text-to-Speech Models Based on Tacontron 2 Architecture:\nThank you for reading until the very end. As a bonus, here are all examples of speech generation from text using Tacotron 2 architecture that is not contained in the training set:"
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech",
    "output": "WaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.",
    "output": "The new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."
  },
  {
    "id_paper": "23",
    "id_story": "23-02",
    "id_chunk": "23-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.",
    "output": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech",
    "output": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.",
    "output": "Text to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM",
    "output": "A Brief Summary\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages",
    "output": "A Brief Summary\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic",
    "output": "A Brief Summary\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.",
    "output": "A Brief Summary\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side",
    "output": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel",
    "output": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better",
    "output": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.",
    "output": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."
  },
  {
    "id_paper": "23",
    "id_story": "23-03",
    "id_chunk": "23-03_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.",
    "output": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\nTacotron 2 is a neural network architecture for text to speech that uses a\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.",
    "output": "While our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."
  },
  {
    "id_paper": "23",
    "id_story": "23-01",
    "id_chunk": "23-01_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.",
    "output": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech",
    "output": "First step transforms the text into time-aligned features, such as mel spectrogram, or F0 frequencies and other linguistic features;\n“William Shakespeare was an English poet, playwright and actor, widely regarded as the greatest writer in the English language and the world’s greatest dramatist. He is often called England’s national poet and the ‘Bard of Avon’.”\nAfter following the steps in the Jupyter notebook, you will be able to provide English text to the model and it will generate an audio output file. All of the scripts to reproduce the results have been published on GitHub in our NVIDIA Deep Learning Examples repository, which contains several high-performance training recipes that use Tensor Cores. Additionally, we developed a Jupyter notebook for users to create their own container image, then download the dataset and reproduce the training and inference results step-by-step.\nTacotron 22 is a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms, as shown in Figure 1.\nThe network is composed of an encoder (blue) and a decoder (orange) with attention. The encoder converts a character sequence into a hidden feature representation, which serves as input to the decoder to predict a spectrogram. Input text (yellow) is presented using a learnt 512-dimensional character embedding, which are passed through a stack of three convolutional layers (each containing 512 filters with shape 5 × 1), followed by batch normalization and ReLU activations. The encoder output is passed to an attention network (gray) which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step.\nWaveGlow 1 is a flow-based network capable of generating high-quality speech from mel spectrograms. WaveGlow combines insights from Glow5 and WaveNet6 in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained with only a single cost function: making the training procedure simple and stable. Our current model synthesizes samples at 55 * 22,050 = 1,212,750, which is 55 times faster than “real-time” at 22,050 samples per second sampling rate. The Mean Opinion Score (MOS) show that it delivers audio quality as good as the best publicly available WaveNet implementation trained on the same dataset.\nRun Jupyter Notebook Step-by-Step\nYou can customize the content of the text file, depending on its length, you may need to increase the –max-decoder-steps option to 2,000. The Tacotron 2 model was trained on the LJ Speech dataset with audio samples no longer than 10 seconds, which corresponds to about 860 mel spectrograms. Therefore the inference is expected to work well with generating audio samples of similar length. We set the mel spectrogram length limit to 2,000 (about 23 seconds), since in practice it still produces the correct voice. If needed, users can split longer phrases into multiple sentences and synthesize them separately."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.",
    "output": "Converting text into high quality, natural-sounding speech in real time has been a challenging conversational AI task for decades. State-of-the-art speech synthesis models are based on parametric neural networks1. Text-to-speech (TTS) synthesis is typically done in two steps.\nInput:\nOutput:\nWaveGlow model2 instead of WaveNet to synthesize waveforms.\nAfter reading this blog, try the Jupyter notebook to get hands-on experience generating audio from text in real-time."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM",
    "output": "Here is an example of what you can achieve using this model:\nThe decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer.\nWaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\nThe Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages",
    "output": "Here is an example of what you can achieve using this model:\nThe decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer.\nWaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\nThe Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic",
    "output": "Here is an example of what you can achieve using this model:\nThe decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer.\nWaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\nThe Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.",
    "output": "Here is an example of what you can achieve using this model:\nThe decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer.\nWaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\nThe Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side",
    "output": "Dropout instead of Zoneout to regularize the LSTM layers;\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures.\nUsing mixed precision training previously required two steps:\nPorting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values.\nMixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed.\nMixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook.\nTraining Performance\nTable 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch.\nNumber of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86\nNumber of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22\nTable 2: Training performance results for WaveGlow model\nAs shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size. Speech quality depends on model size and training set size; using Tensor Cores with automatic mixed precision makes it possible to train higher quality models in the same amount of time.\nConsidering model size and amount of training needed for high quality, GPUs offer a most suitable hardware architecture with an optimal combination of throughput, bandwidth, scalability, and ease of use.\nInference Performance\nTraining\nThe training loss is averaged over an entire training epoch, whereas the validation loss is averaged over the validation dataset. Performance is reported in total input tokens per second for the Tacotron 2 model, and in total output samples per second for the WaveGlow model. Both measures are recorded as train_iter_items/sec (after each iteration) and train_epoch_items/sec (averaged over epoch) in the output log. The result is averaged over an entire training epoch and summed over all GPUs that were included in the training.\nBy default, our training scripts will launch mixed precision training with Tensor cCores. You can change this behavior by removing the –fp16-run flag."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel",
    "output": "Dropout instead of Zoneout to regularize the LSTM layers;\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures.\nUsing mixed precision training previously required two steps:\nPorting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values.\nMixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed.\nMixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook.\nTraining Performance\nTable 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch.\nNumber of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86\nNumber of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22\nTable 2: Training performance results for WaveGlow model\nAs shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size. Speech quality depends on model size and training set size; using Tensor Cores with automatic mixed precision makes it possible to train higher quality models in the same amount of time.\nConsidering model size and amount of training needed for high quality, GPUs offer a most suitable hardware architecture with an optimal combination of throughput, bandwidth, scalability, and ease of use.\nInference Performance\nTraining\nThe training loss is averaged over an entire training epoch, whereas the validation loss is averaged over the validation dataset. Performance is reported in total input tokens per second for the Tacotron 2 model, and in total output samples per second for the WaveGlow model. Both measures are recorded as train_iter_items/sec (after each iteration) and train_epoch_items/sec (averaged over epoch) in the output log. The result is averaged over an entire training epoch and summed over all GPUs that were included in the training.\nBy default, our training scripts will launch mixed precision training with Tensor cCores. You can change this behavior by removing the –fp16-run flag."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better",
    "output": "Dropout instead of Zoneout to regularize the LSTM layers;\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures.\nUsing mixed precision training previously required two steps:\nPorting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values.\nMixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed.\nMixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook.\nTraining Performance\nTable 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch.\nNumber of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86\nNumber of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22\nTable 2: Training performance results for WaveGlow model\nAs shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size. Speech quality depends on model size and training set size; using Tensor Cores with automatic mixed precision makes it possible to train higher quality models in the same amount of time.\nConsidering model size and amount of training needed for high quality, GPUs offer a most suitable hardware architecture with an optimal combination of throughput, bandwidth, scalability, and ease of use.\nInference Performance\nTraining\nThe training loss is averaged over an entire training epoch, whereas the validation loss is averaged over the validation dataset. Performance is reported in total input tokens per second for the Tacotron 2 model, and in total output samples per second for the WaveGlow model. Both measures are recorded as train_iter_items/sec (after each iteration) and train_epoch_items/sec (averaged over epoch) in the output log. The result is averaged over an entire training epoch and summed over all GPUs that were included in the training.\nBy default, our training scripts will launch mixed precision training with Tensor cCores. You can change this behavior by removing the –fp16-run flag."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.",
    "output": "Dropout instead of Zoneout to regularize the LSTM layers;\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures.\nUsing mixed precision training previously required two steps:\nPorting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values.\nMixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed.\nMixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook.\nTraining Performance\nTable 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch.\nNumber of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86\nNumber of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22\nTable 2: Training performance results for WaveGlow model\nAs shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size. Speech quality depends on model size and training set size; using Tensor Cores with automatic mixed precision makes it possible to train higher quality models in the same amount of time.\nConsidering model size and amount of training needed for high quality, GPUs offer a most suitable hardware architecture with an optimal combination of throughput, bandwidth, scalability, and ease of use.\nInference Performance\nTraining\nThe training loss is averaged over an entire training epoch, whereas the validation loss is averaged over the validation dataset. Performance is reported in total input tokens per second for the Tacotron 2 model, and in total output samples per second for the WaveGlow model. Both measures are recorded as train_iter_items/sec (after each iteration) and train_epoch_items/sec (averaged over epoch) in the output log. The result is averaged over an entire training epoch and summed over all GPUs that were included in the training.\nBy default, our training scripts will launch mixed precision training with Tensor cCores. You can change this behavior by removing the –fp16-run flag."
  },
  {
    "id_paper": "23",
    "id_story": "23-04",
    "id_chunk": "23-04_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.",
    "output": "The optimized Tacotron2 model2 and the new WaveGlow model1 take advantage of Tensor Cores on NVIDIA Volta and Turing GPUs to convert text into high quality natural sounding speech in real-time. The generated audio has a clear human-like voice without background noise.\nOur TTS system is a combination of two neural network models:\nThe Tacotron 2 and WaveGlow model form a TTS system that enables users to synthesize natural sounding speech from raw transcripts without any additional prosody information.\nTacotron 2 Model\nOur implementation of the Tacotron 2 model differs from the model described in1, we use:\nTable 1: Training performance results for modified Tacotron 2 model\nTable 3 and Table 4 show inference statistics for the Tacotron2 and WaveGlow text-to-speech system, gathered from 1,000 inference runs, on 1-V100 and 1-T4 GPU, respectively. Latency is measured from the start of Tacotron2 inference to the end of WaveGlow inference. The tables include average latency, standard deviation, and latency confidence intervals (percent values). Throughput is measured as the number of generated audio samples per second. RTF is the real-time factor which tells how many seconds of speech are generated in 1 second of wall time.\nTable 3: Inference statistics for Tacotron2 and WaveGlow system on 1-V100 GPU\nTable 4: Inference statistics for Tacotron2 and WaveGlow system on 1-T4 GPU\nFollow the scripts on GitHub or run the Jupyter notebook step-by-step, to train Tacotron 2 and WaveGlow v1.5 models. In the Jupyter notebook, we provided scripts that are fully automated to download and pre-process the LJ Speech dataset; After the data preparation step, use the provided Dockerfile to build the modified Tacotron 2 and WaveGlow container, and start a detached session in the container; To train our model using AMP with Tensor Cores or using FP32, perform the training step using the default parameters of the Tacrotron 2 and WaveGlow models using a single GPU or multiple GPUs.\nAfter training Tacotron 2 and WaveGlow models, or downloaded the pre-trained checkpoints for the respective models, you can perform inference which takes text as input, and produces an audio file."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: Abstract We propose a cross-lingual neural codec language model, VALL-E X, for crosslingual speech synthesis. Specifically, we extend VALL-E [Wang et al., 2023] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-tospeech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at https://aka.ms/vallex. VALL-E X (Cross-Lingual VALL-E) Cross-Lingual Neural Codec Language Modeling Multilingual G2P Multilingual G2P Audio Codec Encoder Source Text Target Text Source Speech Figure 1: The overall framework of VALL-E X, which can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training, and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation. Personalized Target Speech Audio Codec Decoder Language ID Source phoneme sequences Target phoneme sequences Source acoustic tokens Target acoustic tokens 1",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: Introduction Recent years have witnessed significant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity [Li et al., 2019, Ren et al., 2019, Tan et al., 2022]. However, these models can only generate high-quality speech for a specific speaker in a specific language. Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. Previous methods to tackle these challenges typically augment end-to-end TTS models with specific subnets for speaker and language control [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2020, Ellinas et al., 2022, Cai et al., 2023]. For example, based on the multi-speaker TTS model, Nachmani and Wolf [2019] introduce multiple encoders for each language and additional loss to keep the speaker’s identity. Zhang et al. [2020] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities. Yang and He [2020] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively. Yang and He [2022] further propose a multi-task learning method with additional tasks of speaker similarity and language identification. Moreover, Cai et al. [2023] investigates cross-lingual multi-speaker text-to-speech synthesis with sufficient or limited bilingual speech training data. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [Zhang et al., 2019, Lee et al., 2022]. Table 1: A comparison between VALL-E X and previous cross-lingual TTS systems. Previous Systems VALL-E X Intermediate representation Mel spectrogram Audio codec codes Training data 13K hours Speech accent Foreign Native Speaker similarity Relative low High In-context learning Zero-shot cross-lingual TTS In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, VALL-E X, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis. Based on the knowledge learned from large-scale multi-lingual speech data, VALL-E X is able to transfer the speech characteristics, including the speaker’s voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems. More specifically, we first obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data. Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an offline neural codec encoder. Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model. As illustrated in Figure 1, after training, VALL-E X can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language. The generated acoustic token sequence is decompressed to the target speech waveform by an offline audio codec",
    "output": "Model versions\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\ndecoder. VALL-E X is trained on two large-scale multi-speaker datasets1, LibriLight [Kahn et al., 2020] and WenetSpeech [Zhang et al., 2022a], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively. The combination of LibriLight and Wenet",
    "output": "Model versions\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which significantly improves the coverage of different speakers and enhances VALL-E X’s generalization capacity. The comparison between VALL-E X and the previous cross-lingual TTS systems are listed in Table 1. We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot crosslingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST). For cross-lingual text-to-speech synthesis, the proposed VALL-E X is evaluated with LibriSpeech [Panayotov et al., 2015] and EMIME [Wester, 2010] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers. For zero-shot speech-to-speech translation, EMIME [Wester, 2010] dataset is used for the evaluation of VALL-E X on bidirectional ChineseEnglish translation tasks, and it contains bilingual audio recordings by the same speakers. We evaluate the proposed VALL-E X framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS). Specifically, due to the strong in-context learning capability, VALL-E X achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker. By training on large-scale speech-transcription data, the proposed VALL-E X significantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness. Furthermore, the human evaluation shows that our VALL-E X outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST). Our contributions can be summarized as follows: • We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data. VALL-E X is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts. • The multi-lingual in-context learning framework enables VALL-E X to generate crosslingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language. • Based on the learned cross-lingual speech modeling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks. • We apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation. We encourage readers to listen to the audio samples on our demo page: https://aka.ms/vallex. 2",
    "output": "Model versions\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 3.2 Model Framework Inspired by VALL-E, the cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM MAR is only used to predict the acoustic tokens from the first quantizer of EnCodec model. Formally, based on paired speech-transcription data in any language, let S denote the transcribed phoneme sequence, and A:,1 ai,1|i = 1, . . . , N denotes the first-layer acoustic tokens extracted from the speech X. The decoder MAR, modeling the concatenated sequence S, A:,1, is trained to predict A:,1 autoregressively. It is optimized by maximizing the log-likelihood, Quantization & Phonemization: Quantized tokens Layer L 12 504 31 8 743 ... A:,2:L ...... Layer 2 325 71 435 90 7 ... A:,1 731 284 78 32 669 ... Layer 1 audio codec encoder (Quantization) Multi-lingual speech-transcription pairs (phonemization) G2P tool Phoneme tokens HH AH L OW ... D S N Y Multi-lingual Non-Autoregressive Codec LM Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM MNAR is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer. It is prompted by the phoneme sequence of the current sentence (S) and the acoustic token sequence of another sentence with the same speaker (  A). Here  A is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer l [2, 8], the embeddings of l 1 layers’ acoustic tokens (A:,1:l1) are summed up layerwise as input. The learning objective for the l-layer acoustic tokens A:,l can be calculated as 8 X l=2 log pNAR A:,l | D S,  A:,1:8, A:,1:l1 E ; MNAR (2) LMNAR = where means the sequence concatenation. pNAR(.) computes the pointwise probabilities of A:,l. 3.3 Multi-lingual Training In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-tospeech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (Ss, As) and (St, At) to train our multi-lingual codec LMs MAR and MNAR, where s and t represent two different (source and target) languages. Language ID Module Following multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in VALL-E X. On the one hand, without language ID",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n, VALL-E X may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM MAR is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens. 3.4 Cross-Lingual Inference After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we first concatenate source phonemes Ss and target phonemes St as prompts, and take the first-layer source acoustic tokens As :,1 as the decoding prefix, condition on which the multi-lingual autoregressive codec LM MAR generates the first-layer target acoustic tokens At :,1, at i,1 pAR  at i,1 | Ss, St, As :,1, At i,1 ; MAR , i = 1, . . . , (3) where means probability-based sampling. The sampling is stopped until the end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the final generated speech. After obtaining the first-layer target acoustic tokens At :,1 from MAR, multi-lingual non-autoregressive codec LM MNAR is used to predict the rest layers of acoustic tokens n At :,l | l = 2, . . . , 8 o by greedy search, i.e., choosing the tokens with maximum probabilities, At :,l = argmax At :,l pNAR  At :,l | St, As :,1:8, At :,1:l1 ; MNAR , l = 2, . . . , 8. (4) Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens At :,1:8. tl= 2, ... , 8 VALL-E X Inference: A:,l Multi-lingual Non-Autoregressive Codec LM t s A:,1:l1 St A:,1:8 t A:,1 Multi-lingual Autoregressive Codec LM B t aN,1 t a2,1 t ... s a1,1 bos> Ss St A:,1 Figure 3: Inference illustration of the cross-lingual neural codec language model VALL-E X, with two-stage decoding strategies. VALL-E X can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks. 4 VALL-E X Application VALL-E X can be applied to various cross-lingual speech generation tasks. In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in Figure 3. 4.1 Zero-Shot Cross-Lingual TTS The proposed VALL-E X is naturally suitable for zero-shot cross-lingual TTS tasks. Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker’s voice. Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability. Thanks to the in-context learning capability of large language models, VALL-E X surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis. More specifically, given the source speech, source transcript, and target text, we first convert source speech into source acoustic token As using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes Ss and target phonemes St using G2P tool. More specifically, as introduced in",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nSection 3.4, we let St be the phonemes extracted from the target text, Ss and As be the phonemes and acoustic tokens extracted from the source speech. Then VALL-E X generates the full-layer target acoustic tokens, which are finally decompressed into the target speech by EnCodec decoder. 4.2 Zero-Shot Speech-to-Speech Translation We can also apply our VALL-E X to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences. Speech Recognition & Translation Model We leverage the improved SpeechUT [Zhang et al., 2022c] as our speech recognition & translation model, which is a unified-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss) and MT corpus (source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more As Application: Zero-shot cross-lingual TTS Ss St Codec Encoder G2P Tool G2P Tool Source text Target text Source speech Application: Zero-shot S2ST Ss St As E Codec Encoder Semantic Semantic Encoder Decoder Speech Encoder Source Speech pre-training details about this model. After pre-training, the model is fine-tuned with (X s, Ss, St) triplet data derived from the ST corpus. Specifically, we perform multi-task learning with the CTC [Graves et al., 2006] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes. Inference Figure 3 shows the inference process of speech-to-speech translation. Given a source speech X s, the speech recognition & translation model first generates the source phonemes Ss from the semantic encoder and the target phonemes St from the semantic decoder. Besides, we use the EnCodec encoder to compress X s into source acoustic tokens As. Then, we concatenate Ss, St, and As, as the input of VALL-E X, to produce the acoustic token sequence for the target speech, as introduced in Section 3.4. The generated acoustic tokens are converted to the final target speech with the decoder of EnCodec. 4.3 Evaluation The proposed model is verified using various evaluation criteria, including speaker similarity (ASVScore), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation. Specifically, we measure speaker similarity between synthesized target speech and groudtruth target speech or source speech as an automatic speaker verification (ASV) task, where a WavLM [Chen et al., 2022] based ASV model is used to calculate the score. To verify the quality of generated speech, we first utilize the ASR system from the released HuBERT-Large model [Hsu et al., 2021] to recognize it into text. For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text. For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text. Finally,",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nto better verify our proposed VALL-E X systems, we adopt the open-source NISQA3 [Mittag and Möller, 2021] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS). 5",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: Experiments We evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5, OpenSubtitles20186 and WMT20207, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama8, and news domains, respectively. The EnglishChinese ST data is from GigaST [Ye et al., 2022], which is created by translating the transcripts in GigaSpeech [Chen et al., 2021] using a strong machine translation system. Similarly, we create the ChineseEnglish ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above. 3https://github.com/gabrielmittag/NISQA 4https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech 5https://challenger.ai/competition/translation 6https://opus.nlpl.eu/OpenSubtitles2018.php 7https://www.statmt.org/wmt20/translation-task.html 8http://www.opensubtitles.org/ We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [Wester, 2010], which contains bilingual Chinese/English speech recorded by the same speakers. There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350. Zero-shot cross-lingual TTS is evaluated using Librispeech [Panayotov et al., 2015] dev-clean set and EMIME dataset providing English and Chinese data, respectively. We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts. 5.2 Experimental Setup Phonemization & Quantization The right picture of Figure 2 illustrates the phonemization & quantization processes for different languages. All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets. We use a unified phoneme set called BigCiDian9 for two languages which are based on International Phonetic Alphabet (IPA). The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools10 for additional alignment information used for the pre-training of speech recognition & translation model. The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model EnCodec11, which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks.",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nModel Architecture For the cross-lingual codec language models, MAR and MNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096. The autoregression is implemented by attention masking in the MAR model. Sinuous position embedding is separately computed for each prompt sequence in MAR and MNAR models. Besides, the M",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nNAR model uses individual layer normalization for generating each layer of acoustic tokens. We also introduce the model architecture of speech recognition & translation for S2ST in Appendix A.1.2. We call our cross-lingual TTS model and S2ST model as VALL-E X and VALL-E X Trans in the subsequent experiments, respectively. Training Details We optimize each module of VALL-E X individually, including MAR and MNAR. For both modules, The maximum sentence length is set to 20 seconds, so we re-segment the LibriLight data to an average utterance duration of 12 seconds by detecting the consecutive silence phonemes. Fortunately, the WenetSpeech data has already been segmented into short utterances. The maximum learning rate is 5e-4 with warm-up steps of 8,000. The models are trained on 32 V100 GPUs for 800k steps. MAR is trained with the batch size of 120 seconds per GPU, which is 66 seconds for MNAR due to the memory constraint. When optimizing MNAR, instead of accumulating all layer’s loss in Eqn. (2), we randomly select one layer at each optimization step for efficiency. For speech recognition & translation model, the training details can be found in Appendix A.1.3. Baselines We adopt YourTTS12 [Casanova et al., 2022] as our baseline for zero-shot cross-lingual TTS. YourTTS is a zero-shot multi-speaker TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech. Since previous work shows that current end-to-end S2ST systems underperform cascaded S2ST systems [Jia et al., 2022a, Lee et al., 2021b], we also build an S2ST baseline which is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS. The source speech serves as the reference speech when synthesizing the target speech using YourTTS. The ASR model is the released HuBERT model introduced in Section 4.3, and the MT model is a vanilla Transformer trained by ourselves on the MT data introduced in Section 5.1. Since YourTTS is built only for English, we don’t get its performance for EnglishChinese translation direction. Table 2: Zero-shot cross-lingual TTS evaluation for English TTS with Chinese speech as prompts and Chinese TTS with English speech as prompts, using automatic evaluation matrices, including ASV-Score (hypothesis vs. prompt), ASR-WER, and Naturalness. ASV-Score ASR-WER Naturalness English TTS with Chinese as prompts Baseline (YourTTS) 0.300.10 8.53 3.36 VALL-E X 0.360.11 4.07 3.54 Chinese TTS with English as prompts VALL-E X 0.290.10 8.52 3.36 5.3 Zero-Shot Cross-Lingual TTS Evaluation We first select samples with a length between 4 and 10 seconds from LibriSpeech dev-clean set, resulting in 40 speakers and 1373 samples. For English TTS, we randomly select one audio from EMIME set as the Chinese prompt for each target sentence in LibriSpeech dev-clean set. For Chinese TTS, we use extra 149 Chinese text sentences provided by the EMIME set and repeat them to the total number of 1373 so that they can be prompted by the LibriSpeech audios one-by-one. When synthesizing the target language speech, the whole sequence of the source language speech is used as the prompt. Automatic Evaluation Table 2 summarizes the results of cross-lingual zero-shot TTS tasks, including English TTS prompted by Chinese speech and Chinese TTS prompted by English speech. We measure the speaker similarity using the automatic speaker verification (ASV) model, ranging from -1 to +1 given two speech utterances. The larger the value, the more similar the speakers of",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nthe two utterances are. The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of VALL-E X is superior to that of the baseline (0.36 vs 0.30). (2) VALL-E X reduces the WER significantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\n. (3) VALL-E X has better speech naturalness than the baseline thanks to the large-scale training data and the large language model capacity. The results of Chinese TTS with English prompts are also listed. Table 3: Human evaluation for zero-shot cross-lingual TTS. SMOS means similarity MOS between generated speech and prompt, and CMOS means comparative MOS based on Baseline. SMOS CMOS (v.s. Baseline) Baseline (YourTTS) 3.420.19 0.00 VALL-E X 4.000.20 +0.24 Human Evaluation We further conduct the human evaluation on 50 randomly selected speech records for zero-shot cross-lingual English TTS with Chinese speech as prompts, including SMOS and CMOS. Note that SMOS ranges from 1 to 5 where the larger the value, the higher the voice similarity, and CMOS ranges from -3 to 3 where the positive number means the new system is better than the baseline. The results are listed in Table 3. Baseline gets 3.42 SMOS scores between generated speech and prompts, while our VALL-E X achieves 4.00, which further demonstrates the model’s superiority in keeping the speech characteristic in the cross-lingual setting. Moreover, to directly compare the speech synthesis quality between the proposed VALL-E X and baseline, we calculate the CMOS score between them evaluated by native speakers on the 50 sentences. The last column of Table 3 shows that VALL-E X obtains the gain of +0.24 CMOS scores than the baseline. Table 4: S2ST performance on EMIME dataset for ChineseEnglish directions. Baseline is a cascaded S2ST system based on speaker embedding. Automatic evaluation matrices include ASVScore, ASR-BLEU, and Naturalness. ASV-Score ASR-BLEU Naturalness tgt vs. src hyp vs. src hyp vs. tgt ChineseEnglish S2ST 0.280.10 0.270.12 27.49 3.44 - w/ oracle target text 0.280.10 0.290.11 80.30 3.43 VALL-E X Trans 0.370.10 0.370.11 30.66 3.54 - w/ oracle target text 0.390.10 0.380.10 86.78 3.54 Baseline (S2ST) 0.580.09 EnglishChinese S2ST VALL-E X Trans 0.580.09 0.480.11 0.530.11 34.45 3.41 - w/ oracle target text 0.470.12 0.550.11 84.00 3.42 Speaker Similarity We first evaluate whether the speaker’s voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in Table 4. Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs. src, hyp vs. src, and hyp vs. tgt). From Table 4 we can find that: (1) For ChineseEnglish, the ASV score of VALL-E X Trans significantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker’s voice. (2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs. src) for the EnglishChinese direction, which suggests that the cross-lingual voice transferability still has the improvement space. (3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nincrease notably, indicating that voice transferability is less affected by the quality of translation. Translation Quality Table 4 also shows the translation performance of VALL-E X Trans. Note that ASR-BLEU with oracle target text as the input of VALL-E X can be seen as the upper bound when translations are exactly correct. With oracle target text as input, V",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Experiments_/_Results_chunk4",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nALL-E X Trans can achieve the performance of about 8487 BLEU scores, which also reflects the high performance of our neural codec language model. For ChineseEnglish, VALL-E X Trans achieves higher BLEU over the baseline (30.66 vs. 27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task. Speech Naturalness We also evaluate the Naturalness with the open-source NISQA [Mittag and Möller, 2021] for S2ST outputs. As shown in the last column of Table 4, compared to the baseline, VALL-E X Trans achieves a better naturalness score (3.54 vs. 3.44), which shows that VALL-E X can generate more natural target language speech than the baseline. Human Evaluation We randomly sample 56 translation pairs13 to perform a human evaluation using SMOS and MOS matrics for both ChineseEnglish and EnglishChinese directions. Table 5 lists the results of VALL-E X Trans as well as the ChineseEnglish baseline. We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation. For speaker similarity evaluation, VALL-E X Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs. 3.06), demonstrating its superior ability to model speaker property of the proposed VALL-E X. Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91). For speech quality, our VALL-E X slightly outperforms the baseline in ChineseEnglish S2ST in terms of MOS score (3.87 vs. 3.81). Table 5: Subjection evaluation with SMOS and MOS scores on bidirectional ChineseEnglish S2ST tasks. SMOS is measured by comparing with the ground-truth target speech. EnglishChinese S2ST baseline is not reported since it is not supported by the released YourTTS. ChineseEnglish EnglishChinese SMOS MOS SMOS MOS Baseline (S2ST) 3.060.14 3.810.19 - - VALL-E X Trans 4.120.13 3.870.21 3.940.15 3.480.13 Source speech prompt 4.910.05 - 4.640.06 - Oracle target speech - 3.920.17 - 3.880.13 5.5 Analysis In this section, we first analyze the effect of language ID, then explore the foreign accent problems, and qualitatively investigate the ability to maintain voice emotion and synthesize code-switch speech of our proposed model. Effect of Language ID Our VALL-E X is trained with multi-lingual ASR data, which might increase the modeling difficulty for each specific language. We address it by adding language IDs to guide speech synthesis in the autoregressive language codec model. Here, we verify the effectiveness by removing the language ID (LID) or adding the wrong LID (i.e. the source LID). The ASV-Score and ASR-BLEU are reported in Table 6. Without LID or with the wrong language ID, the translation quality decreases, while the speaker similarity between the hypothesis and source speech increases. These results demonstrate the importance of language ID for the accuracy of the content. It also indicates that target LID reduces the transfer of information, which means the model without LID or with source LID will better maintain the sound of the original speaker. Table 6: Evaluation for the effect of language ID on ChineseEnglish EMIME dataset. ASV-Score is computed between synthesized speech and source prompt speech. The last column lists the subjection evaluation score of the foreign accent (from 1 to 5 scores). ASV-Score (vs. src) ASR-BLEU Accent Score ChineseEnglish S2ST VALL-E X Trans 0.370.10 30.66 4.10 w/o Language ID 0.410.10 29.04 2.98 w/ wrong Language ID 0.410.10 29.07 2.55 EnglishChinese S2ST VALL-E X Trans 0.480.11 34.45 4.03 w/o Language ID 0.490.11 30.86 2.35 w/ wrong Language ID 0.500.11 29.70 2.25 Foreign Accent Control L2 (second-language, or foreign) accent",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Experiments_/_Results_chunk5",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nproblem, the synthesized speech sounds like the accents of a foreigner, has arisen in cross-lingual TTS systems [Zhang et al., 2019, Lee et al., 2022]. Automatic Evaluation has shown that adding LID can boost speech quality. Besides, we conduct a subjection evaluation to label foreign accents from 1 to 5 on randomly selected 20 synthesized speech for both English and Chinese, where each sample is measured with a score from 1 to 5 denoting high-status foreign speakers, low-status foreign speakers, middle-status speakers, low-status native speakers, and high-status native speakers, respectively. As summarized in the last column of Table 6, we observed that our VALL-E X can control the accent for the target speech by LID modules. For example, in EnglishChinese, VALL-E X Trans with right LID and without LID get the score of 4.03 and 2.35, respectively. This indicates that by using correct LID embedding, VALL-E X Trans is able to alleviate the foreign accent problem. Please also see the demo for audio examples of VALL-E X Trans with or without language ID. [Um et al., 2020]. Moreover, it is more tempting to reserve the source speaker’s emotion in generated target speech for the S2ST task, which is not explored in previous S2ST work. In these experiments, we adopt the source prompts from the emotional voices dataset EmoV-DB [Um et al., 2020] as inputs of VALL-E X Trans to generate the translated target speech, whose samples are listed on our demo page. We found that the proposed VALL-E X can maintain emotional consistency to a certain extent between the source prompt and the synthesized speech. The underlying reasons are (1) our VALL-E X is trained with large-scale multi-lingual multi-speaker speech-transcription data, which contains various emotional speech records, and (2) the strong in-context learning ability of VALL-E X, like GPT-3 [Brown et al., 2020], promotes the generated speech to reserve the characteristic of the source prompt. Code-Switch Speech Synthesis It is a common phenomenon to use code-switch utterances in bilingual or multi-lingual communities [Cao et al., 2020, Zhao et al., 2020, Manghat et al., 2022]. Code-switch speech synthesis aims to produce a fluent and consistent voice for code-switch text. Although our proposed VALL-E X is trained on multiple monolingual speech data, without special optimization for code-switch setting, VALL-E X provides a promising solution to code-switch speech synthesis. We put the code-switch samples on our demo page, demonstrating that due to its strong in-context learning ability, VALL-E X can synthesize fluent code-switch speech with a consistent voice. 6",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-01",
    "id_chunk": "09-01_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion In this work, we propose VALL-E X, a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. VALL-E X is free of the requirement for cross-lingual paired data from the same speakers. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.",
    "output": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Abstract\nAbstract: Abstract We propose a cross-lingual neural codec language model, VALL-E X, for crosslingual speech synthesis. Specifically, we extend VALL-E [Wang et al., 2023] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-tospeech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at https://aka.ms/vallex. VALL-E X (Cross-Lingual VALL-E) Cross-Lingual Neural Codec Language Modeling Multilingual G2P Multilingual G2P Audio Codec Encoder Source Text Target Text Source Speech Figure 1: The overall framework of VALL-E X, which can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training, and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation. Personalized Target Speech Audio Codec Decoder Language ID Source phoneme sequences Target phoneme sequences Source acoustic tokens Target acoustic tokens 1",
    "output": "VALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: Introduction Recent years have witnessed significant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity [Li et al., 2019, Ren et al., 2019, Tan et al., 2022]. However, these models can only generate high-quality speech for a specific speaker in a specific language. Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. Previous methods to tackle these challenges typically augment end-to-end TTS models with specific subnets for speaker and language control [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2020, Ellinas et al., 2022, Cai et al., 2023]. For example, based on the multi-speaker TTS model, Nachmani and Wolf [2019] introduce multiple encoders for each language and additional loss to keep the speaker’s identity. Zhang et al. [2020] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities. Yang and He [2020] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively. Yang and He [2022] further propose a multi-task learning method with additional tasks of speaker similarity and language identification. Moreover, Cai et al. [2023] investigates cross-lingual multi-speaker text-to-speech synthesis with sufficient or limited bilingual speech training data. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [Zhang et al., 2019, Lee et al., 2022]. Table 1: A comparison between VALL-E X and previous cross-lingual TTS systems. Previous Systems VALL-E X Intermediate representation Mel spectrogram Audio codec codes Training data 13K hours Speech accent Foreign Native Speaker similarity Relative low High In-context learning Zero-shot cross-lingual TTS In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, VALL-E X, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis. Based on the knowledge learned from large-scale multi-lingual speech data, VALL-E X is able to transfer the speech characteristics, including the speaker’s voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems. More specifically, we first obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data. Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an offline neural codec encoder. Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model. As illustrated in Figure 1, after training, VALL-E X can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language. The generated acoustic token sequence is decompressed to the target speech waveform by an offline audio codec",
    "output": "The traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Introduction\ndecoder. VALL-E X is trained on two large-scale multi-speaker datasets1, LibriLight [Kahn et al., 2020] and WenetSpeech [Zhang et al., 2022a], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively. The combination of LibriLight and Wenet",
    "output": "The traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Introduction\nSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which significantly improves the coverage of different speakers and enhances VALL-E X’s generalization capacity. The comparison between VALL-E X and the previous cross-lingual TTS systems are listed in Table 1. We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot crosslingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST). For cross-lingual text-to-speech synthesis, the proposed VALL-E X is evaluated with LibriSpeech [Panayotov et al., 2015] and EMIME [Wester, 2010] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers. For zero-shot speech-to-speech translation, EMIME [Wester, 2010] dataset is used for the evaluation of VALL-E X on bidirectional ChineseEnglish translation tasks, and it contains bilingual audio recordings by the same speakers. We evaluate the proposed VALL-E X framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS). Specifically, due to the strong in-context learning capability, VALL-E X achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker. By training on large-scale speech-transcription data, the proposed VALL-E X significantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness. Furthermore, the human evaluation shows that our VALL-E X outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST). Our contributions can be summarized as follows: • We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data. VALL-E X is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts. • The multi-lingual in-context learning framework enables VALL-E X to generate crosslingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language. • Based on the learned cross-lingual speech modeling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks. • We apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation. We encourage readers to listen to the audio samples on our demo page: https://aka.ms/vallex. 2",
    "output": "The traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Method\nMethod: 3.2 Model Framework Inspired by VALL-E, the cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM MAR is only used to predict the acoustic tokens from the first quantizer of EnCodec model. Formally, based on paired speech-transcription data in any language, let S denote the transcribed phoneme sequence, and A:,1 ai,1|i = 1, . . . , N denotes the first-layer acoustic tokens extracted from the speech X. The decoder MAR, modeling the concatenated sequence S, A:,1, is trained to predict A:,1 autoregressively. It is optimized by maximizing the log-likelihood, Quantization & Phonemization: Quantized tokens Layer L 12 504 31 8 743 ... A:,2:L ...... Layer 2 325 71 435 90 7 ... A:,1 731 284 78 32 669 ... Layer 1 audio codec encoder (Quantization) Multi-lingual speech-transcription pairs (phonemization) G2P tool Phoneme tokens HH AH L OW ... D S N Y Multi-lingual Non-Autoregressive Codec LM Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM MNAR is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer. It is prompted by the phoneme sequence of the current sentence (S) and the acoustic token sequence of another sentence with the same speaker (  A). Here  A is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer l [2, 8], the embeddings of l 1 layers’ acoustic tokens (A:,1:l1) are summed up layerwise as input. The learning objective for the l-layer acoustic tokens A:,l can be calculated as 8 X l=2 log pNAR A:,l | D S,  A:,1:8, A:,1:l1 E ; MNAR (2) LMNAR = where means the sequence concatenation. pNAR(.) computes the pointwise probabilities of A:,l. 3.3 Multi-lingual Training In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-tospeech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (Ss, As) and (St, At) to train our multi-lingual codec LMs MAR and MNAR, where s and t represent two different (source and target) languages. Language ID Module Following multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in VALL-E X. On the one hand, without language ID",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Method\n, VALL-E X may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM MAR is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens. 3.4 Cross-Lingual Inference After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we first concatenate source phonemes Ss and target phonemes St as prompts, and take the first-layer source acoustic tokens As :,1 as the decoding prefix, condition on which the multi-lingual autoregressive codec LM MAR generates the first-layer target acoustic tokens At :,1, at i,1 pAR  at i,1 | Ss, St, As :,1, At i,1 ; MAR , i = 1, . . . , (3) where means probability-based sampling. The sampling is stopped until the end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the final generated speech. After obtaining the first-layer target acoustic tokens At :,1 from MAR, multi-lingual non-autoregressive codec LM MNAR is used to predict the rest layers of acoustic tokens n At :,l | l = 2, . . . , 8 o by greedy search, i.e., choosing the tokens with maximum probabilities, At :,l = argmax At :,l pNAR  At :,l | St, As :,1:8, At :,1:l1 ; MNAR , l = 2, . . . , 8. (4) Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens At :,1:8. tl= 2, ... , 8 VALL-E X Inference: A:,l Multi-lingual Non-Autoregressive Codec LM t s A:,1:l1 St A:,1:8 t A:,1 Multi-lingual Autoregressive Codec LM B t aN,1 t a2,1 t ... s a1,1 bos> Ss St A:,1 Figure 3: Inference illustration of the cross-lingual neural codec language model VALL-E X, with two-stage decoding strategies. VALL-E X can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks. 4 VALL-E X Application VALL-E X can be applied to various cross-lingual speech generation tasks. In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in Figure 3. 4.1 Zero-Shot Cross-Lingual TTS The proposed VALL-E X is naturally suitable for zero-shot cross-lingual TTS tasks. Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker’s voice. Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability. Thanks to the in-context learning capability of large language models, VALL-E X surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis. More specifically, given the source speech, source transcript, and target text, we first convert source speech into source acoustic token As using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes Ss and target phonemes St using G2P tool. More specifically, as introduced in",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Method\nSection 3.4, we let St be the phonemes extracted from the target text, Ss and As be the phonemes and acoustic tokens extracted from the source speech. Then VALL-E X generates the full-layer target acoustic tokens, which are finally decompressed into the target speech by EnCodec decoder. 4.2 Zero-Shot Speech-to-Speech Translation We can also apply our VALL-E X to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences. Speech Recognition & Translation Model We leverage the improved SpeechUT [Zhang et al., 2022c] as our speech recognition & translation model, which is a unified-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss) and MT corpus (source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more As Application: Zero-shot cross-lingual TTS Ss St Codec Encoder G2P Tool G2P Tool Source text Target text Source speech Application: Zero-shot S2ST Ss St As E Codec Encoder Semantic Semantic Encoder Decoder Speech Encoder Source Speech pre-training details about this model. After pre-training, the model is fine-tuned with (X s, Ss, St) triplet data derived from the ST corpus. Specifically, we perform multi-task learning with the CTC [Graves et al., 2006] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes. Inference Figure 3 shows the inference process of speech-to-speech translation. Given a source speech X s, the speech recognition & translation model first generates the source phonemes Ss from the semantic encoder and the target phonemes St from the semantic decoder. Besides, we use the EnCodec encoder to compress X s into source acoustic tokens As. Then, we concatenate Ss, St, and As, as the input of VALL-E X, to produce the acoustic token sequence for the target speech, as introduced in Section 3.4. The generated acoustic tokens are converted to the final target speech with the decoder of EnCodec. 4.3 Evaluation The proposed model is verified using various evaluation criteria, including speaker similarity (ASVScore), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation. Specifically, we measure speaker similarity between synthesized target speech and groudtruth target speech or source speech as an automatic speaker verification (ASV) task, where a WavLM [Chen et al., 2022] based ASV model is used to calculate the score. To verify the quality of generated speech, we first utilize the ASR system from the released HuBERT-Large model [Hsu et al., 2021] to recognize it into text. For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text. For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text. Finally,",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Method\nto better verify our proposed VALL-E X systems, we adopt the open-source NISQA3 [Mittag and Möller, 2021] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS). 5",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: Experiments We evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5, OpenSubtitles20186 and WMT20207, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama8, and news domains, respectively. The EnglishChinese ST data is from GigaST [Ye et al., 2022], which is created by translating the transcripts in GigaSpeech [Chen et al., 2021] using a strong machine translation system. Similarly, we create the ChineseEnglish ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above. 3https://github.com/gabrielmittag/NISQA 4https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech 5https://challenger.ai/competition/translation 6https://opus.nlpl.eu/OpenSubtitles2018.php 7https://www.statmt.org/wmt20/translation-task.html 8http://www.opensubtitles.org/ We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [Wester, 2010], which contains bilingual Chinese/English speech recorded by the same speakers. There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350. Zero-shot cross-lingual TTS is evaluated using Librispeech [Panayotov et al., 2015] dev-clean set and EMIME dataset providing English and Chinese data, respectively. We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts. 5.2 Experimental Setup Phonemization & Quantization The right picture of Figure 2 illustrates the phonemization & quantization processes for different languages. All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets. We use a unified phoneme set called BigCiDian9 for two languages which are based on International Phonetic Alphabet (IPA). The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools10 for additional alignment information used for the pre-training of speech recognition & translation model. The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model EnCodec11, which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks.",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nModel Architecture For the cross-lingual codec language models, MAR and MNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096. The autoregression is implemented by attention masking in the MAR model. Sinuous position embedding is separately computed for each prompt sequence in MAR and MNAR models. Besides, the M",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nNAR model uses individual layer normalization for generating each layer of acoustic tokens. We also introduce the model architecture of speech recognition & translation for S2ST in Appendix A.1.2. We call our cross-lingual TTS model and S2ST model as VALL-E X and VALL-E X Trans in the subsequent experiments, respectively. Training Details We optimize each module of VALL-E X individually, including MAR and MNAR. For both modules, The maximum sentence length is set to 20 seconds, so we re-segment the LibriLight data to an average utterance duration of 12 seconds by detecting the consecutive silence phonemes. Fortunately, the WenetSpeech data has already been segmented into short utterances. The maximum learning rate is 5e-4 with warm-up steps of 8,000. The models are trained on 32 V100 GPUs for 800k steps. MAR is trained with the batch size of 120 seconds per GPU, which is 66 seconds for MNAR due to the memory constraint. When optimizing MNAR, instead of accumulating all layer’s loss in Eqn. (2), we randomly select one layer at each optimization step for efficiency. For speech recognition & translation model, the training details can be found in Appendix A.1.3. Baselines We adopt YourTTS12 [Casanova et al., 2022] as our baseline for zero-shot cross-lingual TTS. YourTTS is a zero-shot multi-speaker TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech. Since previous work shows that current end-to-end S2ST systems underperform cascaded S2ST systems [Jia et al., 2022a, Lee et al., 2021b], we also build an S2ST baseline which is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS. The source speech serves as the reference speech when synthesizing the target speech using YourTTS. The ASR model is the released HuBERT model introduced in Section 4.3, and the MT model is a vanilla Transformer trained by ourselves on the MT data introduced in Section 5.1. Since YourTTS is built only for English, we don’t get its performance for EnglishChinese translation direction. Table 2: Zero-shot cross-lingual TTS evaluation for English TTS with Chinese speech as prompts and Chinese TTS with English speech as prompts, using automatic evaluation matrices, including ASV-Score (hypothesis vs. prompt), ASR-WER, and Naturalness. ASV-Score ASR-WER Naturalness English TTS with Chinese as prompts Baseline (YourTTS) 0.300.10 8.53 3.36 VALL-E X 0.360.11 4.07 3.54 Chinese TTS with English as prompts VALL-E X 0.290.10 8.52 3.36 5.3 Zero-Shot Cross-Lingual TTS Evaluation We first select samples with a length between 4 and 10 seconds from LibriSpeech dev-clean set, resulting in 40 speakers and 1373 samples. For English TTS, we randomly select one audio from EMIME set as the Chinese prompt for each target sentence in LibriSpeech dev-clean set. For Chinese TTS, we use extra 149 Chinese text sentences provided by the EMIME set and repeat them to the total number of 1373 so that they can be prompted by the LibriSpeech audios one-by-one. When synthesizing the target language speech, the whole sequence of the source language speech is used as the prompt. Automatic Evaluation Table 2 summarizes the results of cross-lingual zero-shot TTS tasks, including English TTS prompted by Chinese speech and Chinese TTS prompted by English speech. We measure the speaker similarity using the automatic speaker verification (ASV) model, ranging from -1 to +1 given two speech utterances. The larger the value, the more similar the speakers of",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nthe two utterances are. The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of VALL-E X is superior to that of the baseline (0.36 vs 0.30). (2) VALL-E X reduces the WER significantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\n. (3) VALL-E X has better speech naturalness than the baseline thanks to the large-scale training data and the large language model capacity. The results of Chinese TTS with English prompts are also listed. Table 3: Human evaluation for zero-shot cross-lingual TTS. SMOS means similarity MOS between generated speech and prompt, and CMOS means comparative MOS based on Baseline. SMOS CMOS (v.s. Baseline) Baseline (YourTTS) 3.420.19 0.00 VALL-E X 4.000.20 +0.24 Human Evaluation We further conduct the human evaluation on 50 randomly selected speech records for zero-shot cross-lingual English TTS with Chinese speech as prompts, including SMOS and CMOS. Note that SMOS ranges from 1 to 5 where the larger the value, the higher the voice similarity, and CMOS ranges from -3 to 3 where the positive number means the new system is better than the baseline. The results are listed in Table 3. Baseline gets 3.42 SMOS scores between generated speech and prompts, while our VALL-E X achieves 4.00, which further demonstrates the model’s superiority in keeping the speech characteristic in the cross-lingual setting. Moreover, to directly compare the speech synthesis quality between the proposed VALL-E X and baseline, we calculate the CMOS score between them evaluated by native speakers on the 50 sentences. The last column of Table 3 shows that VALL-E X obtains the gain of +0.24 CMOS scores than the baseline. Table 4: S2ST performance on EMIME dataset for ChineseEnglish directions. Baseline is a cascaded S2ST system based on speaker embedding. Automatic evaluation matrices include ASVScore, ASR-BLEU, and Naturalness. ASV-Score ASR-BLEU Naturalness tgt vs. src hyp vs. src hyp vs. tgt ChineseEnglish S2ST 0.280.10 0.270.12 27.49 3.44 - w/ oracle target text 0.280.10 0.290.11 80.30 3.43 VALL-E X Trans 0.370.10 0.370.11 30.66 3.54 - w/ oracle target text 0.390.10 0.380.10 86.78 3.54 Baseline (S2ST) 0.580.09 EnglishChinese S2ST VALL-E X Trans 0.580.09 0.480.11 0.530.11 34.45 3.41 - w/ oracle target text 0.470.12 0.550.11 84.00 3.42 Speaker Similarity We first evaluate whether the speaker’s voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in Table 4. Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs. src, hyp vs. src, and hyp vs. tgt). From Table 4 we can find that: (1) For ChineseEnglish, the ASV score of VALL-E X Trans significantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker’s voice. (2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs. src) for the EnglishChinese direction, which suggests that the cross-lingual voice transferability still has the improvement space. (3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nincrease notably, indicating that voice transferability is less affected by the quality of translation. Translation Quality Table 4 also shows the translation performance of VALL-E X Trans. Note that ASR-BLEU with oracle target text as the input of VALL-E X can be seen as the upper bound when translations are exactly correct. With oracle target text as input, V",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk4_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nALL-E X Trans can achieve the performance of about 8487 BLEU scores, which also reflects the high performance of our neural codec language model. For ChineseEnglish, VALL-E X Trans achieves higher BLEU over the baseline (30.66 vs. 27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task. Speech Naturalness We also evaluate the Naturalness with the open-source NISQA [Mittag and Möller, 2021] for S2ST outputs. As shown in the last column of Table 4, compared to the baseline, VALL-E X Trans achieves a better naturalness score (3.54 vs. 3.44), which shows that VALL-E X can generate more natural target language speech than the baseline. Human Evaluation We randomly sample 56 translation pairs13 to perform a human evaluation using SMOS and MOS matrics for both ChineseEnglish and EnglishChinese directions. Table 5 lists the results of VALL-E X Trans as well as the ChineseEnglish baseline. We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation. For speaker similarity evaluation, VALL-E X Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs. 3.06), demonstrating its superior ability to model speaker property of the proposed VALL-E X. Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91). For speech quality, our VALL-E X slightly outperforms the baseline in ChineseEnglish S2ST in terms of MOS score (3.87 vs. 3.81). Table 5: Subjection evaluation with SMOS and MOS scores on bidirectional ChineseEnglish S2ST tasks. SMOS is measured by comparing with the ground-truth target speech. EnglishChinese S2ST baseline is not reported since it is not supported by the released YourTTS. ChineseEnglish EnglishChinese SMOS MOS SMOS MOS Baseline (S2ST) 3.060.14 3.810.19 - - VALL-E X Trans 4.120.13 3.870.21 3.940.15 3.480.13 Source speech prompt 4.910.05 - 4.640.06 - Oracle target speech - 3.920.17 - 3.880.13 5.5 Analysis In this section, we first analyze the effect of language ID, then explore the foreign accent problems, and qualitatively investigate the ability to maintain voice emotion and synthesize code-switch speech of our proposed model. Effect of Language ID Our VALL-E X is trained with multi-lingual ASR data, which might increase the modeling difficulty for each specific language. We address it by adding language IDs to guide speech synthesis in the autoregressive language codec model. Here, we verify the effectiveness by removing the language ID (LID) or adding the wrong LID (i.e. the source LID). The ASV-Score and ASR-BLEU are reported in Table 6. Without LID or with the wrong language ID, the translation quality decreases, while the speaker similarity between the hypothesis and source speech increases. These results demonstrate the importance of language ID for the accuracy of the content. It also indicates that target LID reduces the transfer of information, which means the model without LID or with source LID will better maintain the sound of the original speaker. Table 6: Evaluation for the effect of language ID on ChineseEnglish EMIME dataset. ASV-Score is computed between synthesized speech and source prompt speech. The last column lists the subjection evaluation score of the foreign accent (from 1 to 5 scores). ASV-Score (vs. src) ASR-BLEU Accent Score ChineseEnglish S2ST VALL-E X Trans 0.370.10 30.66 4.10 w/o Language ID 0.410.10 29.04 2.98 w/ wrong Language ID 0.4",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk4_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\n10.10 29.07 2.55 EnglishChinese S2ST VALL-E X Trans 0.480.11 34.45 4.03 w/o Language ID 0.490.11 30.86 2.35 w/ wrong Language ID 0.500.11 29.70 2.25 Foreign Accent Control L2 (second-language, or foreign) accent",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Experiments_/_Results_chunk5",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nproblem, the synthesized speech sounds like the accents of a foreigner, has arisen in cross-lingual TTS systems [Zhang et al., 2019, Lee et al., 2022]. Automatic Evaluation has shown that adding LID can boost speech quality. Besides, we conduct a subjection evaluation to label foreign accents from 1 to 5 on randomly selected 20 synthesized speech for both English and Chinese, where each sample is measured with a score from 1 to 5 denoting high-status foreign speakers, low-status foreign speakers, middle-status speakers, low-status native speakers, and high-status native speakers, respectively. As summarized in the last column of Table 6, we observed that our VALL-E X can control the accent for the target speech by LID modules. For example, in EnglishChinese, VALL-E X Trans with right LID and without LID get the score of 4.03 and 2.35, respectively. This indicates that by using correct LID embedding, VALL-E X Trans is able to alleviate the foreign accent problem. Please also see the demo for audio examples of VALL-E X Trans with or without language ID. [Um et al., 2020]. Moreover, it is more tempting to reserve the source speaker’s emotion in generated target speech for the S2ST task, which is not explored in previous S2ST work. In these experiments, we adopt the source prompts from the emotional voices dataset EmoV-DB [Um et al., 2020] as inputs of VALL-E X Trans to generate the translated target speech, whose samples are listed on our demo page. We found that the proposed VALL-E X can maintain emotional consistency to a certain extent between the source prompt and the synthesized speech. The underlying reasons are (1) our VALL-E X is trained with large-scale multi-lingual multi-speaker speech-transcription data, which contains various emotional speech records, and (2) the strong in-context learning ability of VALL-E X, like GPT-3 [Brown et al., 2020], promotes the generated speech to reserve the characteristic of the source prompt. Code-Switch Speech Synthesis It is a common phenomenon to use code-switch utterances in bilingual or multi-lingual communities [Cao et al., 2020, Zhao et al., 2020, Manghat et al., 2022]. Code-switch speech synthesis aims to produce a fluent and consistent voice for code-switch text. Although our proposed VALL-E X is trained on multiple monolingual speech data, without special optimization for code-switch setting, VALL-E X provides a promising solution to code-switch speech synthesis. We put the code-switch samples on our demo page, demonstrating that due to its strong in-context learning ability, VALL-E X can synthesize fluent code-switch speech with a consistent voice. 6",
    "output": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."
  },
  {
    "id_paper": "09",
    "id_story": "09-03",
    "id_chunk": "09-03_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion In this work, we propose VALL-E X, a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. VALL-E X is free of the requirement for cross-lingual paired data from the same speakers. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.",
    "output": "The multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: Abstract We propose a cross-lingual neural codec language model, VALL-E X, for crosslingual speech synthesis. Specifically, we extend VALL-E [Wang et al., 2023] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-tospeech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at https://aka.ms/vallex. VALL-E X (Cross-Lingual VALL-E) Cross-Lingual Neural Codec Language Modeling Multilingual G2P Multilingual G2P Audio Codec Encoder Source Text Target Text Source Speech Figure 1: The overall framework of VALL-E X, which can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training, and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation. Personalized Target Speech Audio Codec Decoder Language ID Source phoneme sequences Target phoneme sequences Source acoustic tokens Target acoustic tokens 1",
    "output": "VALL-E-X, unlike RVC (Retrieval-based Voice Conversion), directly reflects the characteristics of short audio samples. Therefore, it might be beneficial to prepare several audio prompts with different emotions or intonations and switch between them as needed. This approach allows for greater flexibility and variety in the synthesized speech, adapting to different emotional tones or speaking styles."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: Introduction Recent years have witnessed significant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity [Li et al., 2019, Ren et al., 2019, Tan et al., 2022]. However, these models can only generate high-quality speech for a specific speaker in a specific language. Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. Previous methods to tackle these challenges typically augment end-to-end TTS models with specific subnets for speaker and language control [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2020, Ellinas et al., 2022, Cai et al., 2023]. For example, based on the multi-speaker TTS model, Nachmani and Wolf [2019] introduce multiple encoders for each language and additional loss to keep the speaker’s identity. Zhang et al. [2020] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities. Yang and He [2020] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively. Yang and He [2022] further propose a multi-task learning method with additional tasks of speaker similarity and language identification. Moreover, Cai et al. [2023] investigates cross-lingual multi-speaker text-to-speech synthesis with sufficient or limited bilingual speech training data. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [Zhang et al., 2019, Lee et al., 2022]. Table 1: A comparison between VALL-E X and previous cross-lingual TTS systems. Previous Systems VALL-E X Intermediate representation Mel spectrogram Audio codec codes Training data 13K hours Speech accent Foreign Native Speaker similarity Relative low High In-context learning Zero-shot cross-lingual TTS In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, VALL-E X, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis. Based on the knowledge learned from large-scale multi-lingual speech data, VALL-E X is able to transfer the speech characteristics, including the speaker’s voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems. More specifically, we first obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data. Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an offline neural codec encoder. Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model. As illustrated in Figure 1, after training, VALL-E X can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language. The generated acoustic token sequence is decompressed to the target speech waveform by an offline audio codec",
    "output": "The page linked below contains actual examples of voices synthesized with each system. The results of English voice synthesis with Japanese vocal qualities, found at the end of the page in section `Japanese zero-shot cross-lingual text-to-speech` , are particularly illustrative. You can notice that it’s not just the vocal qualities of the original voice that are captured, but also the emotions.\nThe AR Decoder then generates an intermediate representation A^t_1. Subsequently, S^t, A^s, and A^t_1 are used as inputs to the NAR Decoder to calculate the final intermediate representations A^t_2 to A^t_8. This process enables VALL-E-X to create a nuanced synthesis that mirrors the characteristics of the source voice while delivering the desired speech content.\nModel Size\nvocos: 40.4MB\nwhisper medium: 1.42GB\nFirst, clone the repository.\nThe following commands can be used for speech synthesis. Currently, BLAS runs faster than GPU on macOS, so if necessary, give -e 1 to run using BLAS."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\ndecoder. VALL-E X is trained on two large-scale multi-speaker datasets1, LibriLight [Kahn et al., 2020] and WenetSpeech [Zhang et al., 2022a], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively. The combination of LibriLight and Wenet",
    "output": "The page linked below contains actual examples of voices synthesized with each system. The results of English voice synthesis with Japanese vocal qualities, found at the end of the page in section `Japanese zero-shot cross-lingual text-to-speech` , are particularly illustrative. You can notice that it’s not just the vocal qualities of the original voice that are captured, but also the emotions.\nThe AR Decoder then generates an intermediate representation A^t_1. Subsequently, S^t, A^s, and A^t_1 are used as inputs to the NAR Decoder to calculate the final intermediate representations A^t_2 to A^t_8. This process enables VALL-E-X to create a nuanced synthesis that mirrors the characteristics of the source voice while delivering the desired speech content.\nModel Size\nvocos: 40.4MB\nwhisper medium: 1.42GB\nFirst, clone the repository.\nThe following commands can be used for speech synthesis. Currently, BLAS runs faster than GPU on macOS, so if necessary, give -e 1 to run using BLAS."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which significantly improves the coverage of different speakers and enhances VALL-E X’s generalization capacity. The comparison between VALL-E X and the previous cross-lingual TTS systems are listed in Table 1. We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot crosslingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST). For cross-lingual text-to-speech synthesis, the proposed VALL-E X is evaluated with LibriSpeech [Panayotov et al., 2015] and EMIME [Wester, 2010] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers. For zero-shot speech-to-speech translation, EMIME [Wester, 2010] dataset is used for the evaluation of VALL-E X on bidirectional ChineseEnglish translation tasks, and it contains bilingual audio recordings by the same speakers. We evaluate the proposed VALL-E X framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS). Specifically, due to the strong in-context learning capability, VALL-E X achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker. By training on large-scale speech-transcription data, the proposed VALL-E X significantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness. Furthermore, the human evaluation shows that our VALL-E X outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST). Our contributions can be summarized as follows: • We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data. VALL-E X is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts. • The multi-lingual in-context learning framework enables VALL-E X to generate crosslingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language. • Based on the learned cross-lingual speech modeling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks. • We apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation. We encourage readers to listen to the audio samples on our demo page: https://aka.ms/vallex. 2",
    "output": "The page linked below contains actual examples of voices synthesized with each system. The results of English voice synthesis with Japanese vocal qualities, found at the end of the page in section `Japanese zero-shot cross-lingual text-to-speech` , are particularly illustrative. You can notice that it’s not just the vocal qualities of the original voice that are captured, but also the emotions.\nThe AR Decoder then generates an intermediate representation A^t_1. Subsequently, S^t, A^s, and A^t_1 are used as inputs to the NAR Decoder to calculate the final intermediate representations A^t_2 to A^t_8. This process enables VALL-E-X to create a nuanced synthesis that mirrors the characteristics of the source voice while delivering the desired speech content.\nModel Size\nvocos: 40.4MB\nwhisper medium: 1.42GB\nFirst, clone the repository.\nThe following commands can be used for speech synthesis. Currently, BLAS runs faster than GPU on macOS, so if necessary, give -e 1 to run using BLAS."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 3.2 Model Framework Inspired by VALL-E, the cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM MAR is only used to predict the acoustic tokens from the first quantizer of EnCodec model. Formally, based on paired speech-transcription data in any language, let S denote the transcribed phoneme sequence, and A:,1 ai,1|i = 1, . . . , N denotes the first-layer acoustic tokens extracted from the speech X. The decoder MAR, modeling the concatenated sequence S, A:,1, is trained to predict A:,1 autoregressively. It is optimized by maximizing the log-likelihood, Quantization & Phonemization: Quantized tokens Layer L 12 504 31 8 743 ... A:,2:L ...... Layer 2 325 71 435 90 7 ... A:,1 731 284 78 32 669 ... Layer 1 audio codec encoder (Quantization) Multi-lingual speech-transcription pairs (phonemization) G2P tool Phoneme tokens HH AH L OW ... D S N Y Multi-lingual Non-Autoregressive Codec LM Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM MNAR is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer. It is prompted by the phoneme sequence of the current sentence (S) and the acoustic token sequence of another sentence with the same speaker (  A). Here  A is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer l [2, 8], the embeddings of l 1 layers’ acoustic tokens (A:,1:l1) are summed up layerwise as input. The learning objective for the l-layer acoustic tokens A:,l can be calculated as 8 X l=2 log pNAR A:,l | D S,  A:,1:8, A:,1:l1 E ; MNAR (2) LMNAR = where means the sequence concatenation. pNAR(.) computes the pointwise probabilities of A:,l. 3.3 Multi-lingual Training In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-tospeech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (Ss, As) and (St, At) to train our multi-lingual codec LMs MAR and MNAR, where s and t represent two different (source and target) languages. Language ID Module Following multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in VALL-E X. On the one hand, without language ID",
    "output": "When these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024).\nPositional Embeddings\nFor positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha.\nIn the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution.\nFor example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2.\nOptimisation using kv_cache\nVALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer.\nA Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time.\nTo optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step.\nThis kv_cache mechanism significantly speeds up the operation of the Transformer.\nencodec_24khz: 93.2MB"
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n, VALL-E X may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM MAR is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens. 3.4 Cross-Lingual Inference After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we first concatenate source phonemes Ss and target phonemes St as prompts, and take the first-layer source acoustic tokens As :,1 as the decoding prefix, condition on which the multi-lingual autoregressive codec LM MAR generates the first-layer target acoustic tokens At :,1, at i,1 pAR  at i,1 | Ss, St, As :,1, At i,1 ; MAR , i = 1, . . . , (3) where means probability-based sampling. The sampling is stopped until the end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the final generated speech. After obtaining the first-layer target acoustic tokens At :,1 from MAR, multi-lingual non-autoregressive codec LM MNAR is used to predict the rest layers of acoustic tokens n At :,l | l = 2, . . . , 8 o by greedy search, i.e., choosing the tokens with maximum probabilities, At :,l = argmax At :,l pNAR  At :,l | St, As :,1:8, At :,1:l1 ; MNAR , l = 2, . . . , 8. (4) Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens At :,1:8. tl= 2, ... , 8 VALL-E X Inference: A:,l Multi-lingual Non-Autoregressive Codec LM t s A:,1:l1 St A:,1:8 t A:,1 Multi-lingual Autoregressive Codec LM B t aN,1 t a2,1 t ... s a1,1 bos> Ss St A:,1 Figure 3: Inference illustration of the cross-lingual neural codec language model VALL-E X, with two-stage decoding strategies. VALL-E X can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks. 4 VALL-E X Application VALL-E X can be applied to various cross-lingual speech generation tasks. In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in Figure 3. 4.1 Zero-Shot Cross-Lingual TTS The proposed VALL-E X is naturally suitable for zero-shot cross-lingual TTS tasks. Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker’s voice. Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability. Thanks to the in-context learning capability of large language models, VALL-E X surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis. More specifically, given the source speech, source transcript, and target text, we first convert source speech into source acoustic token As using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes Ss and target phonemes St using G2P tool. More specifically, as introduced in",
    "output": "When these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024).\nPositional Embeddings\nFor positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha.\nIn the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution.\nFor example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2.\nOptimisation using kv_cache\nVALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer.\nA Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time.\nTo optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step.\nThis kv_cache mechanism significantly speeds up the operation of the Transformer.\nencodec_24khz: 93.2MB"
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nSection 3.4, we let St be the phonemes extracted from the target text, Ss and As be the phonemes and acoustic tokens extracted from the source speech. Then VALL-E X generates the full-layer target acoustic tokens, which are finally decompressed into the target speech by EnCodec decoder. 4.2 Zero-Shot Speech-to-Speech Translation We can also apply our VALL-E X to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences. Speech Recognition & Translation Model We leverage the improved SpeechUT [Zhang et al., 2022c] as our speech recognition & translation model, which is a unified-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss) and MT corpus (source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more As Application: Zero-shot cross-lingual TTS Ss St Codec Encoder G2P Tool G2P Tool Source text Target text Source speech Application: Zero-shot S2ST Ss St As E Codec Encoder Semantic Semantic Encoder Decoder Speech Encoder Source Speech pre-training details about this model. After pre-training, the model is fine-tuned with (X s, Ss, St) triplet data derived from the ST corpus. Specifically, we perform multi-task learning with the CTC [Graves et al., 2006] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes. Inference Figure 3 shows the inference process of speech-to-speech translation. Given a source speech X s, the speech recognition & translation model first generates the source phonemes Ss from the semantic encoder and the target phonemes St from the semantic decoder. Besides, we use the EnCodec encoder to compress X s into source acoustic tokens As. Then, we concatenate Ss, St, and As, as the input of VALL-E X, to produce the acoustic token sequence for the target speech, as introduced in Section 3.4. The generated acoustic tokens are converted to the final target speech with the decoder of EnCodec. 4.3 Evaluation The proposed model is verified using various evaluation criteria, including speaker similarity (ASVScore), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation. Specifically, we measure speaker similarity between synthesized target speech and groudtruth target speech or source speech as an automatic speaker verification (ASV) task, where a WavLM [Chen et al., 2022] based ASV model is used to calculate the score. To verify the quality of generated speech, we first utilize the ASR system from the released HuBERT-Large model [Hsu et al., 2021] to recognize it into text. For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text. For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text. Finally,",
    "output": "When these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024).\nPositional Embeddings\nFor positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha.\nIn the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution.\nFor example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2.\nOptimisation using kv_cache\nVALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer.\nA Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time.\nTo optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step.\nThis kv_cache mechanism significantly speeds up the operation of the Transformer.\nencodec_24khz: 93.2MB"
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nto better verify our proposed VALL-E X systems, we adopt the open-source NISQA3 [Mittag and Möller, 2021] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS). 5",
    "output": "When these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024).\nPositional Embeddings\nFor positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha.\nIn the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution.\nFor example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2.\nOptimisation using kv_cache\nVALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer.\nA Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time.\nTo optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step.\nThis kv_cache mechanism significantly speeds up the operation of the Transformer.\nencodec_24khz: 93.2MB"
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: Experiments We evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5, OpenSubtitles20186 and WMT20207, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama8, and news domains, respectively. The EnglishChinese ST data is from GigaST [Ye et al., 2022], which is created by translating the transcripts in GigaSpeech [Chen et al., 2021] using a strong machine translation system. Similarly, we create the ChineseEnglish ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above. 3https://github.com/gabrielmittag/NISQA 4https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech 5https://challenger.ai/competition/translation 6https://opus.nlpl.eu/OpenSubtitles2018.php 7https://www.statmt.org/wmt20/translation-task.html 8http://www.opensubtitles.org/ We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [Wester, 2010], which contains bilingual Chinese/English speech recorded by the same speakers. There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350. Zero-shot cross-lingual TTS is evaluated using Librispeech [Panayotov et al., 2015] dev-clean set and EMIME dataset providing English and Chinese data, respectively. We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts. 5.2 Experimental Setup Phonemization & Quantization The right picture of Figure 2 illustrates the phonemization & quantization processes for different languages. All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets. We use a unified phoneme set called BigCiDian9 for two languages which are based on International Phonetic Alphabet (IPA). The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools10 for additional alignment information used for the pre-training of speech recognition & translation model. The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model EnCodec11, which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks.",
    "output": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nModel Architecture For the cross-lingual codec language models, MAR and MNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096. The autoregression is implemented by attention masking in the MAR model. Sinuous position embedding is separately computed for each prompt sequence in MAR and MNAR models. Besides, the M",
    "output": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nNAR model uses individual layer normalization for generating each layer of acoustic tokens. We also introduce the model architecture of speech recognition & translation for S2ST in Appendix A.1.2. We call our cross-lingual TTS model and S2ST model as VALL-E X and VALL-E X Trans in the subsequent experiments, respectively. Training Details We optimize each module of VALL-E X individually, including MAR and MNAR. For both modules, The maximum sentence length is set to 20 seconds, so we re-segment the LibriLight data to an average utterance duration of 12 seconds by detecting the consecutive silence phonemes. Fortunately, the WenetSpeech data has already been segmented into short utterances. The maximum learning rate is 5e-4 with warm-up steps of 8,000. The models are trained on 32 V100 GPUs for 800k steps. MAR is trained with the batch size of 120 seconds per GPU, which is 66 seconds for MNAR due to the memory constraint. When optimizing MNAR, instead of accumulating all layer’s loss in Eqn. (2), we randomly select one layer at each optimization step for efficiency. For speech recognition & translation model, the training details can be found in Appendix A.1.3. Baselines We adopt YourTTS12 [Casanova et al., 2022] as our baseline for zero-shot cross-lingual TTS. YourTTS is a zero-shot multi-speaker TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech. Since previous work shows that current end-to-end S2ST systems underperform cascaded S2ST systems [Jia et al., 2022a, Lee et al., 2021b], we also build an S2ST baseline which is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS. The source speech serves as the reference speech when synthesizing the target speech using YourTTS. The ASR model is the released HuBERT model introduced in Section 4.3, and the MT model is a vanilla Transformer trained by ourselves on the MT data introduced in Section 5.1. Since YourTTS is built only for English, we don’t get its performance for EnglishChinese translation direction. Table 2: Zero-shot cross-lingual TTS evaluation for English TTS with Chinese speech as prompts and Chinese TTS with English speech as prompts, using automatic evaluation matrices, including ASV-Score (hypothesis vs. prompt), ASR-WER, and Naturalness. ASV-Score ASR-WER Naturalness English TTS with Chinese as prompts Baseline (YourTTS) 0.300.10 8.53 3.36 VALL-E X 0.360.11 4.07 3.54 Chinese TTS with English as prompts VALL-E X 0.290.10 8.52 3.36 5.3 Zero-Shot Cross-Lingual TTS Evaluation We first select samples with a length between 4 and 10 seconds from LibriSpeech dev-clean set, resulting in 40 speakers and 1373 samples. For English TTS, we randomly select one audio from EMIME set as the Chinese prompt for each target sentence in LibriSpeech dev-clean set. For Chinese TTS, we use extra 149 Chinese text sentences provided by the EMIME set and repeat them to the total number of 1373 so that they can be prompted by the LibriSpeech audios one-by-one. When synthesizing the target language speech, the whole sequence of the source language speech is used as the prompt. Automatic Evaluation Table 2 summarizes the results of cross-lingual zero-shot TTS tasks, including English TTS prompted by Chinese speech and Chinese TTS prompted by English speech. We measure the speaker similarity using the automatic speaker verification (ASV) model, ranging from -1 to +1 given two speech utterances. The larger the value, the more similar the speakers of",
    "output": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nthe two utterances are. The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of VALL-E X is superior to that of the baseline (0.36 vs 0.30). (2) VALL-E X reduces the WER significantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method",
    "output": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\n. (3) VALL-E X has better speech naturalness than the baseline thanks to the large-scale training data and the large language model capacity. The results of Chinese TTS with English prompts are also listed. Table 3: Human evaluation for zero-shot cross-lingual TTS. SMOS means similarity MOS between generated speech and prompt, and CMOS means comparative MOS based on Baseline. SMOS CMOS (v.s. Baseline) Baseline (YourTTS) 3.420.19 0.00 VALL-E X 4.000.20 +0.24 Human Evaluation We further conduct the human evaluation on 50 randomly selected speech records for zero-shot cross-lingual English TTS with Chinese speech as prompts, including SMOS and CMOS. Note that SMOS ranges from 1 to 5 where the larger the value, the higher the voice similarity, and CMOS ranges from -3 to 3 where the positive number means the new system is better than the baseline. The results are listed in Table 3. Baseline gets 3.42 SMOS scores between generated speech and prompts, while our VALL-E X achieves 4.00, which further demonstrates the model’s superiority in keeping the speech characteristic in the cross-lingual setting. Moreover, to directly compare the speech synthesis quality between the proposed VALL-E X and baseline, we calculate the CMOS score between them evaluated by native speakers on the 50 sentences. The last column of Table 3 shows that VALL-E X obtains the gain of +0.24 CMOS scores than the baseline. Table 4: S2ST performance on EMIME dataset for ChineseEnglish directions. Baseline is a cascaded S2ST system based on speaker embedding. Automatic evaluation matrices include ASVScore, ASR-BLEU, and Naturalness. ASV-Score ASR-BLEU Naturalness tgt vs. src hyp vs. src hyp vs. tgt ChineseEnglish S2ST 0.280.10 0.270.12 27.49 3.44 - w/ oracle target text 0.280.10 0.290.11 80.30 3.43 VALL-E X Trans 0.370.10 0.370.11 30.66 3.54 - w/ oracle target text 0.390.10 0.380.10 86.78 3.54 Baseline (S2ST) 0.580.09 EnglishChinese S2ST VALL-E X Trans 0.580.09 0.480.11 0.530.11 34.45 3.41 - w/ oracle target text 0.470.12 0.550.11 84.00 3.42 Speaker Similarity We first evaluate whether the speaker’s voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in Table 4. Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs. src, hyp vs. src, and hyp vs. tgt). From Table 4 we can find that: (1) For ChineseEnglish, the ASV score of VALL-E X Trans significantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker’s voice. (2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs. src) for the EnglishChinese direction, which suggests that the cross-lingual voice transferability still has the improvement space. (3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not",
    "output": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nincrease notably, indicating that voice transferability is less affected by the quality of translation. Translation Quality Table 4 also shows the translation performance of VALL-E X Trans. Note that ASR-BLEU with oracle target text as the input of VALL-E X can be seen as the upper bound when translations are exactly correct. With oracle target text as input, V",
    "output": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Experiments_/_Results_chunk4",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nALL-E X Trans can achieve the performance of about 8487 BLEU scores, which also reflects the high performance of our neural codec language model. For ChineseEnglish, VALL-E X Trans achieves higher BLEU over the baseline (30.66 vs. 27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task. Speech Naturalness We also evaluate the Naturalness with the open-source NISQA [Mittag and Möller, 2021] for S2ST outputs. As shown in the last column of Table 4, compared to the baseline, VALL-E X Trans achieves a better naturalness score (3.54 vs. 3.44), which shows that VALL-E X can generate more natural target language speech than the baseline. Human Evaluation We randomly sample 56 translation pairs13 to perform a human evaluation using SMOS and MOS matrics for both ChineseEnglish and EnglishChinese directions. Table 5 lists the results of VALL-E X Trans as well as the ChineseEnglish baseline. We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation. For speaker similarity evaluation, VALL-E X Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs. 3.06), demonstrating its superior ability to model speaker property of the proposed VALL-E X. Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91). For speech quality, our VALL-E X slightly outperforms the baseline in ChineseEnglish S2ST in terms of MOS score (3.87 vs. 3.81). Table 5: Subjection evaluation with SMOS and MOS scores on bidirectional ChineseEnglish S2ST tasks. SMOS is measured by comparing with the ground-truth target speech. EnglishChinese S2ST baseline is not reported since it is not supported by the released YourTTS. ChineseEnglish EnglishChinese SMOS MOS SMOS MOS Baseline (S2ST) 3.060.14 3.810.19 - - VALL-E X Trans 4.120.13 3.870.21 3.940.15 3.480.13 Source speech prompt 4.910.05 - 4.640.06 - Oracle target speech - 3.920.17 - 3.880.13 5.5 Analysis In this section, we first analyze the effect of language ID, then explore the foreign accent problems, and qualitatively investigate the ability to maintain voice emotion and synthesize code-switch speech of our proposed model. Effect of Language ID Our VALL-E X is trained with multi-lingual ASR data, which might increase the modeling difficulty for each specific language. We address it by adding language IDs to guide speech synthesis in the autoregressive language codec model. Here, we verify the effectiveness by removing the language ID (LID) or adding the wrong LID (i.e. the source LID). The ASV-Score and ASR-BLEU are reported in Table 6. Without LID or with the wrong language ID, the translation quality decreases, while the speaker similarity between the hypothesis and source speech increases. These results demonstrate the importance of language ID for the accuracy of the content. It also indicates that target LID reduces the transfer of information, which means the model without LID or with source LID will better maintain the sound of the original speaker. Table 6: Evaluation for the effect of language ID on ChineseEnglish EMIME dataset. ASV-Score is computed between synthesized speech and source prompt speech. The last column lists the subjection evaluation score of the foreign accent (from 1 to 5 scores). ASV-Score (vs. src) ASR-BLEU Accent Score ChineseEnglish S2ST VALL-E X Trans 0.370.10 30.66 4.10 w/o Language ID 0.410.10 29.04 2.98 w/ wrong Language ID 0.410.10 29.07 2.55 EnglishChinese S2ST VALL-E X Trans 0.480.11 34.45 4.03 w/o Language ID 0.490.11 30.86 2.35 w/ wrong Language ID 0.500.11 29.70 2.25 Foreign Accent Control L2 (second-language, or foreign) accent",
    "output": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Experiments_/_Results_chunk5",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nproblem, the synthesized speech sounds like the accents of a foreigner, has arisen in cross-lingual TTS systems [Zhang et al., 2019, Lee et al., 2022]. Automatic Evaluation has shown that adding LID can boost speech quality. Besides, we conduct a subjection evaluation to label foreign accents from 1 to 5 on randomly selected 20 synthesized speech for both English and Chinese, where each sample is measured with a score from 1 to 5 denoting high-status foreign speakers, low-status foreign speakers, middle-status speakers, low-status native speakers, and high-status native speakers, respectively. As summarized in the last column of Table 6, we observed that our VALL-E X can control the accent for the target speech by LID modules. For example, in EnglishChinese, VALL-E X Trans with right LID and without LID get the score of 4.03 and 2.35, respectively. This indicates that by using correct LID embedding, VALL-E X Trans is able to alleviate the foreign accent problem. Please also see the demo for audio examples of VALL-E X Trans with or without language ID. [Um et al., 2020]. Moreover, it is more tempting to reserve the source speaker’s emotion in generated target speech for the S2ST task, which is not explored in previous S2ST work. In these experiments, we adopt the source prompts from the emotional voices dataset EmoV-DB [Um et al., 2020] as inputs of VALL-E X Trans to generate the translated target speech, whose samples are listed on our demo page. We found that the proposed VALL-E X can maintain emotional consistency to a certain extent between the source prompt and the synthesized speech. The underlying reasons are (1) our VALL-E X is trained with large-scale multi-lingual multi-speaker speech-transcription data, which contains various emotional speech records, and (2) the strong in-context learning ability of VALL-E X, like GPT-3 [Brown et al., 2020], promotes the generated speech to reserve the characteristic of the source prompt. Code-Switch Speech Synthesis It is a common phenomenon to use code-switch utterances in bilingual or multi-lingual communities [Cao et al., 2020, Zhao et al., 2020, Manghat et al., 2022]. Code-switch speech synthesis aims to produce a fluent and consistent voice for code-switch text. Although our proposed VALL-E X is trained on multiple monolingual speech data, without special optimization for code-switch setting, VALL-E X provides a promising solution to code-switch speech synthesis. We put the code-switch samples on our demo page, demonstrating that due to its strong in-context learning ability, VALL-E X can synthesize fluent code-switch speech with a consistent voice. 6",
    "output": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."
  },
  {
    "id_paper": "09",
    "id_story": "09-05",
    "id_chunk": "09-05_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion In this work, we propose VALL-E X, a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. VALL-E X is free of the requirement for cross-lingual paired data from the same speakers. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.",
    "output": "RVC is a voice-to-voice model suitable for applications like live chat in games. If this kind of voice-to-voice process is needed, VALL-E-X, being a text-to-speech model, requires converting the original voice to text using tools like Whisper before synthesizing speech.\nContext\nTo use a reference voice, enter the audio file and the transcription of the audio file in the transcript option. The transcription of the voice file can be generated by Whisper or other software if necessary."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Abstract\nAbstract: Abstract We propose a cross-lingual neural codec language model, VALL-E X, for crosslingual speech synthesis. Specifically, we extend VALL-E [Wang et al., 2023] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-tospeech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at https://aka.ms/vallex. VALL-E X (Cross-Lingual VALL-E) Cross-Lingual Neural Codec Language Modeling Multilingual G2P Multilingual G2P Audio Codec Encoder Source Text Target Text Source Speech Figure 1: The overall framework of VALL-E X, which can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training, and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation. Personalized Target Speech Audio Codec Decoder Language ID Source phoneme sequences Target phoneme sequences Source acoustic tokens Target acoustic tokens 1",
    "output": "In the new paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling, a Microsoft research team presents VALL-E X, a simple yet effective cross-lingual neural codec language model that inherits strong in-context learning capabilities from the VALL-E TTS model and demonstrates high-quality zero-shot cross-lingual speech synthesis performance.\nBased on the learned cross-lingual speech modelling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks.\nThe trained VALL-E X is thus able — prompted by a single sentence spoken in the source language — to generate high-quality cross-lingual speech that maintains the source speaker’s voice characteristics, such as emotion and other background information."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Introduction\nIntroduction: Introduction Recent years have witnessed significant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity [Li et al., 2019, Ren et al., 2019, Tan et al., 2022]. However, these models can only generate high-quality speech for a specific speaker in a specific language. Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. Previous methods to tackle these challenges typically augment end-to-end TTS models with specific subnets for speaker and language control [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2020, Ellinas et al., 2022, Cai et al., 2023]. For example, based on the multi-speaker TTS model, Nachmani and Wolf [2019] introduce multiple encoders for each language and additional loss to keep the speaker’s identity. Zhang et al. [2020] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities. Yang and He [2020] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively. Yang and He [2022] further propose a multi-task learning method with additional tasks of speaker similarity and language identification. Moreover, Cai et al. [2023] investigates cross-lingual multi-speaker text-to-speech synthesis with sufficient or limited bilingual speech training data. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [Zhang et al., 2019, Lee et al., 2022]. Table 1: A comparison between VALL-E X and previous cross-lingual TTS systems. Previous Systems VALL-E X Intermediate representation Mel spectrogram Audio codec codes Training data 13K hours Speech accent Foreign Native Speaker similarity Relative low High In-context learning Zero-shot cross-lingual TTS In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, VALL-E X, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis. Based on the knowledge learned from large-scale multi-lingual speech data, VALL-E X is able to transfer the speech characteristics, including the speaker’s voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems. More specifically, we first obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data. Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an offline neural codec encoder. Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model. As illustrated in Figure 1, after training, VALL-E X can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language. The generated acoustic token sequence is decompressed to the target speech waveform by an offline audio codec",
    "output": "Not so long ago, text-to-speech (TTS) outputs were disappointingly deadpan and robotic. The leveraging of deep neural networks in recent years has dramatically transformed TTS, enabling conditioning on factors such as stress and intonation to achieve higher quality and much more humanlike results. Contemporary TTS models however still perform best when dealing with a specific speaker in a specific language. Cross-lingual speech synthesis, which aims to transfer the characteristics of a user’s voice from one language to another, has remained relatively underexplored. That just changed.\nThis paper introduces a promising model with strong potential for cross-lingual speech synthesis. The debut version of VALL-E X was trained on large-scale multi-speaker speech-transcription data in Chinese and English, but the researchers plan to expand their approach with additional data and languages in the future."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Introduction\ndecoder. VALL-E X is trained on two large-scale multi-speaker datasets1, LibriLight [Kahn et al., 2020] and WenetSpeech [Zhang et al., 2022a], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively. The combination of LibriLight and Wenet",
    "output": "Not so long ago, text-to-speech (TTS) outputs were disappointingly deadpan and robotic. The leveraging of deep neural networks in recent years has dramatically transformed TTS, enabling conditioning on factors such as stress and intonation to achieve higher quality and much more humanlike results. Contemporary TTS models however still perform best when dealing with a specific speaker in a specific language. Cross-lingual speech synthesis, which aims to transfer the characteristics of a user’s voice from one language to another, has remained relatively underexplored. That just changed.\nThis paper introduces a promising model with strong potential for cross-lingual speech synthesis. The debut version of VALL-E X was trained on large-scale multi-speaker speech-transcription data in Chinese and English, but the researchers plan to expand their approach with additional data and languages in the future."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Introduction\nSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which significantly improves the coverage of different speakers and enhances VALL-E X’s generalization capacity. The comparison between VALL-E X and the previous cross-lingual TTS systems are listed in Table 1. We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot crosslingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST). For cross-lingual text-to-speech synthesis, the proposed VALL-E X is evaluated with LibriSpeech [Panayotov et al., 2015] and EMIME [Wester, 2010] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers. For zero-shot speech-to-speech translation, EMIME [Wester, 2010] dataset is used for the evaluation of VALL-E X on bidirectional ChineseEnglish translation tasks, and it contains bilingual audio recordings by the same speakers. We evaluate the proposed VALL-E X framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS). Specifically, due to the strong in-context learning capability, VALL-E X achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker. By training on large-scale speech-transcription data, the proposed VALL-E X significantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness. Furthermore, the human evaluation shows that our VALL-E X outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST). Our contributions can be summarized as follows: • We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data. VALL-E X is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts. • The multi-lingual in-context learning framework enables VALL-E X to generate crosslingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language. • Based on the learned cross-lingual speech modeling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks. • We apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation. We encourage readers to listen to the audio samples on our demo page: https://aka.ms/vallex. 2",
    "output": "Not so long ago, text-to-speech (TTS) outputs were disappointingly deadpan and robotic. The leveraging of deep neural networks in recent years has dramatically transformed TTS, enabling conditioning on factors such as stress and intonation to achieve higher quality and much more humanlike results. Contemporary TTS models however still perform best when dealing with a specific speaker in a specific language. Cross-lingual speech synthesis, which aims to transfer the characteristics of a user’s voice from one language to another, has remained relatively underexplored. That just changed.\nThis paper introduces a promising model with strong potential for cross-lingual speech synthesis. The debut version of VALL-E X was trained on large-scale multi-speaker speech-transcription data in Chinese and English, but the researchers plan to expand their approach with additional data and languages in the future."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Method\nMethod: 3.2 Model Framework Inspired by VALL-E, the cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM MAR is only used to predict the acoustic tokens from the first quantizer of EnCodec model. Formally, based on paired speech-transcription data in any language, let S denote the transcribed phoneme sequence, and A:,1 ai,1|i = 1, . . . , N denotes the first-layer acoustic tokens extracted from the speech X. The decoder MAR, modeling the concatenated sequence S, A:,1, is trained to predict A:,1 autoregressively. It is optimized by maximizing the log-likelihood, Quantization & Phonemization: Quantized tokens Layer L 12 504 31 8 743 ... A:,2:L ...... Layer 2 325 71 435 90 7 ... A:,1 731 284 78 32 669 ... Layer 1 audio codec encoder (Quantization) Multi-lingual speech-transcription pairs (phonemization) G2P tool Phoneme tokens HH AH L OW ... D S N Y Multi-lingual Non-Autoregressive Codec LM Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM MNAR is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer. It is prompted by the phoneme sequence of the current sentence (S) and the acoustic token sequence of another sentence with the same speaker (  A). Here  A is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer l [2, 8], the embeddings of l 1 layers’ acoustic tokens (A:,1:l1) are summed up layerwise as input. The learning objective for the l-layer acoustic tokens A:,l can be calculated as 8 X l=2 log pNAR A:,l | D S,  A:,1:8, A:,1:l1 E ; MNAR (2) LMNAR = where means the sequence concatenation. pNAR(.) computes the pointwise probabilities of A:,l. 3.3 Multi-lingual Training In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-tospeech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (Ss, As) and (St, At) to train our multi-lingual codec LMs MAR and MNAR, where s and t represent two different (source and target) languages. Language ID Module Following multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in VALL-E X. On the one hand, without language ID",
    "output": "Not so long ago, text-to-speech (TTS) outputs were disappointingly deadpan and robotic. The leveraging of deep neural networks in recent years has dramatically transformed TTS, enabling conditioning on factors such as stress and intonation to achieve higher quality and much more humanlike results. Contemporary TTS models however still perform best when dealing with a specific speaker in a specific language. Cross-lingual speech synthesis, which aims to transfer the characteristics of a user’s voice from one language to another, has remained relatively underexplored. That just changed.\n\nIn the new paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling, a Microsoft research team presents VALL-E X, a simple yet effective cross-lingual neural codec language model that inherits strong in-context learning capabilities from the VALL-E TTS model and demonstrates high-quality zero-shot cross-lingual speech synthesis performance.\n\n\nThe team summarizes their main contributions as follows:\n\nWe develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data.\nThe multilingual in-context learning framework enables VALL-E X to generate cross-lingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language.\nBased on the learned cross-lingual speech modelling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks.\nWe apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat strong baselines in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nThe proposed VALL-E X is built upon VALL-E, a neural codec language model Microsoft introduced in January that demonstrates strong in-context learning capabilities and achieves state-of-the-art TTS synthesis performance. This study extends VALL-E to enable zero-shot cross-lingual and cross-lingual TTS or speech-to-speech translation (S2ST) capabilities.\n\n\nThe team first extracts multilingual speech-transcription data from ASR (automatic speech recognition) data or pseudo-labelled speech data. They then employ a rule-based converter (a grapheme-to-phoneme conversion / G2P tool) to convert the transcriptions to phoneme sequences; and an offline neural codec encoder to convert the speech data to acoustic tokens. Finally, they train a multilingual conditional language model using the paired phoneme and acoustic token sequences of each language.\n\nThe trained VALL-E X is thus able — prompted by a single sentence spoken in the source language — to generate high-quality cross-lingual speech that maintains the source speaker’s voice characteristics, such as emotion and other background information.\n\n\n\nIn their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness.\n\nThis paper introduces a promising model with strong potential for cross-lingual speech synthesis. The debut version of VALL-E X was trained on large-scale multi-speaker speech-transcription data in Chinese and English, but the researchers plan to expand their approach with additional data and languages in the future.\n\nAudio samples are here, and the code is available on the project’s GitHub. The paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling is on arXiv."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Method\n, VALL-E X may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM MAR is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens. 3.4 Cross-Lingual Inference After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we first concatenate source phonemes Ss and target phonemes St as prompts, and take the first-layer source acoustic tokens As :,1 as the decoding prefix, condition on which the multi-lingual autoregressive codec LM MAR generates the first-layer target acoustic tokens At :,1, at i,1 pAR  at i,1 | Ss, St, As :,1, At i,1 ; MAR , i = 1, . . . , (3) where means probability-based sampling. The sampling is stopped until the end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the final generated speech. After obtaining the first-layer target acoustic tokens At :,1 from MAR, multi-lingual non-autoregressive codec LM MNAR is used to predict the rest layers of acoustic tokens n At :,l | l = 2, . . . , 8 o by greedy search, i.e., choosing the tokens with maximum probabilities, At :,l = argmax At :,l pNAR  At :,l | St, As :,1:8, At :,1:l1 ; MNAR , l = 2, . . . , 8. (4) Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens At :,1:8. tl= 2, ... , 8 VALL-E X Inference: A:,l Multi-lingual Non-Autoregressive Codec LM t s A:,1:l1 St A:,1:8 t A:,1 Multi-lingual Autoregressive Codec LM B t aN,1 t a2,1 t ... s a1,1 bos> Ss St A:,1 Figure 3: Inference illustration of the cross-lingual neural codec language model VALL-E X, with two-stage decoding strategies. VALL-E X can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks. 4 VALL-E X Application VALL-E X can be applied to various cross-lingual speech generation tasks. In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in Figure 3. 4.1 Zero-Shot Cross-Lingual TTS The proposed VALL-E X is naturally suitable for zero-shot cross-lingual TTS tasks. Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker’s voice. Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability. Thanks to the in-context learning capability of large language models, VALL-E X surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis. More specifically, given the source speech, source transcript, and target text, we first convert source speech into source acoustic token As using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes Ss and target phonemes St using G2P tool. More specifically, as introduced in",
    "output": "Not so long ago, text-to-speech (TTS) outputs were disappointingly deadpan and robotic. The leveraging of deep neural networks in recent years has dramatically transformed TTS, enabling conditioning on factors such as stress and intonation to achieve higher quality and much more humanlike results. Contemporary TTS models however still perform best when dealing with a specific speaker in a specific language. Cross-lingual speech synthesis, which aims to transfer the characteristics of a user’s voice from one language to another, has remained relatively underexplored. That just changed.\n\nIn the new paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling, a Microsoft research team presents VALL-E X, a simple yet effective cross-lingual neural codec language model that inherits strong in-context learning capabilities from the VALL-E TTS model and demonstrates high-quality zero-shot cross-lingual speech synthesis performance.\n\n\nThe team summarizes their main contributions as follows:\n\nWe develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data.\nThe multilingual in-context learning framework enables VALL-E X to generate cross-lingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language.\nBased on the learned cross-lingual speech modelling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks.\nWe apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat strong baselines in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nThe proposed VALL-E X is built upon VALL-E, a neural codec language model Microsoft introduced in January that demonstrates strong in-context learning capabilities and achieves state-of-the-art TTS synthesis performance. This study extends VALL-E to enable zero-shot cross-lingual and cross-lingual TTS or speech-to-speech translation (S2ST) capabilities.\n\n\nThe team first extracts multilingual speech-transcription data from ASR (automatic speech recognition) data or pseudo-labelled speech data. They then employ a rule-based converter (a grapheme-to-phoneme conversion / G2P tool) to convert the transcriptions to phoneme sequences; and an offline neural codec encoder to convert the speech data to acoustic tokens. Finally, they train a multilingual conditional language model using the paired phoneme and acoustic token sequences of each language.\n\nThe trained VALL-E X is thus able — prompted by a single sentence spoken in the source language — to generate high-quality cross-lingual speech that maintains the source speaker’s voice characteristics, such as emotion and other background information.\n\n\n\nIn their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness.\n\nThis paper introduces a promising model with strong potential for cross-lingual speech synthesis. The debut version of VALL-E X was trained on large-scale multi-speaker speech-transcription data in Chinese and English, but the researchers plan to expand their approach with additional data and languages in the future.\n\nAudio samples are here, and the code is available on the project’s GitHub. The paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling is on arXiv."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Method\nSection 3.4, we let St be the phonemes extracted from the target text, Ss and As be the phonemes and acoustic tokens extracted from the source speech. Then VALL-E X generates the full-layer target acoustic tokens, which are finally decompressed into the target speech by EnCodec decoder. 4.2 Zero-Shot Speech-to-Speech Translation We can also apply our VALL-E X to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences. Speech Recognition & Translation Model We leverage the improved SpeechUT [Zhang et al., 2022c] as our speech recognition & translation model, which is a unified-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss) and MT corpus (source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more As Application: Zero-shot cross-lingual TTS Ss St Codec Encoder G2P Tool G2P Tool Source text Target text Source speech Application: Zero-shot S2ST Ss St As E Codec Encoder Semantic Semantic Encoder Decoder Speech Encoder Source Speech pre-training details about this model. After pre-training, the model is fine-tuned with (X s, Ss, St) triplet data derived from the ST corpus. Specifically, we perform multi-task learning with the CTC [Graves et al., 2006] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes. Inference Figure 3 shows the inference process of speech-to-speech translation. Given a source speech X s, the speech recognition & translation model first generates the source phonemes Ss from the semantic encoder and the target phonemes St from the semantic decoder. Besides, we use the EnCodec encoder to compress X s into source acoustic tokens As. Then, we concatenate Ss, St, and As, as the input of VALL-E X, to produce the acoustic token sequence for the target speech, as introduced in Section 3.4. The generated acoustic tokens are converted to the final target speech with the decoder of EnCodec. 4.3 Evaluation The proposed model is verified using various evaluation criteria, including speaker similarity (ASVScore), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation. Specifically, we measure speaker similarity between synthesized target speech and groudtruth target speech or source speech as an automatic speaker verification (ASV) task, where a WavLM [Chen et al., 2022] based ASV model is used to calculate the score. To verify the quality of generated speech, we first utilize the ASR system from the released HuBERT-Large model [Hsu et al., 2021] to recognize it into text. For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text. For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text. Finally,",
    "output": "Not so long ago, text-to-speech (TTS) outputs were disappointingly deadpan and robotic. The leveraging of deep neural networks in recent years has dramatically transformed TTS, enabling conditioning on factors such as stress and intonation to achieve higher quality and much more humanlike results. Contemporary TTS models however still perform best when dealing with a specific speaker in a specific language. Cross-lingual speech synthesis, which aims to transfer the characteristics of a user’s voice from one language to another, has remained relatively underexplored. That just changed.\n\nIn the new paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling, a Microsoft research team presents VALL-E X, a simple yet effective cross-lingual neural codec language model that inherits strong in-context learning capabilities from the VALL-E TTS model and demonstrates high-quality zero-shot cross-lingual speech synthesis performance.\n\n\nThe team summarizes their main contributions as follows:\n\nWe develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data.\nThe multilingual in-context learning framework enables VALL-E X to generate cross-lingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language.\nBased on the learned cross-lingual speech modelling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks.\nWe apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat strong baselines in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nThe proposed VALL-E X is built upon VALL-E, a neural codec language model Microsoft introduced in January that demonstrates strong in-context learning capabilities and achieves state-of-the-art TTS synthesis performance. This study extends VALL-E to enable zero-shot cross-lingual and cross-lingual TTS or speech-to-speech translation (S2ST) capabilities.\n\n\nThe team first extracts multilingual speech-transcription data from ASR (automatic speech recognition) data or pseudo-labelled speech data. They then employ a rule-based converter (a grapheme-to-phoneme conversion / G2P tool) to convert the transcriptions to phoneme sequences; and an offline neural codec encoder to convert the speech data to acoustic tokens. Finally, they train a multilingual conditional language model using the paired phoneme and acoustic token sequences of each language.\n\nThe trained VALL-E X is thus able — prompted by a single sentence spoken in the source language — to generate high-quality cross-lingual speech that maintains the source speaker’s voice characteristics, such as emotion and other background information.\n\n\n\nIn their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness.\n\nThis paper introduces a promising model with strong potential for cross-lingual speech synthesis. The debut version of VALL-E X was trained on large-scale multi-speaker speech-transcription data in Chinese and English, but the researchers plan to expand their approach with additional data and languages in the future.\n\nAudio samples are here, and the code is available on the project’s GitHub. The paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling is on arXiv."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Method\nto better verify our proposed VALL-E X systems, we adopt the open-source NISQA3 [Mittag and Möller, 2021] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS). 5",
    "output": "Not so long ago, text-to-speech (TTS) outputs were disappointingly deadpan and robotic. The leveraging of deep neural networks in recent years has dramatically transformed TTS, enabling conditioning on factors such as stress and intonation to achieve higher quality and much more humanlike results. Contemporary TTS models however still perform best when dealing with a specific speaker in a specific language. Cross-lingual speech synthesis, which aims to transfer the characteristics of a user’s voice from one language to another, has remained relatively underexplored. That just changed.\n\nIn the new paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling, a Microsoft research team presents VALL-E X, a simple yet effective cross-lingual neural codec language model that inherits strong in-context learning capabilities from the VALL-E TTS model and demonstrates high-quality zero-shot cross-lingual speech synthesis performance.\n\n\nThe team summarizes their main contributions as follows:\n\nWe develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data.\nThe multilingual in-context learning framework enables VALL-E X to generate cross-lingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language.\nBased on the learned cross-lingual speech modelling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks.\nWe apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat strong baselines in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nThe proposed VALL-E X is built upon VALL-E, a neural codec language model Microsoft introduced in January that demonstrates strong in-context learning capabilities and achieves state-of-the-art TTS synthesis performance. This study extends VALL-E to enable zero-shot cross-lingual and cross-lingual TTS or speech-to-speech translation (S2ST) capabilities.\n\n\nThe team first extracts multilingual speech-transcription data from ASR (automatic speech recognition) data or pseudo-labelled speech data. They then employ a rule-based converter (a grapheme-to-phoneme conversion / G2P tool) to convert the transcriptions to phoneme sequences; and an offline neural codec encoder to convert the speech data to acoustic tokens. Finally, they train a multilingual conditional language model using the paired phoneme and acoustic token sequences of each language.\n\nThe trained VALL-E X is thus able — prompted by a single sentence spoken in the source language — to generate high-quality cross-lingual speech that maintains the source speaker’s voice characteristics, such as emotion and other background information.\n\n\n\nIn their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness.\n\nThis paper introduces a promising model with strong potential for cross-lingual speech synthesis. The debut version of VALL-E X was trained on large-scale multi-speaker speech-transcription data in Chinese and English, but the researchers plan to expand their approach with additional data and languages in the future.\n\nAudio samples are here, and the code is available on the project’s GitHub. The paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling is on arXiv."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Experiments / Results\nExperiments / Results: Experiments We evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5, OpenSubtitles20186 and WMT20207, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama8, and news domains, respectively. The EnglishChinese ST data is from GigaST [Ye et al., 2022], which is created by translating the transcripts in GigaSpeech [Chen et al., 2021] using a strong machine translation system. Similarly, we create the ChineseEnglish ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above. 3https://github.com/gabrielmittag/NISQA 4https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech 5https://challenger.ai/competition/translation 6https://opus.nlpl.eu/OpenSubtitles2018.php 7https://www.statmt.org/wmt20/translation-task.html 8http://www.opensubtitles.org/ We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [Wester, 2010], which contains bilingual Chinese/English speech recorded by the same speakers. There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350. Zero-shot cross-lingual TTS is evaluated using Librispeech [Panayotov et al., 2015] dev-clean set and EMIME dataset providing English and Chinese data, respectively. We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts. 5.2 Experimental Setup Phonemization & Quantization The right picture of Figure 2 illustrates the phonemization & quantization processes for different languages. All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets. We use a unified phoneme set called BigCiDian9 for two languages which are based on International Phonetic Alphabet (IPA). The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools10 for additional alignment information used for the pre-training of speech recognition & translation model. The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model EnCodec11, which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks.",
    "output": "In their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Experiments / Results\nModel Architecture For the cross-lingual codec language models, MAR and MNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096. The autoregression is implemented by attention masking in the MAR model. Sinuous position embedding is separately computed for each prompt sequence in MAR and MNAR models. Besides, the M",
    "output": "In their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Experiments / Results\nNAR model uses individual layer normalization for generating each layer of acoustic tokens. We also introduce the model architecture of speech recognition & translation for S2ST in Appendix A.1.2. We call our cross-lingual TTS model and S2ST model as VALL-E X and VALL-E X Trans in the subsequent experiments, respectively. Training Details We optimize each module of VALL-E X individually, including MAR and MNAR. For both modules, The maximum sentence length is set to 20 seconds, so we re-segment the LibriLight data to an average utterance duration of 12 seconds by detecting the consecutive silence phonemes. Fortunately, the WenetSpeech data has already been segmented into short utterances. The maximum learning rate is 5e-4 with warm-up steps of 8,000. The models are trained on 32 V100 GPUs for 800k steps. MAR is trained with the batch size of 120 seconds per GPU, which is 66 seconds for MNAR due to the memory constraint. When optimizing MNAR, instead of accumulating all layer’s loss in Eqn. (2), we randomly select one layer at each optimization step for efficiency. For speech recognition & translation model, the training details can be found in Appendix A.1.3. Baselines We adopt YourTTS12 [Casanova et al., 2022] as our baseline for zero-shot cross-lingual TTS. YourTTS is a zero-shot multi-speaker TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech. Since previous work shows that current end-to-end S2ST systems underperform cascaded S2ST systems [Jia et al., 2022a, Lee et al., 2021b], we also build an S2ST baseline which is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS. The source speech serves as the reference speech when synthesizing the target speech using YourTTS. The ASR model is the released HuBERT model introduced in Section 4.3, and the MT model is a vanilla Transformer trained by ourselves on the MT data introduced in Section 5.1. Since YourTTS is built only for English, we don’t get its performance for EnglishChinese translation direction. Table 2: Zero-shot cross-lingual TTS evaluation for English TTS with Chinese speech as prompts and Chinese TTS with English speech as prompts, using automatic evaluation matrices, including ASV-Score (hypothesis vs. prompt), ASR-WER, and Naturalness. ASV-Score ASR-WER Naturalness English TTS with Chinese as prompts Baseline (YourTTS) 0.300.10 8.53 3.36 VALL-E X 0.360.11 4.07 3.54 Chinese TTS with English as prompts VALL-E X 0.290.10 8.52 3.36 5.3 Zero-Shot Cross-Lingual TTS Evaluation We first select samples with a length between 4 and 10 seconds from LibriSpeech dev-clean set, resulting in 40 speakers and 1373 samples. For English TTS, we randomly select one audio from EMIME set as the Chinese prompt for each target sentence in LibriSpeech dev-clean set. For Chinese TTS, we use extra 149 Chinese text sentences provided by the EMIME set and repeat them to the total number of 1373 so that they can be prompted by the LibriSpeech audios one-by-one. When synthesizing the target language speech, the whole sequence of the source language speech is used as the prompt. Automatic Evaluation Table 2 summarizes the results of cross-lingual zero-shot TTS tasks, including English TTS prompted by Chinese speech and Chinese TTS prompted by English speech. We measure the speaker similarity using the automatic speaker verification (ASV) model, ranging from -1 to +1 given two speech utterances. The larger the value, the more similar the speakers of",
    "output": "In their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Experiments / Results\nthe two utterances are. The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of VALL-E X is superior to that of the baseline (0.36 vs 0.30). (2) VALL-E X reduces the WER significantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method",
    "output": "In their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Experiments / Results\n. (3) VALL-E X has better speech naturalness than the baseline thanks to the large-scale training data and the large language model capacity. The results of Chinese TTS with English prompts are also listed. Table 3: Human evaluation for zero-shot cross-lingual TTS. SMOS means similarity MOS between generated speech and prompt, and CMOS means comparative MOS based on Baseline. SMOS CMOS (v.s. Baseline) Baseline (YourTTS) 3.420.19 0.00 VALL-E X 4.000.20 +0.24 Human Evaluation We further conduct the human evaluation on 50 randomly selected speech records for zero-shot cross-lingual English TTS with Chinese speech as prompts, including SMOS and CMOS. Note that SMOS ranges from 1 to 5 where the larger the value, the higher the voice similarity, and CMOS ranges from -3 to 3 where the positive number means the new system is better than the baseline. The results are listed in Table 3. Baseline gets 3.42 SMOS scores between generated speech and prompts, while our VALL-E X achieves 4.00, which further demonstrates the model’s superiority in keeping the speech characteristic in the cross-lingual setting. Moreover, to directly compare the speech synthesis quality between the proposed VALL-E X and baseline, we calculate the CMOS score between them evaluated by native speakers on the 50 sentences. The last column of Table 3 shows that VALL-E X obtains the gain of +0.24 CMOS scores than the baseline. Table 4: S2ST performance on EMIME dataset for ChineseEnglish directions. Baseline is a cascaded S2ST system based on speaker embedding. Automatic evaluation matrices include ASVScore, ASR-BLEU, and Naturalness. ASV-Score ASR-BLEU Naturalness tgt vs. src hyp vs. src hyp vs. tgt ChineseEnglish S2ST 0.280.10 0.270.12 27.49 3.44 - w/ oracle target text 0.280.10 0.290.11 80.30 3.43 VALL-E X Trans 0.370.10 0.370.11 30.66 3.54 - w/ oracle target text 0.390.10 0.380.10 86.78 3.54 Baseline (S2ST) 0.580.09 EnglishChinese S2ST VALL-E X Trans 0.580.09 0.480.11 0.530.11 34.45 3.41 - w/ oracle target text 0.470.12 0.550.11 84.00 3.42 Speaker Similarity We first evaluate whether the speaker’s voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in Table 4. Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs. src, hyp vs. src, and hyp vs. tgt). From Table 4 we can find that: (1) For ChineseEnglish, the ASV score of VALL-E X Trans significantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker’s voice. (2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs. src) for the EnglishChinese direction, which suggests that the cross-lingual voice transferability still has the improvement space. (3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not",
    "output": "In their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Experiments / Results\nincrease notably, indicating that voice transferability is less affected by the quality of translation. Translation Quality Table 4 also shows the translation performance of VALL-E X Trans. Note that ASR-BLEU with oracle target text as the input of VALL-E X can be seen as the upper bound when translations are exactly correct. With oracle target text as input, V",
    "output": "In their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Experiments_/_Results_chunk4",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Experiments / Results\nALL-E X Trans can achieve the performance of about 8487 BLEU scores, which also reflects the high performance of our neural codec language model. For ChineseEnglish, VALL-E X Trans achieves higher BLEU over the baseline (30.66 vs. 27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task. Speech Naturalness We also evaluate the Naturalness with the open-source NISQA [Mittag and Möller, 2021] for S2ST outputs. As shown in the last column of Table 4, compared to the baseline, VALL-E X Trans achieves a better naturalness score (3.54 vs. 3.44), which shows that VALL-E X can generate more natural target language speech than the baseline. Human Evaluation We randomly sample 56 translation pairs13 to perform a human evaluation using SMOS and MOS matrics for both ChineseEnglish and EnglishChinese directions. Table 5 lists the results of VALL-E X Trans as well as the ChineseEnglish baseline. We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation. For speaker similarity evaluation, VALL-E X Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs. 3.06), demonstrating its superior ability to model speaker property of the proposed VALL-E X. Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91). For speech quality, our VALL-E X slightly outperforms the baseline in ChineseEnglish S2ST in terms of MOS score (3.87 vs. 3.81). Table 5: Subjection evaluation with SMOS and MOS scores on bidirectional ChineseEnglish S2ST tasks. SMOS is measured by comparing with the ground-truth target speech. EnglishChinese S2ST baseline is not reported since it is not supported by the released YourTTS. ChineseEnglish EnglishChinese SMOS MOS SMOS MOS Baseline (S2ST) 3.060.14 3.810.19 - - VALL-E X Trans 4.120.13 3.870.21 3.940.15 3.480.13 Source speech prompt 4.910.05 - 4.640.06 - Oracle target speech - 3.920.17 - 3.880.13 5.5 Analysis In this section, we first analyze the effect of language ID, then explore the foreign accent problems, and qualitatively investigate the ability to maintain voice emotion and synthesize code-switch speech of our proposed model. Effect of Language ID Our VALL-E X is trained with multi-lingual ASR data, which might increase the modeling difficulty for each specific language. We address it by adding language IDs to guide speech synthesis in the autoregressive language codec model. Here, we verify the effectiveness by removing the language ID (LID) or adding the wrong LID (i.e. the source LID). The ASV-Score and ASR-BLEU are reported in Table 6. Without LID or with the wrong language ID, the translation quality decreases, while the speaker similarity between the hypothesis and source speech increases. These results demonstrate the importance of language ID for the accuracy of the content. It also indicates that target LID reduces the transfer of information, which means the model without LID or with source LID will better maintain the sound of the original speaker. Table 6: Evaluation for the effect of language ID on ChineseEnglish EMIME dataset. ASV-Score is computed between synthesized speech and source prompt speech. The last column lists the subjection evaluation score of the foreign accent (from 1 to 5 scores). ASV-Score (vs. src) ASR-BLEU Accent Score ChineseEnglish S2ST VALL-E X Trans 0.370.10 30.66 4.10 w/o Language ID 0.410.10 29.04 2.98 w/ wrong Language ID 0.410.10 29.07 2.55 EnglishChinese S2ST VALL-E X Trans 0.480.11 34.45 4.03 w/o Language ID 0.490.11 30.86 2.35 w/ wrong Language ID 0.500.11 29.70 2.25 Foreign Accent Control L2 (second-language, or foreign) accent",
    "output": "In their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Experiments_/_Results_chunk5",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Experiments / Results\nproblem, the synthesized speech sounds like the accents of a foreigner, has arisen in cross-lingual TTS systems [Zhang et al., 2019, Lee et al., 2022]. Automatic Evaluation has shown that adding LID can boost speech quality. Besides, we conduct a subjection evaluation to label foreign accents from 1 to 5 on randomly selected 20 synthesized speech for both English and Chinese, where each sample is measured with a score from 1 to 5 denoting high-status foreign speakers, low-status foreign speakers, middle-status speakers, low-status native speakers, and high-status native speakers, respectively. As summarized in the last column of Table 6, we observed that our VALL-E X can control the accent for the target speech by LID modules. For example, in EnglishChinese, VALL-E X Trans with right LID and without LID get the score of 4.03 and 2.35, respectively. This indicates that by using correct LID embedding, VALL-E X Trans is able to alleviate the foreign accent problem. Please also see the demo for audio examples of VALL-E X Trans with or without language ID. [Um et al., 2020]. Moreover, it is more tempting to reserve the source speaker’s emotion in generated target speech for the S2ST task, which is not explored in previous S2ST work. In these experiments, we adopt the source prompts from the emotional voices dataset EmoV-DB [Um et al., 2020] as inputs of VALL-E X Trans to generate the translated target speech, whose samples are listed on our demo page. We found that the proposed VALL-E X can maintain emotional consistency to a certain extent between the source prompt and the synthesized speech. The underlying reasons are (1) our VALL-E X is trained with large-scale multi-lingual multi-speaker speech-transcription data, which contains various emotional speech records, and (2) the strong in-context learning ability of VALL-E X, like GPT-3 [Brown et al., 2020], promotes the generated speech to reserve the characteristic of the source prompt. Code-Switch Speech Synthesis It is a common phenomenon to use code-switch utterances in bilingual or multi-lingual communities [Cao et al., 2020, Zhao et al., 2020, Manghat et al., 2022]. Code-switch speech synthesis aims to produce a fluent and consistent voice for code-switch text. Although our proposed VALL-E X is trained on multiple monolingual speech data, without special optimization for code-switch setting, VALL-E X provides a promising solution to code-switch speech synthesis. We put the code-switch samples on our demo page, demonstrating that due to its strong in-context learning ability, VALL-E X can synthesize fluent code-switch speech with a consistent voice. 6",
    "output": "In their empirical study, the team evaluated VALL-E X on zero-shot cross-lingual TTS and zero-shot S2ST. In the evaluations, VALL-E X surpassed the strong baselines with higher speaker similarity scores, lower word error rates, higher BLEU scores and better speech naturalness."
  },
  {
    "id_paper": "09",
    "id_story": "09-02",
    "id_chunk": "09-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Student, Developer, Researcher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion In this work, we propose VALL-E X, a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. VALL-E X is free of the requirement for cross-lingual paired data from the same speakers. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.",
    "output": "We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data.\nThe multilingual in-context learning framework enables VALL-E X to generate cross-lingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language.\nWe apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat strong baselines in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nAudio samples are here, and the code is available on the project’s GitHub. The paper Speak Foreign Languages with Your Own Voice: Cross-Lingual Neural Codec Language Modeling is on arXiv."
  },
  {
    "id_paper": "62",
    "id_story": "62-01",
    "id_chunk": "62-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Abstract\nAbstract: BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)",
    "output": "During ECG patch monitoring, participants' Apple Watches continued to monitor pulse irregularities. If a participant had an irregular pulse detected, 84% of the time this was confirmed to be atrial fibrillation on the simultaneous ECG patch. This, said Perez, demonstrates that the algorithm in the Apple Watch can successfully identify atrial fibrillation. Information from this study could be used to inform further clinical evaluation.\nThe study also showed how digital health alerts can enhance engagement with the health care system overall. A survey of participants who received an irregular-pulse notification showed that 76% contacted either the telehealth provider or a non-study provider, suggesting that many actively sought medical attention as a result of an irregularity identified by their Apple Watch, said Turakhia."
  },
  {
    "id_paper": "62",
    "id_story": "62-01",
    "id_chunk": "62-01_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Introduction\nIntroduction: Wearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1 Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4 with a lifetime risk as high as 1 in 3.5 Atrial fibrillation is associated with a quintupling of the risk of stroke.6 The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7 Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8 Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.",
    "output": "Wearable technology can safely identify heart rate irregularities that subsequent clinical evaluations confirmed to be atrial fibrillation, reports a study from the Stanford University School of Medicine and Apple now published in the New England Journal of Medicine (NEJM).\n\"The study's findings will help patients and clinicians understand how devices like Apple Watch can play a role in identifying atrial fibrillation, a deadly and often undiagnosed disease,\" said Mintu Turakhia, MD, associate professor of cardiovascular medicine. \"Additionally, these important findings lay the foundation for further research into the use of emerging wearable technologies in clinical practice and demonstrate the unique potential of large-scale app-based studies.\"\nThe study was launched through a research sponsorship by Apple, Inc., in November 2017 to determine whether software on the Apple Watch could use data from the Watch's heart-rate pulse sensor to identify atrial fibrillation, which is one of the most commonly diagnosed significant cardiac arrhythmias in the United States, affecting up to 6 million people.\n\"The performance and accuracy we observed in this study provides important information as we seek to understand the potential impact of wearable technology on the health system,\" said Perez. \"What the Apple Heart Study shows us is that atrial fibrillation is just the beginning. We can look ahead to other areas of preventive medicine. Further research will help people make more informed health decisions.\""
  },
  {
    "id_paper": "62",
    "id_story": "62-01",
    "id_chunk": "62-01_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\nMethod: Study Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with",
    "output": "Courtesy of Apple\nWith more than 400,000 participants enrolled in eight months, the Apple Heart Study is the largest virtual study to date.\nLaunching app-based research\n\"As the number of app-based health studies grows, developing additional methods to maximize self-report data accuracy and engagement will be an important area of investigation,\" said Turakhia.\nThe design of the study provided several valuable insights regarding patient engagement and interventions that will prove helpful in designing further studies. For one, the study's completely virtual nature eliminated the need for participants to be physically present and allowed for the implementation of a massive recruitment strategy in a relatively short period of time. Also, because screening was done via interacting with the app, the study was administered at scale with little incremental cost.\nTo participate in the study, each individual needed an Apple Watch (series 1, 2 or 3) and an iPhone. (The Series 4 and 5 Apple Watch, which feature a built-in ECG, weren't part of the study, as they was released after the study's launch.)\nThe Apple Heart Study app intermittently checked the heart-rate pulse sensor for measurements of an irregular pulse. If it detected an irregular pulse, the participant received a notification and was asked to schedule a telehealth consultation with a doctor involved in the study through American Well. Participants were then sent ECG patches from BioTelemetry, which recorded the electrical rhythm of their hearts for up to a week.\nResearchers from the Lankenau Heart Institute, Jefferson Medical College, the University of Colorado School of Medicine, Cooper Medical School of Rowan University, StopAfib.org, the American Foundation for Women's Health and Duke University also contributed to the study."
  },
  {
    "id_paper": "62",
    "id_story": "62-01",
    "id_chunk": "62-01_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\na health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (65 and 65 years), yielding 97",
    "output": "Courtesy of Apple\nWith more than 400,000 participants enrolled in eight months, the Apple Heart Study is the largest virtual study to date.\nLaunching app-based research\n\"As the number of app-based health studies grows, developing additional methods to maximize self-report data accuracy and engagement will be an important area of investigation,\" said Turakhia.\nThe design of the study provided several valuable insights regarding patient engagement and interventions that will prove helpful in designing further studies. For one, the study's completely virtual nature eliminated the need for participants to be physically present and allowed for the implementation of a massive recruitment strategy in a relatively short period of time. Also, because screening was done via interacting with the app, the study was administered at scale with little incremental cost.\nTo participate in the study, each individual needed an Apple Watch (series 1, 2 or 3) and an iPhone. (The Series 4 and 5 Apple Watch, which feature a built-in ECG, weren't part of the study, as they was released after the study's launch.)\nThe Apple Heart Study app intermittently checked the heart-rate pulse sensor for measurements of an irregular pulse. If it detected an irregular pulse, the participant received a notification and was asked to schedule a telehealth consultation with a doctor involved in the study through American Well. Participants were then sent ECG patches from BioTelemetry, which recorded the electrical rhythm of their hearts for up to a week.\nResearchers from the Lankenau Heart Institute, Jefferson Medical College, the University of Colorado School of Medicine, Cooper Medical School of Rowan University, StopAfib.org, the American Foundation for Women's Health and Duke University also contributed to the study."
  },
  {
    "id_paper": "62",
    "id_story": "62-01",
    "id_chunk": "62-01_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\n.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.",
    "output": "Courtesy of Apple\nWith more than 400,000 participants enrolled in eight months, the Apple Heart Study is the largest virtual study to date.\nLaunching app-based research\n\"As the number of app-based health studies grows, developing additional methods to maximize self-report data accuracy and engagement will be an important area of investigation,\" said Turakhia.\nThe design of the study provided several valuable insights regarding patient engagement and interventions that will prove helpful in designing further studies. For one, the study's completely virtual nature eliminated the need for participants to be physically present and allowed for the implementation of a massive recruitment strategy in a relatively short period of time. Also, because screening was done via interacting with the app, the study was administered at scale with little incremental cost.\nTo participate in the study, each individual needed an Apple Watch (series 1, 2 or 3) and an iPhone. (The Series 4 and 5 Apple Watch, which feature a built-in ECG, weren't part of the study, as they was released after the study's launch.)\nThe Apple Heart Study app intermittently checked the heart-rate pulse sensor for measurements of an irregular pulse. If it detected an irregular pulse, the participant received a notification and was asked to schedule a telehealth consultation with a doctor involved in the study through American Well. Participants were then sent ECG patches from BioTelemetry, which recorded the electrical rhythm of their hearts for up to a week.\nResearchers from the Lankenau Heart Institute, Jefferson Medical College, the University of Colorado School of Medicine, Cooper Medical School of Rowan University, StopAfib.org, the American Foundation for Women's Health and Duke University also contributed to the study."
  },
  {
    "id_paper": "62",
    "id_story": "62-01",
    "id_chunk": "62-01_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Experiments / Results\nExperiments / Results: Baseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (SD) of 1316 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse",
    "output": "Turakhia and Manisha Desai, PhD, professor of medicine and of biomedical data science, are the senior authors of the study. Marco Perez, MD, associate professor of cardiovascular medicine, is the lead author. The study chair is Kenneth Mahaffey, MD, professor of cardiovascular medicine."
  },
  {
    "id_paper": "62",
    "id_story": "62-01",
    "id_chunk": "62-01_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Experiments / Results\nnotifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-",
    "output": "Turakhia and Manisha Desai, PhD, professor of medicine and of biomedical data science, are the senior authors of the study. Marco Perez, MD, associate professor of cardiovascular medicine, is the lead author. The study chair is Kenneth Mahaffey, MD, professor of cardiovascular medicine."
  },
  {
    "id_paper": "62",
    "id_story": "62-01",
    "id_chunk": "62-01_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Experiments / Results\nDay Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.",
    "output": "Turakhia and Manisha Desai, PhD, professor of medicine and of biomedical data science, are the senior authors of the study. Marco Perez, MD, associate professor of cardiovascular medicine, is the lead author. The study chair is Kenneth Mahaffey, MD, professor of cardiovascular medicine."
  },
  {
    "id_paper": "62",
    "id_story": "62-05",
    "id_chunk": "62-05_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Journalist, Policy Maker, Researcher\n\nSection: Abstract\nAbstract: BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)",
    "output": "The study recruited 419,297 people who had an Apple Watch. It tested technology in the watch that uses an optical sensor to detect the wearer’s pulse. Of the cohort, 0.5 percent — or 2,161 people — were notified that they might have atrial fibrillation, a relatively common arrhythmia that can lead to feelings of weakness and increases the risk of stroke and other conditions."
  },
  {
    "id_paper": "62",
    "id_story": "62-05",
    "id_chunk": "62-05_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Journalist, Policy Maker, Researcher\n\nSection: Introduction\nIntroduction: Wearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1 Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4 with a lifetime risk as high as 1 in 3.5 Atrial fibrillation is associated with a quintupling of the risk of stroke.6 The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7 Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8 Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.",
    "output": "And the unknowns could be expensive."
  },
  {
    "id_paper": "62",
    "id_story": "62-05",
    "id_chunk": "62-05_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Journalist, Policy Maker, Researcher\n\nSection: Method\nMethod: Study Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with",
    "output": "NEW ORLEANS — New results of a gigantic study testing how well the Apple Watch detects important heart-rhythm changes offer a tantalizing glimpse of how consumer devices might change medicine and lead to new ways to conduct science — but little more.\nUnfortunately, the data don’t tell us much about whether anyone should buy an Apple Watch, or whether the device is good for the population at large.\nadvertisement\nThe Apple Heart Study is being released at the annual meeting of the American College of Cardiology here. Full results will be presented before thousands of doctors at the opening of the convention Saturday morning. STAT was given a press release in advance containing some data from the study, which was shared with the researchers who commented for this story.\nadvertisement\nNewsletters Sign up for ACC in 30 Seconds Your guide to what's happening at the 2019 ACC meeting in New Orleans. Please enter a valid email address. Privacy Policy\n“The technology has arrived. I am not one of those cardiologists that are blaming Apple for giving us too much information,” said Dr. Rajat Deo, an associate professor of medicine at the University of Pennsylvania. It’s doctors’ job, he said, to figure out how to use this information. He said that 84 percent rate was low enough to be “concerning,” but he expects it to improve over time. The study, therefore, represents a promise of what might be — but it is not yet a device doctors should trust on its own.\n“We are not talking about a medical-grade technology, we’re talking about a fun technology we go running with to check our heart rate,” Deo said. “I would caution clinicians not to act based on Apple Watch data alone.”\nThat mix of hope and skepticism was generally the reaction of doctors given a preview of the data. Some were harsher: Dr. Steven Nissen, the chairman of cardiology at the Cleveland Clinic, worried that the data are still “unpublished, unverified, unreviewed, and not tested by peers.”\nSu said he’s seen about a hundred patients who have come to him after getting a scary result from the Apple Watch, but he doesn’t remember any of them needing a major intervention. He could foresee using the watch to track some patients who have received treatment for atrial fibrillation.\nBut for many experts, the study isn’t really about whether the Apple Watch can have an impact on public health now. It’s about embarking on an entirely new way of doing clinical research and practicing medicine someday.\nDr. Jordan Safirstein, an interventional cardiologist at Morristown Medical Center, said the result was “amazing,” not because of the results, but because researchers were able to recruit a staggering 419,000 patients in a very short period of time, and follow up on them only with telemedicine.\n“It’s a lovely demonstration of how we can enroll patients completely virtually, which I think is a huge extension for us in terms of patient engagement and enrollment,” Safirstein said.\nDr. Mintu Turakhia, one of the Stanford University researchers who designed and conducted the study with Apple, said this was one of the major points of the study: not to reach an immediate conclusion on the Apple Watch — even there, follow-up data on exactly how patients were treated will be needed — but to learn how to use wearable devices in medical research. “We’ve enrolled 400,000 people in just over eight months,” Turakhia said. He hopes that what they learned will allow them to do more ambitious studies in the future.\n“I don’t think any of us view this study as the end in terms of using apps and devices to monitor cardiac performance or any number of other things,” said Dr. Lloyd Minor, dean of the Stanford University School of Medicine. (He owns a small amount of Apple stock.) “But we think it’s an exciting beginning.”\nConsumer-facing technologies, Minor argued, could change medicine just as they have changed every other aspect of our lives. But that still doesn’t tell you whether it’s a good idea to buy an Apple Watch."
  },
  {
    "id_paper": "62",
    "id_story": "62-05",
    "id_chunk": "62-05_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Journalist, Policy Maker, Researcher\n\nSection: Method\na health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (65 and 65 years), yielding 97",
    "output": "NEW ORLEANS — New results of a gigantic study testing how well the Apple Watch detects important heart-rhythm changes offer a tantalizing glimpse of how consumer devices might change medicine and lead to new ways to conduct science — but little more.\nUnfortunately, the data don’t tell us much about whether anyone should buy an Apple Watch, or whether the device is good for the population at large.\nadvertisement\nThe Apple Heart Study is being released at the annual meeting of the American College of Cardiology here. Full results will be presented before thousands of doctors at the opening of the convention Saturday morning. STAT was given a press release in advance containing some data from the study, which was shared with the researchers who commented for this story.\nadvertisement\nNewsletters Sign up for ACC in 30 Seconds Your guide to what's happening at the 2019 ACC meeting in New Orleans. Please enter a valid email address. Privacy Policy\n“The technology has arrived. I am not one of those cardiologists that are blaming Apple for giving us too much information,” said Dr. Rajat Deo, an associate professor of medicine at the University of Pennsylvania. It’s doctors’ job, he said, to figure out how to use this information. He said that 84 percent rate was low enough to be “concerning,” but he expects it to improve over time. The study, therefore, represents a promise of what might be — but it is not yet a device doctors should trust on its own.\n“We are not talking about a medical-grade technology, we’re talking about a fun technology we go running with to check our heart rate,” Deo said. “I would caution clinicians not to act based on Apple Watch data alone.”\nThat mix of hope and skepticism was generally the reaction of doctors given a preview of the data. Some were harsher: Dr. Steven Nissen, the chairman of cardiology at the Cleveland Clinic, worried that the data are still “unpublished, unverified, unreviewed, and not tested by peers.”\nSu said he’s seen about a hundred patients who have come to him after getting a scary result from the Apple Watch, but he doesn’t remember any of them needing a major intervention. He could foresee using the watch to track some patients who have received treatment for atrial fibrillation.\nBut for many experts, the study isn’t really about whether the Apple Watch can have an impact on public health now. It’s about embarking on an entirely new way of doing clinical research and practicing medicine someday.\nDr. Jordan Safirstein, an interventional cardiologist at Morristown Medical Center, said the result was “amazing,” not because of the results, but because researchers were able to recruit a staggering 419,000 patients in a very short period of time, and follow up on them only with telemedicine.\n“It’s a lovely demonstration of how we can enroll patients completely virtually, which I think is a huge extension for us in terms of patient engagement and enrollment,” Safirstein said.\nDr. Mintu Turakhia, one of the Stanford University researchers who designed and conducted the study with Apple, said this was one of the major points of the study: not to reach an immediate conclusion on the Apple Watch — even there, follow-up data on exactly how patients were treated will be needed — but to learn how to use wearable devices in medical research. “We’ve enrolled 400,000 people in just over eight months,” Turakhia said. He hopes that what they learned will allow them to do more ambitious studies in the future.\n“I don’t think any of us view this study as the end in terms of using apps and devices to monitor cardiac performance or any number of other things,” said Dr. Lloyd Minor, dean of the Stanford University School of Medicine. (He owns a small amount of Apple stock.) “But we think it’s an exciting beginning.”\nConsumer-facing technologies, Minor argued, could change medicine just as they have changed every other aspect of our lives. But that still doesn’t tell you whether it’s a good idea to buy an Apple Watch."
  },
  {
    "id_paper": "62",
    "id_story": "62-05",
    "id_chunk": "62-05_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Journalist, Policy Maker, Researcher\n\nSection: Method\n.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.",
    "output": "NEW ORLEANS — New results of a gigantic study testing how well the Apple Watch detects important heart-rhythm changes offer a tantalizing glimpse of how consumer devices might change medicine and lead to new ways to conduct science — but little more.\nUnfortunately, the data don’t tell us much about whether anyone should buy an Apple Watch, or whether the device is good for the population at large.\nadvertisement\nThe Apple Heart Study is being released at the annual meeting of the American College of Cardiology here. Full results will be presented before thousands of doctors at the opening of the convention Saturday morning. STAT was given a press release in advance containing some data from the study, which was shared with the researchers who commented for this story.\nadvertisement\nNewsletters Sign up for ACC in 30 Seconds Your guide to what's happening at the 2019 ACC meeting in New Orleans. Please enter a valid email address. Privacy Policy\n“The technology has arrived. I am not one of those cardiologists that are blaming Apple for giving us too much information,” said Dr. Rajat Deo, an associate professor of medicine at the University of Pennsylvania. It’s doctors’ job, he said, to figure out how to use this information. He said that 84 percent rate was low enough to be “concerning,” but he expects it to improve over time. The study, therefore, represents a promise of what might be — but it is not yet a device doctors should trust on its own.\n“We are not talking about a medical-grade technology, we’re talking about a fun technology we go running with to check our heart rate,” Deo said. “I would caution clinicians not to act based on Apple Watch data alone.”\nThat mix of hope and skepticism was generally the reaction of doctors given a preview of the data. Some were harsher: Dr. Steven Nissen, the chairman of cardiology at the Cleveland Clinic, worried that the data are still “unpublished, unverified, unreviewed, and not tested by peers.”\nSu said he’s seen about a hundred patients who have come to him after getting a scary result from the Apple Watch, but he doesn’t remember any of them needing a major intervention. He could foresee using the watch to track some patients who have received treatment for atrial fibrillation.\nBut for many experts, the study isn’t really about whether the Apple Watch can have an impact on public health now. It’s about embarking on an entirely new way of doing clinical research and practicing medicine someday.\nDr. Jordan Safirstein, an interventional cardiologist at Morristown Medical Center, said the result was “amazing,” not because of the results, but because researchers were able to recruit a staggering 419,000 patients in a very short period of time, and follow up on them only with telemedicine.\n“It’s a lovely demonstration of how we can enroll patients completely virtually, which I think is a huge extension for us in terms of patient engagement and enrollment,” Safirstein said.\nDr. Mintu Turakhia, one of the Stanford University researchers who designed and conducted the study with Apple, said this was one of the major points of the study: not to reach an immediate conclusion on the Apple Watch — even there, follow-up data on exactly how patients were treated will be needed — but to learn how to use wearable devices in medical research. “We’ve enrolled 400,000 people in just over eight months,” Turakhia said. He hopes that what they learned will allow them to do more ambitious studies in the future.\n“I don’t think any of us view this study as the end in terms of using apps and devices to monitor cardiac performance or any number of other things,” said Dr. Lloyd Minor, dean of the Stanford University School of Medicine. (He owns a small amount of Apple stock.) “But we think it’s an exciting beginning.”\nConsumer-facing technologies, Minor argued, could change medicine just as they have changed every other aspect of our lives. But that still doesn’t tell you whether it’s a good idea to buy an Apple Watch."
  },
  {
    "id_paper": "62",
    "id_story": "62-05",
    "id_chunk": "62-05_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Journalist, Policy Maker, Researcher\n\nSection: Experiments / Results\nExperiments / Results: Baseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (SD) of 1316 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse",
    "output": "“If we had everybody go to CVS or Walgreens and get a blood pressure check, we’d probably save a lot more lives and it would cost a lot less money,” said Dr. Wilber Su, chief of cardiac electrophysiology at Banner Health in Phoenix.\n“Most of us have mixed feelings about it,” Su said. “It’s not uncommon for patients to go into complete panic thinking they’re going to have a stroke the next moment because this is positive.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-05",
    "id_chunk": "62-05_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Journalist, Policy Maker, Researcher\n\nSection: Experiments / Results\nnotifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-",
    "output": "“If we had everybody go to CVS or Walgreens and get a blood pressure check, we’d probably save a lot more lives and it would cost a lot less money,” said Dr. Wilber Su, chief of cardiac electrophysiology at Banner Health in Phoenix.\n“Most of us have mixed feelings about it,” Su said. “It’s not uncommon for patients to go into complete panic thinking they’re going to have a stroke the next moment because this is positive.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-05",
    "id_chunk": "62-05_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Journalist, Policy Maker, Researcher\n\nSection: Experiments / Results\nDay Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.",
    "output": "“If we had everybody go to CVS or Walgreens and get a blood pressure check, we’d probably save a lot more lives and it would cost a lot less money,” said Dr. Wilber Su, chief of cardiac electrophysiology at Banner Health in Phoenix.\n“Most of us have mixed feelings about it,” Su said. “It’s not uncommon for patients to go into complete panic thinking they’re going to have a stroke the next moment because this is positive.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-03",
    "id_chunk": "62-03_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Student\n\nSection: Abstract\nAbstract: BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)",
    "output": "Last November,Apple Watch owners began receiving recruitment emails from Apple. The company was looking for owners of its smartwatch to participate in the Apple Heart Study—a Stanford-led investigation into the wearable's ability to sense irregular heart rhythms. Joining was simple: Install an app and wear your watch. If the watch's optical sensors detected an arrhythmia, you might be shipped a dedicated heart monitor—a benchmark to compare against readings from your Apple Watch—to wear for seven days. In true Apple fashion, enrollment and participation were designed to be as user friendly as possible: \"Apple and Stanford Medicine are committed to making it easy for people to participate in medical research,\" the research partners wrote, \"because more data can lead to discoveries that save lives.\" Now, not for the first time, Apple's attention to user experience has been rewarded: According to a paper outlining the study's design in this week's issue of the American Heart Journal, Apple and Stanford have managed to enroll a staggering 419,093 participants. That makes it the largest screening study on atrial fibrillation ever performed. A study of that size is a big deal for researchers. But even if the results (which should be published next year) are positive, Apple will still have much to prove about the public benefits of its popular wearable. First let's talk study size. 400,000 research subjects is huge. By comparison, the Strokestop study—a Swedish investigation into mass arrhythmia screening—has around 25,000 participants. To be fair, the Strokestop study has things going for it that Apple's study doesn't, which we'll get to. But the fact that Apple was able to round up a research population of this size in under a year is impressive. It's also a major selling point of the study's design. Larger sample sizes make for smaller error margins and a greater degree of certainty in one's results, both of which are important when studying the accuracy of a device designed to flag heart problems. Some five million people in the US are affected by atrial fibrillation and atrial flutter (collectively known as AF), irregular heart rhythms that significantly increase one's risk of stroke and heart failure. An estimated 700,000 of those people don't even know they have AF. Cardiologists are particularly interested in that second group. If a product like Apple's could detect undiagnosed arrhythmias across large populations and compel flagged users to take appropriate preemptive action? It could save lives. But here's the thing: Even if the Apple Watch excels at detecting undiagnosed AF (a big if), using it to screen large numbers of asymptomatic people isn't necessarily a good idea. Screening comes with risks: Misdiagnosis. Unnecessary tests. Overtreatment. \"Those are real problems that need to be sorted out,\" says cardiologist Mintu Turakhia, the study's lead author and director of Stanford's Center for Digital Health. That's why he and his team will also observe what happens after Apple Watch users receive an alert: Whether they follow up with a healthcare provider, whether a diagnosis is made, and what treatment they receive. \"We're interested in the patient journey, but we also want to see whether an alert from the watch helps lead to appropriate care,\" Turakhia says. The biggest unknown surrounding AF screening is a simple one: Do its benefits outweigh its costs? \"The current evidence is insufficient to say one way or the other,\" says Seth Landefeld, chair of the department of medicine at University of Alabama Birmingham and a member of the US Preventive Services Task Force, an independent, volunteer panel of national experts in disease prevention. That lack of evidence is why the USPSTF recommends against screening asymptomatic adults. Apple's study doesn't directly address USPSTF's biggest concern. \"The million-dollar question, the one we really need to have answered, is whether people who screen for arrhythmias have fewer strokes, long-term, than those who don't,\" Landefeld says. The present study is designed primarily to see how Apple’s AF detection compares with a dedicated heart monitor—not how the people who receive AF alerts fare in the long haul. For that, you need repeated observations of your participants over a long period of time (a major advantage of the Strokestop study). Randomized control groups wouldn't hurt, either. Turakhia doesn't disagree. But he notes that the thinking on AF has expanded"
  },
  {
    "id_paper": "62",
    "id_story": "62-03",
    "id_chunk": "62-03_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Student\n\nSection: Introduction\nIntroduction: Wearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1 Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4 with a lifetime risk as high as 1 in 3.5 Atrial fibrillation is associated with a quintupling of the risk of stroke.6 The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7 Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8 Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.",
    "output": "It's also a major selling point of the study's design. Larger sample sizes make for smaller error margins and a greater degree of certainty in one's results, both of which are important when studying the accuracy of a device designed to flag heart problems. Some five million people in the US are affected by atrial fibrillation and atrial flutter (collectively known as AF), irregular heart rhythms that significantly increase one's risk of stroke and heart failure. An estimated 700,000 of those people don't even know they have AF."
  },
  {
    "id_paper": "62",
    "id_story": "62-03",
    "id_chunk": "62-03_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Student\n\nSection: Method\nMethod: Study Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with",
    "output": "Last November,Apple Watch owners began receiving recruitment emails from Apple. The company was looking for owners of its smartwatch to participate in the Apple Heart Study—a Stanford-led investigation into the wearable's ability to sense irregular heart rhythms.\nJoining was simple: Install an app and wear your watch. If the watch's optical sensors detected an arrhythmia, you might be shipped a dedicated heart monitor—a benchmark to compare against readings from your Apple Watch—to wear for seven days. In true Apple fashion, enrollment and participation were designed to be as user friendly as possible: \"Apple and Stanford Medicine are committed to making it easy for people to participate in medical research,\" the research partners wrote, \"because more data can lead to discoveries that save lives.\"\nNow, not for the first time, Apple's attention to user experience has been rewarded: According to a paper outlining the study's design in this week's issue of the American Heart Journal, Apple and Stanford have managed to enroll a staggering 419,093 participants. That makes it the largest screening study on atrial fibrillation ever performed. A study of that size is a big deal for researchers. But even if the results (which should be published next year) are positive, Apple will still have much to prove about the public benefits of its popular wearable.\nFirst let's talk study size. 400,000 research subjects is huge. By comparison, the Strokestop study—a Swedish investigation into mass arrhythmia screening—has around 25,000 participants. To be fair, the Strokestop study has things going for it that Apple's study doesn't, which we'll get to. But the fact that Apple was able to round up a research population of this size in under a year is impressive.\nScreening comes with risks: Misdiagnosis. Unnecessary tests. Overtreatment. \"Those are real problems that need to be sorted out,\" says cardiologist Mintu Turakhia, the study's lead author and director of Stanford's Center for Digital Health. That's why he and his team will also observe what happens after Apple Watch users receive an alert: Whether they follow up with a healthcare provider, whether a diagnosis is made, and what treatment they receive. \"We're interested in the patient journey, but we also want to see whether an alert from the watch helps lead to appropriate care,\" Turakhia says.\nOf course, Apple and Stanford could address these questions in follow-up studies. But there's a glaring inconsistency in Apple's plan to study heart-screening technology while making it available in the most popular watch on Earth. In their paper describing the design of the Apple Heart Study, Turakhia and his colleagues write that the ultimate goal of studying the Apple Watch's arrhythmia-sensing potential is \"learning to responsibly release such a technology at scale.\" But Apple plans to release the irregular rhythm notification and ECG features to the public by the end of 2018. The results of the Stanford study won't be finalized until 2019 at the earliest. So is Apple putting the cart before the horse?\nHard to say. On one hand, an investigation like the Apple Heart Study would be all but impossible were it not for the company's decision to incorporate medical technology into the Apple Watch. On the other, the technology is, by definition, unproven. I asked Turakhia whether he thinks Apple is getting ahead of itself by releasing the features before the results of his study are in. \"All I can say is that we at Stanford were not involved in the regulatory submission process,\" Turakhia says1. \"So I can’t comment on that.\"\n1. Correction: While neither Turakhia nor his co-PI Marco Perez were involed in the regulatory submission process, a Stanford spokesperson clarifies that university faculty \"actually were involved in and supported the FDA filing, but were blinded to the findings and the data that was submitted to the agency.\""
  },
  {
    "id_paper": "62",
    "id_story": "62-03",
    "id_chunk": "62-03_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Student\n\nSection: Method\na health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (65 and 65 years), yielding 97",
    "output": "Last November,Apple Watch owners began receiving recruitment emails from Apple. The company was looking for owners of its smartwatch to participate in the Apple Heart Study—a Stanford-led investigation into the wearable's ability to sense irregular heart rhythms.\nJoining was simple: Install an app and wear your watch. If the watch's optical sensors detected an arrhythmia, you might be shipped a dedicated heart monitor—a benchmark to compare against readings from your Apple Watch—to wear for seven days. In true Apple fashion, enrollment and participation were designed to be as user friendly as possible: \"Apple and Stanford Medicine are committed to making it easy for people to participate in medical research,\" the research partners wrote, \"because more data can lead to discoveries that save lives.\"\nNow, not for the first time, Apple's attention to user experience has been rewarded: According to a paper outlining the study's design in this week's issue of the American Heart Journal, Apple and Stanford have managed to enroll a staggering 419,093 participants. That makes it the largest screening study on atrial fibrillation ever performed. A study of that size is a big deal for researchers. But even if the results (which should be published next year) are positive, Apple will still have much to prove about the public benefits of its popular wearable.\nFirst let's talk study size. 400,000 research subjects is huge. By comparison, the Strokestop study—a Swedish investigation into mass arrhythmia screening—has around 25,000 participants. To be fair, the Strokestop study has things going for it that Apple's study doesn't, which we'll get to. But the fact that Apple was able to round up a research population of this size in under a year is impressive.\nScreening comes with risks: Misdiagnosis. Unnecessary tests. Overtreatment. \"Those are real problems that need to be sorted out,\" says cardiologist Mintu Turakhia, the study's lead author and director of Stanford's Center for Digital Health. That's why he and his team will also observe what happens after Apple Watch users receive an alert: Whether they follow up with a healthcare provider, whether a diagnosis is made, and what treatment they receive. \"We're interested in the patient journey, but we also want to see whether an alert from the watch helps lead to appropriate care,\" Turakhia says.\nOf course, Apple and Stanford could address these questions in follow-up studies. But there's a glaring inconsistency in Apple's plan to study heart-screening technology while making it available in the most popular watch on Earth. In their paper describing the design of the Apple Heart Study, Turakhia and his colleagues write that the ultimate goal of studying the Apple Watch's arrhythmia-sensing potential is \"learning to responsibly release such a technology at scale.\" But Apple plans to release the irregular rhythm notification and ECG features to the public by the end of 2018. The results of the Stanford study won't be finalized until 2019 at the earliest. So is Apple putting the cart before the horse?\nHard to say. On one hand, an investigation like the Apple Heart Study would be all but impossible were it not for the company's decision to incorporate medical technology into the Apple Watch. On the other, the technology is, by definition, unproven. I asked Turakhia whether he thinks Apple is getting ahead of itself by releasing the features before the results of his study are in. \"All I can say is that we at Stanford were not involved in the regulatory submission process,\" Turakhia says1. \"So I can’t comment on that.\"\n1. Correction: While neither Turakhia nor his co-PI Marco Perez were involed in the regulatory submission process, a Stanford spokesperson clarifies that university faculty \"actually were involved in and supported the FDA filing, but were blinded to the findings and the data that was submitted to the agency.\""
  },
  {
    "id_paper": "62",
    "id_story": "62-03",
    "id_chunk": "62-03_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Student\n\nSection: Method\n.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.",
    "output": "Last November,Apple Watch owners began receiving recruitment emails from Apple. The company was looking for owners of its smartwatch to participate in the Apple Heart Study—a Stanford-led investigation into the wearable's ability to sense irregular heart rhythms.\nJoining was simple: Install an app and wear your watch. If the watch's optical sensors detected an arrhythmia, you might be shipped a dedicated heart monitor—a benchmark to compare against readings from your Apple Watch—to wear for seven days. In true Apple fashion, enrollment and participation were designed to be as user friendly as possible: \"Apple and Stanford Medicine are committed to making it easy for people to participate in medical research,\" the research partners wrote, \"because more data can lead to discoveries that save lives.\"\nNow, not for the first time, Apple's attention to user experience has been rewarded: According to a paper outlining the study's design in this week's issue of the American Heart Journal, Apple and Stanford have managed to enroll a staggering 419,093 participants. That makes it the largest screening study on atrial fibrillation ever performed. A study of that size is a big deal for researchers. But even if the results (which should be published next year) are positive, Apple will still have much to prove about the public benefits of its popular wearable.\nFirst let's talk study size. 400,000 research subjects is huge. By comparison, the Strokestop study—a Swedish investigation into mass arrhythmia screening—has around 25,000 participants. To be fair, the Strokestop study has things going for it that Apple's study doesn't, which we'll get to. But the fact that Apple was able to round up a research population of this size in under a year is impressive.\nScreening comes with risks: Misdiagnosis. Unnecessary tests. Overtreatment. \"Those are real problems that need to be sorted out,\" says cardiologist Mintu Turakhia, the study's lead author and director of Stanford's Center for Digital Health. That's why he and his team will also observe what happens after Apple Watch users receive an alert: Whether they follow up with a healthcare provider, whether a diagnosis is made, and what treatment they receive. \"We're interested in the patient journey, but we also want to see whether an alert from the watch helps lead to appropriate care,\" Turakhia says.\nOf course, Apple and Stanford could address these questions in follow-up studies. But there's a glaring inconsistency in Apple's plan to study heart-screening technology while making it available in the most popular watch on Earth. In their paper describing the design of the Apple Heart Study, Turakhia and his colleagues write that the ultimate goal of studying the Apple Watch's arrhythmia-sensing potential is \"learning to responsibly release such a technology at scale.\" But Apple plans to release the irregular rhythm notification and ECG features to the public by the end of 2018. The results of the Stanford study won't be finalized until 2019 at the earliest. So is Apple putting the cart before the horse?\nHard to say. On one hand, an investigation like the Apple Heart Study would be all but impossible were it not for the company's decision to incorporate medical technology into the Apple Watch. On the other, the technology is, by definition, unproven. I asked Turakhia whether he thinks Apple is getting ahead of itself by releasing the features before the results of his study are in. \"All I can say is that we at Stanford were not involved in the regulatory submission process,\" Turakhia says1. \"So I can’t comment on that.\"\n1. Correction: While neither Turakhia nor his co-PI Marco Perez were involed in the regulatory submission process, a Stanford spokesperson clarifies that university faculty \"actually were involved in and supported the FDA filing, but were blinded to the findings and the data that was submitted to the agency.\""
  },
  {
    "id_paper": "62",
    "id_story": "62-03",
    "id_chunk": "62-03_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Student\n\nSection: Experiments / Results\nExperiments / Results: Baseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (SD) of 1316 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse",
    "output": "Turakhia doesn't disagree. But he notes that the thinking on AF has expanded beyond concerns about stroke. \"It's still true that, if you have AF, the worst thing that can happen is stroke,” he says. “But more and more, we're learning that, like hypertension, it's a general marker of cardiovascular risk, and associated with a lot of other conditions, from tiredness and shortness of breath to heart failure and cardiomyopathy.\""
  },
  {
    "id_paper": "62",
    "id_story": "62-03",
    "id_chunk": "62-03_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Student\n\nSection: Experiments / Results\nnotifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-",
    "output": "Turakhia doesn't disagree. But he notes that the thinking on AF has expanded beyond concerns about stroke. \"It's still true that, if you have AF, the worst thing that can happen is stroke,” he says. “But more and more, we're learning that, like hypertension, it's a general marker of cardiovascular risk, and associated with a lot of other conditions, from tiredness and shortness of breath to heart failure and cardiomyopathy.\""
  },
  {
    "id_paper": "62",
    "id_story": "62-03",
    "id_chunk": "62-03_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Student\n\nSection: Experiments / Results\nDay Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.",
    "output": "Turakhia doesn't disagree. But he notes that the thinking on AF has expanded beyond concerns about stroke. \"It's still true that, if you have AF, the worst thing that can happen is stroke,” he says. “But more and more, we're learning that, like hypertension, it's a general marker of cardiovascular risk, and associated with a lot of other conditions, from tiredness and shortness of breath to heart failure and cardiomyopathy.\""
  },
  {
    "id_paper": "62",
    "id_story": "62-02",
    "id_chunk": "62-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Abstract\nAbstract: BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)",
    "output": "It’s one of the goals of digital medicine: you wear a device on your wrist that constantly monitors aspects of your health, and if anything is off, it sends you an alert. That’s your cue to connect with your doctor or get a more thorough checkup to head off any potentially serious problems down the road.\nIn the NEJM study, the scientists reported results from nearly 420,000 people who already owned Apple Watches and volunteered to participate. Over a period of nearly four days, the watch notified 0.5% of this group of potential abnormal heart rhythms. These people were then asked to contact the study team, which sent them ECG patches to wear on their chest for up to seven days while still using the app. Because AFib is episodic, and doesn’t occur continuously, this allowed the researchers to compare when the app detected abnormal heart rhythms and when the gold-standard ECG did. They matched up 84% of the time.\n“That says that the deployment of the technology was safe and could indeed accurately identify atrial fibrillation,” says Dr. Mintu Turakhia, director of the Stanford University Center for Digital Health and senior author on the url. That means people using the app could, in theory, be alerted to the fact that they have AFib before they have any symptoms.\nTurakhia notes that the study doesn’t necessarily suggest that the Apple app could be used to definitively screen everyone for AFib — yet. But it showed that in a group of people with varying degrees of vulnerability to AFib, an algorithm used in a smartphone-based device could, relatively accurately, pick up the condition. With further confirmation, the app could help identify AFib in people who are not even aware of their condition. ”These are people who may stand to benefit from knowing their condition,” he says, since they could then take preventive measures or get treatment to stability their abnormal heart rate."
  },
  {
    "id_paper": "62",
    "id_story": "62-02",
    "id_chunk": "62-02_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Introduction\nIntroduction: Wearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1 Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4 with a lifetime risk as high as 1 in 3.5 Atrial fibrillation is associated with a quintupling of the risk of stroke.6 The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7 Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8 Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.",
    "output": "That’s the idea behind the Heart app on the Apple Watch, which can monitor heart pulse patterns, and detect abnormalities, which could indicate a condition called atrial fibrillation, or AFib. The irregular heart beats characteristic of AFib could lead to stroke, blood clots and heart failure."
  },
  {
    "id_paper": "62",
    "id_story": "62-02",
    "id_chunk": "62-02_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\nMethod: Study Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with",
    "output": "Researchers at Stanford University have been working with Apple to study how accurately the tech giant’s app could detect AFib in a general population. Early results of the research project, known as the Apple Heart Study, were presented at the annual meeting of the American College of Cardiology last spring. Now, in a url published Nov. 13 in the New England Journal of Medicine, they report that the app’s alerts matched up with electrocardiography, or ECG readings (the gold standard for detecting atrial fibrillation) 84% of the time.\nWhile the data was stored at Stanford, and the Stanford researchers worked with Apple to analyze the information collected, Apple owns the data gathered from the heart app. That raises concerns about privacy and how well informed people are about how and where their data might be used — questions that remain to be answered.\nIn the meantime, the study highlights a potential shift toward more efficient, and less costly ways to answer certain scientific questions. The study was conducted virtually, with volunteers talking to the researchers on the phone or through video chat. Usually, clinical trials struggle to sign up large numbers of participants quickly; the process is expensive and time-consuming. And, once people do enroll, there are additional costs and time needed to accommodate their travel to the study site. In this case, however, research team was able to sign up a large number of volunteers in just over eight months.\nThe study shows that certain questions might be better answered without the infrastructure that clinical trials have traditionally required, and opens up the possibility that mass collection of data using wearable technologies or other continuous monitoring devices could provide a wealth of information that proves useful in improving people’s health. “We are early in this but I am optimistic that we will continue to see many more great partnerships where scientists and technology can come together to answer questions,” Turakhia says. The challenge, however, is to remain faithful to the data and what the evidence shows. “We need to follow the evidence, and continue to make sure we follow the appropriate guidelines.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-02",
    "id_chunk": "62-02_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\na health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (65 and 65 years), yielding 97",
    "output": "Researchers at Stanford University have been working with Apple to study how accurately the tech giant’s app could detect AFib in a general population. Early results of the research project, known as the Apple Heart Study, were presented at the annual meeting of the American College of Cardiology last spring. Now, in a url published Nov. 13 in the New England Journal of Medicine, they report that the app’s alerts matched up with electrocardiography, or ECG readings (the gold standard for detecting atrial fibrillation) 84% of the time.\nWhile the data was stored at Stanford, and the Stanford researchers worked with Apple to analyze the information collected, Apple owns the data gathered from the heart app. That raises concerns about privacy and how well informed people are about how and where their data might be used — questions that remain to be answered.\nIn the meantime, the study highlights a potential shift toward more efficient, and less costly ways to answer certain scientific questions. The study was conducted virtually, with volunteers talking to the researchers on the phone or through video chat. Usually, clinical trials struggle to sign up large numbers of participants quickly; the process is expensive and time-consuming. And, once people do enroll, there are additional costs and time needed to accommodate their travel to the study site. In this case, however, research team was able to sign up a large number of volunteers in just over eight months.\nThe study shows that certain questions might be better answered without the infrastructure that clinical trials have traditionally required, and opens up the possibility that mass collection of data using wearable technologies or other continuous monitoring devices could provide a wealth of information that proves useful in improving people’s health. “We are early in this but I am optimistic that we will continue to see many more great partnerships where scientists and technology can come together to answer questions,” Turakhia says. The challenge, however, is to remain faithful to the data and what the evidence shows. “We need to follow the evidence, and continue to make sure we follow the appropriate guidelines.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-02",
    "id_chunk": "62-02_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\n.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.",
    "output": "Researchers at Stanford University have been working with Apple to study how accurately the tech giant’s app could detect AFib in a general population. Early results of the research project, known as the Apple Heart Study, were presented at the annual meeting of the American College of Cardiology last spring. Now, in a url published Nov. 13 in the New England Journal of Medicine, they report that the app’s alerts matched up with electrocardiography, or ECG readings (the gold standard for detecting atrial fibrillation) 84% of the time.\nWhile the data was stored at Stanford, and the Stanford researchers worked with Apple to analyze the information collected, Apple owns the data gathered from the heart app. That raises concerns about privacy and how well informed people are about how and where their data might be used — questions that remain to be answered.\nIn the meantime, the study highlights a potential shift toward more efficient, and less costly ways to answer certain scientific questions. The study was conducted virtually, with volunteers talking to the researchers on the phone or through video chat. Usually, clinical trials struggle to sign up large numbers of participants quickly; the process is expensive and time-consuming. And, once people do enroll, there are additional costs and time needed to accommodate their travel to the study site. In this case, however, research team was able to sign up a large number of volunteers in just over eight months.\nThe study shows that certain questions might be better answered without the infrastructure that clinical trials have traditionally required, and opens up the possibility that mass collection of data using wearable technologies or other continuous monitoring devices could provide a wealth of information that proves useful in improving people’s health. “We are early in this but I am optimistic that we will continue to see many more great partnerships where scientists and technology can come together to answer questions,” Turakhia says. The challenge, however, is to remain faithful to the data and what the evidence shows. “We need to follow the evidence, and continue to make sure we follow the appropriate guidelines.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-02",
    "id_chunk": "62-02_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Experiments / Results\nExperiments / Results: Baseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (SD) of 1316 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse",
    "output": "It’s one of the goals of digital medicine: you wear a device on your wrist that constantly monitors aspects of your health, and if anything is off, it sends you an alert. That’s your cue to connect with your doctor or get a more thorough checkup to head off any potentially serious problems down the road.\n\nThat’s the idea behind the Heart app on the Apple Watch, which can monitor heart pulse patterns, and detect abnormalities, which could indicate a condition called atrial fibrillation, or AFib. The irregular heart beats characteristic of AFib could lead to stroke, blood clots and heart failure.\n\nResearchers at Stanford University have been working with Apple to study how accurately the tech giant’s app could detect AFib in a general population. Early results of the research project, known as the Apple Heart Study, were presented at the annual meeting of the American College of Cardiology last spring. Now, in a url published Nov. 13 in the New England Journal of Medicine, they report that the app’s alerts matched up with electrocardiography, or ECG readings (the gold standard for detecting atrial fibrillation) 84% of the time.\n\nIn the NEJM study, the scientists reported results from nearly 420,000 people who already owned Apple Watches and volunteered to participate. Over a period of nearly four days, the watch notified 0.5% of this group of potential abnormal heart rhythms. These people were then asked to contact the study team, which sent them ECG patches to wear on their chest for up to seven days while still using the app. Because AFib is episodic, and doesn’t occur continuously, this allowed the researchers to compare when the app detected abnormal heart rhythms and when the gold-standard ECG did. They matched up 84% of the time.\n“That says that the deployment of the technology was safe and could indeed accurately identify atrial fibrillation,” says Dr. Mintu Turakhia, director of the Stanford University Center for Digital Health and senior author on the url. That means people using the app could, in theory, be alerted to the fact that they have AFib before they have any symptoms.\nTurakhia notes that the study doesn’t necessarily suggest that the Apple app could be used to definitively screen everyone for AFib — yet. But it showed that in a group of people with varying degrees of vulnerability to AFib, an algorithm used in a smartphone-based device could, relatively accurately, pick up the condition. With further confirmation, the app could help identify AFib in people who are not even aware of their condition. ”These are people who may stand to benefit from knowing their condition,” he says, since they could then take preventive measures or get treatment to stability their abnormal heart rate.\n\nWhile the data was stored at Stanford, and the Stanford researchers worked with Apple to analyze the information collected, Apple owns the data gathered from the heart app. That raises concerns about privacy and how well informed people are about how and where their data might be used — questions that remain to be answered.\n\nIn the meantime, the study highlights a potential shift toward more efficient, and less costly ways to answer certain scientific questions. The study was conducted virtually, with volunteers talking to the researchers on the phone or through video chat. Usually, clinical trials struggle to sign up large numbers of participants quickly; the process is expensive and time-consuming. And, once people do enroll, there are additional costs and time needed to accommodate their travel to the study site. In this case, however, research team was able to sign up a large number of volunteers in just over eight months.\nThe study shows that certain questions might be better answered without the infrastructure that clinical trials have traditionally required, and opens up the possibility that mass collection of data using wearable technologies or other continuous monitoring devices could provide a wealth of information that proves useful in improving people’s health. “We are early in this but I am optimistic that we will continue to see many more great partnerships where scientists and technology can come together to answer questions,” Turakhia says. The challenge, however, is to remain faithful to the data and what the evidence shows. “We need to follow the evidence, and continue to make sure we follow the appropriate guidelines.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-02",
    "id_chunk": "62-02_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Experiments / Results\nnotifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-",
    "output": "It’s one of the goals of digital medicine: you wear a device on your wrist that constantly monitors aspects of your health, and if anything is off, it sends you an alert. That’s your cue to connect with your doctor or get a more thorough checkup to head off any potentially serious problems down the road.\n\nThat’s the idea behind the Heart app on the Apple Watch, which can monitor heart pulse patterns, and detect abnormalities, which could indicate a condition called atrial fibrillation, or AFib. The irregular heart beats characteristic of AFib could lead to stroke, blood clots and heart failure.\n\nResearchers at Stanford University have been working with Apple to study how accurately the tech giant’s app could detect AFib in a general population. Early results of the research project, known as the Apple Heart Study, were presented at the annual meeting of the American College of Cardiology last spring. Now, in a url published Nov. 13 in the New England Journal of Medicine, they report that the app’s alerts matched up with electrocardiography, or ECG readings (the gold standard for detecting atrial fibrillation) 84% of the time.\n\nIn the NEJM study, the scientists reported results from nearly 420,000 people who already owned Apple Watches and volunteered to participate. Over a period of nearly four days, the watch notified 0.5% of this group of potential abnormal heart rhythms. These people were then asked to contact the study team, which sent them ECG patches to wear on their chest for up to seven days while still using the app. Because AFib is episodic, and doesn’t occur continuously, this allowed the researchers to compare when the app detected abnormal heart rhythms and when the gold-standard ECG did. They matched up 84% of the time.\n“That says that the deployment of the technology was safe and could indeed accurately identify atrial fibrillation,” says Dr. Mintu Turakhia, director of the Stanford University Center for Digital Health and senior author on the url. That means people using the app could, in theory, be alerted to the fact that they have AFib before they have any symptoms.\nTurakhia notes that the study doesn’t necessarily suggest that the Apple app could be used to definitively screen everyone for AFib — yet. But it showed that in a group of people with varying degrees of vulnerability to AFib, an algorithm used in a smartphone-based device could, relatively accurately, pick up the condition. With further confirmation, the app could help identify AFib in people who are not even aware of their condition. ”These are people who may stand to benefit from knowing their condition,” he says, since they could then take preventive measures or get treatment to stability their abnormal heart rate.\n\nWhile the data was stored at Stanford, and the Stanford researchers worked with Apple to analyze the information collected, Apple owns the data gathered from the heart app. That raises concerns about privacy and how well informed people are about how and where their data might be used — questions that remain to be answered.\n\nIn the meantime, the study highlights a potential shift toward more efficient, and less costly ways to answer certain scientific questions. The study was conducted virtually, with volunteers talking to the researchers on the phone or through video chat. Usually, clinical trials struggle to sign up large numbers of participants quickly; the process is expensive and time-consuming. And, once people do enroll, there are additional costs and time needed to accommodate their travel to the study site. In this case, however, research team was able to sign up a large number of volunteers in just over eight months.\nThe study shows that certain questions might be better answered without the infrastructure that clinical trials have traditionally required, and opens up the possibility that mass collection of data using wearable technologies or other continuous monitoring devices could provide a wealth of information that proves useful in improving people’s health. “We are early in this but I am optimistic that we will continue to see many more great partnerships where scientists and technology can come together to answer questions,” Turakhia says. The challenge, however, is to remain faithful to the data and what the evidence shows. “We need to follow the evidence, and continue to make sure we follow the appropriate guidelines.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-02",
    "id_chunk": "62-02_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Experiments / Results\nDay Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.",
    "output": "It’s one of the goals of digital medicine: you wear a device on your wrist that constantly monitors aspects of your health, and if anything is off, it sends you an alert. That’s your cue to connect with your doctor or get a more thorough checkup to head off any potentially serious problems down the road.\n\nThat’s the idea behind the Heart app on the Apple Watch, which can monitor heart pulse patterns, and detect abnormalities, which could indicate a condition called atrial fibrillation, or AFib. The irregular heart beats characteristic of AFib could lead to stroke, blood clots and heart failure.\n\nResearchers at Stanford University have been working with Apple to study how accurately the tech giant’s app could detect AFib in a general population. Early results of the research project, known as the Apple Heart Study, were presented at the annual meeting of the American College of Cardiology last spring. Now, in a url published Nov. 13 in the New England Journal of Medicine, they report that the app’s alerts matched up with electrocardiography, or ECG readings (the gold standard for detecting atrial fibrillation) 84% of the time.\n\nIn the NEJM study, the scientists reported results from nearly 420,000 people who already owned Apple Watches and volunteered to participate. Over a period of nearly four days, the watch notified 0.5% of this group of potential abnormal heart rhythms. These people were then asked to contact the study team, which sent them ECG patches to wear on their chest for up to seven days while still using the app. Because AFib is episodic, and doesn’t occur continuously, this allowed the researchers to compare when the app detected abnormal heart rhythms and when the gold-standard ECG did. They matched up 84% of the time.\n“That says that the deployment of the technology was safe and could indeed accurately identify atrial fibrillation,” says Dr. Mintu Turakhia, director of the Stanford University Center for Digital Health and senior author on the url. That means people using the app could, in theory, be alerted to the fact that they have AFib before they have any symptoms.\nTurakhia notes that the study doesn’t necessarily suggest that the Apple app could be used to definitively screen everyone for AFib — yet. But it showed that in a group of people with varying degrees of vulnerability to AFib, an algorithm used in a smartphone-based device could, relatively accurately, pick up the condition. With further confirmation, the app could help identify AFib in people who are not even aware of their condition. ”These are people who may stand to benefit from knowing their condition,” he says, since they could then take preventive measures or get treatment to stability their abnormal heart rate.\n\nWhile the data was stored at Stanford, and the Stanford researchers worked with Apple to analyze the information collected, Apple owns the data gathered from the heart app. That raises concerns about privacy and how well informed people are about how and where their data might be used — questions that remain to be answered.\n\nIn the meantime, the study highlights a potential shift toward more efficient, and less costly ways to answer certain scientific questions. The study was conducted virtually, with volunteers talking to the researchers on the phone or through video chat. Usually, clinical trials struggle to sign up large numbers of participants quickly; the process is expensive and time-consuming. And, once people do enroll, there are additional costs and time needed to accommodate their travel to the study site. In this case, however, research team was able to sign up a large number of volunteers in just over eight months.\nThe study shows that certain questions might be better answered without the infrastructure that clinical trials have traditionally required, and opens up the possibility that mass collection of data using wearable technologies or other continuous monitoring devices could provide a wealth of information that proves useful in improving people’s health. “We are early in this but I am optimistic that we will continue to see many more great partnerships where scientists and technology can come together to answer questions,” Turakhia says. The challenge, however, is to remain faithful to the data and what the evidence shows. “We need to follow the evidence, and continue to make sure we follow the appropriate guidelines.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-06",
    "id_chunk": "62-06_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Abstract\nAbstract: BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)",
    "output": "Stanford Medicine today reported results of the Apple Heart Study , the largest study ever of its kind, which enrolled over 400,000 participants from all 50 states in a span of only eight months. Apple and Stanford created the study to evaluate Apple Watch’s irregular rhythm notification, which occasionally checks the heart's rhythm in the background and sends a notification if an irregular heart rhythm appears to be suggestive of atrial fibrillation (AFib). As part of the study, if an irregular heart rhythm was identified, participants received a notification on their Apple Watch and iPhone, a telehealth consultation with a doctor and an electrocardiogram (ECG) patch for additional monitoring.\n\n“We are proud to work with Stanford Medicine as they conduct this important research and look forward to learning more about the impact of Apple Watch alongside the medical community,” said Jeff Williams, Apple’s COO. “We hope consumers will continue to gain useful and actionable information about their heart health through Apple Watch.”\n\nStanford Medicine researchers presented their findings today at the American College of Cardiology’s 68th Annual Scientific Session and Expo. Study results showed 0.5 percent of the over 400,000 participants received an irregular heart rhythm notification, illustrating the feature’s ability to give a user important health information without creating unnecessary burden to their doctor’s schedule. Many participants sought medical advice following their irregular rhythm notification, using the information to have more meaningful conversations with their doctors.\n\n“As physicians, we are always trying to find ways to offer patients health information that is meaningful to them for individualized care,” said Sumbul Desai, MD, Apple’s vice president of Health. “Seeing medical research reflect what we’re hearing from consumers is positive and we’re excited to see Apple Watch helping even more consumers in the future while collaborating with the medical community to further research.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-06",
    "id_chunk": "62-06_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Introduction\nIntroduction: Wearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1 Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4 with a lifetime risk as high as 1 in 3.5 Atrial fibrillation is associated with a quintupling of the risk of stroke.6 The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7 Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8 Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.",
    "output": "Stanford Medicine today reported results of the Apple Heart Study , the largest study ever of its kind, which enrolled over 400,000 participants from all 50 states in a span of only eight months. Apple and Stanford created the study to evaluate Apple Watch’s irregular rhythm notification, which occasionally checks the heart's rhythm in the background and sends a notification if an irregular heart rhythm appears to be suggestive of atrial fibrillation (AFib). As part of the study, if an irregular heart rhythm was identified, participants received a notification on their Apple Watch and iPhone, a telehealth consultation with a doctor and an electrocardiogram (ECG) patch for additional monitoring.\n\n“We are proud to work with Stanford Medicine as they conduct this important research and look forward to learning more about the impact of Apple Watch alongside the medical community,” said Jeff Williams, Apple’s COO. “We hope consumers will continue to gain useful and actionable information about their heart health through Apple Watch.”\n\nStanford Medicine researchers presented their findings today at the American College of Cardiology’s 68th Annual Scientific Session and Expo. Study results showed 0.5 percent of the over 400,000 participants received an irregular heart rhythm notification, illustrating the feature’s ability to give a user important health information without creating unnecessary burden to their doctor’s schedule. Many participants sought medical advice following their irregular rhythm notification, using the information to have more meaningful conversations with their doctors.\n\n“As physicians, we are always trying to find ways to offer patients health information that is meaningful to them for individualized care,” said Sumbul Desai, MD, Apple’s vice president of Health. “Seeing medical research reflect what we’re hearing from consumers is positive and we’re excited to see Apple Watch helping even more consumers in the future while collaborating with the medical community to further research.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-06",
    "id_chunk": "62-06_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\nMethod: Study Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with",
    "output": "Stanford Medicine today reported results of the Apple Heart Study , the largest study ever of its kind, which enrolled over 400,000 participants from all 50 states in a span of only eight months. Apple and Stanford created the study to evaluate Apple Watch’s irregular rhythm notification, which occasionally checks the heart's rhythm in the background and sends a notification if an irregular heart rhythm appears to be suggestive of atrial fibrillation (AFib). As part of the study, if an irregular heart rhythm was identified, participants received a notification on their Apple Watch and iPhone, a telehealth consultation with a doctor and an electrocardiogram (ECG) patch for additional monitoring.\n“We are proud to work with Stanford Medicine as they conduct this important research and look forward to learning more about the impact of Apple Watch alongside the medical community,” said Jeff Williams, Apple’s COO. “We hope consumers will continue to gain useful and actionable information about their heart health through Apple Watch.”\n“As physicians, we are always trying to find ways to offer patients health information that is meaningful to them for individualized care,” said Sumbul Desai, MD, Apple’s vice president of Health. “Seeing medical research reflect what we’re hearing from consumers is positive and we’re excited to see Apple Watch helping even more consumers in the future while collaborating with the medical community to further research.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-06",
    "id_chunk": "62-06_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\na health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (65 and 65 years), yielding 97",
    "output": "Stanford Medicine today reported results of the Apple Heart Study , the largest study ever of its kind, which enrolled over 400,000 participants from all 50 states in a span of only eight months. Apple and Stanford created the study to evaluate Apple Watch’s irregular rhythm notification, which occasionally checks the heart's rhythm in the background and sends a notification if an irregular heart rhythm appears to be suggestive of atrial fibrillation (AFib). As part of the study, if an irregular heart rhythm was identified, participants received a notification on their Apple Watch and iPhone, a telehealth consultation with a doctor and an electrocardiogram (ECG) patch for additional monitoring.\n“We are proud to work with Stanford Medicine as they conduct this important research and look forward to learning more about the impact of Apple Watch alongside the medical community,” said Jeff Williams, Apple’s COO. “We hope consumers will continue to gain useful and actionable information about their heart health through Apple Watch.”\n“As physicians, we are always trying to find ways to offer patients health information that is meaningful to them for individualized care,” said Sumbul Desai, MD, Apple’s vice president of Health. “Seeing medical research reflect what we’re hearing from consumers is positive and we’re excited to see Apple Watch helping even more consumers in the future while collaborating with the medical community to further research.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-06",
    "id_chunk": "62-06_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\n.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.",
    "output": "Stanford Medicine today reported results of the Apple Heart Study , the largest study ever of its kind, which enrolled over 400,000 participants from all 50 states in a span of only eight months. Apple and Stanford created the study to evaluate Apple Watch’s irregular rhythm notification, which occasionally checks the heart's rhythm in the background and sends a notification if an irregular heart rhythm appears to be suggestive of atrial fibrillation (AFib). As part of the study, if an irregular heart rhythm was identified, participants received a notification on their Apple Watch and iPhone, a telehealth consultation with a doctor and an electrocardiogram (ECG) patch for additional monitoring.\n“We are proud to work with Stanford Medicine as they conduct this important research and look forward to learning more about the impact of Apple Watch alongside the medical community,” said Jeff Williams, Apple’s COO. “We hope consumers will continue to gain useful and actionable information about their heart health through Apple Watch.”\n“As physicians, we are always trying to find ways to offer patients health information that is meaningful to them for individualized care,” said Sumbul Desai, MD, Apple’s vice president of Health. “Seeing medical research reflect what we’re hearing from consumers is positive and we’re excited to see Apple Watch helping even more consumers in the future while collaborating with the medical community to further research.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-06",
    "id_chunk": "62-06_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Experiments / Results\nExperiments / Results: Baseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (SD) of 1316 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse",
    "output": "Stanford Medicine today reported results of the Apple Heart Study , the largest study ever of its kind, which enrolled over 400,000 participants from all 50 states in a span of only eight months. Apple and Stanford created the study to evaluate Apple Watch’s irregular rhythm notification, which occasionally checks the heart's rhythm in the background and sends a notification if an irregular heart rhythm appears to be suggestive of atrial fibrillation (AFib). As part of the study, if an irregular heart rhythm was identified, participants received a notification on their Apple Watch and iPhone, a telehealth consultation with a doctor and an electrocardiogram (ECG) patch for additional monitoring.\n\n“We are proud to work with Stanford Medicine as they conduct this important research and look forward to learning more about the impact of Apple Watch alongside the medical community,” said Jeff Williams, Apple’s COO. “We hope consumers will continue to gain useful and actionable information about their heart health through Apple Watch.”\n\nStanford Medicine researchers presented their findings today at the American College of Cardiology’s 68th Annual Scientific Session and Expo. Study results showed 0.5 percent of the over 400,000 participants received an irregular heart rhythm notification, illustrating the feature’s ability to give a user important health information without creating unnecessary burden to their doctor’s schedule. Many participants sought medical advice following their irregular rhythm notification, using the information to have more meaningful conversations with their doctors.\n\n“As physicians, we are always trying to find ways to offer patients health information that is meaningful to them for individualized care,” said Sumbul Desai, MD, Apple’s vice president of Health. “Seeing medical research reflect what we’re hearing from consumers is positive and we’re excited to see Apple Watch helping even more consumers in the future while collaborating with the medical community to further research.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-06",
    "id_chunk": "62-06_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Experiments / Results\nnotifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-",
    "output": "Stanford Medicine today reported results of the Apple Heart Study , the largest study ever of its kind, which enrolled over 400,000 participants from all 50 states in a span of only eight months. Apple and Stanford created the study to evaluate Apple Watch’s irregular rhythm notification, which occasionally checks the heart's rhythm in the background and sends a notification if an irregular heart rhythm appears to be suggestive of atrial fibrillation (AFib). As part of the study, if an irregular heart rhythm was identified, participants received a notification on their Apple Watch and iPhone, a telehealth consultation with a doctor and an electrocardiogram (ECG) patch for additional monitoring.\n\n“We are proud to work with Stanford Medicine as they conduct this important research and look forward to learning more about the impact of Apple Watch alongside the medical community,” said Jeff Williams, Apple’s COO. “We hope consumers will continue to gain useful and actionable information about their heart health through Apple Watch.”\n\nStanford Medicine researchers presented their findings today at the American College of Cardiology’s 68th Annual Scientific Session and Expo. Study results showed 0.5 percent of the over 400,000 participants received an irregular heart rhythm notification, illustrating the feature’s ability to give a user important health information without creating unnecessary burden to their doctor’s schedule. Many participants sought medical advice following their irregular rhythm notification, using the information to have more meaningful conversations with their doctors.\n\n“As physicians, we are always trying to find ways to offer patients health information that is meaningful to them for individualized care,” said Sumbul Desai, MD, Apple’s vice president of Health. “Seeing medical research reflect what we’re hearing from consumers is positive and we’re excited to see Apple Watch helping even more consumers in the future while collaborating with the medical community to further research.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-06",
    "id_chunk": "62-06_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Experiments / Results\nDay Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.",
    "output": "Stanford Medicine today reported results of the Apple Heart Study , the largest study ever of its kind, which enrolled over 400,000 participants from all 50 states in a span of only eight months. Apple and Stanford created the study to evaluate Apple Watch’s irregular rhythm notification, which occasionally checks the heart's rhythm in the background and sends a notification if an irregular heart rhythm appears to be suggestive of atrial fibrillation (AFib). As part of the study, if an irregular heart rhythm was identified, participants received a notification on their Apple Watch and iPhone, a telehealth consultation with a doctor and an electrocardiogram (ECG) patch for additional monitoring.\n\n“We are proud to work with Stanford Medicine as they conduct this important research and look forward to learning more about the impact of Apple Watch alongside the medical community,” said Jeff Williams, Apple’s COO. “We hope consumers will continue to gain useful and actionable information about their heart health through Apple Watch.”\n\nStanford Medicine researchers presented their findings today at the American College of Cardiology’s 68th Annual Scientific Session and Expo. Study results showed 0.5 percent of the over 400,000 participants received an irregular heart rhythm notification, illustrating the feature’s ability to give a user important health information without creating unnecessary burden to their doctor’s schedule. Many participants sought medical advice following their irregular rhythm notification, using the information to have more meaningful conversations with their doctors.\n\n“As physicians, we are always trying to find ways to offer patients health information that is meaningful to them for individualized care,” said Sumbul Desai, MD, Apple’s vice president of Health. “Seeing medical research reflect what we’re hearing from consumers is positive and we’re excited to see Apple Watch helping even more consumers in the future while collaborating with the medical community to further research.”"
  },
  {
    "id_paper": "62",
    "id_story": "62-04",
    "id_chunk": "62-04_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Researcher, Teacher, Student\n\nSection: Abstract\nAbstract: BACKGROUND Optical sensors on wearable devices can detect irregular pulses. The ability of a smartwatch application (app) to identify atrial fibrillation during typical use is unknown. METHODS Participants without atrial fibrillation (as reported by the participants themselves) used a smartphone (Apple iPhone) app to consent to monitoring. If a smartwatchbased irregular pulse notification algorithm identified possible atrial fibrillation, a telemedicine visit was initiated and an electrocardiography (ECG) patch was mailed to the participant, to be worn for up to 7 days. Surveys were administered 90 days after notification of the irregular pulse and at the end of the study. The main objectives were to estimate the proportion of notified participants with atrial fibrillation shown on an ECG patch and the positive predictive value of irregular pulse intervals with a targeted confidence interval width of 0.10. RESULTS We recruited 419,297 participants over 8 months. Over a median of 117 days of monitoring, 2161 participants (0.52%) received notifications of irregular pulse. Among the 450 participants who returned ECG patches containing data that could be analyzed — which had been applied, on average, 13 days after notification — atrial fibrillation was present in 34% (97.5% confidence interval [CI], 29 to 39) overall and in 35% (97.5% CI, 27 to 43) of participants 65 years of age or older. Among participants who were notified of an irregular pulse, the positive predictive value was 0.84 (95% CI, 0.76 to 0.92) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular pulse notification and 0.71 (97.5% CI, 0.69 to 0.74) for observing atrial fibrillation on the ECG simultaneously with a subsequent irregular tachogram. Of 1376 notified participants who returned a 90-day survey, 57% contacted health care providers outside the study. There were no reports of serious app-related adverse events. CONCLUSIONS The probability of receiving an irregular pulse notification was low. Among participants who received notification of an irregular pulse, 34% had atrial fibrillation on subsequent ECG patch readings and 84% of notifications were concordant with atrial fibrillation. This siteless (no on-site visits were required for the participants), pragmatic study design provides a foundation for large-scale pragmatic studies in which outcomes or adherence can be reliably assessed with user-owned devices. (Funded by Apple; Apple Heart Study ClinicalTrials.gov number, NCT03335800.)",
    "output": "The Apple Watch and corresponding Heart Study app uses photoplethysmography to intermittently measure blood flow activity and detect subtle changes that might indicate an irregular heartbeat. A tachogram is then created and is analyzed by an algorithm. The aim of the study, presented by Mintu Turakhia, MD, MS, was to identify patients with an irregular pulse watch notification who have AFib on a subsequent electrocardiogram (ECG) patch.\nA total of 419,297 people self-enrolled in the study. Participants could not have AFib or be taking anticoagulants and were required to have an Apple Watch and compatible iPhone. They were given information about the study when they downloaded the Heart Study app. Detection of five of six repeat tachograms of an irregular pulse within a 48-hour period triggered a notification to be sent via the app. Participants receiving a notification were prompted to contact the study doctor through the app for a video consultation to determine if the participant should wear an ECG patch. The patch was worn for up to seven days."
  },
  {
    "id_paper": "62",
    "id_story": "62-04",
    "id_chunk": "62-04_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Researcher, Teacher, Student\n\nSection: Introduction\nIntroduction: Wearable devices with optical sensors, such as smartwatches, are commonly used to measure wearers’ pulse rates.1 Algorithms that use pulse wave data to detect atrial fibrillation and atrial flutter have been developed.1,2 An Apple Watch application (app) can use intermittent, passively detected pulse rate data in an algorithm that identifies episodes suggestive of atrial fibrillation.3 Atrial fibrillation (which in this article also refers to atrial flutter) is the most commonly diagnosed clinically significant cardiac arrhythmia and affects approximately 6 million people in the United States,4 with a lifetime risk as high as 1 in 3.5 Atrial fibrillation is associated with a quintupling of the risk of stroke.6 The paroxysmal nature of atrial fibrillation may result in diagnostic delays since the electrocardiogram (ECG) can appear normal between episodes. In addition, atrial fibrillation can be minimally symptomatic or clinically silent.7 Approximately 700,000 people in the United States may have undiagnosed atrial fibrillation.8 Continuous traditional heart monitors or implantable devices increase the detection of atrial fibrillation in populations at high risk7,9-12 but have limited monitoring periods and require either invasive procedures or activation by the user. The widespread use of Internet-connected devices provides an opportunity to conduct large, siteless, pragmatic trials at a lower cost. The goal of the Apple Heart Study was to evaluate the ability of an irregular pulse notification algorithm to identify atrial fibrillation with the use of an Apple Watch app by consumers.",
    "output": "Offering a glimpse at how wearable technology may help flag potential health problems, the Apple Watch was able to detect atrial fibrillation (AFib) in a small group of people who had received an alert of an irregular heartbeat, said researchers presenting results from the Apple Heart Study at ACC.19 in New Orleans.\nClinical Topics: Arrhythmias and Clinical EP, Sports and Exercise Cardiology, Implantable Devices, SCD/Ventricular Arrhythmias, Atrial Fibrillation/Supraventricular Arrhythmias"
  },
  {
    "id_paper": "62",
    "id_story": "62-04",
    "id_chunk": "62-04_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Researcher, Teacher, Student\n\nSection: Method\nMethod: Study Design and Oversight Details of the study have been described previously.13 This was a prospective, single-group, open-label, siteless, pragmatic study. The research protocol was approved by the institutional review board at Stanford University and by a central institutional review board (Advarra). Apple sponsored the study and owns the data. All study data are stored at Stanford on Stanford data platforms. The analyses presented here were performed by Stanford quantitative scientists independent of the sponsor. Stanford has the right to publish regardless of the outcome. All the authors, including authors employed by the sponsor, reviewed and approved the manuscript and vouch for the accuracy and completeness of the data. Study Population The app, which used the irregular pulse notification algorithm, was available in the United States for download from the Apple App Store from the time the study launched on November 29, 2017, until August 1, 2018. Major eligibility criteria included possession of a compatible Apple iPhone and Apple Watch, an age of 22 years or older, United States residency, and proficiency in English, as reported by the participant. Participants who reported previous atrial fibrillation or current use of oral anticoagulation agents were not eligible. All participants provided electronically signed informed consent. (The consent form, along with a description of the algorithm, telemedicine visit protocol, and methods used for tachogram sampling, are provided in the Supplementary Appendix, available with the full text of this article at NEJM.org.) Monitoring and Study Intervention The study app was used to verify eligibility, obtain participants’ consent, provide study education, and direct participants through the study procedures. After a participant provided consent, the irregular pulse notification algorithm was activated. The study used the Apple Watch photoplethysmography sensor, which used light-emitting and light-sensitive diodes to intermittently and passively measure changes in blood flow while participants were at rest. These signals were used to generate pulse intervals (tachograms) over 1 minute, which were classified as regular or irregular on the basis of the variation in the pulse interval (Fig. S1 in the Supplementary Appendix). Participants were prompted to initiate a telemedicine visit directly from the app. After the initial notification of an irregular pulse, subsequent tachograms and notifications were recorded but were not provided to the participant. The notification feature was active until September 1, 2018. Study visits were conducted by physicians from a national telehealth servicer (American Well) with the use of a standardized protocol. Participants with urgent symptoms were directed to go to an urgent care clinic or emergency department. Participants whose eligibility was confirmed and whose symptoms were not urgent were mailed an ECG patch (ePatch) to wear for up to 7 days. The ECG patches were returned by mail and initially examined by trained technicians. Participants with serious arrhythmias were contacted immediately and directed to seek urgent medical care. The ECG patch reports were read by two clinicians, and discrepant interpretations were then reconciled by a committee of clinicians coordinated by the Stanford Center for Clinical Research. In addition, 3-minute ECG strips from each patch, time-aligned to sampled tachograms, were separately read by two clinicians, with disagreements resolved by a third clinician and then a committee, if necessary. Participants were prompted to initiate a second telemedicine visit to discuss the ambulatory ECG findings and were directed to subsequent care. Study-visit physicians did not initiate treatments. Participants who received irregular pulse notifications were asked to complete a survey, included in the study app, 90 days after notification. All enrolled participants, regardless of notification status, were directed to a Web-based end-of-study survey to be completed by January 31, 2019. All adverse events were reviewed by personnel at the study safety monitoring desk at the Stanford Center for Clinical Research. Primary and Secondary Outcomes There were two coprimary outcomes: atrial fibrillation of greater than 30 seconds’ duration on ECG patch monitoring in a participant who received an irregular pulse notification, and simultaneous atrial fibrillation on ECG patch monitoring during intervals when the participant had an irregular tachogram. Key secondary outcomes were simultaneous atrial fibrillation on ECG patch monitoring when the pulse notification algorithm detected an irregular pulse and participant report of contact with",
    "output": "The study had several limitations, including reliance on self-reported data from participants and the potential for a high number of false positive heart rhythms that could then lead to further unnecessary tests and undue anxiety for patients. Additionally, the target enrollment of 500,000 participants, with 75,000 aged 65 or older, was not reached.\nACC.org Editor-in-Chief Kim A. Eagle, MD, MACC, also noted that while the watch and corresponding app \"offers promise,\" its accuracy is still far short of more traditional and currently used monitoring techniques. \"This is just a glimpse of the future, but we have a ways to go,\" he said.\nTurakhia said the trial does represent \"a paradigm shift\" for how clinical studies can be conducted. \"We don't have to bring people into a brick and mortar clinic and give the study intervention,\" he said."
  },
  {
    "id_paper": "62",
    "id_story": "62-04",
    "id_chunk": "62-04_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Researcher, Teacher, Student\n\nSection: Method\na health care provider outside the study within 3 months after notification of an irregular pulse. Statistical Analysis We calculated the minimum number of participants with analyzable data from ECG patches that would ensure sufficient precision to estimate both the proportion of atrial fibrillation detected in participants 65 years of age or older and the positive predictive value of the tachogram. We targeted 503 ECG patches in each age group (65 and 65 years), yielding 97",
    "output": "The study had several limitations, including reliance on self-reported data from participants and the potential for a high number of false positive heart rhythms that could then lead to further unnecessary tests and undue anxiety for patients. Additionally, the target enrollment of 500,000 participants, with 75,000 aged 65 or older, was not reached.\nACC.org Editor-in-Chief Kim A. Eagle, MD, MACC, also noted that while the watch and corresponding app \"offers promise,\" its accuracy is still far short of more traditional and currently used monitoring techniques. \"This is just a glimpse of the future, but we have a ways to go,\" he said.\nTurakhia said the trial does represent \"a paradigm shift\" for how clinical studies can be conducted. \"We don't have to bring people into a brick and mortar clinic and give the study intervention,\" he said."
  },
  {
    "id_paper": "62",
    "id_story": "62-04",
    "id_chunk": "62-04_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Researcher, Teacher, Student\n\nSection: Method\n.5% confidence intervals around the atrial fibrillation yield for participants 65 years of age or older and a positive predictive value of tachograms no wider than 0.10. Given the large volume and diversity of data, methods were applied so that key statistics presented were arrived at independently by at least two members of the data team to ensure reproducibility. Statistical analyses are described in the statistical analysis plan, which is included in the Supplementary Appendix. Means and standard deviations are provided for continuous characteristics, and frequency distributions with percentages are presented for binary and categorical characteristics. For participants for whom key data were missing, we compared observed participant-level characteristics of participants with missing values with characteristics of participants without missing values to help in interpreting key relationships. The ECG patch subgroup included participants who received a notification, reported no history of atrial fibrillation before enrollment, were not receiving anticoagulant therapy, had no urgent symptoms at the first study visit, and wore their ECG patch within 14 days after shipment for at least 1 hour and returned it within 45 days after the first study visit. We estimated the yield of atrial fibrillation for participants 65 years of age or older as the proportion who had confirmed atrial fibrillation on subsequent ECG patches. We characterized the positive predictive value of the tachograms by calculating the proportion of sampled irregular tachograms for which atrial fibrillation was confirmed on simultaneous ECG patch strips. We estimated the positive predictive value of the notification by calculating the proportion of participants with atrial fibrillation confirmed on at least one ECG strip that was simultaneous with the tachograms that led to the notification. Finally, we estimated the proportion of participants who reported contact with a health care provider among those who were notified and responded to the 90-day questionnaire. Confidence intervals for relevant quantities were provided on the basis of Gaussian assumptions or, if the Gaussian assumption yielded bounds that crossed 0 or 1, on the bias-corrected and accelerated bootstrap interval; 97.5% confidence intervals were provided for two key quantities (the yield of atrial fibrillation for participants 65 years of age or older and the positive predictive value of the tachogram) and 95% confidence intervals were provided for other estimates.",
    "output": "The study had several limitations, including reliance on self-reported data from participants and the potential for a high number of false positive heart rhythms that could then lead to further unnecessary tests and undue anxiety for patients. Additionally, the target enrollment of 500,000 participants, with 75,000 aged 65 or older, was not reached.\nACC.org Editor-in-Chief Kim A. Eagle, MD, MACC, also noted that while the watch and corresponding app \"offers promise,\" its accuracy is still far short of more traditional and currently used monitoring techniques. \"This is just a glimpse of the future, but we have a ways to go,\" he said.\nTurakhia said the trial does represent \"a paradigm shift\" for how clinical studies can be conducted. \"We don't have to bring people into a brick and mortar clinic and give the study intervention,\" he said."
  },
  {
    "id_paper": "62",
    "id_story": "62-04",
    "id_chunk": "62-04_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Researcher, Teacher, Student\n\nSection: Experiments / Results\nExperiments / Results: Baseline Characteristics During an 8-month period, 419,297 participants were recruited from 50 states and the District of Columbia (Fig. 1). Baseline characteristics of the participants are summarized in Table 1. Irregular Pulse Notifications Over a median monitoring time of 117 days (interquartile range, 113 to 186), irregular pulse notifications were received by 2161 participants (0.52%), ranging from 3.1% of those 65 years of age or older to 0.16% of those 22 to 40 years of age (Fig. 2). Participants who received irregular pulse notifications were older, less likely to be female, more likely to be white, and more likely to have a CHA2 DS2 -VASc score of 2 or higher than the overall cohort (Table 1). (Scores on the CHA2 DS2 -VASc, which is a measure of the risk of stroke among persons with atrial fibrillation, range from 0 to 9, with higher scores indicating a greater risk.) Among participants who received a notification, 50% received notification by day 38 after enrollment, and 90% by day 133 after enrollment (Fig. S2). Atrial Fibrillation on Subsequent Ambulatory ECG Monitoring Among participants who received a notification, 450 (20.8% of all notified) returned an ECG patch, which was applied a mean (SD) of 1316 days after initial notification. ECG patches were worn for an average of 6.3 days. Participants who returned ECG patches had baseline characteristics similar to the full cohort of participants who received a notification. Among the 450 participants who returned ECG patches, atrial fibrillation was identified in 153, resulting in a diagnostic yield of atrial fibrillation on ECG patches of 34% (97.5% confidence interval [CI], 29 to 39) (Fig. 3). The ECG patches worn by participants 65 years of age or older had a diagnostic yield of atrial fibrillation of 35% (97.5% CI, 27 to 43), whereas among participants younger than 40 years of age, the diagnostic yield of atrial fibrillation was 18% (95% CI, 6 to 31). Among the 153 participants with atrial fibrillation confirmed on ambulatory ECG, 20% had continuous atrial fibrillation, whereas most of the remaining participants with atrial fibrillation had atrial fibrillation less than 50% of the time they were monitored and 89% had an episode that lasted at least 1 hour (Figs. S3 and S4). Of the 20 participants who were urgently contacted, 18 had atrial fibrillation with ventricular rates greater than 200 beats per minute for more than 30 seconds, 1 had a pause lasting more than 6 seconds, and 1 had nonsustained ventricular tachycardia lasting more than 6 seconds. Positive Predictive Values Of the 6968 tachograms sampled for adjudication, 270 were excluded because they were not of sufficient quality to be read. Of the 2089 irregular tachograms sampled from participants who had received a notification for analysis, 1489 showed simultaneous atrial fibrillation on ECG patch monitoring, resulting in a positive predictive value of the individual tachogram of 0.71 (97.5% CI, 0.69 to 0.74). For tachograms in the subgroup of participants 65 years of age or older, the positive predictive value was 0.60 (97.5% CI, 0.56 to 0.64). In the 600 irregular tachograms without simultaneous atrial fibrillation on ECG patch monitoring, frequent premature atrial contractions (6 or more over a 3-minute period) were identified in 77%, frequent premature ventricular contractions (6 or more over a 3-minute period) in 16%, and atrial tachycardias (3 or more consecutive beats) in 38%. The identification of these arrhythmias was not mutually exclusive. Sinus arrhythmia alone was found in 28 (4.7%) of the 600 irregular tachograms without atrial fibrillation. Of the 86 participants who had irregular pulse",
    "output": "The positive predictive value (PPV) for the tachogram was 71 percent and the PPV for notification was 84 percent. About half of participants receiving an irregular pulse notification contacted a study doctor. Subsequent surveys showed that 57 percent of participants who received an alert sought medical care outside of the study regardless of whether they had been seen virtually by a study doctor. \"This is encouraging because it tracks with our understanding of AFib as being more common as you get older,\" Turakhia said, adding that the overall study population represented a striking cross section of cardiovascular risk."
  },
  {
    "id_paper": "62",
    "id_story": "62-04",
    "id_chunk": "62-04_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Researcher, Teacher, Student\n\nSection: Experiments / Results\nnotifications during simultaneous use of an ECG patch, 72 showed evidence of atrial fibrillation on concurrent ECG patch strips. This resulted in a positive predictive value for the irregular pulse notification of 0.84 (95% CI, 0.76 to 0.92) among participants who had received an irregular pulse notification. For irregular pulse notifications in participants 65 years of age or older, the positive predictive value was 0.78 (95% CI, 0.64 to 0.92). 90-",
    "output": "The positive predictive value (PPV) for the tachogram was 71 percent and the PPV for notification was 84 percent. About half of participants receiving an irregular pulse notification contacted a study doctor. Subsequent surveys showed that 57 percent of participants who received an alert sought medical care outside of the study regardless of whether they had been seen virtually by a study doctor. \"This is encouraging because it tracks with our understanding of AFib as being more common as you get older,\" Turakhia said, adding that the overall study population represented a striking cross section of cardiovascular risk."
  },
  {
    "id_paper": "62",
    "id_story": "62-04",
    "id_chunk": "62-04_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Researcher, Teacher, Student\n\nSection: Experiments / Results\nDay Survey Of the 2161 participants who received an irregular pulse notification, 1376 (64%) returned a 90-day survey. Of these, 787 (57%) reported contact with a health care provider outside the study, 28% were prescribed a new medication, 33% were recommended to see a specialist (e.g., a cardiologist), and 36% were recommended to have additional testing. In total, 1041 (76%) stated that they had contacted the study visit doctor, a health care provider outside the study, or both. End-of-Study Survey Of the 2161 participants who received a notification, 929 (43%) completed an end-of-study survey; among 417,136 participants who never received a notification, 293,015 (70%) completed the survey (Table 2). Of those notified, 404 (44%) reported a new atrial fibrillation diagnosis, whereas among those who received no notification, 3070 (1.0%) reported a new atrial fibrillation diagnosis. The notification subgroup reported a greater incidence of strokes, heart failure, and myocardial infarctions than did the non-notification group. The notification subgroup was also more likely to start receiving anticoagulant therapy or aspirin. Of the 404 notified participants who reported new atrial fibrillation, 95 (24%) reported undergoing cardioversion, 12 (3%) received an implantable loop recorder, 82 (20%) started antiarrhythmic therapy, and 71 (18%) underwent catheter ablation. Of 1038 adverse events reviewed (Fig. S5), 16 (1.5%) were related to the app; of those, 15 were anxiety-related. None of the adverse events related to the app resulted in hospitalization or urgent medical attention.",
    "output": "The positive predictive value (PPV) for the tachogram was 71 percent and the PPV for notification was 84 percent. About half of participants receiving an irregular pulse notification contacted a study doctor. Subsequent surveys showed that 57 percent of participants who received an alert sought medical care outside of the study regardless of whether they had been seen virtually by a study doctor. \"This is encouraging because it tracks with our understanding of AFib as being more common as you get older,\" Turakhia said, adding that the overall study population represented a striking cross section of cardiovascular risk."
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Abstract\nAbstract: Abstract: Communication barriers pose significant challenges for the Deaf and Hard-ofHearing (DHH) community, limiting their access to essential services, social interactions, and professional opportunities. To bridge this gap, assistive technologies leveraging artificial intelligence (AI) and deep learning have gained prominence. This study presents a real-time American Sign Language (ASL) interpretation system that integrates deep learning with keypoint tracking to enhance accessibility and foster inclusivity. By combining the YOLOv11 model for gesture recognition with MediaPipe for precise hand tracking, the system achieves high accuracy in identifying ASL alphabet letters in real time. The proposed approach addresses challenges such as gesture ambiguity, environmental variations, and computational efficiency. Additionally, this system enables users to spell out names and locations, further improving its practical applications. Experimental results demonstrate that the model attains a mean Average Precision (mAP@0.5) of 98.2%, with an inference speed optimized for real-world deployment. This research underscores the critical role of AI-driven assistive technologies in empowering the DHH community by enabling seamless communication and interaction.",
    "output": "For millions of deaf and hard-of-hearing people, using sign language to communicate in a world built around spoken words can be exhausting.\nWhether it’s ordering food, asking for directions, or taking part in a classroom discussion, barriers show up everywhere.\nThat’s why researchers from Florida Atlantic University’s College of Engineering and Computer Science have created a new system that could change the way we think about accessibility.\n“This project is a great example of how cutting-edge AI can be applied to serve humanity,” said Imad Mahgoub, a co-author on the publication.\nA growing need\nThe deaf and hard-of-hearing community is large and diverse.\nIn the U.S. alone, about 11 million people – roughly 3.6% of the population – are deaf or have significant hearing loss. Around 37.5 million adults experience some level of hearing difficulty.\nThat’s a lot of people who could benefit from better communication tools.\n“The significance of this research lies in its potential to transform communication for the deaf community by providing an AI-driven tool that translates American Sign Language gestures into text, enabling smoother interactions across education, workplaces, health care and social settings,” commented Mohammad Ilyas, co-author of the research study.\nHelping the deaf community say more\nThis would make communication even more natural and fluent, moving from spelling out words to sharing entire ideas.\n“This research highlights the transformative power of AI-driven assistive technologies in empowering the deaf community,” said Stella Batalama, Dean of the Department of Electrical Engineering at FAU.\n“It allows individuals with hearing impairments to interact more seamlessly with the world around them, whether they are introducing themselves, navigating their environment, or simply engaging in everyday conversations.”\nThis technology enhances accessibility and supports greater social integration, helping create a more connected and empathetic community for everyone.\nWith continued development, this AI-powered tool may soon become part of daily life, helping millions to communicate more freely – one gesture at a time."
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Introduction\nIntroduction: 1. Introduction For millions of Deaf and Hard-of-Hearing individuals worldwide, communication barriers hinder their ability to engage effectively in everyday interactions. Traditional methods, such as sign language interpretation services, are often limited in availability, costly, and reliant on human interpreters. In an increasingly digital world, there is a growing need for intelligent assistive technologies that provide real-time, accurate, and accessible solutions to bridge the communication gap [1]. American Sign Language (ASL) is one of the most widely used sign languages, consisting of distinct hand gestures that represent letters, words, and phrases. However, existing ASL recognition systems often struggle with real-time performance, accuracy, and robustness across diverse environments. To address these limitations, this study introduces a real-time ASL interpretation system that integrates deep learning with keypoint tracking. By leveraging YOLOv11 for rapid hand gesture recognition and MediaPipe for precise landmark detection, the proposed approach enhances the efficiency and reliability of ASL interpretation [2,3]. At the core of our system, a built-in webcam functions as a non-contact optical sensor that captures real-time visual data. This vision-based sensor converts light into digital image frames, which serve as the primary input for gesture analysis. MediaPipe extracts 21 keypoints per hand from each frame to generate a skeletal representation, while YOLOv11 detects and classifies specific ASL alphabet letters based on these visual inputs. The webcam’s role as a sensing device enables the system to acquire gesture-based data continuously and reliably. It also ensures that the entire recognition pipeline—from sensing to classification—can operate in real time, even under different lighting conditions and backgrounds, using only standard hardware. This highlights the system’s practical viability as an accessible and scalable assistive technology. In addition to enhancing communication accessibility, AI-driven ASL recognition systems have the potential to revolutionize various industries. In education, they can facilitate more effective learning environments for Deaf students, allowing them to interact seamlessly with teachers and peers. In healthcare, real-time ASL interpretation can bridge the gap between Deaf patients and medical professionals, ensuring better access to quality care [4]. Similarly, workplaces can become more inclusive by integrating ASL recognition tools, enabling Deaf employees to participate fully in discussions and decision-making processes. The integration of AI and deep learning in assistive technologies not only fosters inclusivity but also paves the way for more advancements in human–computer interaction, making communication more intuitive and accessible for individuals with hearing impairments [5,6]. The significance of this research lies in its potential to empower the Deaf community by providing an AI-driven tool that translates ASL gestures into text, facilitating smoother interactions in education, workplaces, healthcare, and social settings. By developing a robust and accessible ASL interpretation system, this study contributes to the advancement of assistive technologies aimed at promoting inclusivity and bridging communication barriers for the DHH population [7]. The key contributions of our research paper can be summarized as follows: • Developed a real-time AI-driven ASL interpretation system capable of accurately spelling out names and locations through hand gesture recognition. • Integrated YOLOv11 for gesture detection and MediaPipe for hand tracking, ensuring both high accuracy and fast inference speeds. • Implemented keypoint-based annotation techniques to enhance the model’s ability to capture subtle variations in finger positioning, improving classification precision. • Utilized a standard built-in webcam as a vision-based optical sensor for gesture data acquisition, demonstrating the system’s practical application in real-time human– computer interaction and AI-driven assistive technology. • Achieved an impressive 98.2% mean Average Precision (mAP@0.5), demonstrating the system’s reliability in recognizing ASL letters. • Curated and processed a large-scale dataset of 130,000 ASL hand gesture images, each annotated with 21 keypoints to enhance model learning and generalization. The research paper is structured as follows: Sections 2 and 3 discuss relevant research and background concepts, including Transfer Learning, YOLO, and MediaPipe",
    "output": "As daily life becomes more digital, the need for smart tools that can translate sign language in real time is more urgent than ever.\nThey’ve developed a real-time American Sign Language (ASL) interpreter powered by artificial intelligence.\nThis system uses deep learning and hand-tracking to convert ASL gestures into written text, all using a regular webcam and off-the-shelf hardware.\n“And all of this is achieved using standard, off-the-shelf hardware. This underscores the system’s practical potential as a highly accessible and scalable assistive technology, making it a viable solution for real-world applications.”\nSeeing and understanding sign language\nAt the center of the system is a basic webcam, which captures live video and turns it into digital frames.\n“By fusing deep learning with hand landmark detection, our team created a system that not only achieves high accuracy but also remains accessible and practical for everyday use. It’s a strong step toward inclusive communication technologies.”\n“By bridging the communication gap through real-time ASL recognition, this system plays a key role in fostering a more inclusive society.”"
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Introduction\n, which are fundamental to real-time ASL alphabet recognition. Section 4 elaborates on the methodology used in this study, detailing dataset preparation, preprocessing, augmentation, annotation, and data balancing techniques. Section 5 presents and discusses the results obtained, evaluating model performance through precision–recall analysis, confidence-based evaluations, and confusion matrix analysis, along with the real-time system’s robustness and accuracy. Finally, a general conclusion is provided in",
    "output": "As daily life becomes more digital, the need for smart tools that can translate sign language in real time is more urgent than ever.\nThey’ve developed a real-time American Sign Language (ASL) interpreter powered by artificial intelligence.\nThis system uses deep learning and hand-tracking to convert ASL gestures into written text, all using a regular webcam and off-the-shelf hardware.\n“And all of this is achieved using standard, off-the-shelf hardware. This underscores the system’s practical potential as a highly accessible and scalable assistive technology, making it a viable solution for real-world applications.”\nSeeing and understanding sign language\nAt the center of the system is a basic webcam, which captures live video and turns it into digital frames.\n“By fusing deep learning with hand landmark detection, our team created a system that not only achieves high accuracy but also remains accessible and practical for everyday use. It’s a strong step toward inclusive communication technologies.”\n“By bridging the communication gap through real-time ASL recognition, this system plays a key role in fostering a more inclusive society.”"
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Introduction\nSection 6.",
    "output": "As daily life becomes more digital, the need for smart tools that can translate sign language in real time is more urgent than ever.\nThey’ve developed a real-time American Sign Language (ASL) interpreter powered by artificial intelligence.\nThis system uses deep learning and hand-tracking to convert ASL gestures into written text, all using a regular webcam and off-the-shelf hardware.\n“And all of this is achieved using standard, off-the-shelf hardware. This underscores the system’s practical potential as a highly accessible and scalable assistive technology, making it a viable solution for real-world applications.”\nSeeing and understanding sign language\nAt the center of the system is a basic webcam, which captures live video and turns it into digital frames.\n“By fusing deep learning with hand landmark detection, our team created a system that not only achieves high accuracy but also remains accessible and practical for everyday use. It’s a strong step toward inclusive communication technologies.”\n“By bridging the communication gap through real-time ASL recognition, this system plays a key role in fostering a more inclusive society.”"
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Method\nMethod: 4. Methodology This study aims to develop a real-time American Sign Language (ASL) alphabet recognition system that accurately identifies and translates ASL hand gestures into text, enabling users to spell names and locations interactively. The proposed approach integrates YOLOv11 for gesture detection and MediaPipe for hand tracking to enhance precision and real-time responsiveness. A major challenge in ASL recognition systems lies in distinguishing visually similar gestures, such as “A” and “T” or “M” and “N”, which often leads to misclassifications. Additionally, the dataset quality presents significant obstacles, including poor image resolution, motion blur, inconsistent lighting, and variations in hand sizes, skin tones, and backgrounds. These factors introduce bias and reduce the model’s ability to generalize across different users and environments [29]. To address these challenges, a large-scale dataset was curated, incorporating custom keypoint-based annotations to enhance the model’s ability to detect subtle variations in finger positioning. This ensures improved accuracy, robustness against variations in hand structure, and better adaptability to real-world conditions. Figure 4 presents an overview of the data processing pipeline for the proposed realtime ASL recognition system. It illustrates the sequential stages, including data preprocessing, augmentation, annotation, and model training, leading to the final real-time recognition system. 4.1. Dataset Description The ASL Alphabet Hand Gesture Dataset is a comprehensive collection of hand gesture images designed to train deep learning models for real-time ASL recognition. This dataset enhances computer vision models by enabling the precise classification of ASL alphabet gestures, thereby supporting real-time sign language interpretation systems . Comprising 130,000 images, the dataset captures diverse hand gestures under varying conditions to improve model generalization. These conditions include • Different lighting environments: bright, dim, and shadowed. • Various backgrounds: natural outdoor settings and indoor scenes. • Multiple angles and hand orientations for robustness. Each image is annotated with 21 keypoints, accurately marking critical hand structures such as fingertips, knuckles, and the wrist (see Figure 5). These annotations provide a skeletal representation of the hand, enabling the model to distinguish between similar gestures with greater precision. Figure 5. Annotated ASL Alphabet Hand Gestures with 21 keypoints for model training. For optimal training and evaluation, the dataset is systematically partitioned as follows: • Training set: 80%. • Validation set: 10%. • Testing set: 10%. This structured division ensures that the trained models achieve high performance in real-world ASL recognition applications, making the dataset a valuable resource for AI-driven sign language accessibility solutions. 4.2. Data Preprocessing • Resizing: Standardizing all images to 640 640 pixels for consistency. • Contrast adjustment: Enhancing visual clarity using contrast stretching. • Grayscale conversion: Applied to 10% of images to improve robustness to color variations. 4.3. Data Augmentation To enhance generalization and mitigate overfitting, 30% of the dataset underwent augmentation with the following transformations: • Flipping: Horizontal and vertical flips to introduce diverse orientations. • Rotation: Random variations between 10and +10to simulate natural hand positioning. • Brightness adjustment: Random modifications within 15% to accommodate lighting variations. • Grayscale conversion: Selectively applied to certain images to enhance the model’s focus on gesture structure. These augmentations significantly improve the model’s ability to recognize ASL gestures across different environmental conditions, ensuring higher accuracy and reliability in real-time applications. 4.4. Data Annotation To optimize YOLO for ASL hand gesture recognition, images were annotated using 21 keypoints extracted from hand landmarks. Unlike traditional bounding boxes that enclose the entire hand without precisely identifying its central structure, this approach prioritizes the 21 keypoints detected by MediaPipe. From these keypoints, a bounding box is computed based on their spatial distribution, ensuring it accurately encapsulates the gesture while preserving crucial articulation details. By leveraging this method, the annotation process enhances precision, as it focuses on the actual structure of the hand rather",
    "output": "“What makes this system especially notable is that the entire recognition pipeline – from capturing the gesture to classifying it operates seamlessly in real time, regardless of varying lighting conditions or backgrounds,” said Bader Alsharif, lead author on the study.\nMediaPipe then pinpoints 21 key spots on each hand – fingertips, knuckles, and the wrist – to build a kind of skeleton map. These points help the system understand hand structure and motion.\nCheck us out on EarthSnap, a free app brought to you by Eric Ralls and Earth.com."
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Method\nthan just its outer boundaries, leading to more reliable recognition and classification. The annotation process was automated and underwent the following steps: 1. Hand Landmark Detection: • MediaPipe extracted 21 keypoints corresponding to joints and finger positions of the hand. • The detected keypoints were scaled and normalized to maintain consistency across varying image resolutions. 2. Bounding Box Calculation: A bounding box was",
    "output": "“What makes this system especially notable is that the entire recognition pipeline – from capturing the gesture to classifying it operates seamlessly in real time, regardless of varying lighting conditions or backgrounds,” said Bader Alsharif, lead author on the study.\nMediaPipe then pinpoints 21 key spots on each hand – fingertips, knuckles, and the wrist – to build a kind of skeleton map. These points help the system understand hand structure and motion.\nCheck us out on EarthSnap, a free app brought to you by Eric Ralls and Earth.com."
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Method\ngenerated to enclose only the 21 keypoints that were previously captured on the hand by determining the minimum and maximum coordinates from the extracted keypoints. 4.4.1. Bounding Box Computation Since YOLO requires object detection labels in the format (x_center, y_center, width, height), the bounding box was calculated using the following mathematical expressions: Finding the Bounding Box Extent The bounding box must enclose all 21 detected keypoints. To determine its position, we compute the top-left corner(xmin, ymin) and bottom-right corner (xmax, ymax): xmax = max(x0, x1, . . . , x20), ymax = max(y0, y1, . . . , y20) Here, xmin and ymin represent the smallest x and y values among all detected keypoints, defining the top-left corner of the bounding box. Similarly, xmax and ymax represent the largest x and y values, defining the bottom-right corner. Computing Bounding Box Dimensions The width and height of the bounding box are calculated as width = xmax xmin, height = ymax ymin These values represent the total span of the hand in the x and y directions. Determining the Bounding Box Center Since YOLO requires bounding box coordinates in a center-based format, the center of the bounding box is computed as xcenter = xmin + xmax This ensures the bounding box is properly centered around the detected hand. Normalization for YOLO Format Given that keypoints are already normalized between 0 and 1 relative to image dimensions, the bounding box coordinates must also be constrained within the same range: xcenter, ycenter, width, height [0, 1] This prevents values from exceeding the image boundaries, ensuring consistency across different image resolutions. 4.4.2. Annotation Format for YOLO Each detected hand was assigned a class label corresponding to the ASL letter and formatted according to YOLO’s labeling convention: • Class Label: An integer (0–25) representing the ASL letter. • where Ki represents the i-th keypoint, and xi, yi are in the range [0, 1]. • Bounding Box: Stored in the YOLO format as Figure 6 illustrates the effectiveness of the annotation process during batch processing. It shows bounding boxes that focus solely on the 21 keypoints, ensuring precise localization. This structured annotation approach enables YOLO to detect and classify ASL gestures with greater precision, leading to a fine-tuned, real-time ASL recognition system optimized for robust gesture classification. 2 , ycenter = ymin + ymax 2 Ki = (xi, yi), i [0, 20] (xcenter, ycenter, width, height) Figure 6. Effectiveness of annotation in batch processing with precise keypoint localization. 4.4.3. Data Balancing Balancing the dataset is essential for training YOLOv11 to detect and interpret the American Sign Language (ASL) alphabet in real time. Without balance, the model favors more frequent classes, leading to bias where certain letters are detected with high accuracy while others are misclassified or ignored. This results in unreliable performance, making the system unsuitable for users relying on ASL [30]. An imbalanced dataset also affects YOLOv11’s detection confidence. If certain letters appear more often, the model assigns higher confidence to them while lowering confidence for less frequent ones, leading to poor recall and inconsistent real-time detection. Balancing the dataset ensures uniform exposure, resulting in stable confidence scores and improved accuracy across all classes. Overfitting is another critical issue. If the model encounters significantly more samples of some letters, it becomes biased toward those while failing to learn the characteristics of rare ones. This weakens its ability to recognize less frequent letters, reducing overall effectiveness [31]. To",
    "output": "“What makes this system especially notable is that the entire recognition pipeline – from capturing the gesture to classifying it operates seamlessly in real time, regardless of varying lighting conditions or backgrounds,” said Bader Alsharif, lead author on the study.\nMediaPipe then pinpoints 21 key spots on each hand – fingertips, knuckles, and the wrist – to build a kind of skeleton map. These points help the system understand hand structure and motion.\nCheck us out on EarthSnap, a free app brought to you by Eric Ralls and Earth.com."
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Method\naddress these challenges, the upsampling technique has been utilized through data augmentation to balance the dataset to 5000 images per class. This ensures equal learning across all ASL letters, prevents bias, enhances generalization, avoids overfitting, and improves real-world performance. A well-balanced dataset is crucial for building a reliable, inclusive, and effective real-time ASL interpretation system. Figure 7 illustrates the",
    "output": "“What makes this system especially notable is that the entire recognition pipeline – from capturing the gesture to classifying it operates seamlessly in real time, regardless of varying lighting conditions or backgrounds,” said Bader Alsharif, lead author on the study.\nMediaPipe then pinpoints 21 key spots on each hand – fingertips, knuckles, and the wrist – to build a kind of skeleton map. These points help the system understand hand structure and motion.\nCheck us out on EarthSnap, a free app brought to you by Eric Ralls and Earth.com."
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Method\ndataset’s distribution and the spatial characteristics of the bounding boxes, offering insight into the overall structure and organization of the data. Top-left (Bar Chart): This plot confirms that the dataset is balanced, with each of the distribution ensures that no letter is overrepresented or underrepresented, preventing model bias. Top-right (Bounding Box Overlays): This visualization displays multiple bounding boxes superimposed on each other. The tight clustering of bounding boxes suggests that the hand positions across different images are well aligned and consistently annotated, reducing variations that could impact the model’s learning. Bottom-left (Heatmap of x-y Coordinates): This heatmap shows the spatial distribution of the hand’s center positions in normalized coordinates. The density around the center suggests that most hand gestures are captured within a specific region of the frame, making it easier for YOLOv11 to detect and recognize them accurately. Bottom-right (Heatmap of Width–Height Distribution): This plot highlights the distribution of bounding box sizes. The concentration of values in a specific region indicates that the hands in the dataset mostly follow a uniform scale, ensuring that the model generalizes well without being biased toward specific sizes. This visualization confirms that the dataset is structured optimally for YOLOv11 training, ensuring consistency across annotations and reducing variability in hand positioning and bounding box dimensions. Figure 7. Overview of the dataset distribution and spatial properties of the bounding boxes.",
    "output": "“What makes this system especially notable is that the entire recognition pipeline – from capturing the gesture to classifying it operates seamlessly in real time, regardless of varying lighting conditions or backgrounds,” said Bader Alsharif, lead author on the study.\nMediaPipe then pinpoints 21 key spots on each hand – fingertips, knuckles, and the wrist – to build a kind of skeleton map. These points help the system understand hand structure and motion.\nCheck us out on EarthSnap, a free app brought to you by Eric Ralls and Earth.com."
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Experiments / Results\nExperiments / Results: 5. Results and Discussion 5.1. Model Performance and Evaluation Metrics The performance evaluation of the proposed American Sign Language (ASL) alphabet recognition system was conducted using standard multiclass classification metrics, including precision, recall, F1-score, and mean Average Precision (mAP). The system is powered by the trained YOLO11 model, a deep learning architecture with 181 layers and 2,594,910 parameters, optimized for efficient real-time inference. With a computational complexity of 6.5 GFLOPs (Giga Floating Point Operations per Second) per inference, the model was rigorously tested on a balanced dataset of ASL alphabet hand gesture images. The training was conducted with 300 epochs, a batch size of 16, and an input image size of 640  640. The AdamW optimizer was employed to enhance training stability. The chosen configuration ensures the model’s reliability, accuracy, and scalability for real-world ASL translation applications. The model’s performance is summarized in Table 3. Table 3. Performance metrics of the ASL recognition model. Table 3. Cont. Metric Value F1-Sore (F) 0.991 mAP@0.5 0.982 mAP@0.5–0.95 0.933 Inference Speed 1.3 ms per image Post-processing Speed 0.3 ms per image These results demonstrate the model’s strong ability to accurately detect and classify ASL hand gestures with a high mean Average Precision (mAP@0.5) of 98.2%, confirming its effectiveness in a real-time sign language interpretation system. 5.2. Precision–Recall Analysis in Multiclass Classification The precision–recall (PR) curve provides insight into how the model balances precision and recall across different confidence thresholds for a multiclass classification setting [32]. In a multiclass classification setting, the precision and recall for each class c are computed as where • TPc (True Positives): Correctly classified instances of class c. • FPc (False Positives): Instances incorrectly classified as class c. • FNc (False Negatives): Instances of class c that were misclassified. From Figure 8, we observe that precision remains above 0.98 across most recall values, confirming minimal false positive detections. The slight degradation in precision at extreme recall values suggests that the model maintains a conservative detection strategy. Precisionc = TPc TPc + FPc (2) Recallc = TPc TPc + FNc (3) 5.3. Confidence-Based Evaluations for Multiclass Classification To further analyze model performance, we examine the recall–confidence, precision– confidence, and F1–confidence curves [33]. 5.3.1. Recall–Confidence Curve In a multiclass classification scenario, the recall–confidence curve illustrates how recall changes as the confidence threshold  increases [34]. The global recall at a given confidence level  is computed as Recall() = N c=1 TPc() As seen in Figure 9, recall remains at its highest value (close to 0.99) for lower confidence thresholds, ensuring that most detections are retained. However, as the confidence threshold surpasses  = 0.8, recall starts to decline sharply. This behavior indicates a trade-off where the model prioritizes precision over recall, filtering out lower-confidence predictions to improve accuracy while reducing the number of retrieved positive instances. The sharp drop in recall near  = 0.99 suggests that only the highest-confidence predictions are retained, which can be useful for applications requiring high precision such ASL interpretation systems. Figure 9. Recall–confidence curve. 5.3.2. Precision–Confidence Curve The global precision at confidence  is computed as Precision() = N c=1 TPc() From Figure 10, precision remains above 0.90 across nearly all confidence levels. The model achieves perfect precision at confidence  = 0.985, indicating that highconfidence detections are nearly always correct. N c=1 TPc() + FNc() (4) N c=1 TPc() + FPc() (5) Figure 10. Precision–confidence curve. 5.3.3. F1–Confidence Curve The F1-score at confidence  balances precision and recall [35]: F1() = 2  Precision()  Recall() Figure 11 shows the optimal F1-score occurs at  = 0.99,",
    "output": "Recognizing sign language is tricky for machines\nASL relies on precise hand shapes and movements to represent letters, words, and phrases. But current recognition tools often miss the mark – especially when signs look alike.\nTogether, they allow the system to detect and classify ASL alphabet letters with an accuracy of 98.2% (mean Average Precision at 0.5). The entire process works in real time and with very little delay.\nYOLOv11 uses this skeletal data to match hand gestures accurately to ASL letters.\n—–\n—–"
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Experiments / Results\nsuggesting the best trade-off between detection accuracy and reliability. Precision() + Recall() (6) 5.4. Confusion Matrix Analysis The confusion matrix provides a class-wise breakdown of model performance [36]. For a multiclass, the confusion matrix element at (i, j) represents CMij = P(ypred = j | ytrue = i) (7) where ypred and ytrue are the predicted and actual labels, respectively. Refer to Figure 12 for a visualization. Figure 12. Normalized confusion matrix for ASL alphabet recognition. Key findings: • The model demonstrates high classification accuracy with minimal misclassifications. • Class-wise performance remains consistent, ensuring robustness across all ASL alphabet classes. • Confusion is minimal, confirming the effectiveness of the YOLO11n architecture in distinguishing ASL gestures. These findings confirm the model’s strong classification ability, making it a promising solution for real-time ASL translation applications. 5.5. Evaluation of a Real-Time ASL Interpretation System Evaluating a real-time American Sign Language (ASL) interpretation system for users to spell names and locations requires a comprehensive assessment of key performance aspects, including accuracy, latency, robustness, and usability. To conduct this evaluation, we utilized a laptop’s built-in webcam for real-time hand tracking and ASL recognition. In our system, the webcam serves as a non-contact vision-based sensor that captures gesture data in the form of image frames. These frames are processed using MediaPipe to extract 21 hand keypoints and passed to YOLOv11 for ASL gesture classification. This sensor-driven pipeline enables real-time processing under various lighting and background conditions using only standard hardware. 5.5.1. System Specifications The system was tested on a laptop with the following specifications: 5.5.2. Performance Analysis Our analysis of image processing per frame revealed the following time distribution: • Preprocessing: 1.1 ms. • Inference (Model Prediction): 70.9 ms. • Loss Calculation: 0.0 ms. • Postprocessing: 0.5 ms. 5.5.3. Robustness Across Different Conditions To further evaluate the system’s effectiveness, we tested it under various conditions, including different hand shapes, skin tones, and background complexities. The system demonstrated strong generalization capabilities, accurately recognizing these variations with minimal error. This robustness can be attributed to the integration of MediaPipe, which focuses solely on detecting 21 keypoints of the hand. Once the hand shape is determined, this information is passed to YOLOv11 for the final recognition process. Despite the two-step approach—first detecting the hand shape and keypoints and then processing the recognition—our model remains fast and highly efficient in detecting hand gestures and translating them into text. 5.5.4. ASL Translation to Names and Locations Figures 13 and 14 illustrate the system’s ability to interpret ASL gestures into meaningful text. (Figure 13. The model translating ASL into a name. Figure 14. The model translating ASL into a location.) These results highlight the system’s efficiency in real-time ASL interpretation, ensuring minimal latency while maintaining high accuracy across diverse conditions",
    "output": "Recognizing sign language is tricky for machines\nASL relies on precise hand shapes and movements to represent letters, words, and phrases. But current recognition tools often miss the mark – especially when signs look alike.\nTogether, they allow the system to detect and classify ASL alphabet letters with an accuracy of 98.2% (mean Average Precision at 0.5). The entire process works in real time and with very little delay.\nYOLOv11 uses this skeletal data to match hand gestures accurately to ASL letters.\n—–\n—–"
  },
  {
    "id_paper": "37",
    "id_story": "37-05",
    "id_chunk": "37-05_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Student, Teacher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 6. Conclusions The development of assistive technologies for the Deaf and Hard-of-Hearing community is crucial in fostering inclusivity and eliminating communication barriers. This study presents a real-time ASL interpretation system that effectively integrates deep learning with keypoint tracking, demonstrating high accuracy and efficiency in recognizing ASL alphabet gestures. By leveraging YOLOv11 for gesture detection and MediaPipe for precise hand tracking, the proposed system ensures robust performance across diverse conditions. Experimental results validate the system’s effectiveness, achieving a mean Average Precision (mAP@0.5) of 98.2% with minimal latency, making it highly suitable for realtime applications. The ability to translate ASL gestures into text in real time significantly enhances accessibility in various domains, including education, healthcare, and professional settings. Future work will focus on extending the system’s capabilities beyond individual letter recognition to full ASL sentence interpretation, enabling more natural and fluid communication. This will require advancements in sequence modeling techniques, such as recurrent neural networks (RNNs), transformers, or temporal convolutional networks (TCNs), to effectively capture contextual dependencies between consecutive gestures. In conclusion, this research underscores the transformative potential of AI-driven assistive technologies in empowering the Deaf community. By bridging the communication gap through real-time ASL recognition, this system contributes to a more inclusive society where individuals with hearing impairments can interact seamlessly with the world around them, whether introducing themselves or indicating locations.",
    "output": "While interpreters and captioning services can help, they’re often limited, costly, or unavailable when most needed.\nFor example, “A” and “T” or “M” and “N” can be hard to tell apart for machines. These tools also struggle in poor lighting, with motion blur, and with differences in hand shape or skin tone, all of which affect how accurate the machine interpretations are.\n“By developing a robust and accessible ASL interpretation system, our study contributes to the advancement of assistive technologies to break down barriers for the deaf and hard-of-hearing population.”\nWhile this new tool already shows strong results for recognizing the ASL alphabet, the team isn’t stopping there. They’re now working to expand the system to understand full ASL sentences.\nLike what you read? Subscribe to our newsletter for engaging articles, exclusive content, and the latest updates."
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Abstract\nAbstract: Abstract We present a method to edit a target portrait footage by taking a sequence of audio as input to synthesize a photorealistic video. This method is unique because it is highly dynamic. It does not assume a person-specific rendering network yet capable of translating arbitrary source audio into arbitrary video output. Instead of learning a highly heterogeneous and nonlinear mapping from audio to the video directly, we first factorize each target video frame into orthogonal parameter spaces, i.e., expression, geometry, and pose, via monocular 3D face reconstruction. Next, a recurrent network is introduced to translate source audio into expression parameters that are primarily related to the audio content. The audio-translated expression parameters are then used to synthesize a photo-realistic human subject in each video frame, with the movement of the mouth regions precisely mapped to the source audio. The geometry and pose parameters of the target human portrait are re- tained, therefore preserving the context of the original video footage. Finally, we introduce a novel video rendering network and a dynamic programming method to construct a temporally coherent and photo-realistic video. Extensive experiments demonstrate the superiority of our method over existing approaches. Our method is end-to-end learnable and robust to voice variations in the source audio. Some results are shown in Fig. 1. Video results are shown on our project page1.",
    "output": "The ability to realistically put words into your mouth could be a serious invasion of privacy and new technology is capable of pulling off astonishing level of originality.\n\nSamantha Cole wrote this piece for Vice describing the alarming progress in AI assisted deepfake capabilities now accessible to any one.\n\nThe process according to the url “Everybody’s Talking’: Let me Talk as You Want,” by researchers at the Nanyang Technological University, Singapore is.. ”To .. first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face,…They then reconstruct a 3D face mesh to match lip movements that match source audio phonemes, or individual sounds—similar to how recent text-to-video methods work.”\n\nSo why should you be concerned? According to the author.. “SenseTime, one of the companies that developed this research, was recently implicated in developing technology that helped the Chinese government profile a Muslim minority group.”\n\nOne way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\n\nWhere I learnt this #420\n\nNew Deepfake Method Can Put Words In Anyone’s Mouth\n\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips\n\nEverybody’s Talkin’: Let Me Talk as You Want\n\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Introduction\nIntroduction: 1. Introduction I’m going where the sun keeps shining. Through the pouring rain. Going where the weather suits my clothes. Fred Neil, Everybody’s Talkin’ Video portrait editing is a highly sought-after technique in view of its wide applications, such as filmmaking, video production, and telepresence. Commercial video editing applications, such as Adobe Premiere and Apple iMovie, are resource-intensive tools. Indeed, editing audio-visual content would require one or more footages to be reshot. Moreover, the quality of the edited video is highly dependent on the prowess of editors. Audio-based approach is an attractive technique for automatic video portrait editing. Several methods [6, 62] are proposed to animate the mouth region of a still image to follow an audio speech. The result is an animated static image rather than a video, hence sacrificing realism. Audiodriven 3D head animation [46] is an audio-based approach but aiming at a different goal, namely to drive stylized 3D computer graphic avatars, rather than to generate a photorealistic video. Suwajanakorn et al. [45] attempted to synthesize photo-realistic videos driven by audio. While impressive performance was achieved, the method assumes the source audio and target video to come from the same identity. The method is only demonstrated on the audio tracks and videos of Barack Obama. Besides, it requires long hours of single-identity data (up to 17 hours [45]) for training using relatively controlled and high-quality shots. In this paper, we investigate a learning-based framework that can perform many-to-many audio-to-video translation, i.e., without assuming a single identity of source audio and the target video. We further assume a scarce number of target video available for training, e.g., at most a 15-minute footage of a person is needed. Such assumptions make our problem non-trivial: 1) Without sufficient data, especially in the absence of source video, it is challenging to learn direct mapping from audio to video. 2) To apply the framework on arbitrary source audios and target videos, our method needs to cope with large audio-video variations between different subjects. 3) Without explicitly specifying scene geometry, materials, lighting, and dynamics, as in the case of a standard graphics rendering engine, it is hard for a learning-based framework to generate photo-realistic yet temporally coherent videos. To overcome the aforementioned challenges, we propose to use the expression parameter space, rather than the full pixels, as the target space for audio-to-video mapping. This facilitates the learning of more effective mapping, since the expression is semantically more relevant to the audio source, compared to other orthogonal spaces, such as geometry and pose. In particular, we manipulate the expression of a target face by generating a new set of parameters through a novel LSTM-based Audio-to-Expression Translation Network. The newly generated expression parameters, combined with geometry and pose parameters of the target human portrait, allow us to reconstruct a 3D face mesh with the same identity and head pose of the target but with new expression (i.e., lip movements) that matches the phonemes of the source audio. We further propose an Audio ID-Removing Network that keeps audio-to-expression translation agnostic to the identity of the source audio. Thus, the translation is robust to variations in the voices of different people in different source audio. Finally, we solve the difficult face generation problem as a face completion problem conditioned on facial landmarks. Specifically, after reconstructing a 3D face mesh with new expression parameters, we extract the associated 2D landmarks from the mouth region and represent them as heatmaps. These heatmaps are combined with target frames where the mouth region is masked. Taking the landmark heatmaps and the masked target frames as inputs, a video rendering network is then used to complete the mouth region of each frame guided by dynamics of the landmarks. We summarize our contributions as follows: 1) We make the first attempt at formulating an end-to-end learnable framework that supports audio-based video portrait editing. We demonstrate",
    "output": "One way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Introduction\ncoherent and photo-realistic results by focusing specifically on expression parameter space as the target space, from which source audios can be effectively translated into target videos. 2) We present an Audio ID-Removing Network that encourages an identity-agnostic audio-to-expression translation. This network allows our framework to cope with large variations in voices that are present in arbitrary audio sources. 3) We propose a Neural Video Rendering Network based on the notion",
    "output": "One way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Introduction\nof face completion with a masked face as input and mesh landmarks as conditions. This approach facilitates the generation of photo-realistic video for arbitrary people within one single network. Ethical Considerations. Our method could contribute greatly towards advancement in video editing. We envisage relevant industries, such as filmmaking, video production, and telepresence to benefit immensely from this technique. We do acknowledge the potential of such forward-looking technology being misused or abused for various malevolent purposes, e.g., aspersion, media manipulation, or dissemination of malicious propaganda. Therefore, we strongly advocate and support all safeguarding measures against such exploitative practices. We welcome enactment and enforcement of legislation to obligate all edited videos to be distinctly labeled as such, to mandate informed consent be collected from all subjects involved in the edited video, as well as to impose hefty levy on all law defaulters. Being at the forefront of developing creative and innovative technologies, we strive to develop methodologies to detect edited video as a countermeasure. We also encourage the public to serve as sentinels in reporting any suspicious-looking videos to the authority. Working in concert, we shall be able to promote cutting-edge and innovative technologies without compromising the personal interest of the general public.",
    "output": "One way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Method\nMethod: 3. Methodology The architecture of the proposed method is shown in Fig. 2. First, we register a parametric 3D face model [5] in the target video, for every portrait video frame to extract face geometry, pose, and expression parameters. Then, the Audio-to-Expression Translation Network learns the mapping from the source audio feature to face expression parameters. We design an Audio ID-Removing Network to alleviate the issues on large variations caused by multiple speakers. Lastly, we formulate the talking face generation problem as a face completion problem guided by mouth region landmarks, in which the landmarks are projected from the restructured 3D facial mesh. We propose a Neural Video Rendering network to complete the mouth region of each frame, guided by the dynamics of the landmarks to generate a photo-realistic portrait video. 3.1. 3D Face Modeling We leverage a parametric 3D face model [5] on portrait video frame to recover low dimensional geometry, expression, and pose parameters. To reduce parameter dimension, geometry and expression bases are computed based on high-quality head scans [2] and facial blendshapes [5, 1] Figure 2: Architecture. Our network contains an Audio-to-Expression Translation Network that learns facial expression parameters from speech audio and a Neural Video Rendering Network that generates mouth region guided by projected mouth landmarks. via principal component analysis (PCA). The geometry parameters s R199 and the expression parameters e R29 are the coefficients of geometry and expression principle components in the PCA, respectively. The pose of the head p R6 which contains 3 head rotation coefficients, 2 translation coefficients (x and y directions on the screen surface), and 1 scaling coefficient. All the parameters are computed by solving a non-linear optimization problem, constrained by the statistical linear 3D face model [2]. By optimizing the geometry, expression, and pose parameters of a given monocular face image based on its detected facial landmarks, protrait video frames will be automatically annotated with low dimensional vectors [2]. The recovered expression parameters are used as the learning target in the Audio-to-Expression Translation Network. Then, the recovered geometry and pose parameters, together with the expression parameters inferred by the Audio-to-Expression Translation Network, are employed for reconstructing the 3.2. Audio-to-Expression Translation 3.2.1 Audio ID-Removing Network We empirically find that identity information embedded in the speech feature degrades the performance of mapping speech to mouth movement. Inspired by recent advances of the speaker adaptation method in the literature of speech recognition [50, 39], we transfer the speech feature lies in different speaker domains onto a “global speaker” domain by applying a linear transformation, in the form [50]: x ′ = Wix + bi = Wi x, where  Wi = (Wi, bi), x = (x; 1),  Wi = I + Pk j=1 j W j (1) Here, x and x ′ represent the raw and transferred speech feature, respectively, while  Wi = I + Pk j=1 j W j represents the speaker-specific adaptation parameter that is factorized into an identity matrix I plus weighted sum of k components W j [39]. In speech recognition, these parameters are iteratively optimized by fMLLR [11, 15] and EM Figure 3: Audio ID-Removing Network. We formulate the speaker adaptation method from speech recognition [50, 39] as a neural network. The network removes identity in speech MFCC spectrum by transferring it to the “global speaker” domain. algorithms. We formulate the above method into a neural network to be integrated with our end-to-end deep learning network. From Eq.(1), the parameters j need to be learned from the input speech feature, while the matrix components W j is general speech features of different speakers. Thus, we design an LSTM+FC network to infer j from the input and set the matrix components W j as the optimizing parameter of the Audio ID-Removing Network. The matrix components W j of the Audio ID-Removing Network are updated by the gradient descent-based algorithm. The details of the network is depicted in Fig. 3. The output of the Audio IDRemoving Network is a new MFCC (Mel-frequency cepstral coefficients) spectrum. We apply a pre-trained speaker identity network",
    "output": "The ability to realistically put words into your mouth could be a serious invasion of privacy and new technology is capable of pulling off astonishing level of originality.\n\nSamantha Cole wrote this piece for Vice describing the alarming progress in AI assisted deepfake capabilities now accessible to any one.\n\nThe process according to the url “Everybody’s Talking’: Let me Talk as You Want,” by researchers at the Nanyang Technological University, Singapore is.. ”To .. first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face,…They then reconstruct a 3D face mesh to match lip movements that match source audio phonemes, or individual sounds—similar to how recent text-to-video methods work.”\n\nSo why should you be concerned? According to the author.. “SenseTime, one of the companies that developed this research, was recently implicated in developing technology that helped the Chinese government profile a Muslim minority group.”\n\nOne way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\n\nWhere I learnt this #420\n\nNew Deepfake Method Can Put Words In Anyone’s Mouth\n\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips\n\nEverybody’s Talkin’: Let Me Talk as You Want\n\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Method\nVGGVox [33, 8] on the new MFCC spectrum and constrain the Audio ID-Removing Network by the following cross-entropy loss function: N X 1 N log p(c|x ′), (2) Lnorm = c=1 where N is the number of speakers, c is the speaker class label. The p(c|x ′) is the probability of assigning MFCC x ′ to speaker c, which is inferred from the pre-trained VGGVox. Eq. (2) enforces the Audio ID-Removing Network to produce an MFCC spectrum that is not distinguishable by the pre-trained VGGVox. 3.2.2 Audio-to-Expression Translation Network We formulate a simple but effective Audio-to-Expression Translation Network that learns the mapping from the IDremoved MFCC feature to the corresponding facial expression parameters. To infer the expression parameters at time t, the translation network observes a sliding window speech clip of 1 second, which contains 0.8 seconds before time t and 0.2 seconds after time t. We empirically find it challenging to train a network to solely regress the expression parameters. The underlying reason could be that the expression parameters are defined and related to the 3DMM model that is hard to model by the network. To facilitate the learning, we introduce a shape constraint. In particular, with the predicted expression parameters from audio and the ground truth geometry/pose parameters of the video portrait, we can obtain a predicted reconstructed 3D facial mesh. Then, we project 3D points of mouth area to the 2D space to obtain the predicted 2D mouth landmarks. Using a similar method, we can obtain a set of ground-truth 2D mouth landmarks from the groundtruth expression parameters. The shape constraint can be introduced between the predicted 2D mouth landmarks and ground-truth 2D mouth landmarks. The whole process of generating mouth landmarks from expression parameters only involves linear operations and thus is differential. The loss function is written as follows: Ltrans = Lexp + Lshape = ||e e||2 + ||l l||2, (3) where e and l are the ground truth expression and landmark, respectively, and e and l are the output expression and landmark of the translation network, respectively. The Audio ID-Removing and Audio-to-Expression Translation Networks are trained jointly, whose objective function is weighted sum of Lnorm (Eq. (2)) and Ltrans (Eq. (3)). 3.3. Neural Video Rendering Network 3.3.1 Network Architecture Our final step is to generate photo-realistic talking face video that is conditioned on dynamic background portrait video and is guided by the mouth region landmark heatmap sequence. We design a completion-based generation network that completes the mouth region guided by mouth landmarks. First, to obtain the masked face images, a tailored dynamic programming based on retiming algorithm inspired by [45] is introduced to select frame sequence whose head shaking and blink of eyes look compatible with the source speech. Then, the mouth area that contains lip, jaw, and nasolabial folds are manually occluded by a square mask filled with random noise. To make the conversion from the landmark coordinates to heatmap differentiable, we follow [22, 56] to generate heatmaps with Gaussianlike functions centered at landmark locations. We mod- ify a Unet [42, 40]-based network as our generation network. The employed skip-connection enables our network to transfer fine-scale structure information. In this way, the landmark heatmap at the input can directly guide the mouth region generation at the output, and the structure of the generated mouth obeys the heatmaps [40, 53]. We composite the generated mouth region over the target face frame according to the input mouth region mask. To obtain the mouth region mask, we connect the outermost mouth landmarks as a polygon and fill it with white color, then we erode the binary mask and smooth its boundaries with a Gaussian filter [26].",
    "output": "The ability to realistically put words into your mouth could be a serious invasion of privacy and new technology is capable of pulling off astonishing level of originality.\n\nSamantha Cole wrote this piece for Vice describing the alarming progress in AI assisted deepfake capabilities now accessible to any one.\n\nThe process according to the url “Everybody’s Talking’: Let me Talk as You Want,” by researchers at the Nanyang Technological University, Singapore is.. ”To .. first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face,…They then reconstruct a 3D face mesh to match lip movements that match source audio phonemes, or individual sounds—similar to how recent text-to-video methods work.”\n\nSo why should you be concerned? According to the author.. “SenseTime, one of the companies that developed this research, was recently implicated in developing technology that helped the Chinese government profile a Muslim minority group.”\n\nOne way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\n\nWhere I learnt this #420\n\nNew Deepfake Method Can Put Words In Anyone’s Mouth\n\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips\n\nEverybody’s Talkin’: Let Me Talk as You Want\n\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Method\nWith the soft mask, we leverage Poisson blending [36] to achieve seamless blending. To improve the temporal continuity of generated video, we apply a sliding window on the input masked video frames and heatmaps [27, 26]. The input of the Neural Video Rendering Network is a tensor stacked by 7 RGB frames and 7 heatmap gray images [26]. It works well in most cases while",
    "output": "The ability to realistically put words into your mouth could be a serious invasion of privacy and new technology is capable of pulling off astonishing level of originality.\n\nSamantha Cole wrote this piece for Vice describing the alarming progress in AI assisted deepfake capabilities now accessible to any one.\n\nThe process according to the url “Everybody’s Talking’: Let me Talk as You Want,” by researchers at the Nanyang Technological University, Singapore is.. ”To .. first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face,…They then reconstruct a 3D face mesh to match lip movements that match source audio phonemes, or individual sounds—similar to how recent text-to-video methods work.”\n\nSo why should you be concerned? According to the author.. “SenseTime, one of the companies that developed this research, was recently implicated in developing technology that helped the Chinese government profile a Muslim minority group.”\n\nOne way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\n\nWhere I learnt this #420\n\nNew Deepfake Method Can Put Words In Anyone’s Mouth\n\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips\n\nEverybody’s Talkin’: Let Me Talk as You Want\n\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Method\na little lip motion jitters and appearance flicker might emerge in the final video. Then, a video temporal flicker removal algorithm improved from [3] is applied to eliminate these artifacts. Please refer to appendix for more details of the flicker removal algorithm. 3.3.2 Loss Functions The loss function for training the Neural Video Rendering Network is written as follows: Lrender = Lrecon + Ladv + Lvgg + Ltv + Lgp. (4) The reconstruction loss Lrecon is the pixel-wise L1 loss between the ground truth and generated images. To improve the realism of the generated video, we apply the LSGAN [30] adversarial loss Ladv and add the gradient penalty term Lgp [19] for faster and more stable training. We also apply the perception loss Lvgg [24] to improve the quality of generated images by constraining the image features at different scales. The total variation regularization term Ltv is used to reduce spike artifact that usually occurs when Lvgg is applied [24]. The network is trained endto-end with Ltotal = Lnorm + Ltrans + Lrender (Equations (2),(3), and (4)) with different coefficients. Due to the limited space, we report the details of the loss function, network architecture, and experimental settings in our appendix.",
    "output": "The ability to realistically put words into your mouth could be a serious invasion of privacy and new technology is capable of pulling off astonishing level of originality.\n\nSamantha Cole wrote this piece for Vice describing the alarming progress in AI assisted deepfake capabilities now accessible to any one.\n\nThe process according to the url “Everybody’s Talking’: Let me Talk as You Want,” by researchers at the Nanyang Technological University, Singapore is.. ”To .. first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face,…They then reconstruct a 3D face mesh to match lip movements that match source audio phonemes, or individual sounds—similar to how recent text-to-video methods work.”\n\nSo why should you be concerned? According to the author.. “SenseTime, one of the companies that developed this research, was recently implicated in developing technology that helped the Chinese government profile a Muslim minority group.”\n\nOne way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\n\nWhere I learnt this #420\n\nNew Deepfake Method Can Put Words In Anyone’s Mouth\n\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips\n\nEverybody’s Talkin’: Let Me Talk as You Want\n\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Experiments / Results\nExperiments / Results: 4. Results We show qualitative and quantitative results on a variety of videos to demonstrate the superiority of our method over existing techniques and the effectiveness of proposed components. Datasets. We evaluate our method on a talking face benchmark dataset GRID [9] and a speech video dataset we newly Figure 4: Many-to-many results. (a) One-to-many results: we use speech audio of 1 speaker to drive face of 3 different speakers. (b) Many-to-one results: we use speech audio of 2 different speakers to drive 1 same speaker. For video results we refer to the video on our project page. collected. The former contains 1,000 sentences spoken by 18 males and 16 females. We follow Chen et al. [6] to split training and testing sets on the GRID dataset. Since GRID only provides frontal face videos of minor head movement, we record a video dataset that contains multiple head poses and time-varying head motion. The collected dataset contains speech videos of 4 speakers. Each speaker contributes 15 minutes video for training and 2 minutes video for testing, all videos are captured from 7 viewpoints to provide 7 head poses. Resolution of each video is 1920 1080. We also take several videos downloaded from YouTube with the same percentage of the training and testing split of recorded data to evaluate our approach. Evaluation Metrics. To evaluate the accuracy of the expression parameters and the projected landmarks under various head poses and motions, we apply the following distance metric: Eexp = 1 Nexp PNexp i=1 ||be(i) e(i)||2, Eldmk = 1 Nldmk PNldmk i=1 ||bl(i) l(i)||2, (5) where Nldmk and Nexp are the number of landmarks and expression parameters respectively and i is the index of landmarks or expression parameters. To quantitatively evaluate the generated quality of portrait videos, we apply common image quality metrics like PSNR [54] and SSIM [54]. To qualitatively evaluate the generated quality of portrait videos, we conduct a user study in Section 4.4 and demonstrate some generated video results on our project page. 4.1. Audio-to-Video Translation Many-to-Many Results. To prove that the audio-toexpression network is capable of handling various speakers and the face completion network is generalized on multiple speakers, we present one-to-many results and many-to-one results in Fig. 4 and on our project page. In the one-to-many results, we use the speech audio of one speaker to drive different speakers. Note that different speakers share a single generator instead of multiple person-specific generators. In the many-to-one results, we use the speech audio of different speakers to drive the same speaker. This is in contrast to recent methods, where the whole pipeline [45] or part of components [26, 13] is designed for a specific person, which disables these methods in handling different voice timbres and facial appearances. Large Pose Results. The main purpose of leveraging 3D face model is to handle head pose variations in generating talking face videos. As far as we know, majority of the recent audio-driving methods focus on generating frontal face video no matter whether a 3D head model is applied [45, 26] or not [51, 61, 7]. Our method, however, can generate portrait videos under various large poses driven by audio input. Thanks to the decomposition of audio-driving facial animation problem in our framework, which makes the audio only relate to expression parameters of face rather than shape or pose parameters. Results are shown in Fig. 5 and the video on our project page. Note that in previous methods [45, 61, 7], they directly learn a mapping from audio to landmarks, which involves the shape and pose information that is actually independent to the input audio. Audio Editing & Singing Results. Our method can also be used to edit the speech contents of a pre-recorded video by splitting and recombining the words or sentences taken from any source audio. We show our audio editing results in Fig. 6 and video on our project",
    "output": "Where I learnt this #420\nEverybody’s Talkin’: Let Me Talk as You Want\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Experiments / Results\npage. In addition, we also ask a person to record singing and the audio is fed into our network. The driving result can be viewed in Fig. 7 and video on our project page. This demonstrates the general- Figure 5: Large pose results. We demonstrate 4 head poses including up, down, right and left. Video results of all of 7 head poses can be viewed the video on our project page. ization capability of our method and its",
    "output": "Where I learnt this #420\nEverybody’s Talkin’: Let Me Talk as You Want\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Experiments / Results\npotential in more complex audio-to-video tasks. Figure 6: Audio editing. We select “knowledge is” and “virtue” from “Knowledge is one thing, virtue is another” in the source audio, then recombine them as “Knowledge is virtue” as input. Figure 7: Singing. We evaluate our network on the singing audio clips. Video result can be viewed in the video on our project page. 4.2. Comparison with State-of-the-Art We compare our method with the recent state-of-the-art portrait video generation methods, e.g., Audio2Obama [45], Face2Face [48], DVP (Deep Video Portrait) [27] and Textbased Editing (TBE) [13]. The comparative results are demonstrated in Fig. 8 and video on our project page. First, the Audio2Obama [45] combines a weighted median texture for synthesizing lower face texture and a teeth proxy for capturing teeth sharp details. Our GAN-based rendering network generates better texture details compare to the weighted median texture synthesis [45], e.g., nasolabial folds (Fig. 8 (a)). Then, we compare our method to Face2Face [48] that supports talking face generation driving by source video in Fig. 8 (b). Face2Face [48] directly transfers facial expression of source video in the parameter space while our method infers facial expression from source audio. The similar lip movement of Face2Face and our method in Fig. 8 (b) suggests the effectiveness of our Audio-to-Expression Translation Network in learning accurate lip movement from speech audio. Moreover, our GAN-based rendering network generates better texture details, such as mouth corners and nasolabial folds. We also compare to another video-driving method DVP [27] that supports talking face generation (Fig. 8 (c)). In DVP, a rendering-to-video translation network is designed to synthesize the whole frame other than the face region. It avoids the blending of face region and background that might be easily detectable. The DVP might fail in a complex and dynamic background as shown in Fig. 8 (c). In contrast, our method uses the original background and achieves seamless blending that is hard to distinguish. Finally, we compare our method with the contemporary text-based talking face editing method TBE [13] in Fig. 8 (d). In TBE, the mouth region is searched by phoneme and a semi-parametric inpainting network is proposed to inpaint the seam between the retrieved mouth and the original face background. This method requires training of a person-specific network per input video while our method can generalize on multiple speakers and head poses. Besides, our generation network produces competitive mouth details as shown in Fig. 8 (d). Figure 8: Comparison to state-of-the-art methods. Comparison between our method with Audio2Obama [45], Face2Face [48], DVP [27], and TBE [13]. 4.3. Ablation Study Evaluation of Parameter Regression. To prove the superiority of incorporating the 3D face model, we compare our network with the one that replaces Audio-to-Expression Translation Network with an Audio-to-Landmark Trans- lation Network as performed in [45]. The Audio-toLandmark Translation Network modifies the last fully connected layer of the Audio-to-Expression Translation Network so that its output dimension is the coordinate number of mouth region landmarks. The visualized comparison can be viewed in the video on our project page and Fig. 9 (a). We also compare the quantitative metric on GRID and collected dataset as shown in Tab. 1. In the collected dataset that contains more head motion and poses, our method achieves better lip synchronization results as the mouth generated by the one that applies Audio-to-Landmark Translation Network does not even open. Table 1: 2D vs 3D quantitative comparison. Eexp, Eldmk, PSNR, and SSIM comparison of 2",
    "output": "Where I learnt this #420\nEverybody’s Talkin’: Let Me Talk as You Want\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Experiments / Results\nD and 3D parameter regression. Parameter Regression Eexp Eldmk PSNR SSIM GRID - 3.99 28.06 0.89 0.65 2.24 31.19 0.95 Collected - 3.13 26.76 0.93 0.595 1.82 29.16 0.95 Figure 9: (a) 2D vs 3D qualitative comparison. 3D parameter regression outperforms 2D parameter regression for head motion and poses. (",
    "output": "Where I learnt this #420\nEverybody’s Talkin’: Let Me Talk as You Want\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Experiments / Results\nb) ID-removing qualitative comparison. Improvement can be observed on a failure case caused by not applying Audio ID-Removing Network. Table 2: ID-removing quantitative comparison. “BL” is the 3D parameter regression baseline; “IR” is “id-removing”. Evaluation of Completion-Based Generation. We evaluate the effects of the proposed completion-based generation that benefits from jointly training on data of different people. As shown in Tab. 3, jointly training completion-based generators outperform separately training person-specific generators with much fewer network parameters when the number of speakers increases, regardless of the time length of the training data. Table 3: Training data size and training style. PSNR/SSIM of different amount of training data and training style. The “Joint” means jointly training one generator for all speakers and “Split” means separately training multiple person-specific generators. One generation network contains 75 million parameters and N(N = 4 in the table) is the speaker number. Time 2 mins 5 mins 10 mins 15 mins PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM Split (N 75M) 29.621/0.868 29.338/0.875 29.487/0.849 29.650/0.876 Joint (75M) 30.421/0.886 30.664/0.888 30.787/0.892 31.072/0.897 4.4. User Study To quantitatively evaluate the visual quality of generated portrait videos, following [13], we conduct a webbased user study involving 100 participants on the collected dataset. The study includes 3 generated video clips for each of the 7 cameras and for each of the 4 speakers, hence a total of 84 video clips. Similarly, we also collect 84 ground truth video clips and mix them up with the generated video clips to perform the user study. We separately calculate the study results of the generated and ground truth video clips. In the user study, all the 84  2 = 168 video clips are randomly shown to the participants and they are asked to evaluate its realism by evaluating if the clips are real on a likert scale of 1-5 (5-absolutely real, 4-real, 3-hard to judge, 2-fake, 1-absolutely fake) [13]. As shown in Tab. 4, the generated and the ground truth video clips are rated as “real”(score 4 and 5) in 55.0% and 70.1% cases, respectively. Since humans are highly tuned to the slight audiovideo misalignment and generation flaws, the user study results demonstrate that our method can generate deceptive audio-video content for large poses in most cases. Table 4: User study. User study results on generated and ground truth video clips for videos of 7 poses. Generated Videos Ground Truth Videos score 1 2 3 4 5 “real” 1 2 3 4 5 “real” front 5.2 8.5 20.6 42.6 23.2 65.8% 0.6 12.1 10.1 29.7 47.5 77.2% up-30 4.6 25.0 14.2 36.9 19.3 56.3% 0.8 13.5 13.2 29.1 43.4 72.5% down-30 4.8 22.0 15.2 39.7 18.3 58.0% 0.9 13.6 14.1 30.2 41.3 71.5% right-30 3.9 22.9 15.8 42.1 15.3 57.4% 1.3 15.6 14.4 29.2 39.6 68.8% right-60 7.3 33.8 11.4 36.9 10.6 47.5% 0.8 17.8 15.1 29.1 37.2 66.3% left-30 3.2 20.9 21.9 40.1 13.9 54.0% 1.1 12.8 16.1 31.5 38.6 70.1% left-60 7.9 33.7 12.1 35.1 11.3 46.3% 0.7 17.5 14.1 27.2 40.5 67.7% all poses 5.3 23.8 15.9 39.0 16.0 55.0% 0.9 14.7 13.9 29.4 41.2 70.6%",
    "output": "Where I learnt this #420\nEverybody’s Talkin’: Let Me Talk as You Want\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-02",
    "id_chunk": "05-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Investor, Developer\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 5. Conclusion In this work, we present the first end-to-end learnable audio-based video editing method. At the core of our approach is the learning from audio to expression space bypassing the highly nonlinearity of directly mapping audio source to target video. Audio ID-Removing Network and Neural Video Rendering Network are introduced to enable generation of photo-realistic videos given arbitrary targets and audio sources. Extensive experiments demonstrate the robustness of our method and the effectiveness of each pivotal component. We believe our approach is a step forward towards solving the important problem of audio-based video editing and we hope it will inspire more researches in this direction.",
    "output": "The ability to realistically put words into your mouth could be a serious invasion of privacy and new technology is capable of pulling off astonishing level of originality.\n\nSamantha Cole wrote this piece for Vice describing the alarming progress in AI assisted deepfake capabilities now accessible to any one.\n\nThe process according to the url “Everybody’s Talking’: Let me Talk as You Want,” by researchers at the Nanyang Technological University, Singapore is.. ”To .. first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face,…They then reconstruct a 3D face mesh to match lip movements that match source audio phonemes, or individual sounds—similar to how recent text-to-video methods work.”\n\nSo why should you be concerned? According to the author.. “SenseTime, one of the companies that developed this research, was recently implicated in developing technology that helped the Chinese government profile a Muslim minority group.”\n\nOne way is to stay aware of the changes else we will not have “the ability of citizens and decision makers to separate truth from fiction.”\n\nWhere I learnt this #420\n\nNew Deepfake Method Can Put Words In Anyone’s Mouth\n\nhttps://www.vice.com/en_us/article/g5xvk7/researchers-created-a-way-to-make-realistic-deepfakes-from-audio-clips\n\nEverybody’s Talkin’: Let Me Talk as You Want\n\nhttps://youtu.be/tNPuAnvijQk"
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Abstract\nAbstract: Abstract We present a method to edit a target portrait footage by taking a sequence of audio as input to synthesize a photorealistic video. This method is unique because it is highly dynamic. It does not assume a person-specific rendering network yet capable of translating arbitrary source audio into arbitrary video output. Instead of learning a highly heterogeneous and nonlinear mapping from audio to the video directly, we first factorize each target video frame into orthogonal parameter spaces, i.e., expression, geometry, and pose, via monocular 3D face reconstruction. Next, a recurrent network is introduced to translate source audio into expression parameters that are primarily related to the audio content. The audio-translated expression parameters are then used to synthesize a photo-realistic human subject in each video frame, with the movement of the mouth regions precisely mapped to the source audio. The geometry and pose parameters of the target human portrait are re- tained, therefore preserving the context of the original video footage. Finally, we introduce a novel video rendering network and a dynamic programming method to construct a temporally coherent and photo-realistic video. Extensive experiments demonstrate the superiority of our method over existing approaches. Our method is end-to-end learnable and robust to voice variations in the source audio. Some results are shown in Fig. 1. Video results are shown on our project page1.",
    "output": "Videos by VICE\nIn this url, at least, the researchers seem to be aware of the risks highly customizable and realistic deepfakes pose to society."
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Introduction\nIntroduction: 1. Introduction I’m going where the sun keeps shining. Through the pouring rain. Going where the weather suits my clothes. Fred Neil, Everybody’s Talkin’ Video portrait editing is a highly sought-after technique in view of its wide applications, such as filmmaking, video production, and telepresence. Commercial video editing applications, such as Adobe Premiere and Apple iMovie, are resource-intensive tools. Indeed, editing audio-visual content would require one or more footages to be reshot. Moreover, the quality of the edited video is highly dependent on the prowess of editors. Audio-based approach is an attractive technique for automatic video portrait editing. Several methods [6, 62] are proposed to animate the mouth region of a still image to follow an audio speech. The result is an animated static image rather than a video, hence sacrificing realism. Audiodriven 3D head animation [46] is an audio-based approach but aiming at a different goal, namely to drive stylized 3D computer graphic avatars, rather than to generate a photorealistic video. Suwajanakorn et al. [45] attempted to synthesize photo-realistic videos driven by audio. While impressive performance was achieved, the method assumes the source audio and target video to come from the same identity. The method is only demonstrated on the audio tracks and videos of Barack Obama. Besides, it requires long hours of single-identity data (up to 17 hours [45]) for training using relatively controlled and high-quality shots. In this paper, we investigate a learning-based framework that can perform many-to-many audio-to-video translation, i.e., without assuming a single identity of source audio and the target video. We further assume a scarce number of target video available for training, e.g., at most a 15-minute footage of a person is needed. Such assumptions make our problem non-trivial: 1) Without sufficient data, especially in the absence of source video, it is challenging to learn direct mapping from audio to video. 2) To apply the framework on arbitrary source audios and target videos, our method needs to cope with large audio-video variations between different subjects. 3) Without explicitly specifying scene geometry, materials, lighting, and dynamics, as in the case of a standard graphics rendering engine, it is hard for a learning-based framework to generate photo-realistic yet temporally coherent videos. To overcome the aforementioned challenges, we propose to use the expression parameter space, rather than the full pixels, as the target space for audio-to-video mapping. This facilitates the learning of more effective mapping, since the expression is semantically more relevant to the audio source, compared to other orthogonal spaces, such as geometry and pose. In particular, we manipulate the expression of a target face by generating a new set of parameters through a novel LSTM-based Audio-to-Expression Translation Network. The newly generated expression parameters, combined with geometry and pose parameters of the target human portrait, allow us to reconstruct a 3D face mesh with the same identity and head pose of the target but with new expression (i.e., lip movements) that matches the phonemes of the source audio. We further propose an Audio ID-Removing Network that keeps audio-to-expression translation agnostic to the identity of the source audio. Thus, the translation is robust to variations in the voices of different people in different source audio. Finally, we solve the difficult face generation problem as a face completion problem conditioned on facial landmarks. Specifically, after reconstructing a 3D face mesh with new expression parameters, we extract the associated 2D landmarks from the mouth region and represent them as heatmaps. These heatmaps are combined with target frames where the mouth region is masked. Taking the landmark heatmaps and the masked target frames as inputs, a video rendering network is then used to complete the mouth region of each frame guided by dynamics of the landmarks. We summarize our contributions as follows: 1) We make the first attempt at formulating an end-to-end learnable framework that supports audio-based video portrait editing. We demonstrate",
    "output": "On Thursday, the Bulletin of the Atomic Scientists, stewards of the Doomsday Clock, included deepfakes as a reason why we’re closer to the end of the world than ever, saying that the emergence of algorithmically-generated video “threatens to further undermine the ability of citizens and decision makers to separate truth from fiction.” But sussing truth from fiction might be relatively low on the list of today’s AI-related concerns: for example, SenseTime, one of the companies that developed this research, was recelty implicated in developing technology that helped the Chinese government profile a Muslim minority group."
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Introduction\ncoherent and photo-realistic results by focusing specifically on expression parameter space as the target space, from which source audios can be effectively translated into target videos. 2) We present an Audio ID-Removing Network that encourages an identity-agnostic audio-to-expression translation. This network allows our framework to cope with large variations in voices that are present in arbitrary audio sources. 3) We propose a Neural Video Rendering Network based on the notion",
    "output": "On Thursday, the Bulletin of the Atomic Scientists, stewards of the Doomsday Clock, included deepfakes as a reason why we’re closer to the end of the world than ever, saying that the emergence of algorithmically-generated video “threatens to further undermine the ability of citizens and decision makers to separate truth from fiction.” But sussing truth from fiction might be relatively low on the list of today’s AI-related concerns: for example, SenseTime, one of the companies that developed this research, was recelty implicated in developing technology that helped the Chinese government profile a Muslim minority group."
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Introduction\nof face completion with a masked face as input and mesh landmarks as conditions. This approach facilitates the generation of photo-realistic video for arbitrary people within one single network. Ethical Considerations. Our method could contribute greatly towards advancement in video editing. We envisage relevant industries, such as filmmaking, video production, and telepresence to benefit immensely from this technique. We do acknowledge the potential of such forward-looking technology being misused or abused for various malevolent purposes, e.g., aspersion, media manipulation, or dissemination of malicious propaganda. Therefore, we strongly advocate and support all safeguarding measures against such exploitative practices. We welcome enactment and enforcement of legislation to obligate all edited videos to be distinctly labeled as such, to mandate informed consent be collected from all subjects involved in the edited video, as well as to impose hefty levy on all law defaulters. Being at the forefront of developing creative and innovative technologies, we strive to develop methodologies to detect edited video as a countermeasure. We also encourage the public to serve as sentinels in reporting any suspicious-looking videos to the authority. Working in concert, we shall be able to promote cutting-edge and innovative technologies without compromising the personal interest of the general public.",
    "output": "On Thursday, the Bulletin of the Atomic Scientists, stewards of the Doomsday Clock, included deepfakes as a reason why we’re closer to the end of the world than ever, saying that the emergence of algorithmically-generated video “threatens to further undermine the ability of citizens and decision makers to separate truth from fiction.” But sussing truth from fiction might be relatively low on the list of today’s AI-related concerns: for example, SenseTime, one of the companies that developed this research, was recelty implicated in developing technology that helped the Chinese government profile a Muslim minority group."
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nMethod: 3. Methodology The architecture of the proposed method is shown in Fig. 2. First, we register a parametric 3D face model [5] in the target video, for every portrait video frame to extract face geometry, pose, and expression parameters. Then, the Audio-to-Expression Translation Network learns the mapping from the source audio feature to face expression parameters. We design an Audio ID-Removing Network to alleviate the issues on large variations caused by multiple speakers. Lastly, we formulate the talking face generation problem as a face completion problem guided by mouth region landmarks, in which the landmarks are projected from the restructured 3D facial mesh. We propose a Neural Video Rendering network to complete the mouth region of each frame, guided by the dynamics of the landmarks to generate a photo-realistic portrait video. 3.1. 3D Face Modeling We leverage a parametric 3D face model [5] on portrait video frame to recover low dimensional geometry, expression, and pose parameters. To reduce parameter dimension, geometry and expression bases are computed based on high-quality head scans [2] and facial blendshapes [5, 1] Figure 2: Architecture. Our network contains an Audio-to-Expression Translation Network that learns facial expression parameters from speech audio and a Neural Video Rendering Network that generates mouth region guided by projected mouth landmarks. via principal component analysis (PCA). The geometry parameters s R199 and the expression parameters e R29 are the coefficients of geometry and expression principle components in the PCA, respectively. The pose of the head p R6 which contains 3 head rotation coefficients, 2 translation coefficients (x and y directions on the screen surface), and 1 scaling coefficient. All the parameters are computed by solving a non-linear optimization problem, constrained by the statistical linear 3D face model [2]. By optimizing the geometry, expression, and pose parameters of a given monocular face image based on its detected facial landmarks, protrait video frames will be automatically annotated with low dimensional vectors [2]. The recovered expression parameters are used as the learning target in the Audio-to-Expression Translation Network. Then, the recovered geometry and pose parameters, together with the expression parameters inferred by the Audio-to-Expression Translation Network, are employed for reconstructing the 3.2. Audio-to-Expression Translation 3.2.1 Audio ID-Removing Network We empirically find that identity information embedded in the speech feature degrades the performance of mapping speech to mouth movement. Inspired by recent advances of the speaker adaptation method in the literature of speech recognition [50, 39], we transfer the speech feature lies in different speaker domains onto a “global speaker” domain by applying a linear transformation, in the form [50]: x ′ = Wix + bi = Wi x, where  Wi = (Wi, bi), x = (x; 1),  Wi = I + Pk j=1 j W j (1) Here, x and x ′ represent the raw and transferred speech feature, respectively, while  Wi = I + Pk j=1 j W j represents the speaker-specific adaptation parameter that is factorized into an identity matrix I plus weighted sum of k components W j [39]. In speech recognition, these parameters are iteratively optimized by fMLLR [11, 15] and EM Figure 3: Audio ID-Removing Network. We formulate the speaker adaptation method from speech recognition [50, 39] as a neural network. The network removes identity in speech MFCC spectrum by transferring it to the “global speaker” domain. algorithms. We formulate the above method into a neural network to be integrated with our end-to-end deep learning network. From Eq.(1), the parameters j need to be learned from the input speech feature, while the matrix components W j is general speech features of different speakers. Thus, we design an LSTM+FC network to infer j from the input and set the matrix components W j as the optimizing parameter of the Audio ID-Removing Network. The matrix components W j of the Audio ID-Removing Network are updated by the gradient descent-based algorithm. The details of the network is depicted in Fig. 3. The output of the Audio IDRemoving Network is a new MFCC (Mel-frequency cepstral coefficients) spectrum. We apply a pre-trained speaker identity network",
    "output": "To do this, the researchers first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face, according to the url, “Everybody’s Talkin’: Let Me Talk as You Want,” published this month to the arxiv preprint server. From there, they draw 2D landmarks of the face, focusing especially on mouth movements. That way, instead of requiring the algorithm to train on the entire scene, it’s only training on the facial features and leaves the background sharp."
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nVGGVox [33, 8] on the new MFCC spectrum and constrain the Audio ID-Removing Network by the following cross-entropy loss function: N X 1 N log p(c|x ′), (2) Lnorm = c=1 where N is the number of speakers, c is the speaker class label. The p(c|x ′) is the probability of assigning MFCC x ′ to speaker c, which is inferred from the pre-trained VGGVox. Eq. (2) enforces the Audio ID-Removing Network to produce an MFCC spectrum that is not distinguishable by the pre-trained VGGVox. 3.2.2 Audio-to-Expression Translation Network We formulate a simple but effective Audio-to-Expression Translation Network that learns the mapping from the IDremoved MFCC feature to the corresponding facial expression parameters. To infer the expression parameters at time t, the translation network observes a sliding window speech clip of 1 second, which contains 0.8 seconds before time t and 0.2 seconds after time t. We empirically find it challenging to train a network to solely regress the expression parameters. The underlying reason could be that the expression parameters are defined and related to the 3DMM model that is hard to model by the network. To facilitate the learning, we introduce a shape constraint. In particular, with the predicted expression parameters from audio and the ground truth geometry/pose parameters of the video portrait, we can obtain a predicted reconstructed 3D facial mesh. Then, we project 3D points of mouth area to the 2D space to obtain the predicted 2D mouth landmarks. Using a similar method, we can obtain a set of ground-truth 2D mouth landmarks from the groundtruth expression parameters. The shape constraint can be introduced between the predicted 2D mouth landmarks and ground-truth 2D mouth landmarks. The whole process of generating mouth landmarks from expression parameters only involves linear operations and thus is differential. The loss function is written as follows: Ltrans = Lexp + Lshape = ||e e||2 + ||l l||2, (3) where e and l are the ground truth expression and landmark, respectively, and e and l are the output expression and landmark of the translation network, respectively. The Audio ID-Removing and Audio-to-Expression Translation Networks are trained jointly, whose objective function is weighted sum of Lnorm (Eq. (2)) and Ltrans (Eq. (3)). 3.3. Neural Video Rendering Network 3.3.1 Network Architecture Our final step is to generate photo-realistic talking face video that is conditioned on dynamic background portrait video and is guided by the mouth region landmark heatmap sequence. We design a completion-based generation network that completes the mouth region guided by mouth landmarks. First, to obtain the masked face images, a tailored dynamic programming based on retiming algorithm inspired by [45] is introduced to select frame sequence whose head shaking and blink of eyes look compatible with the source speech. Then, the mouth area that contains lip, jaw, and nasolabial folds are manually occluded by a square mask filled with random noise. To make the conversion from the landmark coordinates to heatmap differentiable, we follow [22, 56] to generate heatmaps with Gaussianlike functions centered at landmark locations. We mod- ify a Unet [42, 40]-based network as our generation network. The employed skip-connection enables our network to transfer fine-scale structure information. In this way, the landmark heatmap at the input can directly guide the mouth region generation at the output, and the structure of the generated mouth obeys the heatmaps [40, 53]. We composite the generated mouth region over the target face frame according to the input mouth region mask. To obtain the mouth region mask, we connect the outermost mouth landmarks as a polygon and fill it with white color, then we erode the binary mask and smooth its boundaries with a Gaussian filter [26].",
    "output": "To do this, the researchers first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face, according to the url, “Everybody’s Talkin’: Let Me Talk as You Want,” published this month to the arxiv preprint server. From there, they draw 2D landmarks of the face, focusing especially on mouth movements. That way, instead of requiring the algorithm to train on the entire scene, it’s only training on the facial features and leaves the background sharp."
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nWith the soft mask, we leverage Poisson blending [36] to achieve seamless blending. To improve the temporal continuity of generated video, we apply a sliding window on the input masked video frames and heatmaps [27, 26]. The input of the Neural Video Rendering Network is a tensor stacked by 7 RGB frames and 7 heatmap gray images [26]. It works well in most cases while",
    "output": "To do this, the researchers first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face, according to the url, “Everybody’s Talkin’: Let Me Talk as You Want,” published this month to the arxiv preprint server. From there, they draw 2D landmarks of the face, focusing especially on mouth movements. That way, instead of requiring the algorithm to train on the entire scene, it’s only training on the facial features and leaves the background sharp."
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\na little lip motion jitters and appearance flicker might emerge in the final video. Then, a video temporal flicker removal algorithm improved from [3] is applied to eliminate these artifacts. Please refer to appendix for more details of the flicker removal algorithm. 3.3.2 Loss Functions The loss function for training the Neural Video Rendering Network is written as follows: Lrender = Lrecon + Ladv + Lvgg + Ltv + Lgp. (4) The reconstruction loss Lrecon is the pixel-wise L1 loss between the ground truth and generated images. To improve the realism of the generated video, we apply the LSGAN [30] adversarial loss Ladv and add the gradient penalty term Lgp [19] for faster and more stable training. We also apply the perception loss Lvgg [24] to improve the quality of generated images by constraining the image features at different scales. The total variation regularization term Ltv is used to reduce spike artifact that usually occurs when Lvgg is applied [24]. The network is trained endto-end with Ltotal = Lnorm + Ltrans + Lrender (Equations (2),(3), and (4)) with different coefficients. Due to the limited space, we report the details of the loss function, network architecture, and experimental settings in our appendix.",
    "output": "To do this, the researchers first create a three-dimensional face model on every frame of the target video to extract the geometry, pose, and expressions of the face, according to the url, “Everybody’s Talkin’: Let Me Talk as You Want,” published this month to the arxiv preprint server. From there, they draw 2D landmarks of the face, focusing especially on mouth movements. That way, instead of requiring the algorithm to train on the entire scene, it’s only training on the facial features and leaves the background sharp."
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\nExperiments / Results: 4. Results We show qualitative and quantitative results on a variety of videos to demonstrate the superiority of our method over existing techniques and the effectiveness of proposed components. Datasets. We evaluate our method on a talking face benchmark dataset GRID [9] and a speech video dataset we newly Figure 4: Many-to-many results. (a) One-to-many results: we use speech audio of 1 speaker to drive face of 3 different speakers. (b) Many-to-one results: we use speech audio of 2 different speakers to drive 1 same speaker. For video results we refer to the video on our project page. collected. The former contains 1,000 sentences spoken by 18 males and 16 females. We follow Chen et al. [6] to split training and testing sets on the GRID dataset. Since GRID only provides frontal face videos of minor head movement, we record a video dataset that contains multiple head poses and time-varying head motion. The collected dataset contains speech videos of 4 speakers. Each speaker contributes 15 minutes video for training and 2 minutes video for testing, all videos are captured from 7 viewpoints to provide 7 head poses. Resolution of each video is 1920 1080. We also take several videos downloaded from YouTube with the same percentage of the training and testing split of recorded data to evaluate our approach. Evaluation Metrics. To evaluate the accuracy of the expression parameters and the projected landmarks under various head poses and motions, we apply the following distance metric: Eexp = 1 Nexp PNexp i=1 ||be(i) e(i)||2, Eldmk = 1 Nldmk PNldmk i=1 ||bl(i) l(i)||2, (5) where Nldmk and Nexp are the number of landmarks and expression parameters respectively and i is the index of landmarks or expression parameters. To quantitatively evaluate the generated quality of portrait videos, we apply common image quality metrics like PSNR [54] and SSIM [54]. To qualitatively evaluate the generated quality of portrait videos, we conduct a user study in Section 4.4 and demonstrate some generated video results on our project page. 4.1. Audio-to-Video Translation Many-to-Many Results. To prove that the audio-toexpression network is capable of handling various speakers and the face completion network is generalized on multiple speakers, we present one-to-many results and many-to-one results in Fig. 4 and on our project page. In the one-to-many results, we use the speech audio of one speaker to drive different speakers. Note that different speakers share a single generator instead of multiple person-specific generators. In the many-to-one results, we use the speech audio of different speakers to drive the same speaker. This is in contrast to recent methods, where the whole pipeline [45] or part of components [26, 13] is designed for a specific person, which disables these methods in handling different voice timbres and facial appearances. Large Pose Results. The main purpose of leveraging 3D face model is to handle head pose variations in generating talking face videos. As far as we know, majority of the recent audio-driving methods focus on generating frontal face video no matter whether a 3D head model is applied [45, 26] or not [51, 61, 7]. Our method, however, can generate portrait videos under various large poses driven by audio input. Thanks to the decomposition of audio-driving facial animation problem in our framework, which makes the audio only relate to expression parameters of face rather than shape or pose parameters. Results are shown in Fig. 5 and the video on our project page. Note that in previous methods [45, 61, 7], they directly learn a mapping from audio to landmarks, which involves the shape and pose information that is actually independent to the input audio. Audio Editing & Singing Results. Our method can also be used to edit the speech contents of a pre-recorded video by splitting and recombining the words or sentences taken from any source audio. We show our audio editing results in Fig. 6 and video on our project",
    "output": "A woman looks at the camera and says, “Knowledge is one thing, virtue is another.” Then, she says, “Knowledge is virtue.” The same person, with the same voice, says two conflicting statements—but she only said the first in real life. The second statement is the work of an AI system that took audio of her speech and turned it into a video.\nPlay video"
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\npage. In addition, we also ask a person to record singing and the audio is fed into our network. The driving result can be viewed in Fig. 7 and video on our project page. This demonstrates the general- Figure 5: Large pose results. We demonstrate 4 head poses including up, down, right and left. Video results of all of 7 head poses can be viewed the video on our project page. ization capability of our method and its",
    "output": "A woman looks at the camera and says, “Knowledge is one thing, virtue is another.” Then, she says, “Knowledge is virtue.” The same person, with the same voice, says two conflicting statements—but she only said the first in real life. The second statement is the work of an AI system that took audio of her speech and turned it into a video.\nPlay video"
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\npotential in more complex audio-to-video tasks. Figure 6: Audio editing. We select “knowledge is” and “virtue” from “Knowledge is one thing, virtue is another” in the source audio, then recombine them as “Knowledge is virtue” as input. Figure 7: Singing. We evaluate our network on the singing audio clips. Video result can be viewed in the video on our project page. 4.2. Comparison with State-of-the-Art We compare our method with the recent state-of-the-art portrait video generation methods, e.g., Audio2Obama [45], Face2Face [48], DVP (Deep Video Portrait) [27] and Textbased Editing (TBE) [13]. The comparative results are demonstrated in Fig. 8 and video on our project page. First, the Audio2Obama [45] combines a weighted median texture for synthesizing lower face texture and a teeth proxy for capturing teeth sharp details. Our GAN-based rendering network generates better texture details compare to the weighted median texture synthesis [45], e.g., nasolabial folds (Fig. 8 (a)). Then, we compare our method to Face2Face [48] that supports talking face generation driving by source video in Fig. 8 (b). Face2Face [48] directly transfers facial expression of source video in the parameter space while our method infers facial expression from source audio. The similar lip movement of Face2Face and our method in Fig. 8 (b) suggests the effectiveness of our Audio-to-Expression Translation Network in learning accurate lip movement from speech audio. Moreover, our GAN-based rendering network generates better texture details, such as mouth corners and nasolabial folds. We also compare to another video-driving method DVP [27] that supports talking face generation (Fig. 8 (c)). In DVP, a rendering-to-video translation network is designed to synthesize the whole frame other than the face region. It avoids the blending of face region and background that might be easily detectable. The DVP might fail in a complex and dynamic background as shown in Fig. 8 (c). In contrast, our method uses the original background and achieves seamless blending that is hard to distinguish. Finally, we compare our method with the contemporary text-based talking face editing method TBE [13] in Fig. 8 (d). In TBE, the mouth region is searched by phoneme and a semi-parametric inpainting network is proposed to inpaint the seam between the retrieved mouth and the original face background. This method requires training of a person-specific network per input video while our method can generalize on multiple speakers and head poses. Besides, our generation network produces competitive mouth details as shown in Fig. 8 (d). Figure 8: Comparison to state-of-the-art methods. Comparison between our method with Audio2Obama [45], Face2Face [48], DVP [27], and TBE [13]. 4.3. Ablation Study Evaluation of Parameter Regression. To prove the superiority of incorporating the 3D face model, we compare our network with the one that replaces Audio-to-Expression Translation Network with an Audio-to-Landmark Trans- lation Network as performed in [45]. The Audio-toLandmark Translation Network modifies the last fully connected layer of the Audio-to-Expression Translation Network so that its output dimension is the coordinate number of mouth region landmarks. The visualized comparison can be viewed in the video on our project page and Fig. 9 (a). We also compare the quantitative metric on GRID and collected dataset as shown in Tab. 1. In the collected dataset that contains more head motion and poses, our method achieves better lip synchronization results as the mouth generated by the one that applies Audio-to-Landmark Translation Network does not even open. Table 1: 2D vs 3D quantitative comparison. Eexp, Eldmk, PSNR, and SSIM comparison of 2",
    "output": "A woman looks at the camera and says, “Knowledge is one thing, virtue is another.” Then, she says, “Knowledge is virtue.” The same person, with the same voice, says two conflicting statements—but she only said the first in real life. The second statement is the work of an AI system that took audio of her speech and turned it into a video.\nPlay video"
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\nD and 3D parameter regression. Parameter Regression Eexp Eldmk PSNR SSIM GRID - 3.99 28.06 0.89 0.65 2.24 31.19 0.95 Collected - 3.13 26.76 0.93 0.595 1.82 29.16 0.95 Figure 9: (a) 2D vs 3D qualitative comparison. 3D parameter regression outperforms 2D parameter regression for head motion and poses. (",
    "output": "A woman looks at the camera and says, “Knowledge is one thing, virtue is another.” Then, she says, “Knowledge is virtue.” The same person, with the same voice, says two conflicting statements—but she only said the first in real life. The second statement is the work of an AI system that took audio of her speech and turned it into a video.\nPlay video"
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\nb) ID-removing qualitative comparison. Improvement can be observed on a failure case caused by not applying Audio ID-Removing Network. Table 2: ID-removing quantitative comparison. “BL” is the 3D parameter regression baseline; “IR” is “id-removing”. Evaluation of Completion-Based Generation. We evaluate the effects of the proposed completion-based generation that benefits from jointly training on data of different people. As shown in Tab. 3, jointly training completion-based generators outperform separately training person-specific generators with much fewer network parameters when the number of speakers increases, regardless of the time length of the training data. Table 3: Training data size and training style. PSNR/SSIM of different amount of training data and training style. The “Joint” means jointly training one generator for all speakers and “Split” means separately training multiple person-specific generators. One generation network contains 75 million parameters and N(N = 4 in the table) is the speaker number. Time 2 mins 5 mins 10 mins 15 mins PSNR/SSIM PSNR/SSIM PSNR/SSIM PSNR/SSIM Split (N 75M) 29.621/0.868 29.338/0.875 29.487/0.849 29.650/0.876 Joint (75M) 30.421/0.886 30.664/0.888 30.787/0.892 31.072/0.897 4.4. User Study To quantitatively evaluate the visual quality of generated portrait videos, following [13], we conduct a webbased user study involving 100 participants on the collected dataset. The study includes 3 generated video clips for each of the 7 cameras and for each of the 4 speakers, hence a total of 84 video clips. Similarly, we also collect 84 ground truth video clips and mix them up with the generated video clips to perform the user study. We separately calculate the study results of the generated and ground truth video clips. In the user study, all the 84  2 = 168 video clips are randomly shown to the participants and they are asked to evaluate its realism by evaluating if the clips are real on a likert scale of 1-5 (5-absolutely real, 4-real, 3-hard to judge, 2-fake, 1-absolutely fake) [13]. As shown in Tab. 4, the generated and the ground truth video clips are rated as “real”(score 4 and 5) in 55.0% and 70.1% cases, respectively. Since humans are highly tuned to the slight audiovideo misalignment and generation flaws, the user study results demonstrate that our method can generate deceptive audio-video content for large poses in most cases. Table 4: User study. User study results on generated and ground truth video clips for videos of 7 poses. Generated Videos Ground Truth Videos score 1 2 3 4 5 “real” 1 2 3 4 5 “real” front 5.2 8.5 20.6 42.6 23.2 65.8% 0.6 12.1 10.1 29.7 47.5 77.2% up-30 4.6 25.0 14.2 36.9 19.3 56.3% 0.8 13.5 13.2 29.1 43.4 72.5% down-30 4.8 22.0 15.2 39.7 18.3 58.0% 0.9 13.6 14.1 30.2 41.3 71.5% right-30 3.9 22.9 15.8 42.1 15.3 57.4% 1.3 15.6 14.4 29.2 39.6 68.8% right-60 7.3 33.8 11.4 36.9 10.6 47.5% 0.8 17.8 15.1 29.1 37.2 66.3% left-30 3.2 20.9 21.9 40.1 13.9 54.0% 1.1 12.8 16.1 31.5 38.6 70.1% left-60 7.9 33.7 12.1 35.1 11.3 46.3% 0.7 17.5 14.1 27.2 40.5 67.7% all poses 5.3 23.8 15.9 39.0 16.0 55.0% 0.9 14.7 13.9 29.4 41.2 70.6%",
    "output": "A woman looks at the camera and says, “Knowledge is one thing, virtue is another.” Then, she says, “Knowledge is virtue.” The same person, with the same voice, says two conflicting statements—but she only said the first in real life. The second statement is the work of an AI system that took audio of her speech and turned it into a video.\nPlay video"
  },
  {
    "id_paper": "05",
    "id_story": "05-01",
    "id_chunk": "05-01_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 5. Conclusion In this work, we present the first end-to-end learnable audio-based video editing method. At the core of our approach is the learning from audio to expression space bypassing the highly nonlinearity of directly mapping audio source to target video. Audio ID-Removing Network and Neural Video Rendering Network are introduced to enable generation of photo-realistic videos given arbitrary targets and audio sources. Extensive experiments demonstrate the robustness of our method and the effectiveness of each pivotal component. We believe our approach is a step forward towards solving the important problem of audio-based video editing and we hope it will inspire more researches in this direction.",
    "output": "The researchers say this is the first end-to-end learnable audio-based video editing method. If you’re going to make your deepfake speak, however, an unconvincing voice can make or break its believability—for example, the deepfakes of Mark Zuckerberg last year, with a voice that’s comically unrealistic. Faked audio has been a focus of AI engineers and deepfake developers for years, and algorithmically-generated voices alone can sound incredibly real. A generated voice mimicking Jordan Peterson was so realistic that Peterson himself threatened to sue its creator."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Abstract\nAbstract: IMPORTANCE A deep learning system (DLS) is a machine learning technology with potential for screening diabetic retinopathy and related eye diseases. OBJECTIVE To evaluate the performance of a DLS in detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, possible glaucoma, and age-related macular degeneration (AMD) in community and clinic-based multiethnic populations with diabetes. DESIGN, SETTING, AND PARTICIPANTS Diagnostic performance of a DLS for diabetic retinopathy and related eye diseases was evaluated using 494 661 retinal images. A DLS was trained for detecting diabetic retinopathy (using 76 370 images), possible glaucoma (125 189 images), and AMD (72 610 images), and performance of DLS was evaluated for detecting diabetic retinopathy (using 112 648 images), possible glaucoma (71 896 images), and AMD (35 948 images). Training of the DLS was completed in May 2016, and validation of the DLS was completed in May 2017 for detection of referable diabetic retinopathy (moderate nonproliferative diabetic retinopathy or worse) and vision-threatening diabetic retinopathy (severe nonproliferative diabetic retinopathy or worse) using a primary validation data set in the Singapore National Diabetic Retinopathy Screening Program and 10 multiethnic cohorts with diabetes. EXPOSURES Use of a deep learning system. MAIN OUTCOMES AND MEASURES Area under the receiver operating characteristic curve (AUC) and sensitivity and specificity of the DLS with professional graders (retinal specialists, general ophthalmologists, trained graders, or optometrists) as the reference standard. RESULTS In the primary validation dataset (n = 14 880 patients; 71 896 images; mean [SD] age, 60.2 [2.2] years; 54.6% men), the prevalence of referable diabetic retinopathy was 3.0%; vision-threatening diabetic retinopathy, 0.6%; possible glaucoma, 0.1%; and AMD, 2.5%. The AUC of the DLS for referable diabetic retinopathy was 0.936 (95% CI, 0.925-0.943), sensitivity was 90.5% (95% CI, 87.3%-93.0%), and specificity was 91.6% (95% CI, 91.0%-92.2%). For vision-threatening diabetic retinopathy, AUC was 0.958 (95% CI, 0.956-0.961), sensitivity was 100% (95% CI, 94.1%-100.0%), and specificity was 91.1% (95% CI, 90.7%-91.4%). For possible glaucoma, AUC was 0.942 (95% CI, 0.929-0.954), sensitivity was 96.4% (95% CI, 81.7%-99.9%), and specificity was 87.2% (95% CI, 86.8%-87.5%). For AMD, AUC was 0.931 (95% CI, 0.928-0.935), sensitivity was 93.2% (95% CI, 91.1%-99.8%), and specificity was 88.7% (95% CI, 88.3%-89.0%). For referable diabetic retinopathy in the 10 additional datasets, AUC range was 0.889 to 0.983 (n = 40 752 images). CONCLUSIONS AND RELEVANCE In this evaluation of retinal images from multiethnic cohorts of patients with diabetes, the DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases. Further research is necessary to evaluate the applicability of the DLS in health care settings and the utility of the DLS to improve vision outcomes.",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Introduction\nIntroduction: By 2040,it is projected that approximately 600 million people will have diabetes, with one-third expected to have diabetic retinopathy.1-3 Screening for diabetic retinopathy,coupled with timely referral and treatment,is a universally accepted strategy for blindness prevention.2 However, programs for screening diabetic retinopathy are challenged by issues related to implementation, availability of human assessors,and long-term financial sustainability.2,4-7 A deep learning system (DLS) uses artificial intelligence and representation learning methods to process large data and extract meaningful patterns.8,9 A few DLSs have re- cently shown high sensitivity and specificity (>90%) in detecting referable diabetic retinopathy from retinal photo- graphs, primarily using high-quality images from publicly available databases from homogenous populations of white individuals.10-12 The performance of a DLS in screening for diabeticretinopathyshouldideallybeevaluatedinclinicalor population settings in which retinal images from patients of different races and ethnicities (and therefore with varying fundi pigmentation) have varying qualities (eg, due to poor pupil dilation, media opacity, poor contrast or focus).13,14 Furthermore, in screening programs for diabetic retinopathy, the detection of incidental but related vision-threatening eye diseases, such as glaucoma and age-related macular degen- eration (AMD), should be incorporated because missing such casesisclinicallyunacceptable.15 The primary aim of this study was to train and vali- date a DLS to detect referable diabetic retinopathy, vision- threatening diabetic retinopathy, and related eye diseases (referablepossibleglaucomaandreferableAMD)byevaluat- ing retinal images obtained primarily from patients with diabetes in an ongoing community-based national diabetic retinopathy screening program in Singapore, with further external validation on referable diabetic retinopathy in 10 additional multiethnic datasets from different countries with diverse community- and clinic-based populations withdiabetes.Thesecondaryaimwastodeterminehowthe DLS could fit in 2 potential models of diabetic retinopathy screening—a fully automated model for communities with noexistingscreeningprogramsandasemiautomatedmodel in which referable cases from the DLS undergo a secondary assessment by human graders.",
    "output": "A few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\nMethod: Methods This study was approved by the centralized institutional review board (IRB) of SingHealth, Singapore (protocol SHF/FG648S/2015) and conducted in accordance with the Declaration of Helsinki. Information on race/ethnicity was collected to evaluate the consistency of DLS diagnostic performance across races/ethnicities. Patients’ informed consent was exempted by the IRB because of the retrospective nature of study using fully anonymized retinal images. Key Points Question How does a deep learning system (DLS) using artificial intelligence compare with professional human graders in identifying diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes? Findings In the primary validation dataset (71 896 images; 14 880 patients), the DLS had a sensitivity of 90.5% and specificity of 91.6% for detecting referable diabetic retinopathy; 100% sensitivity and 91.1% specificity for vision-threatening diabetic retinopathy; 96.4% sensitivity and 87.2% specificity for possible glaucoma; and 93.2% sensitivity and 88.7% specificity for age-related macular degeneration, compared with professional graders. Meaning The DLS had high sensitivity and specificity for identifying diabetic retinopathy and related eye diseases using retinal images from multiethnic populations with diabetes. who participated in the ongoing Singapore National Diabetic Retinopathy Screening Program (SIDRP) between 2010 and 2013 (SIDRP 2010-2013; Table 1 and Table 2). The SIDRP was established from 2010, progressively covered all 18 primary care clinics across Singapore, and screened half of the diabetes population by 2015.16 SIDRP uses digital retinal photography, a tele-ophthalmology platform, and assessment of diabetic retinopathy by a team of trained professional graders. For each patient, 2 retinal photographs (optic disc and fovea) were taken of each eye. All trained graders received 3 to 6 months of training before certification and underwent annual reaccreditation. Specifically for this study, in the training set (SIDRP 2010-2013), each retinal image was analyzed by 2 trained senior certified nonmedical professional graders (>5 years’ experience)17; if there were discordant findings between the nonmedical professional graders, arbitration was performed by a retinal specialist (PhD-trained with >5 years’ experience in conducting diabetic retinopathy assessment) to generate final grading. For referable possible glaucoma and AMD, the DLS was trained using images from SIDRP 2010-2013 and several additional population- and clinic-based studies of patients with glaucoma and AMD (Table 1; eTable 1 in the Supplement).17-20,22,23,25-29 Architecture of the DLS The DLS consisted of a convolutional neural network to implicitly recognize characteristics of referable diabetic retinopathy, possible glaucoma, and AMD from the appearance in retinal images. Training of the DLS entailed exposure of multiple examples of retinal images (with and without each of the 3 conditions) to the neural networks, allowing the networks to gradually adapt their weight parameters to model and differentiate between conditions. Once the training was complete, the DLS could be used to classify unseen images. Technical details are shown in eFigure 1 in the Supplement. Guangdong (China) Chinese Community-based FundusVue 15 798 7899 3970 African American Eye Disease Study22 (United States) African American Population-based Topcon 1968 968 484 Singapore Indian Eye Study17-20 (Singapore) Indian Population-based Canon 4512 2256 1128 Chinese, Malay, Indian Community-based Topcon by 1 retinal specialist 76 370 38 185 13 099 Chinese, Malay, Indian Community-based Topcon 2 professional senior graders 71 896 35 948 14 880 Singapore Chinese Eye Study17-20 (Singapore) Chinese Population-based Canon 1936 968 4",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\n84 Royal Victoria Eye and Ear Hospital23 (Australia) White Clinic-based Topcon 2302 1151 588 Singapore Malay Eye Study17-20 (Singapore) Malay Population-based Canon 3052 1526 763 Beijing Eye Study21 (China) Chinese Population-based Canon 1052 526 263 Images Eyes Patients No. Table 1. Summary of the Training and Validation Datasets for Referable Diabetic Retinopathy, Referable",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\nPossible Glaucoma, and Referable Age-Related Macular Degeneration Source Datasets (Location) Race/Ethnicity Cohort Camera Assessor Singapore National Diabetic Retinopathy Screening Program 2010-201316 (Singapore) Singapore National Diabetic Retinopathy Screening Program 2014-201516 (Singapore) Referable Diabetic Retinopathy External validation Primary validation (continued) Mexican (Mexico) Hispanic Clinic-based Topcon 1172 586 343 University of Hong Kong (Hong Kong) Chinese Clinic-based Carl Zeiss 7706 3853 1932 Categorical total, validation 112 648 56 324 15 157 Categorical total, training and validation 189 018 94 509 38 256 Categorical total, training 125 189 59 616 23 978 Categorical total, training and validation 197 085 95 564 38 858 Singapore Malay Eye Study25 (Singapore) Malay Population-based Canon 10 114 6560 3280 Singapore Indian Eye Study25 (Singapore) Indian Population-based Canon 10 819 6800 3400 Singapore Chinese Eye Study25 (Singapore) Chinese Population-based Canon 26 731 6706 3353 Chinese, Malay, Indian Community-based Topcon by 1 retinal specialist 76 108 38 185 13 099 Chinese, Malay, Indian Community-based Topcon 71 896 35 948 14 880 Chinese University of Hong Kong24 (Hong Kong) Chinese Clinic-based Topcon 1254 627 314 Singapore National Eye Centre, Glaucoma (Singapore) Chinese, Malay, Indian Clinic-based Topcon 1417 1365 846 Singapore National Diabetic Retinopathy Screening Program 2010-201316 (Singapore) Singapore National Diabetic Retinopathy Screening Program 2014-201516 (Singapore) Referable Possible Glaucoma Primary validation Singapore Malay Eye Study26 (Singapore) Malay Population-based Canon 8616 6560 3280 Singapore Indian Eye Study26 (Singapore) Indian Population-based Canon 7447 6800 3400 Singapore Chinese Eye Study26 (Singapore) Chinese Population-based Canon 16 182 6706 3353 494 661 111 538 46 934 Singapore National Eye Centre, AMD27-29 (Singapore) Chinese, Malay, Indian Clinic-based Topcon 2 retinal specialists 2180 348 174 Categorical total, training 72 610 58 599 23 306 Chinese, Malay, Indian Community-based Topcon 1 retinal specialist; 2 trained professional senior graders 35 948 35 948 14 880 Categorical total, training and validation 108 558 94 547 38 189 Singapore National Diabetic Retinopathy Screening Program 2010-201316 (Singapore) Chinese, Malay, Indian Community-based Topcon by 1 retinal specialist 38 185 38 185 13 099 Images Eyes Patients Table 1. Summary of the Training and Validation Datasets for Referable Diabetic Retinopathy, Referable Possible Glaucoma, and Referable Age-Related Macular Degeneration (continued) No. Source Datasets (Location) Race/Ethnicity Cohort Camera Assessor Total images for referable diabetic retinopathy, referable possible glaucoma, and referable age-related macular degeneration training and validation Referable Age-Related Macular Degeneration Singapore National Diabetic Retinopathy Screening Program 2014-201516 (Singapore) Primary validation same SIDRP among patients seen between 2014 and 2015 (SIDRP 2014-2015). The primary analysis was to determine if the DLS was equivalent or better than 2 trained senior nonmedical professional graders (>5 years’ experience) currently employed in the SIDRP in detecting referable diabetic retinopathy and vision-threatening diabetic retinopathy, with reference to a retina",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\nl specialist (>5 years’ experience in diabetic retinopathy grading). The DLS was then externally validated using 10 additional multiethnic cohorts of participants with diabetes from different settings (community, population-based, and clinicbased). A range of retinal cameras were used, and assessment of diabetic retinopathy was facilitated by retinal specialists, general ophthalmologists, trained nonmedical professional graders, or opt",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\nometrists across the cohorts (Table 1). All retinal images were captured with JPEG compression format (resolutions 5-7 megapixels, except for images of eyes in the Hispanic cohort [1 megapixel]). Training, Experience, and Credentials of the Grading Team for External Validation Datasets Guangdong: 5 nonmedical United Kingdom–certified professional graders (>2 years’ experience), supervised by 1 retinal specialist (>10 years’ experience). Singapore Malay Eye Study, Singapore Indian Eye Study, and Singapore Chinese Eye Study: 1 certified professional senior grader (>7 years’ experience), supervised by 2 senior retinal specialists from Australia (>15 years’ experience). Beijing Eye Study: experience). African American Eye Study: 2 retinal specialists (>5 years’ experience). Royal Victorian Eye and Ear Hospital: 4 professional senior graders (>7 years’ experience). Mexican study: 2 retinal specialists (>5 years’ experience). Chinese University of Hong Kong: 3 retinal specialists (>6 years’ experience). The University of Hong Kong: 6 optometrists (>4 years’ experience). Singapore National Eye Center Glaucoma Study: 3 glaucoma specialists (>5 years’ experience). Singapore National Eye Center AMD Phenotyping Study: 10 retinal specialists (>5 years’ experience). Definition of Referable Diabetic Retinopathy, Vision-Threatening Diabetic Retinopathy, Referable Possible Glaucoma, and Referable AMD Diabetic retinopathy levels from all retinal images were defined using the International Classification Diabetic Retinopathy Scale.30 Referable diabetic retinopathy was defined as a diabetic retinopathy severity level of moderate nonproliferative diabetic retinopathy or worse, diabetic macular edema, and/or ungradable image. Vision-threatening diabetic retinopathy was defined as severe nonproliferative diabetic retinopathy and proliferative diabetic retinopathy. Diabetic macular edema was assessed as present if hard exudates were detected at the posterior pole of the retinal images. If more than one-third of the photograph was obscured, it was considered ungradable and the individual was considered referable. Referable possible glaucoma was defined as a ratio of vertical cup to disc diameter of 0.8 Diabetic Macular Edema Ungradable African American Eye Disease Study22 492 1968 984 807 (82.0) 50 (5.1) 37 (3.8) 5 (0.5) 16 (1.6) 28 (2.85) 41 (4.17) Singapore Malay Eye Study17-20 763 3052 1526 1143 (74.9) 215 (14.1) 113 (7.4) 18 (1.2) 9 (0.6) 53 (3.5) 28 (1.8) Singapore Indian Eye Study17-20 1128 4512 2256 1639 (72.7) 422 (18.7) 125 (5.5) 5 (0.2) 17 (0.8) 71 (3.1) 48 (2.1) Beijing Eye Study21 263 1052 526 493 (93.7) 4 (0.8) 11 (2.1) 4 (0.8) 0 12 (2.3) 2 (0.4) Guangdongf 3970 15 798 7899 5665 (71.7) 1235 (15.6) 737 (9.3) 0 154 (1.9) 0 108 (1.4) Singapore National Diabetic Retinopathy Screening Program 2010-201316,d 13 099 76 370 38 185 33 709 (88.3) 3310 (8.7) 597 (1.6) 478 (1.3) 70 (0.2) 2026 (5.3) 21 (0.1) Singapore National Diabetic Retinopathy Screening Program 2014-201516,e 14 880 71 896 35 948 33 087 (92.0) 1808 (5.0) 455 (1.3) 170 (0.5) 24 (0.1) 320 (0.9) 404 (1.1) Singapore Chinese Eye Study17-20 484 1936 968 759 (78.4) 131 (13.5) 60 (6.2) 1 (0.1) 7 (0.7) 17 (1.8",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\n) 10 (1) Proliferative Diabetic Retinopathyc Severe Nonproliferative Diabetic Retinopathyc Moderate Nonproliferative Diabetic Retinopathy Nonreferable Eyes Referable Eyesb Table 2. Training and Validation Datasets for Diabetic Retinopathy on Training, Primary Validation, and 10 External Validation Datasetsa Mild Nonproliferative Diabetic Re",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk4_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\ntinopathy Patients Images Eyes No Diabetic Retinopathy No. (%) No. External Validation Primary Validation Community-based Population-based Royal Victoria Eye and Ear Hospital23 588 2302 1151 432 (37.5) 121 (10.5) 159 (13.8) 123 (10.7) 191 (16.6) 249 (21.6) 125 (10.9) Total 38 253 189 018 94 509 79 980 (84.9) 9179 (9.74) 2876 (3.04) 912 (0.97) 517 (0.55) 3309 (3.50) 791 (0.41) University of Hong Kongg 1932 7706 3853 1984 (51.5) 1485 (38.5) 155 (4.0) 14 (0.4) 0 214 (5.55) 1 (0.03) Mexican 343 1172 586 38 (6.5) 284 (48.5) 192 (32.8) 51 (8.7) 18 (3.1) 223 (38.1) 3 (0.5) e In the Singapore Diabetic Retinopathy Screening Program 2014-2015, there were 6291 patients who were repeats from the Singapore Diabetic Retinopathy Screening Program 2010-2013, and 8589 were unique patients. Chinese University of Hong Kong24 314 1254 627 224 (35.7) 114 (18.2) 235 (37.5) 43 (6.9) 11 (1.8) 96 (15.3) 0 f Forty-one patients had only 1 eye. g Eleven patients had only 1 eye. c Vision-threatening diabetic retinopathy is defined as severe nonproliferative diabetic retinopathy and proliferative diabetic retinopathy. bReferable diabetic retinopathy is defined as moderate nonproliferative diabetic retinopathy, severe nonproliferative diabetic retinopathy, proliferative diabetic retinopathy, and diabetic macular edema. a For study locations and race/ethnicity data, see Table 1. dNine patients had only 1 eye. Table 3. Demographics, Diabetes History, and Systemic Risk Factors of Patients Attending the Singapore National Diabetes Retinopathy Screening Program Between 2010 to 2013 (Training Dataset) and 2014 to 2015 (Primary Validation Dataset) Primary Validation Dataset (SIDRP 2014-2015) No. of retinal images 76 370 71 896 Primary Training Dataset (SIDRP 2010-2013) Demographics and Vascular Risk Factors No. of eyes 38 185 35 948 No. of patients 13 099 14 880a Age, mean (SD), y 62.77 (11.32) 60.16 (12.19) Men, No. (%) 6518 (49.76) 4334 (51.02) Race/ethnicity, No. (%) Chinese 9615 (73.79) 6160 (72.51) Indian 1427 (10.95) 1037 (12.21) Malay 1582 (12.14) 1020 (12) Other 407 (3.12) 278 (3.27) Systemic risk factors, mean (SD) Body mass indexb 26.54 (4.69) 27.22 (4.99) Diabetes duration, median (IQR), y 6.4 (1.6-8.7) 3.7 (0.4-6.1) Blood pressure, mm Hg Systolic 129.9 (16.85) 132.05 (17.57) Diastolic 70.46 (10.06) 72.77 (10.78) HbA1c, % 7.25 (1.41) 7.54 (1.88) Lipids, mg/dL Total cholesterol 81.90 (17.28) 83.70 (19.26) HDL cholesterol 24.12 (6.48) 23.58 (6.30) LDL cholesterol 45.54 (14.94) 46.98 (15.66) Triglycerides 27.36 (16.20) 30.24 (39.96) Creatinine, mg/dL 0.92 (0.42) 0.85 (0.41) Abbreviations: HbA1c, glycated hemoglobin; HDL, high-density lipoprotein;",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk4_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\nIQR, interquartile range; LDL, low-density lipoprotein; SIDRP, Singapore National Diabetic Retinopathy Screening Program. SI conversion factors: To convert values for creatinine to mol/L, multiply by 88.4; total cholesterol, HDL cholesterol, and LDL cholesterol values to mmol/L, multiply by 0.0259; triglyceride values to mmol/L",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk5_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\n, multiply by 0.0113. a In the Singapore National Diabetic Retinopathy Screening Program 2014-2015, a total of 14 880 patients visited the primary eye care clinics for diabetic retinopathy screening. Of those, 6291 were follow-up patients who attended the Singapore National Diabetic Retinopathy Screening Program 2010-2013 and were excluded for analysis in eTable 3 in the Supplement to eliminate the risk of overfitting diagnostic performance of the deep learning system. bBody mass index was calculated as weight in kilograms divided by height in meters squared. or greater, focal thinning or notching of the neuroretinal rim, optic disc hemorrhages, or localized retinal nerve fiber layer defects—features sometimes referred to as glaucoma suspects. Referable AMD was defined as the presence of intermediate AMD (numerous medium-sized drusen, 1 large drusen 125 m in greatest linear diameter, noncentral geographical atrophy, and/or advanced AMD [central geographical atrophy or neovascular AMD]) according to the AgeRelated Eye Disease Study grading system.31 assessment) who was masked to the grading of the trained nonmedical professional graders. For all other retinal images from the 10 external validation datasets, reference standards were based on individual studies’ assessment of diabetic retinopathy, which was based on retinal specialists, general ophthalmologists, trained nonmedical professional graders, or optometrists (Table 1). The DLS performance for identifying referable diabetic retinopathy in the 10 external validation datasets was compared against these reference standards. For the analysis on referable possible glaucoma and referable AMD, the reference standard was the retinal specialist (Table 1). Statistical Analysis Initially the area under the curve (AUC) of the receiver operating characteristic (ROC) curve of DLS was calculated on the training dataset of the SIDRP 2010-2013 across a range of classification thresholds, and one was selected that achieved a predetermined optimal sensitivity of 90% for detecting referable diabetic retinopathy, vision-threatening diabetic retinopathy, referable possible glaucoma, and referable AMD. For diabetic retinopathy screening, international guidelines recommended a minimum sensitivity of 60% (Australia) to 80% (United Kingdom).32,33 In Singapore, the DLS sensitivity was preset at 90% based on the trained professional graders’ past performances and criteria set by the Ministry of Health, Singapore. The hypothesis determined was that the DLS was at least comparable to the professional graders’ performance. Primary analysis was to evaluate the performance of the DLS in the setting of the ongoing SIDRP 2014-2015 (the primary validation set) by determining whether the DLS was equivalent or superior to professional graders in the screening program. Thus, the AUC, sensitivity, and specificity of the DLS vs the professional graders in detecting referable diabetic retinopathy and vision-threatening diabetic retinopathy was computed to the reference standard (retinal specialist) at individual-eye levels. Next, the following subsidiary analyses were performed: (1) the analyses were repeated excluding patients who appeared in both the SIDRP 2010-2013 training set and the primary validation set of SIDRP 2014-2015 (n = 6291 seen more than once in SIDRP), with the patient treated as having referable diabetic retinopathy if either eye had referable diabetic retinopathy; (2) performance of the DLS was evaluated using higher-quality images with no media opacity (eg, cataracts) as noted by professional graders; (3) AUC subgroups were computed stratified by age, sex, and glycemic control; and (4) the analysis was repeated by calculating the AUC, sensitivity, and specificity of the DLS and the proportion of concordant and discordant eyes on the 10 external validation datasets, compared with the reference standards in these studies (retinal specialists, general",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk5_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\nophthalmologists, trained graders, or optometrists; Table 1). The DLS performance was then evaluated in detection of referable possible glaucoma and referable AMD, with reference to a retinal specialist, using the primary validation dataset (SIDRP 2014-2015). (Table 4. Primary Validation Dataset Showing the Area Under the Curve, Sensitivity, and Specificity of the Deep Learning System vs",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Method_chunk6",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Method\nTrained Professional Graders in Patients With Diabetes, SIDRP 2014-2015, With Reference to a Retinal Specialist’s Grading Referable diabetic retinopathyc Area under the curved 0.936 (0.925-0.943) Sensitivity, % 90.5 (87.3-93.0) 91.2 (88.0-93.6) .68 Specificity, % 91.6 (91.0-92.2) 99.3 (99.2-99.4) .001 Vision-threatening diabetic retinopathye Area under the curved 0.958 (0.956-0.961) Sensitivity, % 100 (94.1-100.0)f 88.5 (75.3-95.1) .001 Specificity, % 91.1 (90.7-91.4) 99.6 (99.6-99.7) .001 a Eyes were the units of analysis (n = 35 948). Asymptotic 95% CI was computed for the logit of each proportion and using the cluster sandwich estimator of standard error to account for possible dependency of eyes within each individual (exception, sensitivity calculation for the deep learning system). bP value was calculated between the deep learning system vs trained professional graders using the McNemar test. c Referable diabetic retinopathy was defined as moderate non proliferative Value (95% CI)a P Valueb Deep Learning System Trained Professional Graders diabetic retinopathy, severe nonproliferative diabetic retinopathy, proliferative diabetic retinopathy, diabetic macular edema, and ungradable eye. dCluster-bootstrap, biased-corrected 95% CI was computed for each area under the curve, with individual patients as the bootstrap sampling clusters. e Vision-threatening diabetic retinopathy was defined as severe nonproliferative diabetic retinopathy and proliferative diabetic retinopathy. f Exact Clopper-Pearson left-sided 97.5% CI was calculated owing to estimate being at the boundary.) For a secondary aim, an examination of how the DLS could fit in 2 potential diabetic retinopathy screening models was performed: a fully-automated model for communities with no existing screening programs, vs a semiautomated model in which referable cases from the DLS have a secondary assessment by human graders—a method currently used in some communities and countries (eg, United States, United Kingdom, and Singapore) (eFigure 2 in the Supplement).32-35 For this analysis, in the fully-automated model, eyes were considered referable if any one of the 3 conditions (referable diabetic retinopathy, referable possible glaucoma, or referable AMD) were present. In the semiautomated model, eyes classified as referable by the DLS would undergo a secondary assessment by trained professional graders to reclassify eyes if necessary. For semiautomated models, evaluation was made of the proportion of images requiring secondary assessment when presetting the DLS sensitivity threshold at 90%, 95%, and 99% in detection of referable status. Cluster-bootstrap, biased-corrected, asymptotic 2-sided 95%CIsadjustedforclusteringbypatientswerecalculatedand presented for proportions (sensitivity, specificity) and AUC, respectively. In a few exceptional cases with estimate of sensitivity at the boundary of 100%, the exact Clopper-Pearson method was used instead to obtain CI estimates.36 All hypotheses tested were 2-sided, and a P value of less than .05 was considered statistically significant. No adjustment for multiple comparisons was made because the study was restricted to a small number of planned comparisons. All analyses were performed using Stata version 14 (StataCorp).",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\nExperiments / Results: Results From a total of 494661 retinal images, the DLS was trained for detection of referable diabetic retinopathy (using 76370 images), referable possible glaucoma (using 125 189 images), and referable AMD (using 72 610 images); performance of the DLS was evaluated using 112 648 images for detection of referable diabetic retinopathy, 71 896 images for referable possible glaucoma, and 35 948 images for referable AMD. All images were assembled between January 2016 and March 2017 (Table 1), the DLS training was completed in May 2016, and validation was completed in May 2017. Among 76 370 images in the training dataset, 11.7% demonstrated any diabetic retinopathy, 5.3% referable diabetic retinopathy, and 1.5% vision-threatening diabetic retinopathy. In the primary validation dataset, estimates were 8.0% for having any diabetic retinopathy, 3.0% for referable diabetic retinopathy, and 0.6% for vision-threatening diabetic retinopathy (n = 71 896 images). In the 10 external validation datasets, estimates were 35.3% for any diabetic retinopathy, 15.4% for referable diabetic retinopathy, and 3.4% for visionthreatening diabetic retinopathy (n = 40 752 images; Table 2). For possible glaucoma, 2630 images (1907 eyes) were considered referable; for AMD, 2900 images (1017 eyes) were considered referable (eTable 1 in the Supplement). The overall patients demographics, diabetes history, and systemic risk factors of the training and validation datasets are listed in Table 3 (SIDRP 2010-2013 and SIDRP 2014-2015, primary validation set) and eTable 2 in the Supplement (10 external validation datasets for referable diabetic retinopathy and training datasets for referable possible glaucoma and referable AMD). The diagnostic performance of the DLS as compared with trained professional graders, both with reference to the retinal specialist standard using this primary validation dataset, is shown in Table 4. The AUC of the DLS was 0.936 for referable diabetic retinopathy and 0.958 for vision-threatening diabetic retinopathy (Figure 1). Sensitivity of the DLS in detecting referable diabetic retinopathy was comparable with that of trained graders (90.5% vs 91.1%; P = .68), although the Figure 1. Receiver Operating Characteristic Curve and Area Under the Curve of the Deep Learning System for Detection of Referable Diabetic Retinopathy and Vision-Threatening Diabetic Retinopathy in the Singapore National Diabetic Retinopathy Screening Program (SIDRP 2014-2015; Primary Validation Dataset), Compared with Professional Graders’ Performance, With Retinal Specialists’ Grading as Reference Standard Deep learning system for detection of referable diabetic retinopathy A 100 80 60 Sensitivity, % 40 Professional grader Professional grader Sensitivity, %: 91.18 (95% CI, 87.97-93.60) Specificity, %: 99.34 (95% CI, 99.24-99.42) 20 Deep learning system AUC = 0.936 (95% CI, 0.925-0.943) 0 0 100 80 60 40 20 100 – Specificity, % Deep learning system for detection of vision-threatening diabetic retinopathy B 100 80 60 Sensitivity, % 40 Professional grader Professional grader Sensitivity, %: 88.52 (95% CI, 75.30-95.13) Specificity, %: 99.63 (95% CI, 99.45-99.70) 20 Deep learning system AUC = 0.958 (95% CI, 0.956-0.961) 0 0 100 80 60 40 20 100 – Specificity, % AUC indicates area under the receiver operating characteristic curve; SIDRP, Singapore National Diabetic Retinopathy Screening Program. graders had higher specific",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\nity (91.6% vs 99.3%; P .001) (Table 4; Figure 1). For vision-threatening diabetic retinopathy, the DLS had higher sensitivity compared with trained graders (100% vs 88.5%; P .001), but lower specificity (91.1% vs 99.6%; P .001). Among eyes with referable diabetic retinopathy",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\n, the sensitivity of diabetic macular edema was 92.1% for the DLS and 98.2% for professional graders. Five subsidiary analyses were performed. First, the DLS showed similar diagnostic performance in 8589 unique patients of SIDRP 2014-2015 (with no overlap with training set) as in the primary analysis (eTable 3 in the Supplement). Second, in a subset of 97.4% eyes (n = 35 055) with excellent retinal image quality (no media opacity), the AUC of the DLS for referable diabetic retinopathy increased to 0.949 (95% CI, 0.940-0.957); for vision-threatening diabetic 100 95 90 85 Professional grader 80 0 40 35 20 25 30 15 10 5 100 – Specificity, % 100 95 90 85 Professional grader 80 0 35 40 20 25 30 15 10 5 100 – Specificity, % retinopathy, it increased to 0.970 (0.968-0.973). Third, the DLS showed comparable performance in different subgroups of patients stratified by age, sex, and glycemic control (Figure 2). Fourth, the DLS showed clinically acceptable performance (sensitivity 90%) for referable diabetic retinopathy with respect to multiethnic populations of different communities, clinics, and settings (Table 5). Among the 10 external validation datasets, the AUC of referable diabetic retinopathy ranged from 0.889 to 0.983. The DLS showed clinically acceptable AUCs of greater than 0.90 for different cameras (eg, FundusVue, Canon, Topcon, and Carl Zeiss). Most datasets (except for Singapore Chinese, Malay, and Indian patients) had more than 80% concordance between the DLS and trained professional graders, with sensitivity of more Figure 2. Receiver Operating Characteristic Curve and Area Under the Curve of the Deep Learning System for Detection of Referable Diabetic Retinopathy in SIDRP 2014-2015 (Primary Validation Set) by Age, Sex, and HbA1c Level Deep learning system for detection of diabetic retinopathy referable by age, y (No. of eyes, 35 948) A 100 80 60 Sensitivity, % 40 20 Age 60 y; AUC, 0.980 (95% CI, 0.975-0.984) Age 60 y; AUC, 0.920 (95% CI, 0.899-0.940) 0 0 100 80 60 40 20 100 – Specificity, % Deep learning system for detection of diabetic retinopathy referable by HbA1c (No. of eyes, 18 729) C 100 80 60 Sensitivity, % 40 20 HbA1c 8%; AUC, 0.938 (95% CI, 0.892-0.958) HbA1c 8%; AUC, 0.954 (95% CI, 0.942-0.964) 0 0 100 80 60 40 20 100 – Specificity, % Eyes are the units of analysis. Glycated hemoglobin (HbA1c) levels were available for only 52.1% of patients. Cluster-bootstrap biased-corrected 95% CI was computed for each area under the receiver operating characteristic curve than 91% in the eyes classified as referable by retinal specialists, general ophthalmologists, trained graders, or optometrists (Table 5). Fifth, for referable possible glaucoma, the AUC of the DLS was 0.942 (95% CI, 0.929-0.954), sensitivity was 96.4% (95% CI, 81.7%-99.9%), and specificity was 87.2% (86.8%-87.5%); for referable AMD, the AUC was 0.931 (95% CI, 0.928-0.935), sensitivity was 93.2% (95% CI, 91.1%-99.8%) and specificity was 88.7% (95% CI, 88.3%",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\n-89.0%) (Figure 3). For the secondary aim, we evaluated the performance of the DLS in 2 diabetic retinopathy screening models (eFigure 2 in the Supplement): the fully- automated model had sensitivity of 93.0% (95% CI, 91.5%-94.3%) and specificity of 77.5% (95% CI, 77.0%-77.9%) to detect overall referable",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\ncases (refer- Deep learning system for detection of diabetic retinopathy referable by sex (No. of eyes, 35 948) B 100 80 60 Sensitivity, % 40 20 Men; AUC, 0.952 (95% CI, 0.925-0.963) Women; AUC, 0.948 (95% CI, 0.933-0.956) 0 0 100 80 60 40 20 100 – Specificity, % (AUC), with individual patients as the bootstrap sampling clusters. See Methods for defintions of referable conditions. A, P .001. B, P= .74. C, P = .34. SIDRP indicates Singapore National Diabetic Retinopathy Screening Program. able diabetic retinopathy, possible glaucoma, or AMD), while the semiautomated model (DLS followed by graders) had sensitivity of 91.3% (95% CI, 89.7%-92.8%) and specificity of 99.5% (95% CI, 99.5%-99.6%) to detect overall referable status. The performance of different semiautomated models with a preset sensitivity threshold of 90%, 95%, and 99% are shown in eTable 4 in the Supplement. Table 5. External Validation Datasets Showing the Area Under the Curve, Sensitivity, Specificity, Concordant and Discordant Rates of the Deep Learning System in Detecting Referable Diabetic Retinopathy Among Populations With Diabetes, With Comparison to Retinal Specialists, General Ophthalmologists, Trained Graders, or Optometristsa % (95% CI) Concordance Between DLS and Grader, No. (%)d Sensitivityc Specificityc DLS+ Graders+ DLS+ Graders DLS Graders+ DLS Graders Datasets (No. of Images) AUC (95% CI)b Community-based Guangdong (N = 15 798) 0.949 (0.943-0.955) 98.7 (97.7-99.3) 81.6 (80.7-82.5) 1785 (11.3) 2575 (16.3) 16 (0.1) 11 422 (72.3) 13 207 (83.6) Population-based 0.889 (0.863-0.908) 97.1 (92.5-98.9) 82.0 (79.4-84.4) 282 (9.2) 611 (20.0) 3 (0.1) 2156 (70.6) 2438 (79.9) Singapore Malay Eye Study,17-20 (N = 3052) 0.917 (0.899-0.933) 99.3 (95.1-99.9) 73.3 (70.9-75.5) 298 (6.6) 1543 (34.2) 0 2671 (59.2) 2969 (65.8) Singapore Indian Eye Study,17-20 (N = 4512) 0.919 (0.900-0.942) 100 (92.5-100.0)e 76.3 (72.7-79.6) 138 (7.1) 560 (28.9) 0 1239 (64.0) 1377 (71.1) Singapore Chinese Eye Study17-20 (N = 1936) Beijing Eye Study,21 (N = 1052) 0.929 (0.903-0.955) 94.4 (72.7-99.9) 88.5 (85.4-91.2) 35 (3.3) 117 (11.1) 1 (0.1) 899 (85.5) 934 (88.8) 0.980 (0.971-0.989) 98.8 (93.5-100.0) 86.5 (84.1-88.7) 171 (8.7) 242 (12.3) 2 (0.1) 1553 (78.9) 1724 (87.6) African American Eye Disease Study22 (N = 1968) Clinic-based 0.983 (0.972-0.991) 98.9 (97.5-99.6) 92.2 (89.5-94.3) 1066 (46.3) 198 (8.6) 5 (0.2) 1034 (44.9) 2100 (91.2) Royal Victoria Eye and Ear Hospital8 (N = 2302) Mexican (N = 1172) 0.950 (0.934-0.966) 91.8 (88.4-94.4) 84.8 (80.4-88.5)",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\n571 (48.7) 83 (7.1) 52 (4.4) 466 (39.8) 1037 (88.5) 0.948 (0.921-0.972) 99.3 (97.3-99.8) 83.1 (77.9-87.3) 576 (45.9) 165 (13.2) 4 (0.3) 509 (40.6) 1085 (86.5) Chinese University of Hong Kong (N = 1254) 0.964",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk4_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\n(0.958-0.970) 100 (99.0-100)e 81.3 (80.0-82.6) 701 (9.1) 1310 (17.0) 0 5695 (73.9) 6396 (83.0) University of Hong Kong (N = 7706) c Asymptotic 95% CI was computed for the logit of each proportion and using the cluster sandwich estimator of standard error to account for possible dependency of eyes within each individual. Abbreviations: AUC, area under the receiver operating characteristic curve; DLS, deep learning system. a For study locations and race/ethnicity data, see Table 1. Referable diabetic retinopathy was defined as moderate nonproliferative diabetic retinopathy, severe, proliferative diabetic retinopathy, and ungradable images. dDLS+ and grader+ indicates positive concordance; DLSand grader, negative concordance. Last column reports total concordance (sum of these 2 values). e Exact Clopper-Pearson left-sided 97.5% CI was calculated owing to estimate being at the boundary. bCluster-bootstrap, biased-corrected 95% CI was computed for each area under the curve, with individual patients as the bootstrap sampling clusters. identifying referable diabetic retinopathy and visionthreatening diabetic retinopathy, as well as for identifying related eye diseases, including referable possible glaucoma and referable age-related macular degeneration. The performance of the DLS was comparable and clinically acceptable to the current model based on assessment of retinal images by trained professional graders and showed consistency in 10 external validation datasets of multiple ethnicities and settings, using diverse reference standards in assessment of diabetic retinopathy by professional graders, optometrists, or retinal specialists. This study also examined how the DLS could be deployed in 2 common diabetic retinopathy screening models: a “fully-automated” screening model that showed clinically acceptable performance to detect all 3 conditions, useful in communities without any existing diabetic retinopathy screening programs; and a “semi-automated” model in which diabetic retinopathy screening programs using trained professional graders already exist, and the DLS could be incorporated. There have been previous studies of automated software for diabetic retinopathy screening14,37,38; most recent ones used a DLS.10-12 Gulshan et al10 reported a DLS with high sensitivity and specificity (>90%) and an AUC of 0.99 for referable diabetic retinopathy using approximately 10 000 images retrieved from 2 publicly available databases (EyePAC-1 and Messidor-2). Similarly, Gargeya and Leng11 showed optimal DLS diagnostic performance in detecting any diabetic retinopathy using 2 other public databases (Messidor-2 and E-Ophtha). To facilitate translation, it is important to develop and test the DLS in clinical scenarios using diverse retinal images of varying quality from different camera types and in representative diabetic retinopathy screening populations.13 The current study therefore substantially added to other current studies. First, the DLS was trained to also detect other related eye diseases including referable possible glaucoma and referable AMD in addition to diabetic retinopathy. Second, the training and validation data sets were substantially larger (nearly Total Concordant Images Figure 3. Primary Validation Dataset and Area Under the Curve of the Deep Learning System in Detecting Referable Possible Glaucoma and Referable Age-Related Macular Degeneration (AMD) Among Patients With Diabetes, SIDRP 2014-2015, With Reference to a Retinal Specialist Deep learning system for detecting possible glaucoma A 100 80 60 Sensitivity, % 40 20 AUC, 0.942 (95% CI, 0.929-0.954) 0 0 100 80 60 40 20 100 – Specificity, % Eyes are the units of analysis. Cluster-bootstrap biased-correcte",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk4_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\nd 95% CI was computed for each area under the receiver operating characteristic curve (AUC), with individual patients as the bootstrap sampling clusters. Referable possible glaucoma defined as ratio of vertical cup to disc diameter of 0.8 or greater, focal thinning or notching of the neuroretinal rim, optic disc hemorrhages, or localized retinal nerve fiber layer defects. Referable acute macular degeneration (AM",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "57",
    "id_story": "57-01",
    "id_chunk": "57-01_Experiments_/_Results_chunk5",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Developer, Student, Journalist\n\nSection: Experiments / Results\nD) defined as the presence of intermediate AMD (numerous intermediate drusens, 1 large drusen >125um) and/or advanced 500 000 images) and included images from patients of diverse racial and ethnic groups (ie, darker fundus pigmentation in African American and Indian individuals to lighter fundus in white individuals). The DLS showed consistent diagnostic performance across images of varying quality and different camera types, and across patients with varying systemic glycemic control level. Third, primary validation of the DLS was conducted in an ongoing diabetic retinopathy screening program in which there were poorer quality images, including ungradable ones. This results in somewhat lower performance of the DLS (AUC, 0.936) than the system by Gulshan et al that used higher-quality images.10 Fourth, this study also had fewer cases of severe disease (eg, vision-threatening diabetic retinopathy, referable possible glaucoma, and referable AMD), but this is more representative of populations for routine diabetic retinopathy screening.39 To ensure no degradation in health outcomes, a threshold was set to ensure false-negative rates were no worse than human assessment by trained professional graders. Although the results suggest that professional nonmedical graders may outperform the DLS (with high specificity of 99% for referable diabetic retinopathy and vision-threatening diabetic retinopathy), given the very low marginal cost of the DLS, the low prevalence rate of the conditions in the target screening population (5%), and equality in health outcomes, the DLS could be used with a semiautomated model in which firstline screening with the DLS is followed by human assessment Deep learning system for detecting referable AMD B 100 80 60 Sensitivity, % 40 20 AUC, 0.932 (95% CI, 0.928-0.935) 0 0 100 80 60 40 20 100 – Specificity, % AMD, geographic atrophy, or neovascular AMD, using the Age-Related Eye Disease Study grading system.30 Repeats from the Singapore National Diabetes Retinopathy Screening Program (SIDRP) 2014-2015 were excluded from the analysis. Asymptotic 95% CI was computed for the logit of each proportion and using the cluster sandwich estimator of standard error to account for possible dependency of eyes within each individual. Cluster-bootstrap biased-corrected 95% CI was computed for each AUC, with individual patients as the bootstrap sampling clusters. for patients who test positive. This will allow increasing screening episodes with lower cost and no degradation in health outcomes.",
    "output": "Diabetic retinopathy — an eye condition that affects people with diabetes — is the fastest growing cause of blindness, with nearly 415 million diabetic patients at risk worldwide. The disease can be treated if detected early, but if not, it can lead to irreversible blindness.\n\nOne of the most common ways to detect diabetic eye disease is to have a specialist examine pictures of the back of the eye and determine whether there are signs of the disease, and if so, how severe it is. While annual screening is recommended for all patients with diabetes, many people live in areas without easy access to specialist care. That means millions of people aren’t getting the care they need to prevent loss of vision.\n\nA few years ago, a Google research team began studying whether machine learning could be used to screen for diabetic retinopathy (DR). Today, in the Journal of the American Medical Association, we’ve published our results: a deep learning algorithm capable of interpreting signs of DR in retinal photographs, potentially helping doctors screen more patients, especially in underserved communities with limited resources.\n\nWorking with a team of doctors in India and the U.S., we created a dataset of 128,000 images and used them to train a deep neural network to detect diabetic retinopathy. We then compared our algorithm’s performance to another set of images examined by a panel of board-certified ophthalmologists. Our algorithm performs on par with the ophthalmologists, achieving both high sensitivity and specificity. For more details, see our post on the Research blog.\n\nWe’re excited by the results, but there’s a lot more to do before an algorithm like this can be used widely. For example, interpretation of a 2D retinal photograph is only one step in the process of diagnosing diabetic eye disease — in some cases, doctors use a 3D imaging technology to examine various layers of a retina in detail. Our colleagues at DeepMind are working on applying machine learning to that method. In the future, these two complementary methods might be used together to assist doctors in the diagnosis of a wide spectrum of eye diseases.\n\nAutomated, highly accurate screening methods have the potential to assist doctors in evaluating more patients and quickly routing those who need help to a specialist. We hope this study will be one of many examples to come demonstrating the ability of machine learning to help solve important problems in healthcare."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Abstract\nAbstract: Robust Speech Recognition via Large-Scale Weak Supervision Alec Radford * 1 Jong Wook Kim * 1 Tao Xu 1 Greg Brockman 1 Christine McLeavey 1 Ilya Sutskever 1 Abstract We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing. arXiv:2212.04356v1 [eess.AS] 6 Dec 2022",
    "output": "“The primary intended users of [the Whisper] models are AI researchers studying robustness, generalization, capabilities, biases and constraints of the current model. However, Whisper is also potentially quite useful as an automatic speech recognition solution for developers, especially for English speech recognition,” OpenAI wrote in the GitHub repo for Whisper, from where several versions of the system can be downloaded. “[The models] show strong ASR results in ~10 languages. They may exhibit additional capabilities … if fine-tuned on certain tasks like voice activity detection, speaker classification or speaker diarization but have not been robustly evaluated in these area.”\nThat last bit is nothing new to the world of speech recognition, unfortunately. Biases have long plagued even the best systems, with a 2020 Stanford study finding systems from Amazon, Apple, Google, IBM and Microsoft made far fewer errors — about 19% — with users who are white than with users who are Black."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\nIntroduction: 1. Introduction Progress in speech recognition has been energized by the development of unsupervised pre-training techniques exemplified by Wav2Vec 2.0 (Baevski et al., 2020). Since these methods learn directly from raw audio without the need for human labels, they can productively use large datasets of unlabeled speech and have been quickly scaled up to 1,000,000 hours of training data (Zhang et al., 2021), far more than the 1,000 or so hours typical of an academic supervised dataset. When fine-tuned on standard benchmarks, this approach has improved the state of the art, especially in a low-data setting. These pre-trained audio encoders learn high-quality representations of speech, but because they are purely unsupervised they lack an equivalently performant decoder mapping those representations to usable outputs, necessitating a finetuning stage in order to actually perform a task such as speech recognition1. This unfortunately limits their usefulness and impact as fine-tuning can still be a complex process requiring a skilled practitioner. There is an additional risk with requiring fine-tuning. Machine learning Wook Kim jongwook@openai.com>. methods are exceedingly adept at finding patterns within a training dataset which boost performance on held-out data from the same dataset. However, some of these patterns are brittle and spurious and don’t generalize to other datasets and distributions. In a particularly disturbing example, Radford et al. (2021) documented a 9.2% increase in object classification accuracy when fine-tuning a computer vision model on the ImageNet dataset (Russakovsky et al., 2015) without observing any improvement in average accuracy when classifying the same objects on seven other natural image datasets. A model that achieves “superhuman” performance when trained on a dataset can still make many basic errors when evaluated on another, possibly precisely because it is exploiting those dataset-specific quirks that humans are oblivious to (Geirhos et al., 2020). This suggests that while unsupervised pre-training has improved the quality of audio encoders dramatically, the lack of an equivalently high-quality pre-trained decoder, combined with a recommended protocol of dataset-specific finetuning, is a crucial weakness which limits their usefulness and robustness. The goal of a speech recognition system should be to work reliably “out of the box” in a broad range of environments without requiring supervised fine-tuning of a decoder for every deployment distribution. As demonstrated by Narayanan et al. (2018), Likhomanenko et al. (2020), and Chan et al. (2021) speech recognition systems that are pre-trained in a supervised fashion across many datasets/domains exhibit higher robustness and generalize much more effectively to held-out datasets than models trained on a single source. These works achieve this by combining as many existing high-quality speech recognition datasets as possible. However, there is still only a moderate amount of this data easily available. SpeechStew (Chan et al., 2021) mixes together 7 pre-existing datasets totalling 5,140 hours of supervision. While not insignificant, this is still tiny compared to the previously mentioned 1,000,000 hours of unlabeled speech data utilized in Zhang et al. (2021). Recognizing the limiting size of existing high-quality supervised datasets, recent efforts have created larger datasets for speech recognition. By relaxing the requirement of goldstandard human-validated transcripts, Chen et al. (2021) and Galvez et al. (2021) make use of sophisticated automated pipelines to scale weakly supervised speech recognition to 10,000 and 30,000 hours of noisier training data. This trade-off between quality and quantity is often the right call. Although understudied so far for speech recognition, recent work in computer vision has demonstrated that moving beyond gold-standard crowdsourced datasets such as ImageNet (Russakovsky et",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English.\n\nCountless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\n\n“The primary intended users of [the Whisper] models are AI researchers studying robustness, generalization, capabilities, biases and constraints of the current model. However, Whisper is also potentially quite useful as an automatic speech recognition solution for developers, especially for English speech recognition,” OpenAI wrote in the GitHub repo for Whisper, from where several versions of the system can be downloaded. “[The models] show strong ASR results in ~10 languages. They may exhibit additional capabilities … if fine-tuned on certain tasks like voice activity detection, speaker classification or speaker diarization but have not been robustly evaluated in these area.”\n\nWhisper has its limitations, particularly in the area of text prediction. Because the system was trained on a large amount of “noisy” data, OpenAI cautions Whisper might include words in its transcriptions that weren’t actually spoken — possibly because it’s both trying to predict the next word in audio and trying to transcribe the audio itself. Moreover, Whisper doesn’t perform equally well across languages, suffering from a higher error rate when it comes to speakers of languages that aren’t well-represented in the training data.\n\nThat last bit is nothing new to the world of speech recognition, unfortunately. Biases have long plagued even the best systems, with a 2020 Stanford study finding systems from Amazon, Apple, Google, IBM and Microsoft made far fewer errors — about 19% — with users who are white than with users who are Black.\n\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools.\n\n“While Whisper models cannot be used for real-time transcription out of the box, their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation,” the company continues on GitHub. “The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications … [W]e hope the technology will be used primarily for beneficial purposes, making automatic speech recognition technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication.”\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe release of Whisper isn’t necessarily indicative of OpenAI’s future plans. While increasingly focused on commercial efforts like DALL-E 2 and GPT-3, the company is pursuing several purely theoretical research threads, including AI systems that learn by observing videos."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\nal., 2015) to much larger but weakly supervised datasets significantly improves the robustness and generalization of models (Mahajan et al., 2018; Kolesnikov et al., 2020). Yet these new datasets are only a few times larger than the sum of existing high-quality datasets and still much smaller than prior unsupervised work. In this work we close that gap, scaling weakly supervised speech recognition",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English.\n\nCountless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\n\n“The primary intended users of [the Whisper] models are AI researchers studying robustness, generalization, capabilities, biases and constraints of the current model. However, Whisper is also potentially quite useful as an automatic speech recognition solution for developers, especially for English speech recognition,” OpenAI wrote in the GitHub repo for Whisper, from where several versions of the system can be downloaded. “[The models] show strong ASR results in ~10 languages. They may exhibit additional capabilities … if fine-tuned on certain tasks like voice activity detection, speaker classification or speaker diarization but have not been robustly evaluated in these area.”\n\nWhisper has its limitations, particularly in the area of text prediction. Because the system was trained on a large amount of “noisy” data, OpenAI cautions Whisper might include words in its transcriptions that weren’t actually spoken — possibly because it’s both trying to predict the next word in audio and trying to transcribe the audio itself. Moreover, Whisper doesn’t perform equally well across languages, suffering from a higher error rate when it comes to speakers of languages that aren’t well-represented in the training data.\n\nThat last bit is nothing new to the world of speech recognition, unfortunately. Biases have long plagued even the best systems, with a 2020 Stanford study finding systems from Amazon, Apple, Google, IBM and Microsoft made far fewer errors — about 19% — with users who are white than with users who are Black.\n\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools.\n\n“While Whisper models cannot be used for real-time transcription out of the box, their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation,” the company continues on GitHub. “The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications … [W]e hope the technology will be used primarily for beneficial purposes, making automatic speech recognition technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication.”\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe release of Whisper isn’t necessarily indicative of OpenAI’s future plans. While increasingly focused on commercial efforts like DALL-E 2 and GPT-3, the company is pursuing several purely theoretical research threads, including AI systems that learn by observing videos."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\nthe next order of magnitude to 680,000 hours of labeled audio data. We call our approach Whisper2. We demonstrate models trained at this scale transfer well to existing datasets zeroshot, removing the need for any dataset-specific fine-tuning to achieve high-quality results. In addition to scale, our work also focuses on broadening the scope of weakly supervised pre-training beyond English-only speech recognition to be both multilingual and multitask. Of those 680,000 hours of audio, 117,000 hours cover 96 other languages. The dataset also includes 125,000 hours of Xen translation data. We find that for sufficiently large models there is no drawback and even benefits to joint multilingual and multitask training. Our work suggests that simple scaling of weakly supervised pre-training has been underappreciated so far for speech recognition. We achieve these results without the need for the self-supervision or self-training techniques that have been a mainstay of recent large-scale speech recognition work. To serve as a foundation for further research on robust speech recognition, we release inference code and models at the following URL: https://github.com/openai/whisper.",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English.\n\nCountless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\n\n“The primary intended users of [the Whisper] models are AI researchers studying robustness, generalization, capabilities, biases and constraints of the current model. However, Whisper is also potentially quite useful as an automatic speech recognition solution for developers, especially for English speech recognition,” OpenAI wrote in the GitHub repo for Whisper, from where several versions of the system can be downloaded. “[The models] show strong ASR results in ~10 languages. They may exhibit additional capabilities … if fine-tuned on certain tasks like voice activity detection, speaker classification or speaker diarization but have not been robustly evaluated in these area.”\n\nWhisper has its limitations, particularly in the area of text prediction. Because the system was trained on a large amount of “noisy” data, OpenAI cautions Whisper might include words in its transcriptions that weren’t actually spoken — possibly because it’s both trying to predict the next word in audio and trying to transcribe the audio itself. Moreover, Whisper doesn’t perform equally well across languages, suffering from a higher error rate when it comes to speakers of languages that aren’t well-represented in the training data.\n\nThat last bit is nothing new to the world of speech recognition, unfortunately. Biases have long plagued even the best systems, with a 2020 Stanford study finding systems from Amazon, Apple, Google, IBM and Microsoft made far fewer errors — about 19% — with users who are white than with users who are Black.\n\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools.\n\n“While Whisper models cannot be used for real-time transcription out of the box, their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation,” the company continues on GitHub. “The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications … [W]e hope the technology will be used primarily for beneficial purposes, making automatic speech recognition technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication.”\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe release of Whisper isn’t necessarily indicative of OpenAI’s future plans. While increasingly focused on commercial efforts like DALL-E 2 and GPT-3, the company is pursuing several purely theoretical research threads, including AI systems that learn by observing videos."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nMethod: 2. Approach 2.1. Data Processing Following the trend of recent work leveraging web-scale text from the internet for training machine learning systems, we take a minimalist approach to data pre-processing. In contrast to a lot of work on speech recognition, we train Whisper models to predict the raw text of transcripts without any significant standardization, relying on the expressiveness of sequence-to-sequence models to learn to map between utterances and their transcribed form. This simplifies the speech recognition pipeline since it removes the need for a separate inverse text normalization step in order to produce naturalistic transcriptions. We construct the dataset from audio that is paired with transcripts on the Internet. This results in a very diverse dataset covering a broad distribution of audio from many different environments, recording setups, speakers, and languages. While diversity in audio quality can help train a model to be robust, diversity in transcript quality is not similarly beneficial. Initial inspection showed a large amount of subpar transcripts in the raw dataset. To address this, we developed several automated filtering methods to improve transcript quality. Many transcripts on the internet are not actually humangenerated but the output of existing ASR systems. Recent research has shown that training on datasets of mixed human and machine-generated data can significantly impair the performance of translation systems (Ghorbani et al., 2021). In order to avoid learning “transcript-ese”, we developed many heuristics to detect and remove machine-generated transcripts from the training dataset. Many existing ASR systems output only a limited subset of written language which removes or normalizes away aspects that are difficult to predict from only audio signals such as complex punctuation (exclamation points, commas, and question marks), formatting whitespace such as paragraphs, or stylistic aspects such as capitalization. An all-uppercase or all-lowercase transcript is very unlikely to be human generated. While many ASR systems include some level of inverse text normalization, it is often simple or rule-based and still detectable from other unhandled aspects such as never including commas. We also use an audio language detector, which was created by fine-tuning a prototype model trained on a prototype version of the dataset on VoxLingua107 (Valk & Alum ae, 2021) to ensure that the spoken language matches the language of the transcript according to CLD2. If the two do not match, we don’t include the (audio, transcript) pair as a speech recognition training example in the dataset. We make an exception if the transcript language is English and add these pairs to the dataset as Xen speech translation training examples instead. We use fuzzy de-duping of transcript texts to reduce the amount of duplication and automatically generated content in the training dataset. We break audio files into 30-second segments paired with the subset of the transcript that occurs within that time segment. We train on all audio, including segments where there is no speech (though with sub-sampled probability) and use these segments as training data for voice activity detection. data sources and performed manual inspection of these data sources sorting by a combination of both high error rate and data source size in order to identify and remove low-quality ones efficiently. This inspection showed a large amount of only partially transcribed or poorly aligned/misaligned transcripts as well as remaining low-quality machine-generated captions that filtering heuristics did not detect. To avoid contamination, we perform de-duplication at a transcript level between the training dataset and the evaluation datasets we thought were at higher risk of overlap, namely TED-LIUM 3 (Hernandez et al., 2018). 2.2. Model Since the focus of our work is on studying the capabilities of large-scale supervised pre-training for speech recognition, we use an off-the-shelf architecture to avoid confounding our findings with model improvements. We chose an encoder-decoder Transformer (Vaswani et al., 2017) as this architecture has been well validated to scale reliably. All audio is re-sampled to 16,000 Hz,",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nand an 80-channel logmagnitude Mel spectrogram representation is computed on 25-millisecond windows with a stride of 10 milliseconds. For feature normalization, we globally scale the input to be between -1 and 1 with approximately zero mean across the pre-training dataset. The encoder processes this input representation with a small stem consisting of two convolution layers with a filter width of 3 and the GELU activation",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nfunction (Hendrycks & Gimpel, 2016) where the second convolution layer has a stride of two. Sinusoidal position embeddings are then added to the output of the stem after which the encoder Transformer blocks are applied. The transformer uses pre-activation residual blocks (Child et al., 2019), and a final layer normalization is applied to the encoder output. The decoder uses learned position embeddings and tied input-output token representations (Press & Wolf, 2017). The encoder and decoder have the same width and number of transformer blocks. Figure 1 summarizes the model architecture. We use the same byte-level BPE text tokenizer used in GPT2 (Sennrich et al., 2015; Radford et al., 2019) for the Englishonly models and refit the vocabulary (but keep the same size) for the multilingual models to avoid excessive fragmentation on other languages since the GPT-2 BPE vocabulary is English only. 2.3. Multitask Format Although predicting which words were spoken in a given audio snippet is a core part of the full speech recognition problem and extensively studied in research, it is not the only part. A fully featured speech recognition system can involve many additional components such as voice activity detection, speaker diarization, and inverse text normal- ization. These components are often handled separately, resulting in a relatively complex system around the core speech recognition model. To reduce this complexity, we would like to have a single model perform the entire speech processing pipeline, not just the core recognition part. An important consideration here is the interface for the model. There are many different tasks that can be performed on the same input audio signal: transcription, translation, voice activity detection, alignment, and language identification are some examples. For this kind of one-to-many mapping to work with a single model, some form of task specification is necessary. We use a simple format to specify all tasks and conditioning information as a sequence of input tokens to the decoder. Since our decoder is an audio-conditional language model, we also train it to condition on the history of text of the transcript in the hope that it will learn to use longer-range text context to resolve ambiguous audio. Specifically, with some probability we add the transcript text preceding the current audio segment to the decoder’s context. We indicate the beginning of prediction with a |startoftranscript|> token. First, we predict the language being spoken which is represented by a unique token for each language in our training set (99 total). These language targets are sourced from the aforementioned VoxLingua107 model. In the case where there is no speech in an audio segment, the model is trained to predict a |nospeech|> token indicating this. The next token specifies the task (either transcription or translation) with an |transcribe|> or |translate|> token. After this, we specify whether to predict timestamps or not by including a |notimestamps|> token for that case. At this point, the task and desired format is fully specified, and the output begins. For timestamp prediction, we predict time relative to the current audio segment, quantizing all times to the nearest 20 milliseconds which matches the native time resolution of Whisper models, and add additional tokens to our vocabulary for each of these. We interleave their prediction with the caption tokens: the start time token is predicted before each caption’s text, and the end time token is predicted after. When a final transcript segment is only partially included in the current 30second audio chunk, we predict only its start time token for the segment when in timestamp mode, to indicate that the subsequent decoding should be performed on an audio window aligned with that time, otherwise we truncate the audio to not include the segment. Lastly, we add a |endoftranscript|> token. We only mask out the training loss over the previous context text, and train the model to predict all other tokens. Please see Figure 1 for an overview of our format and training setup. Multitask training data (680k hours) Sequence",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\n-to-sequence learning English transcription “Ask not what your country can do for ” Ask not what your country can do for Any-to-English speech translation “El rápido zorro marrón salta sobre ” The quick brown fox jumps over Transformer Encoder Blocks Transformer Decoder Blocks Non-English transcription",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\n“       ”          Sinusoidal Positional Encoding No speech (background music playing)  Multitask training format X  X Transcription Language identification LANGUAGE TRANSCRIBE TAG previous text tokens START OF TRANSCRIPT PREV NO SPEECH TRANSLATE Custom vocabulary / prompting Voice activity X  English detection Translation special tokens text tokens timestamp (VAD) tokens Figure 1. Overview of our approach. A sequence-to-sequence Transformer model is trained on many different speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. All of these tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing for a single model to replace many different stages of a traditional speech processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets, as further explained in Section 2.3. 2.4. Training Details We train a suite of models of various sizes in order to study the scaling properties of Whisper. Please see Table 1 for an overview. We train with data parallelism across accelerators using FP16 with dynamic loss scaling and activation checkpointing (Griewank & Walther, 2000; Chen et al., 2016). Models were trained with AdamW (Loshchilov & Hutter, 2017) and gradient norm clipping (Pascanu et al., 2013) with a linear learning rate decay to zero after a warmup over the first 2048 updates. A batch size of 256 segments was used, and the models are trained for 220 updates which is between two and three passes over the dataset. Due to only training for a few epochs, over-fitting is not a large concern, and we do not use any data augmentation or regularization and instead rely on the diversity contained within such a TRANS-  EN 0.0 The quick brown CRIBE next-token prediction MLP MLP cross attention self attention self attention cross attention    MLP MLP self attention cross attention self attention MLP self attention MLP cross attention self attention Learned Positional Encoding 2  Conv1D + GELU TRANS-  SOT EN CRIBE 0.0 The quick Log-Mel Spectrogram Tokens in Multitask Training Format Time-aligned transcription begin  end time text tokens begin end time text tokens time time EOT NO TIMESTAMPS text tokens Text-only transcription (allows dataset-specific fine-tuning) large dataset to encourage generalization and robustness. Please see Appendix F for full training hyperparameters.3 During early development and evaluation we observed that Whisper models had a tendency to transcribe plausible but almost always incorrect guesses for the names of speakers. This happens because many transcripts in the pre-training dataset include the name of the person who is speaking, encouraging the model to try to predict them, but this information is only rarely inferable from only the most recent 30 Large model (denoted V2) for 2.5X more epochs while adding SpecAugment (Park et al., 2019), Stochastic Depth (Huang et al., 2016), and BPE Dropout (Provilkov et al., 2019) for regularization. Reported results have been updated to this improved model unless otherwise specified. Model Layers Width Heads Parameters Tiny 4 384 6 Base 6 512 8 Small 12 768 12 Medium 24 1024 16 Large 32 1280 20 Table 1. Architecture details of the Whisper model family. seconds of audio context. To avoid this, we fine-tune Whisper models briefly on the subset of transcripts that do not include speaker annotations which removes this behavior.",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nExperiments / Results: 3. Experiments 3.1. Zero-shot Evaluation The goal of Whisper is to develop a single robust speech processing system that works reliably without the need for dataset specific fine-tuning to achieve high-quality results on specific distributions. To study this capability, we reuse a wide set of existing speech processing datasets to check whether Whisper is able to generalize well across domains, tasks, and languages. Instead of using the standard evaluation protocol for these datasets, which include both a train and test split, we evaluate Whisper in a zero-shot setting without using any of the training data for each of these datasets so that we are measuring broad generalization. 3.2. Evaluation Metrics Speech recognition research typically evaluates and compares systems based on the word error rate (WER) metric. However, WER, which is based on string edit distance, penalizes all differences between the model’s output and the reference transcript including innocuous differences in transcript style. As a result, systems that output transcripts that would be judged as correct by humans can still have a large WER due to minor formatting differences. While this poses a problem for all transcribers, it is particularly acute for zero-shot models like Whisper, which do not observe any examples of specific datasets transcript formats. This is not a novel observation; the development of evaluation metrics that better correlate with human judgement is an active area of research, and while there are some promising methods, none have seen widespread adoption for speech recognition yet. We opt to address this problem with extensive standardization of text before the WER calculation to minimize penalization of non-semantic differences. Our text normalizer was developed through iterative manual inspection to identify common patterns where naive WER penalized Whisper models for an innocuous difference. Appendix C includes full details. For several datasets, we observe WER drops of up to 50 percent usually due to a quirk such as a dataset’s reference transcripts seperating contractions from words with whitespace. We caution this development procedure comes at a risk of overfitting to the transcription style of Whisper models which we investigate in Section 4.4. We are releasing the code for our text normalizer to allow for easy comparison and to help others study the performance of speech recognition systems in out-of-distribution settings. 3.3. English Speech Recognition In 2015, Deep Speech 2 (Amodei et al., 2015) reported a speech recognition system matched human-level performance when transcribing the LibriSpeech test-clean split. As part of their analysis they concluded: “Given this result, we suspect that there is little room for a generic speech system to further improve on clean read speech without further domain adaptation.” Yet seven years later the SOTA WER on LibriSpeech test-clean has dropped another 73% from their 5.3% to 1.4% (Zhang et al., 2021), far below their reported human-level error rate of 5.8%. Despite this massive and unanticipated further improvement in performance on held-out but in-distribution data, speech recognition models trained on LibriSpeech remain far above human error rates when used in other settings. What explains this gap between reportedly superhuman performance in-distribution and subhuman performance out-of-distribution? We suspect a large part of this gap between human and machine behavior is due to conflating different capabilities being measured by human and machine performance on a test set. This claim may seem confusing at first; if both humans and machines are taking the same test, how can it be that different skills are being tested? The difference arises not in the testing but in how they trained for it. Humans are often asked to perform a task given little to no supervision on the specific data distribution being studied. Thus human performance is a measure of out-of-distribution generalization. But machine learning models are usually evaluated after training on a large amount of supervision from the evaluation distribution, meaning that machine performance is instead a measure of in-distribution generalization. While both humans and machines are being evaluated on the same test data,",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\ntwo quite different abilities are being measured due to a difference in train data. Whisper models, which are trained on a broad and diverse distribution of audio and evaluated in a zero-shot setting, could potentially match human behavior much better than existing systems. To study whether this is the case (or whether the difference between machine and human performance is due to yet-to-be-understood factors) we can compare Whisper models with both human performance and standard fine-",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\ntuned machine learning models and check which they more closely match. 50 Average WER on [Common Voice, CHiME-6, TED-LIUM] (%) 40 30 20 10 0 0 1 2 3 4 5 6 7 8 WER on LibriSpeech dev-clean (%) Figure 2. Zero-shot Whisper models close the gap to human robustness. Despite matching or outperforming a human on LibriSpeech dev-clean, supervised LibriSpeech models make roughly twice as many errors as a human on other datasets demonstrating their brittleness and lack of robustness. The estimated robustness frontier of zero-shot Whisper models, however, includes the 95% confidence interval for this particular human. To quantify this difference, we examine both overall robustness, that is average performance across many distributions/datasets, and effective robustness, introduced by Taori et al. (2020), which measures the difference in expected performance between a reference dataset, which is usually in-distribution, and one or more out-of-distribution datasets. A model with high effective robustness does better than expected on out-of-distribution datasets as a function of its performance on the reference dataset and approaches the ideal of equal performance on all datasets. For our analysis, we use LibriSpeech as the reference dataset due to its central role in modern speech recognition research and the availability of many released models trained on it, which allows for characterizing robustness behaviors. We use a suite of 12 other academic speech recognition datasets to study out-of-distribution behaviors. Full details about these datasets can be found in Appendix A. Our main findings are summarized in Figure 2 and Table 2. Although the best zero-shot Whisper model has a relatively unremarkable LibriSpeech clean-test WER of 2.5, which is roughly the performance of modern supervised baseline or the mid-2019 state of the art, zero-shot Whisper models have very different robustness properties than supervised LibriSpeech models and out-perform all benchmarked LibriSpeech models by large amounts on other datasets. Even wav2vec 2.0 Whisper RER Dataset Large (no LM) Large V2 (%) LibriSpeech Clean 2.7 2.7 0.0 Supervised LibriSpeech models Zero-shot Whisper models Zero-shot Human (Alec) Ideal robustness (y = x) Artie 24.5 6.2 74.7 Common Voice 29.9 9.0 69.9 Fleurs En 14.6 4.4 69.9 Tedlium 10.5 4.0 61.9 CHiME6 65.8 25.5 61.2 VoxPopuli En 17.9 7.3 59.2 CORAAL 35.6 16.2 54.5 AMI IHM 37.0 16.9 54.3 Switchboard 28.3 13.8 51.2 CallHome 34.8 17.6 49.4 WSJ 7.7 3.9 49.4 AMI SDM1 67.6 36.4 46.2 LibriSpeech Other 6.2 5.2 16.1 Average 29.3 12.8 55.2 Table 2. Detailed comparison of effective robustness across various datasets. Although both models perform within 0.1% of each other on LibriSpeech, a zero-shot Whisper model performs much better on other datasets than expected for its LibriSpeech performance and makes 55.2% less errors on average. Results reported in word error rate (WER) for both models after applying our text normalizer. the smallest zero-shot Whisper model, which has only 39 million parameters and a 6.7 WER on LibriSpeech test-clean is roughly competitive with the best supervised LibriSpeech model when evaluated on other datasets. When compared to a human in Figure 2, the best zero-shot Whisper models roughly match their accuracy and robustness. For a detailed breakdown of this large improvement in robustness, Table 2 compares the performance of the best zero-shot Whisper model with a supervised LibriSpeech model that has the closest performance to it on LibriSpeech test-clean. Despite their very close performance on the reference distribution",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\n, the zero-shot Whisper model achieves an average relative error reduction of 55.2% when evaluated on other speech recognition datasets. This finding suggests emphasizing zero-shot and out-ofdistribution evaluations of models, particularly when attempting to compare to human performance, to avoid overstating the capabilities of machine learning systems due to misleading comparisons. 3.4. Multi-lingual Speech Recognition In order to compare to prior work on multilingual speech recognition, we",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nreport results on two low-data benchmarks: Multilingual LibriSpeech (MLS) (Pratap et al., 2020b) and VoxPopuli (Wang et al., 2021) in Table 3. 160 MY KA BN GU PA LO TE ML KM UZ 80 TG MT NE BE HY 40 SW MR IS KN Word Error Rate (WER) KK SR AF CY FA LT HE AZ LV UR SL HI ET 20 TA HU MK AR RO GL ZH BG FIL BS KO DA EL HR CS SK TH VI 10 FI NB MS UK SV TR FR CA ID NL RU PL JA 5 DE PT EN IT r2 = 0.83 ES 2.5 0.1 1 10 100 Hours of transcribed audio Figure 3. Correlation of pre-training supervision amount with downstream speech recognition performance. The amount of pre-training speech recognition data for a given language is very predictive of zero-shot performance on that language in Fleurs. Model MLS VoxPopuli VP-10K + FT - 15.3 XLS-R (1B) 10.9 10.6 mSLAM-CTC (2B) 9.7 9.1 Maestro - 8.1 Zero-Shot Whisper 7.3 13.6 Table 3. Multilingual speech recognition performance. Zeroshot Whisper improves performance on Multilingual LibriSpeech (MLS) but is still significantly behind both Maestro, XLS-R, and mSLAM on VoxPopuli. et al., 2022), and Maestro (Chen et al., 2022b) in a zero-shot setting. We caution that we do use a simple text standardizer for this result which prevents direct comparison or claims of SOTA performance. On VoxPopuli, however, Whisper significantly underperforms prior work and only beats the VP-10K+FT baseline from the original paper. We suspect the underperformance of Whisper models on VoxPopuli could be due to other models including this distribution as a major source for their unsupervised pre-training data and the dataset having significantly more supervised data, which benefits fine-tuning. While MLS has 10 hours of training data per language, the average amount of training data per language is roughly 10 higher for VoxPopuli. 40 PT CA DE SV 35 FR AF DA SR RO 30 NB BS GL RU HR SK UK CS ID BG MK MS IT 25 TR EL AR ES PL FIL FI OC NL HE KO HI HU BLEU VI 20 FA ET ZH SL ML HY UR TH GU JA LB LV 15 NE PA TE BN LT TG CY MR MT KN BE AZ 10 TA MI LO IS SW SD UZ KM AS 5 KK KA PS r2 = 0.24 MN SN LN AM YO SO MY HA 0 1 10 100 Hours of translated audio Figure 4. Correlation of pre-training supervision amount with downstream translation performance. The amount of pretraining translation data for a given language is only moderately predictive of Whisper’s zero-shot performance on that language in Fleurs. the Indo-European language family and many of which are high-resource languages. These benchmarks only provide limited coverage and room to study Whisper models multilingual capabilities which include training data for speech recognition in 75 languages. To study the performance of Whisper more broadly we also report performance on the Fleurs dataset (Conneau et al., 2022). In particular, we were interested in studying the relationship between the amount of training data we have for a given language and the resulting downstream zero-shot performance for that language. We visualize this relation in Figure 3. We find a strong squared correlation coefficient of 0.83 between the log of the word error rate and the log of the amount of training data per language. Checking the regression coefficient for a",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nlinear fit to these log-log values results in an estimate that WER halves for every 16 increase in training data. We also observed that many of the largest outliers in terms of worse than expected performance according to this trend are languages that have unique scripts and are more distantly related to the Indo-European languages making up the majority of the training dataset such as Hebrew (HE), Telugu (TE), Chinese (ZH), and Korean (KO). These",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk4_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\ndifferences could be due to a lack of transfer due to linguistic distance, our byte level BPE tokenizer being a poor match for these languages, or variations in data quality. X English High Mid Low All XMEF-X 34.2 20.2 5.9 14.7 XLS-R (2B) 36.1 27.7 15.1 22.1 mSLAM-CTC (2B) 37.8 29.6 18.5 24.8 Maestro 38.2 31.3 18.4 25.2 Zero-Shot Whisper 36.2 32.6 25.2 29.1 Table 4. Xen Speech translation performance. Zero-shot Whisper outperforms existing models on CoVoST2 in the overall, medium, and low resource settings but still moderately underperforms on high-resource languages compared to prior directly supervised work. Language ID Fleurs w2v-bert-51 (0.6B) 71.4 mSLAM-CTC (2B) 77.7 Zero-shot Whisper 64.5 Table 5. Language identification performance. Zero-shot Whisper’s accuracy at language identification is not competitive with prior supervised results on Fleurs. This is partially due to Whisper being heavily penalized for having no training data for 20 of Fleurs languages. 3.5. Translation We study the translation capabilities of Whisper models by measuring their performance on the Xen subset of CoVoST2 (Wang et al., 2020b). We compare with Maestro, mSLAM, and XLS-R, the highest-performing prior work. We achieve a new state of the art of 29.1 BLEU zero-shot without using any of the CoVoST2 training data. We attribute this to the 68,000 hours of Xen translation data for these languages in our pre-training dataset which, although noisy, is vastly larger than the 861 hours of training data for Xen translation in CoVoST2. Since Whisper evaluation is zero-shot, it does particularly well on the lowest resource grouping of CoVoST2, improving over mSLAM by 6.7 BLEU. Conversely, the best Whisper model does not actually improve over Maestro and mSLAM on average for the highest resource languages. For an additional analysis on an even wider set of languages, we also re-purpose Fleurs, which is a speech recognition dataset, as a translation dataset. Since the same sentences are transcribed for every language we use the English transcripts as reference translations. In Figure 4 we visualize the correlation between the amount of translation training data per language and the resulting zero-shot BLEU score on Fleurs. While there is a clear trend of improvement with increasing training data, the squared correlation coefficient is much lower than the 0.83 observed for speech recognition white noise pub noise 100 WER on LibriSpeech test-clean (%) 50 20 10 5 2 1 40 30 20 10 0 -10 signal-to-noise ratio (dB) 40 30 20 10 0 -10 signal-to-noise ratio (dB) unispeech-sat-base-100h-libri-ft wav2vec2-base-100h wav2vec2-base-960h wav2vec2-large-960h wav2vec2-large-robust-ft-libri-960h wav2vec2-large-960h-lv60-self asr-crdnn-rnnlm-librispeech asr-transformer-transformerlm-librispeech hubert-large-ls960-ft hubert-xlarge-ls960-ft s2t-medium-librispeech-asr s2t-large-librispeech-asr stt_en_conformer_ctc_large stt_en_conformer_transducer_xlarge Whisper Figure 5. WER on LibriSpeech test-clean as a function of SNR under additive white noise (left) and pub noise (right).",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk4_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nThe accuracy of LibriSpeech-trained models degrade faster than the best Whisper model (). NVIDIA STT models (•) perform best under low noise but are outperformed by Whisper under high noise (SNR 10 dB). The second-best model under low noise () is fine-tuned on LibriSpeech only and degrades even more quickly. and only 0.",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk5_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\n24. We suspect this is partly caused by the noisier training data due to errors in audio language identification. As an example, Welsh (CY) is an outlier with much worse than expected performance at only 13 BLEU despite supposedly having 9,000 hours of translation data. This large amount of Welsh translation data is surprising, ranking 4th overall for translation data and ahead of some of the most spoken languages in the world like French, Spanish, and Russian. Inspection shows the majority of supposedly Welsh translation data is actually English audio with English captions where the English audio was mis-classified as Welsh by the language identification system, resulting in it being included as translation training data rather transcription data according to our dataset creation rules. 3.6. Language Identification To evaluate language identification, we use the Fleurs dataset (Conneau et al., 2022). The zero-shot performance of Whisper is not competitive with prior supervised work here and underperforms the supervised SOTA by 13.6%. However, Whisper is heavily disadvantaged for language identification on Fleurs, since the Whisper dataset contains no training data for 20 of the 102 languages in Fleurs, upperbounding accuracy at 80.4%. On the 82 overlapping languages the best Whisper model achieves 80.3% accuracy. 3.7. Robustness to Additive Noise We tested the noise robustness of Whisper models and 14 LibriSpeech-trained models by measuring the WER when either white noise or pub noise from the Audio Degradation Toolbox (Mauch & Ewert, 2013) was added to the audio. The pub noise represents a more natural noisy environment with ambient noise and indistinct chatter typical in a crowded restaurant or a pub. Among the 14 models, twelve are pre-trained and/or fine-tuned on LibriSpeech, and the other two are NVIDIA STT models trained on a mixture dataset similar to prior work like SpeechStew that includes LibriSpeech. The level of additive noise corresponding to a given signal-to-noise ratio (SNR) is calculated based on the signal power of individual examples. Figure 5 shows how the ASR performance degrades as the additive noise becomes more intensive. There are many models that outperform our zero-shot performance under low noise (40 dB SNR), which is unsurprising given those models are trained primarily on LibriSpeech, but all models quickly degrade as the noise becomes more intensive, performing worse than the Whisper model under additive pub noise of SNR below 10 dB. This showcases Whisper’s robustness to noise, especially under more natural distribution shifts like the pub noise. 3.8. Long-form Transcription Whisper models are trained on 30-second audio chunks and cannot consume longer audio inputs at once. This is not a 40 35 30 Word Error Rate (%) 25 20 15 10 5 TED-LIUM3 Meanwhile Kincaid46 Rev16 Earnings-21 Earnings-22 CORAAL 0 Whisper Company A Company B Company C Company D NVIDIA STT (CTC large) Figure 6. Whisper is competitive with state-of-the-art commercial and open-source ASR systems in long-form transcription. The distribution of word error rates from six ASR systems on seven long-form datasets are compared, where the input lengths range from a few minutes to a few hours. The boxes show the quartiles of per-example WERs, and the per-dataset aggregate WERs are annotated on each box. Our model outperforms the best open source model (NVIDIA STT) on all datasets, and in most cases, commercial ASR systems as well. problem with most academic datasets comprised of short utterances but presents challenges in real-world applications which often require transcribing minutes- or hours-long audio. We developed a strategy to perform buffered transcription of long audio by consecutively transcribing 30-second segments of audio and shifting the window according to the timestamps predicted by the model. We observed that it is crucial to have beam search and temperature scheduling based on the repetitiveness and",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk5_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nthe log probability of the model predictions in order to reliably transcribe long audio. The full procedure is described in Section 4.5. We evaluate the long-form transcription performance on seven datasets consisting of speech recordings of various lengths and recording conditions, to cover as diverse a data distribution as possible. These include a long-form adaptation of TED-LIUM3 (Hernandez et al., 2018) concatenated so that each example",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Experiments_/_Results_chunk6",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nis a full-length TED talk, a collection of jargon-laden segments taken from The Late Show with Stephen Colbert (Meanwhile), sets of videos/podcasts that has been used as ASR benchmarks in online blogs (Rev16 and Kincaid46), recordings of earnings calls (Del Rio et al., 2021), and the full-length interviews from the Corpus of Regional African American Language (CORAAL) (Gunter et al., 2021). Full details about the long-form datasets can be found in Appendix A. We compare the performance with open-source models as well as 4 commercial ASR services. The results are summarized in Figure 6, showing the distribution of word error rates from Whisper and the 4 commercial ASR services, as well as the NVIDIA STT Conformer-CTC Large model from the NeMo toolkit (Kuchaiev et al., 2019) which performed the best among the open-source models. All commercial ASR services are queried using their default English transcription settings as of September 1st, 2022, and for the NVIDIA STT model we used their buffered inference implementation in the FrameBatchASR class to enable long-form transcription. The results show that Whisper performs better than the compared models on most datasets, especially on the Meanwhile dataset which is heavy with uncommon words. Additionally, we note the possibility that some of the commercial ASR systems have been trained on some of these publicly available datasets, and therefore these results may not be accurately reflecting the relative robustness of the systems. 3.9. Comparison with Human Performance Because of ambiguous or indistinct speech as well as labeling errors, there are different levels of irreducible error in each dataset, and with WER metrics from ASR systems alone it is difficult to make sense of how much room for improvement exists in each dataset. To quantify how close Whisper’s performance is to the human performance, we selected 25 recordings from the Kincaid46 dataset and used 5 services to obtain transcripts produced by professional transcribers, among which one provides computer-assisted transcription and the other four are entirely human-transcribed. The audio selection covers various recording conditions such as scripted and unscripted broadcast, telephone and VoIP calls, and meetings. Figure 7 shows the distribution of per-example WERs and aggregate WER across the 25 recordings, where the computer-assisted service has the lowest aggregate WER that is 1.15% point better than Whisper’s, and the pure-human performance is only a fraction of a percentage point better than Whisper’s. These results indicate that Whisper’s English ASR performance is not perfect but very close to human-level accuracy.",
    "output": "Countless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools."
  },
  {
    "id_paper": "25",
    "id_story": "25-02",
    "id_chunk": "25-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 7. Conclusion Whisper suggests that scaling weakly supervised pretraining has been underappreciated so far in speech recognition research. We achieve our results without the need for the self-supervision and self-training techniques that have been a mainstay of recent large-scale speech recognition work and demonstrate how simply training on a large and diverse supervised dataset and focusing on zero-shot transfer can significantly improve the robustness of a speech recognition system.",
    "output": "Speech recognition remains a challenging problem in AI and machine learning. In a step toward solving it, OpenAI today open-sourced Whisper, an automatic speech recognition system that the company claims enables “robust” transcription in multiple languages as well as translation from those languages into English.\n\nCountless organizations have developed highly capable speech recognition systems, which sit at the core of software and services from tech giants like Google, Amazon and Meta. But what makes Whisper different, according to OpenAI, is that it was trained on 680,000 hours of multilingual and “multitask” data collected from the web, which lead to improved recognition of unique accents, background noise and technical jargon.\n\n“The primary intended users of [the Whisper] models are AI researchers studying robustness, generalization, capabilities, biases and constraints of the current model. However, Whisper is also potentially quite useful as an automatic speech recognition solution for developers, especially for English speech recognition,” OpenAI wrote in the GitHub repo for Whisper, from where several versions of the system can be downloaded. “[The models] show strong ASR results in ~10 languages. They may exhibit additional capabilities … if fine-tuned on certain tasks like voice activity detection, speaker classification or speaker diarization but have not been robustly evaluated in these area.”\n\nWhisper has its limitations, particularly in the area of text prediction. Because the system was trained on a large amount of “noisy” data, OpenAI cautions Whisper might include words in its transcriptions that weren’t actually spoken — possibly because it’s both trying to predict the next word in audio and trying to transcribe the audio itself. Moreover, Whisper doesn’t perform equally well across languages, suffering from a higher error rate when it comes to speakers of languages that aren’t well-represented in the training data.\n\nThat last bit is nothing new to the world of speech recognition, unfortunately. Biases have long plagued even the best systems, with a 2020 Stanford study finding systems from Amazon, Apple, Google, IBM and Microsoft made far fewer errors — about 19% — with users who are white than with users who are Black.\n\nDespite this, OpenAI sees Whisper’s transcription capabilities being used to improve existing accessibility tools.\n\n“While Whisper models cannot be used for real-time transcription out of the box, their speed and size suggest that others may be able to build applications on top of them that allow for near-real-time speech recognition and translation,” the company continues on GitHub. “The real value of beneficial applications built on top of Whisper models suggests that the disparate performance of these models may have real economic implications … [W]e hope the technology will be used primarily for beneficial purposes, making automatic speech recognition technology more accessible could enable more actors to build capable surveillance technologies or scale up existing surveillance efforts, as the speed and accuracy allow for affordable automatic transcription and translation of large volumes of audio communication.”\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe release of Whisper isn’t necessarily indicative of OpenAI’s future plans. While increasingly focused on commercial efforts like DALL-E 2 and GPT-3, the company is pursuing several purely theoretical research threads, including AI systems that learn by observing videos."
  },
  {
    "id_paper": "03",
    "id_story": "03-02",
    "id_chunk": "03-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: Learning to Simulate Dynamic Environments with GameGAN Seung Wook Kim1,2,3 Yuhao Zhou2 Jonah Philion1,2,3 Antonio Torralba4 Sanja Fidler1,2,3 seungwookk,jphilion,sfidler@nvidia.com henryzhou@cs.toronto.edu torralba@mit.edu Abstract Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this paper, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN “renders” the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist. We will release the code, enabling human players to play generated games with our GameGAN.",
    "output": "Simulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this url, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN \"renders\" the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist."
  },
  {
    "id_paper": "03",
    "id_story": "03-02",
    "id_chunk": "03-02_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: 1. Introduction Before deployment to the real world, an artificial agent needs to undergo extensive testing in challenging simulated environments. Designing good simulators is thus extremely important. This is traditionally done by writing procedural models to generate valid and diverse scenes, and complex behavior trees that specify how each actor in the scene behaves and reacts to actions made by other actors, including the ego agent. However, writing simulators that encompass a large number of diverse scenarios is extremely time consuming and requires highly skilled graphics experts. Learn- Figure 1. If you look at the person on the left picture, you might think she is playing Pacman of Toru Iwatani, but she is not! She is actually playing with a GAN generated version of Pacman. In this paper, we introduce GameGAN that learns to reproduce games by just observing lots of playing rounds. Moreover, our model can disentangle background from dynamic objects, allowing us to create new games by swapping components as shown in the center and right images. ing to simulate by simply observing the dynamics of the real world is the most scaleable way going forward. A plethora of existing work aims at learning behavior models [2, 28, 16, 3]. However, these typically assume a significant amount of supervision such as access to agents’ ground-truth trajectories. We aim to learn a simulator by simply watching an agent interact with an environment. To simplify the problem, we frame this as a 2D image generation problem. Given sequences of observed image frames and the corresponding actions the agent took, we wish to emulate image creation as if “rendered” from a real dynamic environment that is reacting to the agent’s actions. We focus on graphics games as a proxy of the real environment. Our goal is to replace the graphics engine at test time, by visually imitating the game using a learned model. This is a challenging problem: different games have different number of components as well as different physical dynamics. Furthermore, many games require long-term consistency in the environment. For example, imagine a game where an agent navigates through a maze. When the agent moves away and later returns to a location, it expects the scene to look consistent with what it has encountered before. In visual SLAM, detecting loop closure (returning to a previous location) is already known to be challenging, let alone generating one. Last but not least, both deterministic and stochastic behaviors typically exist in a game, and modeling the latter is known to be particularly hard. In this paper, we introduce GameGAN, a generative model that learns to imitate a desired game. GameGAN ingests screenplay and keyboard actions during training and aims to predict the next frame by conditioning on the action, i.e. a key pressed by the agent. It learns from rollouts of image and action pairs directly without having access to the underlying game logic or engine. We make several advancements over the recently introduced World Model [11] that aims to solve a similar problem. While [11] employs a straightforward conditional decoder, GameGAN features a carefully designed architecture. In particular, we propose a new memory module that encourages the model to build an internal map of the environment, allowing the agent to return to previously visited locations with high visual consistency. Furthermore, we introduce a purposely designed decoder that learns to disentangle static and dynamic components within the image. This makes the behavior of the model more interpretable, and it further allows us to modify existing games by swapping out different components. We test GameGAN on a modified version of Pacman and the VizDoom environment [17], and propose several synthetic tasks for both quantitative and qualitative evaluation. We further introduce a come-back-home task to test the long-term consistency of learned simulators. Note that GameGAN supports several applications such as transferring a given game from one operating system to the other, without requiring to re-write code. Our GameGAN will be made available to human players, enabling them to play games with a GAN and modify them in creative ways.",
    "output": "Overview\nGameGAN trained on the official version of Pac-Man from Bandai-Namco entertainment GameGAN learns to follow the user command - how Pacman moves - as well as other rules of the game. It turns ghosts to purple when a capsule is consumed, and also learns their policy. You can see that ghosts chase Pacman in the normal state, but they start to run away when a capsule is consumed.\nGameGAN trained on a custom version of Pac-Man and VizDoom\nWith disentangled components, we can swap out the static/dynamic components with anything we like! We can play Pac-man with mario or play VizDoom in your favorite scenes rather than staying in the depressing room from VizDoom."
  },
  {
    "id_paper": "03",
    "id_story": "03-02",
    "id_chunk": "03-02_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: 4. Experiments t=0 t=11 Action-LSTM World Model GameGAN-M GameGAN Figure 8. Rollout of models from the same initial screen. Action-LSTM trained with reconstruction loss produces frames without refined details (e.g. foods). World Model has difficulty keeping temporal consistency, resulting in occasional significant discontinuities. GameGAN can produce consistent simulation. model trained only with reconstruction loss which is in essence similar to [5], 2) World Model [11], 3) GameGANM: our model without the memory module and with the simple rendering engine, and 4) GameGAN: the full model with the memory module and the rendering engine for disentanglement. Experiments are conducted on the following three datasets (Figure 7): Pacman: We use a modified version of the Pacman game3 in which the Pacman agent observes an egocentric 7x7 grid from the full 14x14 environment. The environment is randomly generated for each episode. This is an ideal environment to test the quality of a simulator since it has both deterministic (e.g., game rules & viewpoint shift) and highly stochastic components (e.g., game layout of foods and walls; game dynamics with moving ghosts). Images in the episodes are 84x84 and the action space is A = left, right, up, down, stay. 45K episodes of length greater than or equal to 18 are extracted and 40K are used for training. Training data is generated by using a trained DQN [25] agent that observes the full environment with high entropy to allow exploring diverse action sequences. Each episode consists of a sequence of 7x7 Pacman-centered grids along with actions. Pacman-Maze: This game is similar to Pacman except that it does not have ghosts, and its walls are randomly generated from a maze-generation algorithm, thus are structured better. The same number of data is used as Pacman. Vizdoom: We follow the experiment set-up of Ha and Schmidhuber [11] that uses takecover mode of the VizDoom platform [17]. Training data consists of 10k episodes extracted with random policy. Images in the episodes are 64x64 and the action space is A = left, right, stay 4.1. Qualitative Evaluation Time construction loss, produces blurry images as it fails to capture the multi-modal future distribution, and the errors accumulate quickly. World model [11] generates realistic images for VizDoom, but it has trouble simulating the highly stochastic Pacman environment. In particular, it sometimes suffers from large unexpected discontinuities (e.g. t = 0 to t = 1). On the other hand, GameGAN produces temporally consistent and realistic sharp images. GameGAN consists of only a few convolution layers to roughly match the number of parameters of World Model. We also provide a version of GameGAN that can produce higher quality images in the supplementary materials Section A.5. Disentangling static & dynamic elements: Our GameGAN with the memory module is trained to disentangle static elements from dynamic elements. Figure 5 shows how walls from the Pacman environment and the room from the VizDoom environment are separated from dynamic objects such as ghosts and fireballs. With this, we can make interesting environments in which each element is swapped with other objects. Instead of the depressing room of VizDoom, enemies can be placed in the user’s favorite place, or alternatively have Mario run around the room (Figure 9). We can swap the background without having to modify the code of the original games. Our approach treats games as a black box and learns to reproduce the game, allowing us to easily modify it. Disentangled models also open up many promising future directions that are not possible with existing models. One interesting direction would be learning multiple disentangled models and swapping certain components. As the dynamics engine learns the rules of an environment and the rendering engine learns to render images, simply learning a linear transformation from the hidden state of one model to make use of the rendering engine of the other could work. Pacman-Maze generation: GameGAN on the Pacman-Maze produces a partial grid at each time step which can be connected to generate the full maze. It can",
    "output": "Abstract\n\nSimulation is a crucial component of any robotic system. In order to simulate correctly, we need to write complex rules of the environment: how dynamic agents behave, and how the actions of each of the agents affect the behavior of others. In this url, we aim to learn a simulator by simply watching an agent interact with an environment. We focus on graphics games as a proxy of the real environment. We introduce GameGAN, a generative model that learns to visually imitate a desired game by ingesting screenplay and keyboard actions during training. Given a key pressed by the agent, GameGAN \"renders\" the next screen using a carefully designed generative adversarial network. Our approach offers key advantages over existing work: we design a memory module that builds an internal map of the environment, allowing for the agent to return to previously visited locations with high visual consistency. In addition, GameGAN is able to disentangle static and dynamic components within an image making the behavior of the model more interpretable, and relevant for downstream tasks that require explicit reasoning over dynamic elements. This enables many interesting applications such as swapping different components of the game to build new games that do not exist.\nOverview \n We are interested in training a game simulator that can model both deterministic and stochastic nature of the environment. GameGAN is composed of three modules. 1) The dynamics engine maintains an internal state variable which is recurrently updated. 2) For environments that require long-term consistency, an external memory module is used to remember what the model has generated so far 3) Finally, the rendering engine is used to decode the output image at each time instance. All modules are neural networks and trained end-to-end. \n GameGAN trained on the official version of Pac-Man from Bandai-Namco entertainment GameGAN learns to follow the user command - how Pacman moves - as well as other rules of the game. It turns ghosts to purple when a capsule is consumed, and also learns their policy. You can see that ghosts chase Pacman in the normal state, but they start to run away when a capsule is consumed. \n GameGAN trained on a custom version of Pac-Man and VizDoom \n GameGAN trained with memory module & disentangling rendering engine\n GameGAN learns to disentangle static components such as background and blue wall from dynamic components such as the firball-shooting enemies. In the Pac-man video, observe how it remembers the layouts (blue walls) that it has generated. It learns to recover the generated layout when Pac-man comes back to the same location.\n Swapping foreground / background \n With disentangled components, we can swap out the static/dynamic components with anything we like! We can play Pac-man with mario or play VizDoom in your favorite scenes rather than staying in the depressing room from VizDoom."
  },
  {
    "id_paper": "03",
    "id_story": "03-02",
    "id_chunk": "03-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 5. Conclusion We propose GameGAN which leverages adversarial training to learn to simulate games. GameGAN is trained by observing screenplay along with user’s actions and does not require access to the game’s logic or engine. GameGAN features a new memory module to ensure long-term consistency and is trained to separate static and dynamic elements. Thorough ablation studies showcase the modeling power of GameGAN . In future works, we aim to extend our model to capture more complex real-world environments.",
    "output": "We are interested in training a game simulator that can model both deterministic and stochastic nature of the environment. GameGAN is composed of three modules. 1) The dynamics engine maintains an internal state variable which is recurrently updated. 2) For environments that require long-term consistency, an external memory module is used to remember what the model has generated so far 3) Finally, the rendering engine is used to decode the output image at each time instance. All modules are neural networks and trained end-to-end.\nGameGAN trained with memory module & disentangling rendering engine"
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Abstract\nAbstract: Abstract. Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 100 pixels in gigapixel microscopy images sized 100,000 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92 .4% of the tumors, relative to 82 .7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection.",
    "output": "Each year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Introduction\nIntroduction: Introduction The treatment and management of breast cancer is determined by the disease stage. A central component of breast cancer staging involves the microscopic examination of lymph nodes adjacent to the breast for evidence that the cancer has spread, or metastasized [3]. This process requires highly skilled pathologists and is fairly time-consuming and error-prone, particularly for lymph nodes with either no or small tumors. Computer assisted detection of lymph node metastasis could increase the sensitivity, speed, and consistency of metastasis detection [16]. In recent years, deep CNNs have significantly improved accuracy on a wide range of computer vision tasks such as image recognition [14,11,19], object detection [8], and semantic segmentation [17]. Similarly, deep CNNs have been applied productively to improve healthcare (e.g.,[9]). This paper presents a CNN framework to aid breast cancer metastasis detection in lymph nodes. We build on [23] by leveraging a more recent Inception architecture [20], careful image patch sampling and data augmentations. Despite performing inference with stride 128 (instead of 4), we halve the error rate at 8 false positives (FPs) per slide, setting a new state-of-the-art. We also found that several approaches yielded no benefits: (1) a multi-scale approach that mimics the human cognition of a pathologist’s examination of biological tissue, (2) pretraining the model on ImageNet image recognition, and (3) color normalization. Finally, we dispense with the random forest classifier and feature engineering used in [23] and find that the maximum function is an effective whole-slide classification procedure. Work done as a Google Brain Resident (g.co/brainresidency). Work done as a Google intern.",
    "output": "Abstract\n\nEach year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Method\nMethod: Methods Given a gigapixel pathology image (slide1), the goal is to classify if the image contains tumor and localize the tumors for a pathologist’s review. This use case and the difficulty of pixel-accurate annotation (Fig. 2) renders detection and localization more important than pixel-level segmentation. Because of the large size of the slide and the limited number of slides (270), we train models using (H&E), and is scanned at the most common high magnification in a microscope, “40X”. We also experimented with 2- and 4-times down-sampled patches (“20X” and “10X”). Fig. 1. Left: three tumor patches and right: three challenging normal patches. Fig. 2. Difficulty of pixel-accurate annotations for scattered tumor cells. Ground truth annotation is overlaid with a lighter shade. Note that the tumor annotations include both tumor cells and normal cells e.g.,white space representing adipose tissue (fat). Fig. 3. The three colorful blocks represent Inception (V3) towers up to the second-last layer (PreLogit). Single scale utilizes one tower with input images at 40X magnification; multi-scale utilizes multiple (e.g.,2) input magnifications that are input to separate towers and merged. smaller image patches extracted from the slide (Fig. 1). Similarly, we perform inference over patches in a sliding window across the slide, generating a tumor probability heatmap. For each slide, we report the maximum value in the heatmap as the slide-level tumor prediction. We utilize the Inception (V3) architecture [20] with inputs sized 299299 (the default) to assess the value of initializing from existing models pre-trained on another domain. For each input patch, we predict the label of the center 128128 region. A 128 pixel region can span several tumor cells and was also used in [16]. We label a patch as tumor if at least one pixel in the center region is annotated as tumor. We explored the influence of the number of parameters by reducing the number of filters per layer while keeping the number of layers constant (e.g., depth multiplier = 0.1 in TensorFlow). We denote these models “small”. We also experimented with multi-scale approaches that utilize patches at multiple magnifications centered on the same region (Fig. 3). Because preliminary experiments did not show a benefit from using up to four magnifications, we present results only for up to two magnifications. Training and evaluating our models was challenging because of the large number of patches and the tumor class imbalance. Each slide contains 10, 000 3 4 to 400, 000 patches (median 90, 000). However, each tumor slide contains 20 to 150, 000 tumors patches (median 2, 000), corresponding to tumor patch percentages ranging from 0.01% to 70% (median 2%). Avoiding biases towards slides containing more patches (both normal and tumor) required careful sampling. First, we select “normal” or “tumor” with equal probability. Next, we select a slide that contains that class of patches uniformly at random, and sample patches from that slide. By contrast, some existing methods pre-sample a set of patches from each slide [23], which limits the breadth of patches seen during training. To combat the rarity of tumor patches, we apply several data augmentations. First, we rotate the input patch by 4 multiples of 90, apply a left-right flip and repeat the rotations. All 8 orientations are valid because pathology slides do not have canonical orientations. Next, we use TensorFlow’s image library (tensorflow.image.random X ) to perturb color: brightness with a maximum delta of 64/255, saturation with a maximum delta of 0.25, hue with a maximum delta of 0.04, and contrast with a maximum delta of 0.75. Lastly, we add jitter to the patch extraction process such that each patch has a small x,y",
    "output": "Abstract\n\nEach year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Method\noffset of up to 8 pixels. The magnitudes of the color perturbations and jitter were lightly tuned using our validation set. Pixel values are clipped to [0, 1] and scaled to [1, 1]. We run inference across the slide in a sliding window with a stride of 128 to match the center region’s size. For each patch, we apply the rotations and left-right flip to obtain predictions for each",
    "output": "Abstract\n\nEach year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Method\nof the 8 orientations, and average the 8 predictions. Implementation Details We trained our networks with stochastic gradient descent in TensorFlow [2], with 8 replicas each running on a NVIDIA Pascal GPU with asynchronous gradient updates and batch size of 32 per replica. We used RMSProp [21] with momentum of 0.9, decay of 0.9 and  = 1.0. The initial learning rate was 0.05, with a decay of 0.5 every 2 million examples. For refining a model pretrained on ImageNet, we used an initial learning rate of 0.002. 3 Evaluation and Datasets We use the two Camelyon16 evaluation metrics [1]. The first metric, the area under receiver operating characteristic, (Area Under ROC, AUC) [10] evaluates slide-level classification. This metric is challenging because of the potential for FPs when 105 patch-level predictions are obtained per slide. We obtained 95% confidence intervals using a bootstrap approach2. The second metric, FROC [5], evaluates tumor detection and localization. We first generate a list of coordinates and corresponding predictions from each heatmap. Among all coordinates that fall within each annotated tumor region, the highest prediction is retained. Coordinates falling outside tumor regions are FPs. We use these values to compute the ROC. The FROC is defined as the sensitivity at 0.25, 0.5, 1, 2, 4, 8 average FPs per tumor-negative slide [16]. This of slides in the dataset/split, and compute the AUC. Repeat for a total of 2000 bootstrap samples, and report the 2.5 and 97.5 percentile values. metric is challenging because reporting multiple points per FP region can quickly erode the score. We focused on the FROC as opposed to the AUC because there are approximately twice as many tumors as slides, which improves the reliability of the evaluation metric. Similar to the AUC, we report 95% confidence intervals by computing the FROC over 2000 bootstrap samples of the predicted points. In addition, we report the sensitivity at 8 FP per slide (“@8FP”) to assess the false negative rate. To generate points for FROC computation, the Camelyon winners [23,1] thresholded the heatmap to produce a bit-mask, and reported a single prediction for each connected component in the bit-mask. By contrast, we use a non-maxima suppression method similar to [6] that repeats two steps until no values in the heatmap remain above a threshold t: (1) report the maximum and corresponding coordinate, and (2) set all values within a radius r of the maximum to 0. Because we apply this procedure to the heatmap, r has units of 128 pixels. t controls the number of points reported and has no effect on the FROC unless the curve plateaus before 8 FP. To avoid erroneously dropping tumor predictions, we used a conservative threshold of t = 0.5. Datasets Our work utilizes the Camelyon16 dataset [1], which contains 400 slides: 270 slides with pixel-level annotations, and 130 unlabeled slides as a test set.3 We split the 270 slides into train and validation sets (Appendix) for hyperparameter tuning. Typically only a small portion of a slide contains biological tissue of interest, with background and fat comprising the remainder (e.g., Fig. 2). To reduce computation, we removed background patches (gray value > 0.8 [12]), and verified visually that lymph node tissue was not discarded. Additional Evaluation: NHO-1 We digitized another set of 110 slides (57 containing tumor) from H&E-stained lymph nodes extracted from 20 patients (86 biological tissue blocks4) as an additional evaluation set. These slides came with patient- or block-level labels. To determine the slide labels, a board-certified pathologist blinded to the predictions adjudicated any differences, and briefly reviewed all 110 slides.",
    "output": "Abstract\n\nEach year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Experiments / Results\nExperiments / Results: 4 Experiments & Results To perform slide-level classification, the current state-of-the-art methods apply a random forest to features extracted from a heatmap prediction [1]. Unfortunately, we were unable to train slide-level classifiers because the 100% validationset AUC (Table 1) rendered internal evaluation of improvements impossible. Nonetheless, using the maximum value of each slide’s heatmap achieved AUCs > 97%, statistically indistinguishable from the current best results. For tumor-level classification, we find that the connected component approach [23] provides a 15% gain in FROC when the FROC is modest ( 80%), by masking FP regions. However, this approach is sensitive to the threshold (up 5 6 Input & Validation Test model size FROC @8FP AUC FROC @8FP AUC 98.1 100 99.0 87.3 (83.2, 91.1) 91.1 (87.2, 94.5) 96.7 (92.6, 99.6) 99.3 100 100 85.5 (81.0, 89.5) 91.1 (86.8, 94.6) 97.5 (93.8, 99.8) 99.3 100 100 86.4 (82.2, 90.4) 92.4 (88.8, 95.7) 97.1 (93.2, 99.8) ensemble-of-3 - - - 88.5 (84.3, 92.2) 92.4 (88.7, 95.6) 97.7 (93.0, 100) 94.7 100 99.6 85.5 (81.0, 89.7) 91.1 (86.9, 94.8) 98.6 (96.7, 100) 88.7 97.2 97.7 79.3 (74.2, 84.1) 84.9 (80.0, 89.4) 96.5 (91.9, 99.7) 94.9 98.6 99.0 85.9 (81.6, 89.9) 92.9 (89.3, 96.1) 97.0 (93.1, 99.9) 93.8 98.6 100 82.2 (77.0, 86.7) 87.6 (83.2, 91.7) 98.6 (96.2, 99.9) Pathologist [1] - - - 73.3* 73.3* 96.6 Camelyon16 winner [1,23] - - - 80.7 82.7 99.4 Table 1. Results on Camelyon16 dataset (95% confidence intervals, CI). Bold indicates results within the CI of the best model. “Small” models contain 300K parameters per Inception tower instead of 20M. -: not reported. *A pathologist achieved this sensitivity (with no FP) using 30 hours. to 10 20% variance), and can confound evaluation of model improvements by grouping multiple nearby tumors as one. By contrast, our non-maxima suppression approach is relatively insensitive to r between 4 and 6, although less accurate models benefited from tuning r using the validation set (e.g., 8). Finally, we achieve 100% FROC on larger tumors (macrometastasis), indicating that most false negatives are comprised of smaller tumors. Previous work (e.g., [24,9]) has shown that pre-training on a different domain improves performance. However, we find that although pre-training significantly improved convergence speed, it did not improve the FROC (see Table 1: 40X vs. addition, our large dataset size (107 patches) and data augmentation may have enabled the training of accurate models without pre-training. Next, we studied the effect of model size. Although we were originally motivated by improved experiment turn-around time, we surprisingly found that slimmed-down Inception architectures with only 3% of the parameters achieved similar performance to the full version (Table 1: 40X vs. 40X-small). Thus, we performed the remaining experiments using this smaller model. We also experimented with a multi-scale approach inspired by pathologists’ workflow of examining a slide at multiple magnifications to get context. However, we find no performance benefit in combining 40X with an additional input at lower magnification (Fig. 3). However, these combinations output smoother heatmaps (Fig",
    "output": "Abstract\n\nEach year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Experiments / Results\n. 4), likely because of translational invariance of the CNN and overlap in adjacent patches. These visual improvements can be deceptive: some of the speckles in the 40X models reveal small non-tumor regions surrounded by tumor. Figures 1 and 3 highlight the variability in the images. Although the current leading approaches report improvements from color normalization, our experi- Fig. 4. Left to right: sample image, ground truth",
    "output": "Abstract\n\nEach year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Experiments / Results\n(tumor in white), and heatmap outputs (40X-ensemble-of-3, 40X+20X, and 40X+10X). Heatmaps of 40X and 40Xensemble-of-3 look identical. The red circular regions at the bottom left quadrant of the heatmaps are unannotated tumor. Some of the speckles are either out of focus patches on the image or non-tumor patches within a large tumor. ments revealed no benefit (Appendix). This could be explained by our extensive data augmentations causing our models to learn color-invariant features. Finally, we experimented with ensembling models in two ways. First, averaging predictions across the 8 rotations/flips yielded a few percent improvement in the metrics. Second, ensembling across independently trained models yield additional but smaller improvements, and gave diminishing returns after 3 models. Additional Validation We also tested our models on another 110 slides that were digitized on different scanners, from different patients, and treated with different tissue preparation protocols. Encouragingly, we obtained an AUC of 97.6 (93.6, 100), on-par with our Camelyon16 test set performance. Qualitative Evaluation We discovered tumors in two “normal” slides: 086 and 144. Fortunately, the challenge organizers confirmed that both were data processing errors, and the patients were unaffected. Remarkably, both slides were in our training set, suggesting that our model was relatively resilient to label noise. In addition, we discovered an additional 7 tumor slides with incomplete annotations: 5 in train, 2 in validation (Appendix). Samples of our predictions and corresponding patches are shown in the Appendix. Limitations Our errors were related to out-of-focus tissues (macrophages, germinal centers, stroma), and tissue preparation artifacts. These errors could be reduced by better scanning quality, tissue preparation, and more comprehensive labels for different tissue types. In addition, we were unable to exhaustively tune our hyperparameters owing to the near-perfect FROC and AUC on our validation set. We plan to further develop our work on larger datasets. 5",
    "output": "Abstract\n\nEach year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "47",
    "id_story": "47-01",
    "id_chunk": "47-01_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Researcher, Developer, Student, Teacher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion Our method yields state-of-the-art sensitivity on the challenging task of detecting small tumors in gigapixel pathology slides, reducing the false negative rate to a quarter of a pathologist and less than half of the previous best result. We further achieve pathologist-level slide-level AUCs in two independent test sets. Our method could improve accuracy and consistency of evaluating breast cancer cases, and potentially improve patient outcomes. Future work will focus on improvements utilizing larger datasets. 7 8",
    "output": "Abstract\n\nEach year, the treatment decisions for more than 230,000 breast cancer patients in the U.S. hinge on whether the cancer has metastasized away from the breast. Metastasis detection is currently performed by pathologists reviewing large expanses of biological tissues. This process is labor intensive and error-prone. We present a framework to automatically detect and localize tumors as small as 100 x 100 pixels in gigapixel microscopy images sized 100,000 x 100,000 pixels. Our method leverages a convolutional neural network (CNN) architecture and obtains state-of-the-art results on the Camelyon16 dataset in the challenging lesion-level tumor detection task. At 8 false positives per image, we detect 92.4% of the tumors, relative to 82.7% by the previous best automated approach. For comparison, a human pathologist attempting exhaustive search achieved 73.2% sensitivity. We achieve image-level AUC scores above 97% on both the Camelyon16 test set and an independent set of 110 slides. In addition, we discover that two slides in the Camelyon16 training set were erroneously labeled normal. Our approach could considerably reduce false negative rates in metastasis detection."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Abstract\nAbstract: Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, Abstract We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and imagetext alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, GLIDE and DALL-E 2, and find that human raters prefer Imagen over other models in side-byside comparisons, both in terms of sample quality and image-text alignment. See imagen.research.google for an overview of the results. 1",
    "output": "As for Chirp, it’s a speech model trained on “millions” of hours of audio that supports more than 100 languages and can be used to caption videos, offer voice assistance and generally power a range of speech tasks and apps."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\nIntroduction: Introduction Multimodal learning has come into prominence recently, with text-to-image synthesis [53, 12, 57] and image-text contrastive learning [49, 31, 74] at the forefront. These models have transformed the research community and captured widespread public attention with creative image generation [22, 54] and editing applications [21, 41, 34]. To pursue this research direction further, we introduce Imagen, a text-to-image diffusion model that combines the power of transformer language models (LMs) [15, 52] with high-fidelity diffusion models [28, 29, 16, 41] to deliver an unprecedented degree of photorealism and a deep level of language understanding in text-to-image synthesis. In contrast to prior work that uses only image-text data for model training [e.g., 53, 41], the key finding behind Imagen is that text embeddings from large LMs [52, 15], pretrained on text-only corpora, are remarkably effective for text-to-image synthesis. See Fig. 1 for select samples. Imagen comprises a frozen T5-XXL [52] encoder to map input text into a sequence of embeddings and a 6464 image diffusion model, followed by two super-resolution diffusion models for generating Sprouts in the shape of text ‘Imagen’ coming out of a fairytale book. Teddy bears swimming at the Olympics 400m Butterfly event. A brain riding a rocketship heading towards the moon. A dragon fruit wearing karate belt in the snow. A strawberry mug filled with white sesame seeds. The mug is floating in a dark chocolate sea. A high contrast portrait of a very happy fuzzy panda dressed as a chef in a high end kitchen making dough. There is a painting of flowers on the wall behind him. A photo of a Shiba Inu dog with a backpack riding a bike. It is wearing sunglasses and a beach hat. A cute corgi lives in a house made out of sushi. A cute sloth holding a small treasure chest. A bright golden glow is coming from the chest. 256256 and 10241024 images (see Fig. A.4). All diffusion models are conditioned on the text embedding sequence and use classifier-free guidance [27]. Imagen relies on new sampling techniques to allow usage of large guidance weights without sample quality degradation observed in prior work, resulting in images with higher fidelity and better image-text alignment than previously possible. While conceptually simple and easy to train, Imagen yields surprisingly strong results. Imagen outperforms other methods on COCO [36] with zero-shot FID-30K of 7.27, significantly outperforming prior work such as GLIDE [41] (at 12.4) and the concurrent work of DALL-E 2 [54] (at 10.4). Our zero-shot FID score is also better than state-of-the-art models trained on COCO, e.g., Make-A-Scene [22] (at 7.6). Additionally, human raters indicate that generated samples from Imagen are on-par in image-text alignment to the reference images on COCO captions. We introduce DrawBench, a new structured suite of text prompts for text-to-image evaluation. DrawBench enables deeper insights through a multi-dimensional evaluation of text-to-image models, with text prompts designed to probe different semantic properties of models. These include compositionality, cardinality, spatial relations, the ability to handle complex text prompts or prompts with rare words, and they include creative prompts that push the limits of models’ ability to generate highly implausible scenes well beyond the scope of the training data. With DrawBench, extensive human evaluation shows that Imagen outperforms other recent methods [57, 12, 54] by a significant margin. We further demonstrate some of the clear advantages of the use of large pre-trained language models [52] over multi-modal embeddings such as CLIP [49] as a text encoder for Imagen. Key contributions of the paper include: 1. We discover that large frozen language models",
    "output": "To paraphrase Andreessen Horowitz, generative AI, particularly on the text-to-art side, is eating the world. At least, investors believe so — judging by the billions of dollars they’ve poured into startups developing AI that creates text and images from prompts. Not to be left behind, Big Tech is investing in its own generative AI art solutions, whether through partnerships with the aforementioned startups or in-house R&D. (See: Microsoft teaming up with OpenAI for Image Creator.) Google, leveraging its robust R&D wing, has decided to go the latter route, commercializing its work in generative AI to compete with the platforms already out there. Today at its annual I/O developer conference, Google announced new AI models heading to Vertex AI, its fully managed AI service, including a text-to-image model called Imagen. Imagen, which Google previewed via its AI Test Kitchen app last November, can generate and edit images as well as write captions for existing images. “Any developer can use this technology using Google Cloud,” Nenshad Bardoliwalla, director of Vertex AI at Google Cloud, told TechCrunch in a phone interview. “You don’t need to be a data scientist or developer.” Imagen in Vertex Getting started with Imagen in Vertex is, indeed, a relatively straightforward process. A UI for the model is accessible from what Google calls the Model Garden, a selection of Google-developed models alongside curated open source models. Within the UI, similar to generative art platforms such as Midjourney and NightCafe, customers can enter prompts (e.g. “a purple handbag”) to have Imagen generate a handful of candidate images. Editing tools and follow-up prompts refine the Imagen-generated images, for example adjusting the color of the objects depicted in them. Vertex also offers upscaling for sharpening images, in addition to fine-tuning that allows customers to steer Imagen toward certain styles and preferences. As alluded to earlier, Imagen can also generate captions for images, optionally translating those captions leveraging Google Translate. To comply with privacy regulations like GDPR, generated images that aren’t saved are deleted within 24 hours, Bardoliwalla says. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW “We make it very easy for people to start working with generative AI and their images,” he added. Of course, there’s a host of ethical and legal challenges associated with all forms of generative AI — no matter how polished the UI. AI models like Imagen “learn” to generate images from text prompts by “training” on existing images, which often come from datasets that were scraped together by trawling public image hosting websites. Some experts suggest that training models using public images, even copyrighted ones, will be covered by the fair use doctrine in the U.S. But it’s a matter that’s unlikely to be settled anytime soon. To wit, two companies behind popular AI art tools, Midjourney and Stability AI, are in the crosshairs of a legal case that alleges they infringed on the rights of millions of artists by training their tools on web-scraped images. Stock image supplier Getty Images has taken Stability AI to court, separately, for reportedly using millions of images from its site without permission to train the art-generating model Stable Diffusion. I asked Bardoliwalla whether Vertex customers should be concerned that Imagen might’ve been trained on copyrighted materials. Understandably, they might be deterred from using it if that were the case. Bardoliwalla didn’t say outright that Imagen wasn’t trained on trademarked images — only that Google conducts broad “data governance reviews” to “look at the source data” inside its models to ensure that they’re “free of copyright claims.” (The hedged language doesn’t come as a massive surprise considering that"
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\ntrained only on text data are surprisingly very effective text encoders for text-to-image generation, and that scaling the size of frozen text encoder improves sample quality significantly more than scaling the size of image diffusion model. 2. We introduce dynamic thresholding, a new diffusion sampling technique to leverage high guidance weights and generating more photorealistic and detailed images than previously possible. 3. We highlight several important diffusion architecture design choices and propose Efficient U-Net",
    "output": "To paraphrase Andreessen Horowitz, generative AI, particularly on the text-to-art side, is eating the world. At least, investors believe so — judging by the billions of dollars they’ve poured into startups developing AI that creates text and images from prompts. Not to be left behind, Big Tech is investing in its own generative AI art solutions, whether through partnerships with the aforementioned startups or in-house R&D. (See: Microsoft teaming up with OpenAI for Image Creator.) Google, leveraging its robust R&D wing, has decided to go the latter route, commercializing its work in generative AI to compete with the platforms already out there. Today at its annual I/O developer conference, Google announced new AI models heading to Vertex AI, its fully managed AI service, including a text-to-image model called Imagen. Imagen, which Google previewed via its AI Test Kitchen app last November, can generate and edit images as well as write captions for existing images. “Any developer can use this technology using Google Cloud,” Nenshad Bardoliwalla, director of Vertex AI at Google Cloud, told TechCrunch in a phone interview. “You don’t need to be a data scientist or developer.” Imagen in Vertex Getting started with Imagen in Vertex is, indeed, a relatively straightforward process. A UI for the model is accessible from what Google calls the Model Garden, a selection of Google-developed models alongside curated open source models. Within the UI, similar to generative art platforms such as Midjourney and NightCafe, customers can enter prompts (e.g. “a purple handbag”) to have Imagen generate a handful of candidate images. Editing tools and follow-up prompts refine the Imagen-generated images, for example adjusting the color of the objects depicted in them. Vertex also offers upscaling for sharpening images, in addition to fine-tuning that allows customers to steer Imagen toward certain styles and preferences. As alluded to earlier, Imagen can also generate captions for images, optionally translating those captions leveraging Google Translate. To comply with privacy regulations like GDPR, generated images that aren’t saved are deleted within 24 hours, Bardoliwalla says. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW “We make it very easy for people to start working with generative AI and their images,” he added. Of course, there’s a host of ethical and legal challenges associated with all forms of generative AI — no matter how polished the UI. AI models like Imagen “learn” to generate images from text prompts by “training” on existing images, which often come from datasets that were scraped together by trawling public image hosting websites. Some experts suggest that training models using public images, even copyrighted ones, will be covered by the fair use doctrine in the U.S. But it’s a matter that’s unlikely to be settled anytime soon. To wit, two companies behind popular AI art tools, Midjourney and Stability AI, are in the crosshairs of a legal case that alleges they infringed on the rights of millions of artists by training their tools on web-scraped images. Stock image supplier Getty Images has taken Stability AI to court, separately, for reportedly using millions of images from its site without permission to train the art-generating model Stable Diffusion. I asked Bardoliwalla whether Vertex customers should be concerned that Imagen might’ve been trained on copyrighted materials. Understandably, they might be deterred from using it if that were the case. Bardoliwalla didn’t say outright that Imagen wasn’t trained on trademarked images — only that Google conducts broad “data governance reviews” to “look at the source data” inside its models to ensure that they’re “free of copyright claims.” (The hedged language doesn’t come as a massive surprise considering that"
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\n, a new architecture variant which is simpler, converges faster and is more memory efficient. 4. We achieve a new state-of-the-art COCO FID of 7.27. Human raters find Imagen to be on-par with the reference images in terms of image-text alignment. 5. We introduce DrawBench, a new comprehensive and challenging evaluation benchmark for the text-to-image task. On DrawBench human evaluation, we find Imagen to outperform all other work, including the concurrent work of DALL-E 2 [54].",
    "output": "To paraphrase Andreessen Horowitz, generative AI, particularly on the text-to-art side, is eating the world. At least, investors believe so — judging by the billions of dollars they’ve poured into startups developing AI that creates text and images from prompts. Not to be left behind, Big Tech is investing in its own generative AI art solutions, whether through partnerships with the aforementioned startups or in-house R&D. (See: Microsoft teaming up with OpenAI for Image Creator.) Google, leveraging its robust R&D wing, has decided to go the latter route, commercializing its work in generative AI to compete with the platforms already out there. Today at its annual I/O developer conference, Google announced new AI models heading to Vertex AI, its fully managed AI service, including a text-to-image model called Imagen. Imagen, which Google previewed via its AI Test Kitchen app last November, can generate and edit images as well as write captions for existing images. “Any developer can use this technology using Google Cloud,” Nenshad Bardoliwalla, director of Vertex AI at Google Cloud, told TechCrunch in a phone interview. “You don’t need to be a data scientist or developer.” Imagen in Vertex Getting started with Imagen in Vertex is, indeed, a relatively straightforward process. A UI for the model is accessible from what Google calls the Model Garden, a selection of Google-developed models alongside curated open source models. Within the UI, similar to generative art platforms such as Midjourney and NightCafe, customers can enter prompts (e.g. “a purple handbag”) to have Imagen generate a handful of candidate images. Editing tools and follow-up prompts refine the Imagen-generated images, for example adjusting the color of the objects depicted in them. Vertex also offers upscaling for sharpening images, in addition to fine-tuning that allows customers to steer Imagen toward certain styles and preferences. As alluded to earlier, Imagen can also generate captions for images, optionally translating those captions leveraging Google Translate. To comply with privacy regulations like GDPR, generated images that aren’t saved are deleted within 24 hours, Bardoliwalla says. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW “We make it very easy for people to start working with generative AI and their images,” he added. Of course, there’s a host of ethical and legal challenges associated with all forms of generative AI — no matter how polished the UI. AI models like Imagen “learn” to generate images from text prompts by “training” on existing images, which often come from datasets that were scraped together by trawling public image hosting websites. Some experts suggest that training models using public images, even copyrighted ones, will be covered by the fair use doctrine in the U.S. But it’s a matter that’s unlikely to be settled anytime soon. To wit, two companies behind popular AI art tools, Midjourney and Stability AI, are in the crosshairs of a legal case that alleges they infringed on the rights of millions of artists by training their tools on web-scraped images. Stock image supplier Getty Images has taken Stability AI to court, separately, for reportedly using millions of images from its site without permission to train the art-generating model Stable Diffusion. I asked Bardoliwalla whether Vertex customers should be concerned that Imagen might’ve been trained on copyrighted materials. Understandably, they might be deterred from using it if that were the case. Bardoliwalla didn’t say outright that Imagen wasn’t trained on trademarked images — only that Google conducts broad “data governance reviews” to “look at the source data” inside its models to ensure that they’re “free of copyright claims.” (The hedged language doesn’t come as a massive surprise considering that"
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nMethod: 2 Imagen Imagen consists of a text encoder that maps text to a sequence of embeddings and a cascade of conditional diffusion models that map these embeddings to images of increasing resolutions (see Fig. A.4). In the following subsections, we describe each of these components in detail. 2.1 Pretrained text encoders Text-to-image models need powerful semantic text encoders to capture the complexity and compositionality of arbitrary natural language text inputs. Text encoders trained on paired image-text data are standard in current text-to-image models; they can be trained from scratch [41, 53] or pretrained on image-text data [54] (e.g., CLIP [49]). The image-text training objectives suggest that these text encoders may encode visually semantic and meaningful representations especially relevant for the text-to-image generation task. Large language models can be another models of choice to encode text for text-to-image generation. Recent progress in large language models (e.g., BERT [15], GPT [47, 48, 7], T5 [52]) have led to leaps in textual understanding and generative capabilities. Language models are trained on text only corpus significantly larger than paired image-text data, thus being exposed to a very rich and wide distribution of text. These models are also generally much larger than text encoders in current image-text models [49, 31, 80] (e.g. PaLM [11] has 540B parameters, while CoCa [80] has a 1B parameter text encoder). It thus becomes natural to explore both families of text encoders for the text-to-image task. Imagen explores pretrained text encoders: BERT [15], T5 [51] and CLIP [46]. For simplicity, we freeze the weights of these text encoders. Freezing has several advantages such as offline computation of embeddings, resulting in negligible computation or memory footprint during training of the textto-image model. In our work, we find that there is a clear conviction that scaling the text encoder size improves the quality of text-to-image generation. We also find that while T5-XXL and CLIP text encoders perform similarly on simple benchmarks such as MS-COCO, human evaluators prefer T5-XXL encoders over CLIP text encoders in both image-text alignment and image fidelity on DrawBench, a set of challenging and compositional prompts. We refer the reader to Section 4.4 for summary of our findings, and Appendix D.1 for detailed ablations. 2.2 Diffusion models and classifier-free guidance Here we give a brief introduction to diffusion models; a precise description is in Appendix A. Diffusion models [63, 28, 65] are a class of generative models that convert Gaussian noise into samples from a learned data distribution via an iterative denoising process. These models can be conditional, for example on class labels, text, or low-resolution images [e.g. 16, 29, 59, 58, 75, 41, 54]. A diffusion model x is trained on a denoising objective of the form Ex,c,,t wtx(tx + t, c) x2 2 (1) where (x, c) are data-conditioning pairs, t U([0, 1]),  N(0, I), and t, t, wt are functions of t that influence sample quality. Intuitively, x is trained to denoise zt := tx + t into x using a squared error loss, weighted to emphasize certain values of t. Sampling such as the ancestral sampler [28] and DDIM [64] start from pure noise z1 N(0, I) and iteratively generate points zt1, . . . , ztT , where 1 = t1 >    > tT = 0, that gradually decrease in noise content. These points are functions of the x-predictions xt 0 := x(zt, c). Classifier guidance [16] is a technique to improve sample quality while reducing diversity in conditional diffusion models using gradients from a pret",
    "output": "Today at its annual I/O developer conference, Google announced new AI models heading to Vertex AI, its fully managed AI service, including a text-to-image model called Imagen. Imagen, which Google previewed via its AI Test Kitchen app last November, can generate and edit images as well as write captions for existing images.\nImagen in Vertex\nGetting started with Imagen in Vertex is, indeed, a relatively straightforward process. A UI for the model is accessible from what Google calls the Model Garden, a selection of Google-developed models alongside curated open source models. Within the UI, similar to generative art platforms such as Midjourney and NightCafe, customers can enter prompts (e.g. “a purple handbag”) to have Imagen generate a handful of candidate images.\nAs alluded to earlier, Imagen can also generate captions for images, optionally translating those captions leveraging Google Translate. To comply with privacy regulations like GDPR, generated images that aren’t saved are deleted within 24 hours, Bardoliwalla says.\nBardoliwalla didn’t say outright that Imagen wasn’t trained on trademarked images — only that Google conducts broad “data governance reviews” to “look at the source data” inside its models to ensure that they’re “free of copyright claims.” (The hedged language doesn’t come as a massive surprise considering that the original Imagen was trained on a public data set, LAION, known to contain copyrighted works.)\nIn a related announcement at I/O, Google launched the Embeddings API for Vertex in preview, which can convert text and image data into representations called vectors that map specific semantic relationships. Google says that it’ll be used to build semantic search and text classification functionality like Q&A chatbots based on an organization’s data, sentiment analysis and anomaly detection.\nCodey, Imagen, the Embeddings API for images and RLHF are available in Vertex AI to “trusted testers,” Google says. Chirp, the Embeddings API and Generative AI Studio, a suite for interacting with and deploying AI models, meanwhile, are accessible in preview in Vertex to anyone with a Google Cloud account."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nrained model p(c|zt) during sampling. Classifierfree guidance [27] is an alternative technique that avoids this pretrained model by instead jointly training a single diffusion model on conditional and unconditional objectives via randomly dropping c during training (e.g. with 10% probability). Sampling is performed using the adjusted x-prediction (zt  )/t, where (zt, c) = w(zt, c) + (1 w)(zt). (2) Here, (zt, c) and (zt) are conditional and unconditional -predictions, given by  := (zt  tx)/t, and w is the guidance weight. Setting w = 1 disables classifier-free guidance, while increasing w > 1 strengthens the effect of guidance. Imagen depends critically on classifier-free guidance for effective text conditioning. 2.3 Large guidance weight samplers We corroborate the results of recent text-guided diffusion work [16, 41, 54] and find that increasing the classifier-free guidance weight improves image-text alignment, but damages image fidelity producing highly saturated and unnatural images [27]. We find that this is due to a train-test mismatch arising from high guidance weights. At each sampling step t, the x-prediction xt 0 must be within the same bounds as training data x, i.e. within [1, 1], but we find empirically that high guidance weights cause x-predictions to exceed these bounds. This is a train-test mismatch, and since the diffusion model is iteratively applied on its own output throughout sampling, the sampling process produces unnatural images and sometimes even diverges. To counter this problem, we investigate static thresholding and dynamic thresholding. See Appendix Fig. A.31 for reference implementation of the techniques and Appendix Fig. A.9 for visualizations of their effects. Static thresholding: We refer to elementwise clipping the x-prediction to [1, 1] as static thresholding. This method was in fact used but not emphasized in previous work [28], and to our knowledge its importance has not been investigated in the context of guided sampling. We discover that static thresholding is essential to sampling with large guidance weights and prevents generation of blank images. Nonetheless, static thresholding still results in over-saturated and less detailed images as the guidance weight further increases. Dynamic thresholding: We introduce a new dynamic thresholding method: at each sampling step we set s to a certain percentile absolute pixel value in xt 0, and if s > 1, then we threshold xt 0 to the range [s, s] and then divide by s. Dynamic thresholding pushes saturated pixels (those near -1 and 1) inwards, thereby actively preventing pixels from saturation at each step. We find that dynamic thresholding results in significantly better photorealism as well as better image-text alignment, especially when using very large guidance weights. 2.4 Robust cascaded diffusion models Imagen utilizes a pipeline of a base 6464 model, and two text-conditional super-resolution diffusion models to upsample a 64  64 generated image into a 256  256 image, and then to 1024  1024 image. Cascaded diffusion models with noise conditioning augmentation [29] have been extremely effective in progressively generating high-fidelity images. Furthermore, making the super-resolution models aware of the amount of noise added, via noise level conditioning, significantly improves the sample quality and helps improving the robustness of the super-resolution models to handle artifacts generated by lower resolution models [29]. Imagen uses noise conditioning augmentation for both the super-resolution models. We find this to be a critical for generating high fidelity images. Given a conditioning low-resolution image and augmentation level (a.k.a aug_level) (e.g., strength of Gaussian noise or blur), we corrupt the low-resolution image with the augmentation (corresponding to aug_level), and condition the diffusion model on aug_level. During training, aug_level is chosen randomly, while during inference, we sweep over its different values to find the best sample quality. In our case, we use Gaussian noise as a form of augmentation, and apply variance preserving Gaussian noise augmentation",
    "output": "Today at its annual I/O developer conference, Google announced new AI models heading to Vertex AI, its fully managed AI service, including a text-to-image model called Imagen. Imagen, which Google previewed via its AI Test Kitchen app last November, can generate and edit images as well as write captions for existing images.\nImagen in Vertex\nGetting started with Imagen in Vertex is, indeed, a relatively straightforward process. A UI for the model is accessible from what Google calls the Model Garden, a selection of Google-developed models alongside curated open source models. Within the UI, similar to generative art platforms such as Midjourney and NightCafe, customers can enter prompts (e.g. “a purple handbag”) to have Imagen generate a handful of candidate images.\nAs alluded to earlier, Imagen can also generate captions for images, optionally translating those captions leveraging Google Translate. To comply with privacy regulations like GDPR, generated images that aren’t saved are deleted within 24 hours, Bardoliwalla says.\nBardoliwalla didn’t say outright that Imagen wasn’t trained on trademarked images — only that Google conducts broad “data governance reviews” to “look at the source data” inside its models to ensure that they’re “free of copyright claims.” (The hedged language doesn’t come as a massive surprise considering that the original Imagen was trained on a public data set, LAION, known to contain copyrighted works.)\nIn a related announcement at I/O, Google launched the Embeddings API for Vertex in preview, which can convert text and image data into representations called vectors that map specific semantic relationships. Google says that it’ll be used to build semantic search and text classification functionality like Q&A chatbots based on an organization’s data, sentiment analysis and anomaly detection.\nCodey, Imagen, the Embeddings API for images and RLHF are available in Vertex AI to “trusted testers,” Google says. Chirp, the Embeddings API and Generative AI Studio, a suite for interacting with and deploying AI models, meanwhile, are accessible in preview in Vertex to anyone with a Google Cloud account."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nresembling the forward process used in diffusion models (Appendix A). The augmentation level is specified using aug_level [0, 1]. See Fig. A.32 for reference pseudocode. 2.5 Neural network architecture Base model: We adapt the U-Net architecture from [40] for our base 64 64 text-to-image diffusion model. The network is conditioned on text embeddings via a pooled embedding vector, added to the diffusion timestep embedding similar to the class embedding conditioning method used in [16, 29]. We further condition on the entire sequence of text embeddings by adding cross attention [57] over the text embeddings at multiple resolutions. We study various methods of text conditioning in Appendix D.3.1. Furthermore, we found Layer Normalization [2] for text embeddings in the attention and pooling layers to help considerably improve performance. Super-resolution models: For 64 64 256 256 super-resolution, we use the U-Net model adapted from [40, 58]. We make several modifications to this U-Net model for improving memory efficiency, inference time and convergence speed (our variant is 2-3x faster in steps/second over the U-Net used in [40, 58]). We call this variant Efficient U-Net (See Appendix B.1 for more details and comparisons). Our 256256 10241024 super-resolution model trains on 6464 256256 crops of the 1024 1024 image. To facilitate this, we remove the self-attention layers, however we keep the text cross-attention layers which we found to be critical. During inference, the model receives the full 256 256 low-resolution images as inputs, and returns upsampled 1024 1024 images as outputs. Note that we use text cross attention for both our super-resolution models. 3 Evaluating Text-to-Image Models The COCO [36] validation set is the standard benchmark for evaluating text-to-image models for both the supervised [82, 22] and the zero-shot setting [53, 41]. The key automated performance metrics used are FID [26] to measure image fidelity, and CLIP score [25, 49] to measure image-text alignment. Consistent with previous works, we report zero-shot FID-30K, for which 30K prompts are drawn randomly from the validation set, and the model samples generated on these prompts are compared with reference images from the full validation set. Since guidance weight is an important ingredient to control image quality and text alignment, we report most of our ablation results using trade-off (or pareto) curves between CLIP and FID scores across a range of guidance weights. Both FID and CLIP scores have limitations, for example FID is not fully aligned with perceptual quality [42], and CLIP is ineffective at counting [49]. Due to these limitations, we use human evaluation to assess image quality and caption similarity, with ground truth reference caption-image pairs as a baseline. We use two experimental paradigms: 1. To probe image quality, the rater is asked to select between the model generation and reference image using the question: “Which image is more photorealistic (looks more real)?”. We report the percentage of times raters choose model generations over reference images (the preference rate). 2. To probe alignment, human raters are shown an image and a prompt and asked “Does the caption accurately describe the above image?”. They must respond with “yes”, “somewhat”, or “no”. These responses are scored as 100, 50, and 0, respectively. These ratings are obtained independently for model samples and reference images, and both are reported. A brown bird and a blue bear. One cat and two dogs sitting on the grass. A sign that says ’NeurIPS’. A small blue book sitting on a large red book. A blue coloured pizza. A wine glass on top of a dog. A pear cut into seven pieces A photo of a confused grizzly bear A small vessel propelled on water arranged in a ring. in calculus class. by oar",
    "output": "Today at its annual I/O developer conference, Google announced new AI models heading to Vertex AI, its fully managed AI service, including a text-to-image model called Imagen. Imagen, which Google previewed via its AI Test Kitchen app last November, can generate and edit images as well as write captions for existing images.\nImagen in Vertex\nGetting started with Imagen in Vertex is, indeed, a relatively straightforward process. A UI for the model is accessible from what Google calls the Model Garden, a selection of Google-developed models alongside curated open source models. Within the UI, similar to generative art platforms such as Midjourney and NightCafe, customers can enter prompts (e.g. “a purple handbag”) to have Imagen generate a handful of candidate images.\nAs alluded to earlier, Imagen can also generate captions for images, optionally translating those captions leveraging Google Translate. To comply with privacy regulations like GDPR, generated images that aren’t saved are deleted within 24 hours, Bardoliwalla says.\nBardoliwalla didn’t say outright that Imagen wasn’t trained on trademarked images — only that Google conducts broad “data governance reviews” to “look at the source data” inside its models to ensure that they’re “free of copyright claims.” (The hedged language doesn’t come as a massive surprise considering that the original Imagen was trained on a public data set, LAION, known to contain copyrighted works.)\nIn a related announcement at I/O, Google launched the Embeddings API for Vertex in preview, which can convert text and image data into representations called vectors that map specific semantic relationships. Google says that it’ll be used to build semantic search and text classification functionality like Q&A chatbots based on an organization’s data, sentiment analysis and anomaly detection.\nCodey, Imagen, the Embeddings API for images and RLHF are available in Vertex AI to “trusted testers,” Google says. Chirp, the Embeddings API and Generative AI Studio, a suite for interacting with and deploying AI models, meanwhile, are accessible in preview in Vertex to anyone with a Google Cloud account."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\ns, sails, or an engine. Figure 2: Non-cherry picked Imagen samples for different categories of prompts from DrawBench. For both cases we use 200 randomly chosen image-caption pairs from the COCO validation set. Subjects were shown batches of 50 images. We also used interleaved “control\" trials, and only include rater",
    "output": "Today at its annual I/O developer conference, Google announced new AI models heading to Vertex AI, its fully managed AI service, including a text-to-image model called Imagen. Imagen, which Google previewed via its AI Test Kitchen app last November, can generate and edit images as well as write captions for existing images.\nImagen in Vertex\nGetting started with Imagen in Vertex is, indeed, a relatively straightforward process. A UI for the model is accessible from what Google calls the Model Garden, a selection of Google-developed models alongside curated open source models. Within the UI, similar to generative art platforms such as Midjourney and NightCafe, customers can enter prompts (e.g. “a purple handbag”) to have Imagen generate a handful of candidate images.\nAs alluded to earlier, Imagen can also generate captions for images, optionally translating those captions leveraging Google Translate. To comply with privacy regulations like GDPR, generated images that aren’t saved are deleted within 24 hours, Bardoliwalla says.\nBardoliwalla didn’t say outright that Imagen wasn’t trained on trademarked images — only that Google conducts broad “data governance reviews” to “look at the source data” inside its models to ensure that they’re “free of copyright claims.” (The hedged language doesn’t come as a massive surprise considering that the original Imagen was trained on a public data set, LAION, known to contain copyrighted works.)\nIn a related announcement at I/O, Google launched the Embeddings API for Vertex in preview, which can convert text and image data into representations called vectors that map specific semantic relationships. Google says that it’ll be used to build semantic search and text classification functionality like Q&A chatbots based on an organization’s data, sentiment analysis and anomaly detection.\nCodey, Imagen, the Embeddings API for images and RLHF are available in Vertex AI to “trusted testers,” Google says. Chirp, the Embeddings API and Generative AI Studio, a suite for interacting with and deploying AI models, meanwhile, are accessible in preview in Vertex to anyone with a Google Cloud account."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\ndata from those who correctly answered at least 80% of the control questions. This netted 73 and 51 ratings per image for image quality and image-text alignment evaluations, respectively. DrawBench: While COCO is a valuable benchmark, it is increasingly clear that it has a limited spectrum of prompts that do not readily provide insight into differences between models (e.g., see Sec. 4.2). Recent work by [10] proposed a new evaluation set called PaintSkills to systematically evaluate visual reasoning skills and social biases beyond COCO. With similar motivation, we introduce DrawBench, a comprehensive and challenging set of prompts that support the evaluation and comparison of text-to-image models. DrawBench contains 11 categories of prompts, testing different capabilities of models such as the ability to faithfully render different colors, numbers of objects, spatial relations, text in the scene, and unusual interactions between objects. Categories also include complex prompts, including long, intricate textual descriptions, rare words, and also misspelled prompts. We also include sets of prompts collected from DALL-E [53], Gary Marcus et al. [38] and Reddit. Across these 11 categories, DrawBench comprises 200 prompts in total, striking a good balance between the desire for a large, comprehensive dataset, and small enough that human evaluation remains feasible. (Appendix C provides a more detailed description of DrawBench. Fig. 2 shows example prompts from DrawBench with Imagen samples.) We use DrawBench to directly compare different models. To this end, human raters are presented with two sets of images, one from Model A and one from Model B, each of which has 8 samples. Human raters are asked to compare Model A and Model B on sample fidelity and image-text alignment. They respond with one of three choices: Prefer Model A; Indifferent; or Prefer Model B.",
    "output": "Today at its annual I/O developer conference, Google announced new AI models heading to Vertex AI, its fully managed AI service, including a text-to-image model called Imagen. Imagen, which Google previewed via its AI Test Kitchen app last November, can generate and edit images as well as write captions for existing images.\nImagen in Vertex\nGetting started with Imagen in Vertex is, indeed, a relatively straightforward process. A UI for the model is accessible from what Google calls the Model Garden, a selection of Google-developed models alongside curated open source models. Within the UI, similar to generative art platforms such as Midjourney and NightCafe, customers can enter prompts (e.g. “a purple handbag”) to have Imagen generate a handful of candidate images.\nAs alluded to earlier, Imagen can also generate captions for images, optionally translating those captions leveraging Google Translate. To comply with privacy regulations like GDPR, generated images that aren’t saved are deleted within 24 hours, Bardoliwalla says.\nBardoliwalla didn’t say outright that Imagen wasn’t trained on trademarked images — only that Google conducts broad “data governance reviews” to “look at the source data” inside its models to ensure that they’re “free of copyright claims.” (The hedged language doesn’t come as a massive surprise considering that the original Imagen was trained on a public data set, LAION, known to contain copyrighted works.)\nIn a related announcement at I/O, Google launched the Embeddings API for Vertex in preview, which can convert text and image data into representations called vectors that map specific semantic relationships. Google says that it’ll be used to build semantic search and text classification functionality like Q&A chatbots based on an organization’s data, sentiment analysis and anomaly detection.\nCodey, Imagen, the Embeddings API for images and RLHF are available in Vertex AI to “trusted testers,” Google says. Chirp, the Embeddings API and Generative AI Studio, a suite for interacting with and deploying AI models, meanwhile, are accessible in preview in Vertex to anyone with a Google Cloud account."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nExperiments / Results: 4 Experiments Section 4.1 describes training details, Sections 4.2 and 4.3 analyze results on MS-COCO and DrawBench, and Section 4.4 summarizes our ablation studies and key findings. For all experiments below, the images are fair random samples from Imagen with no post-processing or re-ranking. 4.1 Training details Table 1: MS-COCO 256  256 FID-30K. We use a guidance weight of 1.35 for our 64  64 model, and a guidance weight of 8.0 for our super-resolution model. Model FID-30K Zero-shot FID-30K AttnGAN [76] 35.49 DM-GAN [83] 32.64 DF-GAN [69] 21.42 DM-GAN + CL [78] 20.79 XMC-GAN [81] 9.33 LAFITE [82] 8.12 Make-A-Scene [22] 7.55 DALL-E [53] 17.89 LAFITE [82] 26.94 GLIDE [41] 12.24 DALL-E 2 [54] 10.39 Imagen (Our Work) 7.27 models. We do not find over-fitting to be an issue, and we believe further training might improve overall performance. We use Adafactor for our base 64  64 model, because initial comparisons with Adam suggested similar performance with much smaller memory footprint for Adafactor. For superresolution models, we use Adam as we found Adafactor to hurt model quality in our initial ablations. For classifier-free guidance, we joint-train unconditionally via zeroing out the text embeddings with 10% probability for all three models. We train on a combination of internal datasets, with 460M image-text pairs, and the publicly available Laion dataset [61], with 400M image-text pairs. There are limitations in our training data, and we refer the reader to Section 6 for details. See Appendix F for more implementation details. 4.2 Results on COCO We evaluate Imagen on the COCO validation set using FID score, similar to [53, 41]. Table 1 displays the results. Imagen achieves state of the art zero-shot FID on COCO at 7.27, outperforming the concurrent work of DALL-E 2 [54] and even models trained on COCO. Table 2 reports the human evaluation to test image quality and alignment on the COCO validation set. We report results on the original COCO validation set, as well as a filtered version in which all reference data with people have been removed. For photorealism, Imagen achieves 39.2% preference rate indicating high image quality generation. On the set with no people, there is a boost in preference rate of Imagen to 43.6%, indicating Imagen’s limited ability to generate photorealistic people. On caption similarity, Imagen’s score is on-par with the original reference images, suggesting Imagen’s ability to generate images that align well with COCO captions. 4.3 Results on DrawBench Using DrawBench, we compare Imagen with DALL-E 2 (the public version) [54], GLIDE [41], Latent Diffusion [57], and CLIP-guided VQ-GAN [12]. Fig. 3 shows the human evaluation results for pairwise comparison of Imagen with each of the three models. We report the percentage of time raters prefer Model A, Model B, or are indifferent for both image fidelity and image-text alignment. We aggregate the scores across all the categories and raters. We find the human raters to exceedingly prefer Imagen over all others models in both image-text alignment and image fidelity. We refer the reader to Appendix E for a more detailed category wise comparison and qualitative comparison. 4.4 Analysis of Imagen Table 2: COCO 256  256 human evaluation comparing model outputs and original images. For the bottom part (no people), we filter out prompts containing one of man, men, woman, women, person, people, child, adult, adults, boy, boys, girl, girls, guy, lady, ladies, someone, toddler, (sport) player, workers, spectators. Model Photorealism Alignment  Original Original 50.0% 91.9  0.42 Imagen 39.5  0.75% 91.4  0.44 No people Original 50.0% 92.2  0.54 Imagen 43.9  1.01% 92.1  0.55 Imagen DALL-E 2 Imagen GLIDE 100% 50% Alignment",
    "output": "Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nFidelity 0% Alignment Fidelity Figure 3: Comparison between Imagen and DALL-E 2 [54], GLIDE [41], VQ-GAN+CLIP [12] and Latent Diffusion [57] on DrawBench: User preference rates (with 95% confidence intervals) for image-text alignment and image fidelity. T5-Small T-Large T5-XL T5-XXL 25 25 FID-10K FID-10K 20 20 15 15 10 10 0.22 0.24 0.26 0.28 0.24 0.25 0.26 0.27 0.28 0.29 CLIP Score CLIP Score (a) Impact of encoder size. (b) Impact of U-Net size. Figure 4: Summary of some of the critical findings of Imagen with pareto curves sweeping over different guidance values. See Appendix D for more details. Scaling text encoder size is more important than U-Net size. While scaling the size of the diffusion model U-Net improves sample quality, we found scaling the text encoder size to be significantly more impactful than the U-Net size (Fig. 4b). Dynamic thresholding is critical. We show that dynamic thresholding results in samples with significantly better photorealism and alignment with text, over static or no thresholding, especially under the presence of large classifier-free guidance weights (Fig. 4c). Human raters prefer T5-XXL over CLIP on DrawBench. The models trained with T5-XXL and CLIP text encoders perform similarly on the COCO validation set in terms of CLIP and FID scores. However, we find that human raters prefer T5-XXL over CLIP on DrawBench across all 11 categories. Noise conditioning augmentation is critical. We show that training the super-resolution models with noise conditioning augmentation leads to better CLIP and FID scores. We also show that noise conditioning augmentation enables stronger text conditioning for the super-resolution model, resulting in improved CLIP and FID scores at higher guidance weights. Adding noise to the low-res image during inference along with the use of large guidance weights allows the super-resolution models to generate diverse upsampled outputs while removing artifacts from the low-res image. Text conditioning method is critical. We observe that conditioning over the sequence of text embeddings with cross attention significantly outperforms simple mean or attention based pooling in both sample fidelity as well as image-text alignment. Efficient U-Net is critical. Our Efficient U-Net implementation uses less memory, converges faster, and has better sample quality with faster inference. 5",
    "output": "Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Conclusion_/_Future_Work_chunk1_sub1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 6 Conclusions, Limitations and Societal Impact Imagen showcases the effectiveness of frozen large pretrained language models as text encoders for the text-to-image generation using diffusion models. Our observation that scaling the size of these language models have significantly more impact than scaling the U-Net size on overall performance encourages future research directions on exploring even bigger language models as text encoders. Furthermore, through Imagen we re-emphasize the importance of classifier-free guidance, and we introduce dynamic thresholding, which allows usage of much higher guidance weights than seen in previous works. With these novel components, Imagen produces 1024 1024 samples with unprecedented photorealism and alignment with text. Our primary aim with Imagen is to advance research on generative methods, using text-to-image synthesis as a test bed. While end-user applications of generative methods remain largely out of scope, we recognize the potential downstream applications of this research are varied and may impact society in complex ways. On the one hand, generative models have a great potential to complement, extend, and augment human creativity [30]. Text-to-image generation models, in particular, have the potential to extend image-editing capabilities and lead to the development of new tools for creative practitioners. On the other hand, generative methods can be leveraged for malicious purposes, including harassment and misinformation spread [20], and raise many concerns regarding social and cultural exclusion and bias [67, 62, 68]. These considerations inform our decision to not to release code or a public demo. In future work we will explore a framework for responsible externalization that balances the value of external auditing with the risks of unrestricted open-access. Another ethical challenge relates to the large scale data requirements of text-to-image models, which have have led researchers to rely heavily on large, mostly uncurated, web-scraped datasets. While this approach has enabled rapid algorithmic advances in recent years, datasets of this nature have been critiqued and contested along various ethical dimensions. For example, public and academic discourse regarding appropriate use of public data has raised concerns regarding data subject awareness and consent [24, 18, 60, 43]. Dataset audits have revealed these datasets tend to reflect social stereotypes, oppressive viewpoints, and derogatory, or otherwise harmful, associations to marginalized identity groups [44, 4]. Training text-to-image models on this data risks reproducing these associations and causing significant representational harm that would disproportionately impact individuals and communities already experiencing marginalization, discrimination and exclusion within society. As such, there are a multitude of data challenges that must be addressed before text-to-image models like Imagen can be safely integrated into user-facing applications. While we do not directly address these challenges in this work, an awareness of the limitations of our training data guide our decision not to release Imagen for public use. We strongly caution against the use text-to-image generation methods for any user-facing tools without close care and attention to the contents of the training dataset. Imagen’s training data was drawn from several pre-existing datasets of image and English alt-text pairs. A subset of this data was filtered to removed noise and undesirable content, such as pornographic imagery and toxic language. However, a recent audit of one of our data sources, LAION-400M [61], uncovered a wide range of inappropriate content including pornographic imagery, racist slurs, and harmful social stereotypes [4]. This finding informs our assessment that Imagen is not suitable for public use at this time and also demonstrates the value of rigorous dataset audits and comprehensive dataset documentation (e.g. [23, 45]) in informing consequent decisions about the model’s appropriate and safe use. Imagen also relies on text encoders trained on uncurated web-scale data, and thus inherits the social biases and limitations of large language models [5, 3, 50]. consequence of dataset bias. Second, Imagen exhibits serious limitations when generating images depicting people. Our human evaluations found Imagen obtains significantly higher preference rates when evaluated on images that do not portray people, indicating a degradation in image fidelity. Finally, our preliminary assessment also suggests Image",
    "output": "To paraphrase Andreessen Horowitz, generative AI, particularly on the text-to-art side, is eating the world. At least, investors believe so — judging by the billions of dollars they’ve poured into startups developing AI that creates text and images from prompts.\nNot to be left behind, Big Tech is investing in its own generative AI art solutions, whether through partnerships with the aforementioned startups or in-house R&D. (See: Microsoft teaming up with OpenAI for Image Creator.) Google, leveraging its robust R&D wing, has decided to go the latter route, commercializing its work in generative AI to compete with the platforms already out there.\nEditing tools and follow-up prompts refine the Imagen-generated images, for example adjusting the color of the objects depicted in them. Vertex also offers upscaling for sharpening images, in addition to fine-tuning that allows customers to steer Imagen toward certain styles and preferences.\n“We make it very easy for people to start working with generative AI and their images,” he added.\nOf course, there’s a host of ethical and legal challenges associated with all forms of generative AI — no matter how polished the UI. AI models like Imagen “learn” to generate images from text prompts by “training” on existing images, which often come from datasets that were scraped together by trawling public image hosting websites. Some experts suggest that training models using public images, even copyrighted ones, will be covered by the fair use doctrine in the U.S. But it’s a matter that’s unlikely to be settled anytime soon.\nOne solution — not to the problem of ownership, per se, but to questions around copyrighted training data — is allowing artists to “opt out” of AI training altogether. AI startup Spawning is attempting to establish industry-wide standards and tools for opting out of generative AI tech. Adobe is pursuing its own opt-out mechanisms and tooling. So is DeviantArt, which in November launched an HTML-tag-based protection to prohibit software robots from crawling pages for images.\n“We have a very good sense from our consumer properties of the type of content that may not be the kind of content that our customers are looking for these generative AI models to produce,” he continued. “This is an area of significant investment as well as market leadership for Google — for us to make sure that our customers are able to produce the results that they’re looking for that doesn’t harm or damage their brand value.”\nTo that end, Google is launching reinforcement learning from human feedback (RLHF) as a managed service offering in Vertex, which it claims will help organizations maintain model performance over time and deploy safer — and measurably more accurate — models in production. RLHF, a popular technique in machine learning, trains a “reward model” directly from human feedback, like asking contract workers to rate responses from an AI chatbot. It then uses this reward model to optimize a generative AI model along the lines of Imagen.\nBeyond Imagen, several other generative AI models are now available to select Vertex customers, Google announced today: Codey and Chirp.\nCodey, Google’s answer to GitHub’s Copilot, can generate code in over 20 languages including Go, Java, JavaScript, Python and TypeScript. Codey can suggest the next few lines based on the context of code entered into a prompt or, like OpenAI’s ChatGPT, the model can answer questions about debugging, documentation and high-level coding concepts."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Conclusion_/_Future_Work_chunk1_sub2",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Conclusion / Future Work\nn encodes several social biases and stereotypes, including an overall bias towards generating images of people with lighter skin tones and a tendency for images portraying different professions to align with Western gender stereotypes. Even when we focus generations away from people, our preliminary analysis indicates Imagen encodes a range of social and cultural biases when generating images of activities, events, and objects. While there has been extensive work auditing image-to-text",
    "output": "To paraphrase Andreessen Horowitz, generative AI, particularly on the text-to-art side, is eating the world. At least, investors believe so — judging by the billions of dollars they’ve poured into startups developing AI that creates text and images from prompts.\nNot to be left behind, Big Tech is investing in its own generative AI art solutions, whether through partnerships with the aforementioned startups or in-house R&D. (See: Microsoft teaming up with OpenAI for Image Creator.) Google, leveraging its robust R&D wing, has decided to go the latter route, commercializing its work in generative AI to compete with the platforms already out there.\nEditing tools and follow-up prompts refine the Imagen-generated images, for example adjusting the color of the objects depicted in them. Vertex also offers upscaling for sharpening images, in addition to fine-tuning that allows customers to steer Imagen toward certain styles and preferences.\n“We make it very easy for people to start working with generative AI and their images,” he added.\nOf course, there’s a host of ethical and legal challenges associated with all forms of generative AI — no matter how polished the UI. AI models like Imagen “learn” to generate images from text prompts by “training” on existing images, which often come from datasets that were scraped together by trawling public image hosting websites. Some experts suggest that training models using public images, even copyrighted ones, will be covered by the fair use doctrine in the U.S. But it’s a matter that’s unlikely to be settled anytime soon.\nOne solution — not to the problem of ownership, per se, but to questions around copyrighted training data — is allowing artists to “opt out” of AI training altogether. AI startup Spawning is attempting to establish industry-wide standards and tools for opting out of generative AI tech. Adobe is pursuing its own opt-out mechanisms and tooling. So is DeviantArt, which in November launched an HTML-tag-based protection to prohibit software robots from crawling pages for images.\n“We have a very good sense from our consumer properties of the type of content that may not be the kind of content that our customers are looking for these generative AI models to produce,” he continued. “This is an area of significant investment as well as market leadership for Google — for us to make sure that our customers are able to produce the results that they’re looking for that doesn’t harm or damage their brand value.”\nTo that end, Google is launching reinforcement learning from human feedback (RLHF) as a managed service offering in Vertex, which it claims will help organizations maintain model performance over time and deploy safer — and measurably more accurate — models in production. RLHF, a popular technique in machine learning, trains a “reward model” directly from human feedback, like asking contract workers to rate responses from an AI chatbot. It then uses this reward model to optimize a generative AI model along the lines of Imagen.\nBeyond Imagen, several other generative AI models are now available to select Vertex customers, Google announced today: Codey and Chirp.\nCodey, Google’s answer to GitHub’s Copilot, can generate code in over 20 languages including Go, Java, JavaScript, Python and TypeScript. Codey can suggest the next few lines based on the context of code entered into a prompt or, like OpenAI’s ChatGPT, the model can answer questions about debugging, documentation and high-level coding concepts."
  },
  {
    "id_paper": "11",
    "id_story": "11-05",
    "id_chunk": "11-05_Conclusion_/_Future_Work_chunk2",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Conclusion / Future Work\nand image labeling models for forms of social bias (e.g. [8, 9, 68]), there has been comparatively less work on social bias evaluation methods for text-to-image models, with the recent exception of [10]. We believe this is a critical avenue for future research and we intend to explore benchmark evaluations for social and cultural bias in future work—for example, exploring whether it is possible to generalize the normalized pointwise mutual information metric [1] to the measurement of biases in image generation models. There is also a great need to develop a conceptual vocabulary around potential harms of text-to-image models that could guide the development of evaluation metrics and inform responsible model release. We aim to address these challenges in future work.",
    "output": "To paraphrase Andreessen Horowitz, generative AI, particularly on the text-to-art side, is eating the world. At least, investors believe so — judging by the billions of dollars they’ve poured into startups developing AI that creates text and images from prompts.\nNot to be left behind, Big Tech is investing in its own generative AI art solutions, whether through partnerships with the aforementioned startups or in-house R&D. (See: Microsoft teaming up with OpenAI for Image Creator.) Google, leveraging its robust R&D wing, has decided to go the latter route, commercializing its work in generative AI to compete with the platforms already out there.\nEditing tools and follow-up prompts refine the Imagen-generated images, for example adjusting the color of the objects depicted in them. Vertex also offers upscaling for sharpening images, in addition to fine-tuning that allows customers to steer Imagen toward certain styles and preferences.\n“We make it very easy for people to start working with generative AI and their images,” he added.\nOf course, there’s a host of ethical and legal challenges associated with all forms of generative AI — no matter how polished the UI. AI models like Imagen “learn” to generate images from text prompts by “training” on existing images, which often come from datasets that were scraped together by trawling public image hosting websites. Some experts suggest that training models using public images, even copyrighted ones, will be covered by the fair use doctrine in the U.S. But it’s a matter that’s unlikely to be settled anytime soon.\nOne solution — not to the problem of ownership, per se, but to questions around copyrighted training data — is allowing artists to “opt out” of AI training altogether. AI startup Spawning is attempting to establish industry-wide standards and tools for opting out of generative AI tech. Adobe is pursuing its own opt-out mechanisms and tooling. So is DeviantArt, which in November launched an HTML-tag-based protection to prohibit software robots from crawling pages for images.\n“We have a very good sense from our consumer properties of the type of content that may not be the kind of content that our customers are looking for these generative AI models to produce,” he continued. “This is an area of significant investment as well as market leadership for Google — for us to make sure that our customers are able to produce the results that they’re looking for that doesn’t harm or damage their brand value.”\nTo that end, Google is launching reinforcement learning from human feedback (RLHF) as a managed service offering in Vertex, which it claims will help organizations maintain model performance over time and deploy safer — and measurably more accurate — models in production. RLHF, a popular technique in machine learning, trains a “reward model” directly from human feedback, like asking contract workers to rate responses from an AI chatbot. It then uses this reward model to optimize a generative AI model along the lines of Imagen.\nBeyond Imagen, several other generative AI models are now available to select Vertex customers, Google announced today: Codey and Chirp.\nCodey, Google’s answer to GitHub’s Copilot, can generate code in over 20 languages including Go, Java, JavaScript, Python and TypeScript. Codey can suggest the next few lines based on the context of code entered into a prompt or, like OpenAI’s ChatGPT, the model can answer questions about debugging, documentation and high-level coding concepts."
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Abstract\nAbstract: The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2–6. Here we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture that is capable of predicting the joint structure of complexes including proteins, nucleic acids, small molecules, ions and modified residues. The new AlphaFold model demonstrates substantially improved accuracy over many previous specialized tools: far greater accuracy for protein–ligand interactions compared with state-of-the-art docking tools, much higher accuracy for protein–nucleic acid interactions compared with nucleic-acid-specific predictors and substantially higher antibody–antigen prediction accuracy compared with AlphaFold-Multimer v.2.37,8. Together, these results show that high-accuracy modelling across biomolecular space is possible within a single unified deep-learning framework.",
    "output": "The latest iteration of the protein-structure-prediction algorithm AlphaFold has generated a great deal of interest since its release, accompanied by a url in Nature, earlier this month1. Unlike its predecessor AlphaFold2, AlphaFold3 can predict not just the structures of protein complexes, but also when proteins interact with other kinds of molecule, including DNA and RNA. The artificial intelligence tool will be important in both fundamental research and drug discovery.\nBut its release has also prompted questions, and criticism, of both the AlphaFold team at Google DeepMind in London and Nature. When AlphaFold2 was published2, the full underlying code was made accessible to all researchers. But AlphaFold3 comes with ‘pseudocode’ — a detailed description of what the code can do and how it works.\nMajor AlphaFold upgrade offers boost for drug discovery\nThe basics of how the community can use the new version of AlphaFold remain the same: anyone with a Google account can use the tool for free, for non-commercial applications. But there are important differences between the latest iteration and previous ones. Notably, for AlphaFold2, the DeepMind team worked with the European Molecular Biology Laboratory’s European Bioinformatics Institute, an intergovernmental (and publicly funded) research organization based in Cambridge, UK. DeepMind researchers have trained their software on thousands of published records of protein structures and their known shapes."
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Introduction\nIntroduction: Accurate models of biological complexes are critical to our understanding of cellular functions and for the rational design of thera­ peutics2–4,9. Enormous progress has been achieved in protein structure prediction with the development of AlphaFold1, and the field has grown tremendously with a number of later methods that build on the ideas and techniques of AlphaFold 2 (AF2)10–12. Almost immediately after AlphaFold became available, it was shown that simple input modifications would enable surprisingly accurate protein interaction predictions13–15 and that training AF2 specifically for protein inter­ action prediction yielded a highly accurate system7. These successes lead to the question of whether it is possible to accurately predict the structure of complexes containing a much wider range of biomolecules, including ligands, ions, nucleic acids and modified residues, within a deep-learning framework. A wide range of predictors for various specific interaction types has been developed16–28, as well as one generalist method developed concurrently with the present work29, but the accuracy of such deep-learning attempts has been mixed and often below that of physics-inspired methods30,31. Almost all of these methods are also highly specialized to particular interaction types and cannot predict the structure of general biomolecular complexes containing many types of entities. Here we present AlphaFold 3 (AF3)—a model that is capable of high-accuracy prediction of complexes containing nearly all molecular types present in the Protein Data Bank32 (PDB) (Fig. 1a,b). In all but one category, it achieves a substantially higher performance than strong methods that specialize in just the given task (Fig. 1c and Extended Data Table 1), including higher accuracy at protein structure and the structure of protein–protein interactions. This is achieved by a substantial evolution of the AF2 architecture and training procedure (Fig. 1d) both to accommodate more general chemical structures and to improve the data efficiency of learning. The system reduces the amount of multiple-sequence alignment (MSA) processing by replacing the AF2 evoformer with the simpler pairformer module (Fig. 2a). Furthermore it directly predicts the raw atom coordinates with a diffusion module, replacing the AF2 structure module that operated on amino-acid-specific frames and side-chain torsion angles (Fig. 2b). The multiscale nature of the diffusion process (low noise levels induce the network to improve local structure) also enable us to eliminate stereochemical losses and most special handling of bonding patterns in the network, easily accommodating arbitrary chemical components.",
    "output": "A technology cluster in Cambridge, Massachusetts, gives researchers from various sectors somewhere to collaborate easily.Credit: Brooks Kraft LLC/Corbis/Getty The latest iteration of the protein-structure-prediction algorithm AlphaFold has generated a great deal of interest since its release, accompanied by a url in Nature, earlier this month1. Unlike its predecessor AlphaFold2, AlphaFold3 can predict not just the structures of protein complexes, but also when proteins interact with other kinds of molecule, including DNA and RNA. The artificial intelligence tool will be important in both fundamental research and drug discovery. But its release has also prompted questions, and criticism, of both the AlphaFold team at Google DeepMind in London and Nature. When AlphaFold2 was published2, the full underlying code was made accessible to all researchers. But AlphaFold3 comes with ‘pseudocode’ — a detailed description of what the code can do and how it works. Major AlphaFold upgrade offers boost for drug discovery This was not a decision we took lightly, and this editorial briefly explains our reasoning. We think that research, regardless of the sector that does it, should be evaluated through peer review and published for the benefit of society and science. At the same time, we have no wish for this to be the final word. This is an opportunity for an important conversation among all research stakeholders at a time when the majority of global research is privately funded. The basics of how the community can use the new version of AlphaFold remain the same: anyone with a Google account can use the tool for free, for non-commercial applications. But there are important differences between the latest iteration and previous ones. Notably, for AlphaFold2, the DeepMind team worked with the European Molecular Biology Laboratory’s European Bioinformatics Institute, an intergovernmental (and publicly funded) research organization based in Cambridge, UK. DeepMind researchers have trained their software on thousands of published records of protein structures and their known shapes. Now, DeepMind has partnered with Isomorphic Labs, a London-based drug-development company owned by Google’s parent, Alphabet. In addition to the non-availability of the full code, there are other restrictions on the use of the tool — for example, in drug development. There are also daily limits on the numbers of predictions that individual researchers can perform. Many researchers are disappointed by these constraints. In an open letter to Nature (see go.nature.com/3k9acav), its authors point out that the lack of an open-source code limits verification and reproducibility efforts. The letter has had an impact. Shortly after it was published, DeepMind’s vice-president of research, Pushmeet Kohli, posted on the social-media platform X that the team is “working on releasing the AF3 model (incl weights) for academic use” within six months. This is an important step, and Nature will update the published url once the code is released. AI’s potential to accelerate drug discovery needs a reality check But why allow the complete data and code to be restricted at the time of publication? Nature’s editorial policies have been developed to support open science and state: “Authors are required to make materials, data, code, and associated protocols promptly available to readers without undue qualifications.” One way we enable researchers to do so is through a partnership with Code Ocean, a computational-science platform that makes the information reproducible and traceable. But our policies also acknowledge that there can be restrictions, which “must be disclosed to the editors at the time of submission” and agreed with editors. The policy also states: “Any reason that would preclude the need for code or algorithm sharing will be evaluated by the editors who reserve the right to decline the url if important code is unavailable.” Reasons for restrictions include a lack of discipline-wide data-reporting standards or of the technological infrastructure necessary for depositing data openly and in structured repositories. In other cases, confidentiality might have to be protected, or data might be withheld for safety, security or legal reasons. Similar principles apply to the availability of code, which is why Nature’s policies include an option for releasing the training model and pseudocode or limited code, as attested by examples of studies we have published in chemistry3, climate change4 and virology5. The private sector funds most global research and development, and many of the results of such work are not published in peer-reviewed journals. We at Nature think it’s important that journals engage with the private sector and work with its scientists so they can submit their research for peer review and publication. This promotes the sharing of knowledge,"
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Method\nMethod: Network architecture and training The overall structure of AF3 (Fig. 1d and Supplementary Methods 3) echoes that of AF2, with a large trunk evolving a pairwise representation of the chemical complex followed by a structure module that uses the pairwise representation to generate explicit atomic positions, but there are large differences in each major component. These modifications were driven both by the need to accommodate a wide range of chemical entities without excessive special casing and by observations of AF2 performance with different modifications. Within the trunk, MSA processing is substantially de-emphasized, with a much smaller and simpler MSA embedding block (Supplementary Methods 3.3). Compared with the original evoformer from AF2, the number of blocks is reduced to four, the processing of the MSA representation uses an inexpensive pair-weighted averaging and only the pair representation is used for later processing steps. The ‘pairformer’ (Fig. 2a and Supplementary Methods 3.6) replaces the evoformer of AF2 as the dominant processing block. It operates only on the pair representation and the single representation; the MSA representation is not retained and all information passes through the pair representation. The pair processing and the number of blocks (48) is largely unchanged from AF2. The resulting pair and single representation together with the input representation are passed to the new diffusion module (Fig. 2b) that replaces the structure module of AF2. The diffusion module (Fig. 2b and Supplementary Methods 3.7) operates directly on raw atom coordinates, and on a coarse abstract token representation, without rotational frames or any equivariant processing. We had observed in AF2 that removing most of the complexity of the structure module had only a modest effect on the prediction accuracy, and maintaining the backbone frame and side-chain torsion representation add quite a bit of complexity for general molecular graphs. Similarly AF2 required carefully tuned stereochemical violation penalties during training to enforce chemical plausibility of the resulting structures. We use a relatively standard diffusion approach33 in which the diffusion model is trained to receive ‘noised’ atomic coordinates and then predict the true coordinates. This task requires the network to learn protein structure at a variety of length scales, whereby the denoising task at small noise emphasizes understanding very local stereochemistry and the denoising task at high noise emphasizes the large-scale structure of the system. At the inference time, random noise is sampled and then recurrently denoised to produce a final structure. Importantly, this is a generative training procedure that produces a distribution of answers. This means that, for each answer, the local structure will be sharply defined (for example, side-chain bond geometry) even when the network is uncertain about the positions. For this reason, we are able to avoid both torsion-based parametrizations of the residues and violation losses on the structure, while handling the full complexity of general ligands. Similarly to some recent work34, we find that no invariance or equivariance with respect to global rotations and translation of the molecule are required in the architecture and we therefore omit them to simplify the machine learning architecture. The use of a generative diffusion approach comes with some technical challenges that we needed to address. The biggest issue is that generative models are prone to hallucination35, whereby the model may invent plausible-looking structure even in unstructured regions. To counteract this effect, we use a cross-distillation method in which we enrich the training data with structures predicted by AlphaFold-Multimer (v.2.3)7,8 . In these structures, unstructured regions are typically represented by long extended loops instead of compact structures, and training on them ‘teaches’ AF3 to mimic this behaviour. This cross-distillation greatly reduced the hallucination behaviour of AF3 (Extended Data Fig. 1 for disorder prediction results on the CAID 236 benchmark set). We also developed confidence measures that predict the atom-level and pairwise errors in our final structures. In AF2, this was done directly by",
    "output": "Many researchers are disappointed by these constraints. In an open letter to Nature (see go.nature.com/3k9acav), its authors point out that the lack of an open-source code limits verification and reproducibility efforts. The letter has had an impact. Shortly after it was published, DeepMind’s vice-president of research, Pushmeet Kohli, posted on the social-media platform X that the team is “working on releasing the AF3 model (incl weights) for academic use” within six months. This is an important step, and Nature will update the published url once the code is released."
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Method\nregressing the error in the output of the structure module during training. However, this procedure is not applicable to diffusion training, as only a single step of the diffusion is trained instead of a full-structure generation (Fig. 2c). To remedy this, we developed a diffusion ‘rollout’ procedure for the full-structure prediction generation during training (using a larger step size than normal; Fig. 2c (mini-rollout)). This predicted structure is",
    "output": "Many researchers are disappointed by these constraints. In an open letter to Nature (see go.nature.com/3k9acav), its authors point out that the lack of an open-source code limits verification and reproducibility efforts. The letter has had an impact. Shortly after it was published, DeepMind’s vice-president of research, Pushmeet Kohli, posted on the social-media platform X that the team is “working on releasing the AF3 model (incl weights) for academic use” within six months. This is an important step, and Nature will update the published url once the code is released."
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Method\nthen used to permute the symmetric ground-truth chains and ligands, and to compute the performance metrics to train the confidence head. The confidence head uses the pairwise representation to predict a modified local distance difference test (pLDDT) and a predicted aligned error (PAE) matrix as in AF2, as well as a distance error matrix (PDE), which is the error in the distance matrix of the predicted structure as compared to the true structure (details are provided in Supplementary Methods 4.3). Figure 2d shows that, during initial training, the model learns quickly to predict the local structures (all intrachain metrics go up quickly and reach 97% of the maximum performance within the first 20,000 training steps), while the model needs considerably longer to learn the global constellation (the interface metrics go up slowly and protein–protein interface LDDT passes the 97% bar only after 60,000 steps). During AF3 development, we observed that some model abilities topped out relatively early and started to decline (most likely due to overfitting to the limited number of training samples for this capability), while other abilities were still undertrained. We addressed this by increasing or decreasing the sampling probability for the corresponding training sets (Supplementary Methods 2.5.1) and by performing early stopping using a weighted average of all of the above metrics and some additional metrics to select the best model checkpoint (Supplementary Table 7). The fine-tuning stages with the larger crop sizes improve the model on all metrics with an especially high uplift on protein–protein interfaces (Extended Data Fig. 2).Accuracy across complex types AF3 can predict structures from input polymer sequences, residue modifications and ligand SMILES (simplified molecular-input line-entry system). In Fig. 3 we show a selection of examples highlighting the ability of the model to generalize to a number of biologically important and therapeutically relevant modalities. In selecting these examples, we considered novelty in terms of the similarity of individual chains and interfaces to the training set (additional information is provided in Supplementary Methods 8.1). We evaluated the performance of the system on recent interfacespecific benchmarks for each complex type (Fig. 1c and Extended Data Table 1). Performance on protein–ligand interfaces was evaluated on the PoseBusters benchmark set, which is composed of 428 protein–ligand structures released to the PDB in 2021 or later. As our standard training cut-off date is in 2021, we trained a separate AF3 model with an earlier training-set cutoff (Methods). Accuracy on the PoseBusters set is reported as the percentage of protein–ligand pairs with pocket-aligned ligand root mean squared deviation (r.m.s.d.) of less than 2 . The baseline models come in two categories: those that use only protein sequence and ligand SMILES as an input and those that additionally leak information from the solved protein–ligand test structure. Traditional docking methods use the latter privileged information, even though that information would not be available in real-world use cases. Even so, AF3 greatly outperforms classical docking tools such as Vina37,38 even while not using any structural inputs (Fisher’s exact test, P = 2.27 1013) and greatly outperforms all other true blind docking like RoseTTAFold All-Atom (P = 4.45 1025). Extended Data Fig. 3 shows three examples in which AF3 achieves accurate predictions but docking tools Vina and Gold do not37. PoseBusters analysis was performed using a training cut-off of 30 September 2019 for AF3 to ensure that the model was not trained on any PoseBusters structures. To compare with the RoseTTAFold All-Atom results, we used PoseBusters version 1. Version 2 (crystal contacts removed from the benchmark set) results including quality metrics are shown in Extended Data Fig. 4b–f and Extended Data Table 1. We use multiple seeds to ensure correct chirality and avoid slight protein–ligand clashing (as",
    "output": "Many researchers are disappointed by these constraints. In an open letter to Nature (see go.nature.com/3k9acav), its authors point out that the lack of an open-source code limits verification and reproducibility efforts. The letter has had an impact. Shortly after it was published, DeepMind’s vice-president of research, Pushmeet Kohli, posted on the social-media platform X that the team is “working on releasing the AF3 model (incl weights) for academic use” within six months. This is an important step, and Nature will update the published url once the code is released."
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Method\nopposed to a method like diffusion guidance to enforce) but we are typically able to produce high-quality stereochemistry. Separately, we also train a version of AF3 that receives the ‘pocket information’ as used in some recent deep-learning work24,26 (the results are shown in Extended Data Fig. 4a). AF3 predicts protein–nucleic complexes and RNA structures",
    "output": "Many researchers are disappointed by these constraints. In an open letter to Nature (see go.nature.com/3k9acav), its authors point out that the lack of an open-source code limits verification and reproducibility efforts. The letter has had an impact. Shortly after it was published, DeepMind’s vice-president of research, Pushmeet Kohli, posted on the social-media platform X that the team is “working on releasing the AF3 model (incl weights) for academic use” within six months. This is an important step, and Nature will update the published url once the code is released."
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Method\nwith higher accuracy than RoseTTAFold2NA15 (Fig. 1c (second plot)). As RoseTTAFold2NA is validated only on structures below 1,000 residues, we use only structures below 1,000 residues from our recent PDB evaluation set for this comparison (Methods). AF3 is able to predict protein–nucleic structures with thousands of residues, an example of which is shown in Fig. 3a. Note that we do not compare directly to RoseTTAFold All-Atom, but benchmarks indicate that RoseTTAFold All-Atom is slightly less accurate than RoseTTAFold2NA for nucleic acid predictions29. We also evaluated AF3 performance on the ten publicly available Critical Assessment of Structure Prediction 15 (CASP15) RNA targets: we achieve a higher average performance than RoseTTAFold2NA and AIchemy_RNA27 (the best AI-based submission in CASP1518,31) on the respective common subsets of our and their predictions (detailed results are shown in Extended Data Fig. 5a). We did not reach the performance of the best human-expert-aided CASP15 submission AIchemy_RNA239 (Fig. 1c (centre left)). Owing to limited dataset sizes, we do not report significance test statistics here. Further analysis of the accuracy of predicting nucleic acids alone (without proteins) is shown in Extended Data Fig. 5b. Covalent modifications (bonded ligands, glycosylation, and modified protein residues and nucleic acid bases) are also accurately predicted by AF3 (Fig. 1c (centre right)). Modifications include those to any polymer residue (protein, RNA or DNA). We report accuracy as the percentage of successful predictions (pocket r.m.s.d. 2 ). We apply quality filters to the bonded ligands and glycosylation dataset (as does PoseBusters): we include only ligands with high-quality experimental data (ranking_model_fit > 0.5, according to the RCSB structure validation report, that is, X-ray structures with a model quality above the median). As with the PoseBusters set, the bonded ligands and glycosylation datasets are not filtered by homology to the training dataset. Filtering on the basis of the bound polymer chain homology (using polymer template similarity 40) yielded only five clusters for bonded ligands and seven clusters for glycosylation. We exclude multi-residue glycans here because the RCSB validation report does not provide a ranking_model_fit value for them. The percentage of successful predictions (pocket r.m.s.d. 2 ) for multi-residue glycans on all-quality experimental data is 42.1% (n = 131 clusters), which is slightly lower than the success rate for single-residue glycans on all-quality experimental data of 46.1% (n = 167). The modified residues dataset is filtered similarly to our other polymer test sets: it contains only modified residues in polymer chains with low homology to the training set (Methods). See Extended Data Table 1 for detailed results, and Extended Data Fig. 6 for examples of predicted protein, DNA and RNA structures with covalent modifications, including analysis of the impact of phosphorylation on predictions. While expanding in modelling abilities, AF3 has also improved in protein complex accuracy relative to AlphaFold-Multimer (v.2.3)7,8 . Generally, protein–protein prediction success (DockQ > 0.23)40 has increased (paired Wilcoxon signed-rank test, P = 1.8 1018), with antibody–protein interaction prediction in particular showing a marked improvement (Fig. 1c (right); paired Wilcoxon signed-rank test, P = 6.5 105, predictions top-ranked from 1,000 rather than the typical 5 seeds; further details are provided in Fig. 5a). Protein monomer LD",
    "output": "Many researchers are disappointed by these constraints. In an open letter to Nature (see go.nature.com/3k9acav), its authors point out that the lack of an open-source code limits verification and reproducibility efforts. The letter has had an impact. Shortly after it was published, DeepMind’s vice-president of research, Pushmeet Kohli, posted on the social-media platform X that the team is “working on releasing the AF3 model (incl weights) for academic use” within six months. This is an important step, and Nature will update the published url once the code is released."
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Method\nDT improvement is also significant (paired Wilcoxon signed-rank test, P = 1.7 1034). AF3 has a very similar dependence on MSA depth to AlphaFold-Multimer v.2.3; proteins with shallow MSAs are predicted with lower accuracy (a comparison of the dependence of single-chain LDDT on MSA depth is shown in Extended Data Fig.",
    "output": "Many researchers are disappointed by these constraints. In an open letter to Nature (see go.nature.com/3k9acav), its authors point out that the lack of an open-source code limits verification and reproducibility efforts. The letter has had an impact. Shortly after it was published, DeepMind’s vice-president of research, Pushmeet Kohli, posted on the social-media platform X that the team is “working on releasing the AF3 model (incl weights) for academic use” within six months. This is an important step, and Nature will update the published url once the code is released."
  },
  {
    "id_paper": "58",
    "id_story": "58-02",
    "id_chunk": "58-02_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Student\n\nSection: Method\n7a). Predicted confidences track accuracy As with AF2, AF3 confidence measures are well calibrated with accuracy. Our confidence analysis is performed on the recent PDB evaluation set, with no homology filtering and including peptides. The ligands category is filtered to high-quality experimental structures as described above, and considers standard non-bonded ligands only. See Extended Data Fig. 8 for a similar assessment on bonded ligand and other interfaces. All statistics are cluster-weighted (Methods) and consider the top-ranked prediction only (ranking details are provided in Supplementary Methods 5.9.3). In Fig. 4a (top row), we plot the chain pair interface-predicted TM (ipTM) score41 (Supplementary Methods 5.9.1) against interface accuracy measures: protein–protein DockQ, protein–nucleic interface LDDT (iLDDT) and protein–ligand success, with success defined as the percentage of examples under thresholded pocket-aligned r.m.s.d. values. In Fig. 4a (bottom row), we plot the average pLDDT per protein, nucleotide or ligand entity against our bespoke LDDT_to_polymer metric (metrics details are provided in the Methods), which is closely related to the training target of the pLDDT predictor. In Fig. 4b–e, we highlight a single example prediction of 7T82, in which per-atom pLDDT colouring identifies unconfident chain tails, somewhat confident interfaces and otherwise confident secondary structure. In Fig. 4c, the same prediction is coloured by chain, along with DockQ interface scores in Fig. 4d and per-chain colouring displayed on the axes for reference. We see from Fig. 4e that PAE confidence is high for pink–grey and blue–orange residue pairs for which DockQ > 0.7, and least confident about pink–orange and pink–blue residue pairs that have DockQ  0. A similar PAE analysis of an example with protein and nucleic acid chains is shown in Extended Data Fig. 5c,d.",
    "output": "Many researchers are disappointed by these constraints. In an open letter to Nature (see go.nature.com/3k9acav), its authors point out that the lack of an open-source code limits verification and reproducibility efforts. The letter has had an impact. Shortly after it was published, DeepMind’s vice-president of research, Pushmeet Kohli, posted on the social-media platform X that the team is “working on releasing the AF3 model (incl weights) for academic use” within six months. This is an important step, and Nature will update the published url once the code is released."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Abstract\nAbstract: Among individuals with diabetes, the prevalence of diabetic retinopathy is approximately 28.5% in the United States1 and 18% in India.2 Most guidelines recommend annual screening for those with no retinopathy or mild diabetic retinopathy, repeat examination in 6 months for moderate diabetic retinopathy, and an ophthalmologist referral for treatment evaluation within a few weeks to months for severe or worse diabetic retinopathy or the presence of referable diabetic macular edema, known as clinically significant macular edema.3 Referable diabetic retinopathy has been defined as moderate or worse diabetic retinopathy or referable diabetic macular edema,4 given that recommended managementchangesfromyearlyscreeningtocloserfollow-upatmoderate disease severity.3 Retinal photography with manual interpretation is a widely accepted screening tool for diabetic retinopathy, with performance that can exceed that of inperson dilated eye examinations.3 Automated grading of diabetic retinopathy has potential benefits such as increasing efficiency, reproducibility, and coverage of screening programs; reducing barriers to access; and improving patient outcomes by providing early detection and treatment. To maximize the clinical utility of automated grading, an algorithm to detect referable diabetic retinopathy is needed. Machine learning (a discipline within computer science that focuses on teaching machines to detect patterns in data) has been leveraged for a variety of classification tasks including automated classification of diabetic retinopathy. However, much of the work has focused on “featureengineering,” which involves computing explicit features specified by experts, resulting in algorithms designed to detect specific lesions or predicting the presence of any level of diabetic retinopathy.5 Deep learning6 is a machine learning technique that avoids such engineering by learning the most predictive features directly from the images given a large data set of labeled examples. This technique uses an optimization algorithm called back-propagation to indicate how a machine should change its internal parameters to best predict the desired output of an image. In this study, deep learning7,8 was used to train an algorithm to detect referable diabetic retinopathy and assess the performance of the algorithm in 2 clinical validation sets.",
    "output": "Google's artificial intelligence can play the ancient game of Go better than any human. It can identify faces, recognize spoken words, and pull answers to your questions from the web. But the promise is that this same kind of technology will soon handle far more serious work than playing games and feeding smartphone apps. One day, it could help care for the human body. Demonstrating this promise, Google researchers have worked with doctors to develop an AI that can automatically identify diabetic retinopathy, a leading cause blindness among adults. Using deep learning—the same breed of AI that identifies faces, animals, and objects in pictures uploaded to Google's online services—the system detects the condition by examining retinal photos. In a recent study, it succeeded at about the same rate as human opthamologists, according to a paper published today in the Journal of the American Medical Association. \"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google. But the idea behind this AI isn't to replace doctors. Blindness is often preventable if diabetic retinopathy is caught early. The hope is that the technology can screen far more people for the condition than doctors could on their own, particularly in countries where healthcare is limited, says Peng. The project began, she says, when a Google researcher realized that doctors in his native India were struggling to screen all the locals that needed to be screened. In many places, doctors are already using photos to diagnose the condition without seeing patients in person. \"This is a well validated technology that can bring screening services to remote locations where diabetic retinal eye screening is less available,\" says David McColloch, a clinical professor of medicine at the University of Washington who specializes in diabetes. That could provide a convenient on-ramp for an AI that automates the process. Peng's project is part of a much wider effort to detect disease and illness using deep neural networks, pattern recognition systems that can learn discrete tasks by analyzing vast amounts of data. Researchers at DeepMind, a Google AI lab in London, have teamed with Britain's National Health Service to build various technologies that can automatically detect when patients are at risk of disease and illness, and several other companies, including Salesforce.com and a startup called Enlitic, are exploring similar systems. At Kaggle, an internet site where data scientists compete to solve real-world problems using algorithms, groups have worked to build their own machine learning systems that can automatically identify diabetic retinopathy. Medical Brains Peng is part of Google Brain, a team inside the company that provides AI software and services for everything from search to security to Android. Within this team, she now leads a group spanning dozens of researchers that focuses solely on medical applications for AI. The work on diabetic retinopathy started as a \"20 Percent project\" about two years ago, before becoming a full-time effort. Researchers began working with hospitals in the Indian cities of Aravind and Sankara that were already collecting retinal photos for doctors to examine. Then the Google team asked more than four dozen doctors in India and the US to identify photos where mini-aneurysms, hemorrhages, and other issues indicated that diabetic patients could be at risk for blindness. At least three doctors reviewed each photo, before Peng and team fed about 128,000 of these images into their neural network. Ultimately, the system identified the condition slightly more consistently than the original group of doctors. At its most sensitive, the system avoided both false negatives and false positives more than 90 percent of the time, exceeding the National Institutes of Health's recommended standard of at least 80 percent accuracy and precision for diabetic retinopathy screens. Given the success of deep learning algorithms with other machine vision tasks, the results of the original trial aren't surprising. But Yaser Sheikh, a professor of computer science at Carnegie Mellon who is working on other forms of AI for healthcare, says that actually moving this kind of thing into the developing world can be difficult. \"It is the kind of thing that sounds good, but actually making it work has proven to be far more difficult,\" he says. \"Getting technology to actually help in the developing world---there are many, many systematic barriers.\" But Peng and her team are pushing forward. She says Google is now running additional trials with photos taken specifically to train its diagnostic AI. Preliminary results, she says, indicate that the system once again performs as well as trained doctors. The machines, it seems, are gaining new kinds of sight."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nMethod: Methods Data Sets For algorithm development, macula-centered retinal fundus images were retrospectively obtained from EyePACS in the United States and 3 eye hospitals in India (Aravind Eye Hospital, Sankara Nethralaya, and Narayana Nethralaya) among patients presenting for diabetic retinopathy screening. All images were deidentified according to Health Insurance Portability and Accountability Act Safe Harbor prior to transfer to study investigators. Ethics review and institutional review board exemption was obtained using Quorum Review IRB. Two further data sets were used for clinical validation. The first deidentified data set consisted of a random sample Key Points Question How does the performance of an automated deep learning algorithm compare with manual grading by ophthalmologists for identifying diabetic retinopathy in retinal fundus photographs? Finding In 2 validation sets of 9963 images and 1748 images, at the operating point selected for high specificity, the algorithm had 90.3% and 87.0% sensitivity and 98.1% and 98.5% specificity for detecting referable diabetic retinopathy, defined as moderate or worse diabetic retinopathy or referable macular edema by the majority decision of a panel of at least 7 US board-certified ophthalmologists. At the operating point selected for high sensitivity, the algorithm had 97.5% and 96.1% sensitivity and 93.4% and 93.9% specificity in the 2 validation sets. Meaning Deep learning algorithms had high sensitivity and specificity for detecting diabetic retinopathy and macular edema in retinal fundus photographs. of macula-centered images taken at EyePACS screening sites between May 2015 and October 2015. A variety of cameras were used, including Centervue DRS, Optovue iCam, Canon CR1/DGi/CR2, and Topcon NW using 45° fields of view. EyePACS images were acquired as a part of routine clinical care for diabetic retinopathy screening, and approximately 40% of the images were acquired with pupil dilation. This data set did not overlap with the EyePACS data used in development. The second data set was the publicly available Messidor-2 data set,9,10 which has been used by other groups for benchmarking performance of automated detection algorithms for diabetic retinopathy.11-13 The images were obtained between January 2005 and December 2010 at 3 hospitals in France using a Topcon TRC NW6 nonmydriatic camera and 45° fields of view centered on the fovea. Approximately 44% of images were acquired with pupil dilation. Grading All images in the development and clinical validation sets were graded by ophthalmologists for the presence of diabetic retinopathy, diabetic macular edema, and image quality using an annotation tool (eFigures 1 and 2 in the Supplement). Diabetic retinopathy severity (none, mild, moderate, severe, or proliferative) was graded according to the International Clinical Diabetic Retinopathy scale.14 Referable diabetic macular edema was defined as any hard exudates within 1 disc diameter of the macula,15 which is a proxy for macular edema when stereoscopic views are not available. Image quality was assessed by graders using the rubric in the “Grading Instructions” section in the Supplement. Images of excellent, good, and adequate quality were considered gradable. Derivation The 54 graders for the development set were US-licensed ophthalmologists or ophthalmology trainees in their last year of residency (postgraduate year 4). Each individual graded between 20 and 62 508 images (mean, 9774; median, 2021). Trainee performance was not worse than that of licensed ophthalmologists, although only 3 trainees graded more than 1000 images. Intergrader reliability was measured for each physician using pairwise comparisons by taking the number of times a grader was in agreement with another grader over the total number of pairwise comparisons. Approximately 10% of the derivation set (1",
    "output": "\"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\n28 175 images) were randomly selected to be overread by the same grader to determine intragrader reliability. All graders were paid for their work. Validation Graders who were US board-certified ophthalmologists with the highest rate of self-consistency were invited to grade the clinical validation sets, EyePACS-1 (n = 8) and Messidor-2 (n = 7). A simple majority decision (an image was classified as referable",
    "output": "\"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nif 50% of ophthalmologists graded it referable) served as the reference standard for both referability and gradability. Graders were masked to judgments by other graders. (See “Grading Quality Control” section in the Supplement for more details.) Development of the Algorithm Deep learning is the process of training a neural network (a large mathematical function with millions of parameters) to perform a given task. The function computes diabetic retinopathy severity from the intensities of the pixels in a fundus image. Creating or “training” this function requires a large set of images for which the diabetic retinopathy severity is already known (training set). During the training process, the parameters of the neural network (mathematical function) are initially set to random values. Then, for each image, the severity grade given by the function is compared with the known grade from the training set, and parameters of the function are then modified slightly to decrease the error on that image. This process is repeated for every image in the training set many times over, and the function “learns” how to accurately compute the diabetic retinopathy severity from the pixel intensities of the image for all images in the training set. With the right training data, the result is a function general enough to compute diabetic retinopathy severity on new images. The network used in this study is a convolutional neural network that uses a function that first combines nearby pixels into local features, then aggregates those into global features. Although the algorithm does not explicitly detect lesions (eg, hemorrhages, microaneurysms), it likely learns to recognize them using the local features. The specific neural network used in this work is the Inception-v3 architecture proposed by Szegedy et al.8 Data were preprocessed according to a protocol described in the Supplement. The optimization algorithm used to train the network weights was a distributed stochastic gradient descent implementation by Dean et al.16 To speed up the training, batch normalization7 as well as preinitialization using weights from the same network trained to classify objects in the ImageNet data set17 were used. Preinitialization also improved performance. A single network was trained to make multiple binary predictions, including whether the image was (1) moderate or worse diabetic retinopathy (ie, moderate, severe, or proliferative), (2) severe or worse dia- betic retinopathy, (3) referable diabetic macular edema, or (4) fully gradable. Referable diabetic retinopathy was defined as any image that fulfilled either criterion 1, criterion 3, or both. The performance of the algorithm was measured by the area under the receiver operating curve (AUC) generated by plotting sensitivity vs 1 specificity. Because the network in this study had a large number of parameters (22 million), an early stopping criteria18 (that stops training when peak AUC isreachedonaseparatetuningset)wasusedtoterminatetraining before convergence. The development set was divided into 2 parts: (1) training: 80% of the data was used to optimize the network weights and (2) tuning: 20% of the data was used to optimize hyperparameters (such as early stopping for training, image preprocessing options). An ensemble19 of 10 networks trained on the same data was used, and the final prediction was computed by a linear average over the predictions of the ensemble. Evaluating the Algorithm The trained neural network generates a continuous number between 0 and 1 for referable diabetic retinopathy and other diabeticretinopathyclassifications,correspondingtotheprobability of that condition being present in the image. Receiver operating curves were plotted by varying the operating thresholdand2operatingpointsforthealgorithmwereselectedfrom the development set. The first operating point approximated the specificity of the ophthalmologists in the derivation set for detecting referable diabetic retinopathy (approximately 98%) andallowedforbettercomparisonbetweenthealgori",
    "output": "\"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nthm’sperformance and that of the 7 or 8 ophthalmologists that graded the validation set. The second operating point corresponded to a sensitivity of 97% for detecting referable diabetic retinopathy because a high sensitivity is a prerequisite in a potential screening tool. Statistical Analysis and Performance Comparison on Clinical Validation Sets Based on the 2 operating points, 2 2 tables were generated to character",
    "output": "\"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nize the sensitivity and specificity of the algorithm with respect to the reference standard, which was defined as the majority decision of the ophthalmologists’ readings based on all available grades. The 95% confidence intervals for the sensitivity and specificity of the algorithm at the 2 operating points were calculated to be “exact” ClopperPearson intervals,20 which corresponded to separate 2-sided confidence intervals with individual coverage probabilities of sqrt(0.95) 0.975. The 95% confidence intervals for the intragrader and intergrader reliabilities are z confidence intervals. Statistical significance and simultaneous 2-sided confidence intervals were computed using the StatsModels version 0.6.1 and SciPy version 0.15.1 python packages. Subsampling Experiments Experiments to understand the relationship between the amount of development data on the performance of the resulting algorithms also were conducted. To understand the effect of reducing the number of images in the training set, Table. Baseline Characteristicsa Characteristics Development Data Set EyePACS-1 Validation Data Set Messidor-2 Validation Data Set No. of images 128 175 9963 1748 No. of ophthalmologists 54 8 7 No. of grades per image 3-7 8 7 Grades per ophthalmologist, median (interquartile range) 2021 (304-8366) 8906 (8744-9360) 1745 (1742-1748) Patient demographics No. of unique individuals 69 573b 4997 874 Age, mean (SD), y 55.1 (11.2)c 54.4 (11.3) 57.6 (15.9) Female, No./total (%) among images for which sex was known 50 769/84 734 (59.9)c 5463/8784 (62.2) 743/1745 (42.6) Image quality distribution Fully gradable, No./total (%) among images for which image quality was assessed 52 311/69 598 (75.1)d 8788/9946 (88.4) 1745/1748 (99.8) Disease severity distribution classified by majority decision of ophthalmologists (reference standard) Total images for which both diabetic retinopathy and diabetic macular edema were assessed, No. (%) No diabetic retinopathy 53 759 (45.4) 7252 (82.5) 1217 (69.7) Mild diabetic retinopathy 30 637 (25.9) 842 (9.6) 264 (15.1) Moderate diabetic retinopathy 24 366 (20.6) 545 (6.2) 211 (12.1) Severe diabetic retinopathy 5298 (4.5) 54 (0.6) 28 (1.6) Proliferative diabetic retinopathy 4359 (3.7) 95 (1.1) 25 (1.4) Referable diabetic macular edema 18 224 (15.4) 272 (3.1) 125 (7.2) Referable diabetic retinopathye 33 246 (28.1) 683 (7.8) 254 (14.6) a Summary of image characteristics and available demographic information in the development and clinical validation data sets (EyePACS-1 and Messidor-2). Abnormal images were oversampled for the development set for algorithm training. The clinical validation sets were not enriched for abnormal images. bUnique patient codes (deidentified) were available for 89.3% of the development set (n = 114 398 images). c Individual-level data including age and sex were available for 66.1% of the development set (n = 84 734 images). images were sampled at rates of 0.2%, 2%, and N  10%, in which N ranged from 1 to 10; a new algorithm was trained for each data set; and its performance was measured on a fixed tuning set. To understand the effect of reducing the number ofgradesperimage,2experimentswererun:(1)training:grades in the training set were subsampled at rates of N  20%, in which N ranged from 0 to 5, with a restriction that the minimum number of grades to sample per image was 1. A new algorithm was trained for each N, and its performance was measured on a fixed tuning set with all the available grades and (2) tuning: grades in the tuning set (used to measure performance) were sampled using the same procedure as in the training experiment. The training set and the algorithm were fixed and used all available grades.",
    "output": "\"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\nExperiments / Results: Results Patient demographics and image characteristics are summarized in the Table. The development set included 128 175 images, of which 118 419 were assessed for referable diabetic retinopathy and 33 246 (28.1%) had referable diabetic retinopathy. Each image was graded by ophthalmologists between 3 and 7 times. The EyePACS-1 and Messidor-2 clinical validation sets consisted of 9963 images (8788 fully gradable; 683 [7.8%] referable) and 1748 images (1745 fully gradable; 254 [14.6%] referable), respectively. Image quality was assessed only for a subset of the development set, and fully gradable images ranged from 75.1% (52 311 of 69 598 images that were assessed for image quality) for the development set to 99.8% (1745 of 1748) for the Messidor-2 validation set (Table and Figure 1). In the development set, intragrader reliability among the ophthalmologists could be assessed among 16 graders who had graded a sufficient volume of repeat images. The mean intragrader reliability for referable diabetic retinopathy of these graders was 94.0% (95% CI, 91.2%-96.8%). Intergrader reliability could be assessed on 26 graders. The mean intergrader reliability for these graders was 95.5% (95% CI, 94.0%-96.9%). In the validation sets, a total of 8 grades per image were obtained for the EyePACS-1 data set and 7 grades per image for Messidor-2. The mean intragrader reliability for referable diabetic retinopathy for EyePACS-1 was 95.8% (95% CI, 92.8%-98.7%). (Figure 1. EyePACS-1 and Messidor-2 Clinical Validation Sets for Detection of Diabetic Retinopathy and All-Cause Referable Diabetic Retinopathy Referable diabetic retinopathy (RDR) A EYEPACS-1 MESSIDOR-2 mask detection failed) fully gradable) (index test) (index test) decision of 8 ophthalmologists (reference standard) decision of 8 ophthalmologists (reference standard) decision of 7 ophthalmologists (reference standard) Final diagnosis Final diagnosis Final diagnosis A, Referable diabetic retinopathy, defined as moderate or worse diabetic retinopathy or referable diabetic macular edema. B, All-cause referable cases, defined as moderate or worse diabetic retinopathy, referable diabetic macular edema, or ungradable image quality.) Intragrader reliability was not assessed for Messidor-2. The mean intergrader reliability was 95.9% (95% CI, 94.0%-97.8%) for EyePACS-1 and 94.6% (95% CI, 93.0%- 96.1%) for Messidor-2. On EyePACS-1, the mean agreement among ophthalmologists on referable diabetic retinopathy images was 77.7% (SD, 16.3%), with complete agreement on 19.6% of the referable cases. On nonreferable images, the average agreement was 97.4% (SD, 7.3%), with complete agreement on 85.6% of the nonreferable cases. On Messidor-2, the average agreement among ophthalmologists on referable diabetic retinopathy imageswas82.4%(SD,16.9%),withcompleteagreementon37.8% of the referable cases. On nonreferable images, the average agreement was 96.3% (SD, 9.9%), with complete agreement on 85.1% of the nonreferable cases. The distribution of agreement among ophthalmologists is reported in eFigure 3 in the Supplement. Figure 2 summarizes the performance of the algorithm in detecting referable diabetic retinopathy in the EyePACS-1 and Messidor-2 validation data sets for fully gradable images. For referable diabetic retin",
    "output": "Google's artificial intelligence can play the ancient game of Go better than any human. It can identify faces, recognize spoken words, and pull answers to your questions from the web. But the promise is that this same kind of technology will soon handle far more serious work than playing games and feeding smartphone apps. One day, it could help care for the human body. Demonstrating this promise, Google researchers have worked with doctors to develop an AI that can automatically identify diabetic retinopathy, a leading cause blindness among adults. Using deep learning—the same breed of AI that identifies faces, animals, and objects in pictures uploaded to Google's online services—the system detects the condition by examining retinal photos. In a recent study, it succeeded at about the same rate as human opthamologists, according to a paper published today in the Journal of the American Medical Association. \"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google. But the idea behind this AI isn't to replace doctors. Blindness is often preventable if diabetic retinopathy is caught early. The hope is that the technology can screen far more people for the condition than doctors could on their own, particularly in countries where healthcare is limited, says Peng. The project began, she says, when a Google researcher realized that doctors in his native India were struggling to screen all the locals that needed to be screened. In many places, doctors are already using photos to diagnose the condition without seeing patients in person. \"This is a well validated technology that can bring screening services to remote locations where diabetic retinal eye screening is less available,\" says David McColloch, a clinical professor of medicine at the University of Washington who specializes in diabetes. That could provide a convenient on-ramp for an AI that automates the process. Peng's project is part of a much wider effort to detect disease and illness using deep neural networks, pattern recognition systems that can learn discrete tasks by analyzing vast amounts of data. Researchers at DeepMind, a Google AI lab in London, have teamed with Britain's National Health Service to build various technologies that can automatically detect when patients are at risk of disease and illness, and several other companies, including Salesforce.com and a startup called Enlitic, are exploring similar systems. At Kaggle, an internet site where data scientists compete to solve real-world problems using algorithms, groups have worked to build their own machine learning systems that can automatically identify diabetic retinopathy. Medical Brains Peng is part of Google Brain, a team inside the company that provides AI software and services for everything from search to security to Android. Within this team, she now leads a group spanning dozens of researchers that focuses solely on medical applications for AI. The work on diabetic retinopathy started as a \"20 Percent project\" about two years ago, before becoming a full-time effort. Researchers began working with hospitals in the Indian cities of Aravind and Sankara that were already collecting retinal photos for doctors to examine. Then the Google team asked more than four dozen doctors in India and the US to identify photos where mini-aneurysms, hemorrhages, and other issues indicated that diabetic patients could be at risk for blindness. At least three doctors reviewed each photo, before Peng and team fed about 128,000 of these images into their neural network. Ultimately, the system identified the condition slightly more consistently than the original group of doctors. At its most sensitive, the system avoided both false negatives and false positives more than 90 percent of the time, exceeding the National Institutes of Health's recommended standard of at least 80 percent accuracy and precision for diabetic retinopathy screens. Given the success of deep learning algorithms with other machine vision tasks, the results of the original trial aren't surprising. But Yaser Sheikh, a professor of computer science at Carnegie Mellon who is working on other forms of AI for healthcare, says that actually moving this kind of thing into the developing world can be difficult. \"It is the kind of thing that sounds good, but actually making it work has proven to be far more difficult,\" he says. \"Getting technology to actually help in the developing world---there are many, many systematic barriers.\" But Peng and her team are pushing forward. She says Google is now running additional trials with photos taken specifically to train its diagnostic AI. Preliminary results, she says, indicate that the system once again performs as well as trained doctors. The machines, it seems, are gaining new kinds of sight."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\nopathy, the algorithm achieved an AUC of 0.991 (95% CI, 0.988-0.993) on EyePACS-1 and an AUC of 0.990 (95% CI, 0.986-0.995) on Messidor-2. Using the first operating cut point with high specificity, approximating the specificity of ophthalmologists in the development set, on EyePACS-1, the algorithm’s sensitivity was 90.3% and",
    "output": "Google's artificial intelligence can play the ancient game of Go better than any human. It can identify faces, recognize spoken words, and pull answers to your questions from the web. But the promise is that this same kind of technology will soon handle far more serious work than playing games and feeding smartphone apps. One day, it could help care for the human body. Demonstrating this promise, Google researchers have worked with doctors to develop an AI that can automatically identify diabetic retinopathy, a leading cause blindness among adults. Using deep learning—the same breed of AI that identifies faces, animals, and objects in pictures uploaded to Google's online services—the system detects the condition by examining retinal photos. In a recent study, it succeeded at about the same rate as human opthamologists, according to a paper published today in the Journal of the American Medical Association. \"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google. But the idea behind this AI isn't to replace doctors. Blindness is often preventable if diabetic retinopathy is caught early. The hope is that the technology can screen far more people for the condition than doctors could on their own, particularly in countries where healthcare is limited, says Peng. The project began, she says, when a Google researcher realized that doctors in his native India were struggling to screen all the locals that needed to be screened. In many places, doctors are already using photos to diagnose the condition without seeing patients in person. \"This is a well validated technology that can bring screening services to remote locations where diabetic retinal eye screening is less available,\" says David McColloch, a clinical professor of medicine at the University of Washington who specializes in diabetes. That could provide a convenient on-ramp for an AI that automates the process. Peng's project is part of a much wider effort to detect disease and illness using deep neural networks, pattern recognition systems that can learn discrete tasks by analyzing vast amounts of data. Researchers at DeepMind, a Google AI lab in London, have teamed with Britain's National Health Service to build various technologies that can automatically detect when patients are at risk of disease and illness, and several other companies, including Salesforce.com and a startup called Enlitic, are exploring similar systems. At Kaggle, an internet site where data scientists compete to solve real-world problems using algorithms, groups have worked to build their own machine learning systems that can automatically identify diabetic retinopathy. Medical Brains Peng is part of Google Brain, a team inside the company that provides AI software and services for everything from search to security to Android. Within this team, she now leads a group spanning dozens of researchers that focuses solely on medical applications for AI. The work on diabetic retinopathy started as a \"20 Percent project\" about two years ago, before becoming a full-time effort. Researchers began working with hospitals in the Indian cities of Aravind and Sankara that were already collecting retinal photos for doctors to examine. Then the Google team asked more than four dozen doctors in India and the US to identify photos where mini-aneurysms, hemorrhages, and other issues indicated that diabetic patients could be at risk for blindness. At least three doctors reviewed each photo, before Peng and team fed about 128,000 of these images into their neural network. Ultimately, the system identified the condition slightly more consistently than the original group of doctors. At its most sensitive, the system avoided both false negatives and false positives more than 90 percent of the time, exceeding the National Institutes of Health's recommended standard of at least 80 percent accuracy and precision for diabetic retinopathy screens. Given the success of deep learning algorithms with other machine vision tasks, the results of the original trial aren't surprising. But Yaser Sheikh, a professor of computer science at Carnegie Mellon who is working on other forms of AI for healthcare, says that actually moving this kind of thing into the developing world can be difficult. \"It is the kind of thing that sounds good, but actually making it work has proven to be far more difficult,\" he says. \"Getting technology to actually help in the developing world---there are many, many systematic barriers.\" But Peng and her team are pushing forward. She says Google is now running additional trials with photos taken specifically to train its diagnostic AI. Preliminary results, she says, indicate that the system once again performs as well as trained doctors. The machines, it seems, are gaining new kinds of sight."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\nAll-cause referable cases B EYEPACS-1 mask detection failed) fully gradable) referable decision of 7 ophthalmologists (reference standard) decision of 8 ophthalmologists (reference standard) decision of 8 ophthalmologists (reference standard) Final diagnosis Final diagnosis cases Final diagnosis cases specificity was 98.1%. In Messidor-2, the sensitivity was 87.0% and specificity was 98.5%. A second operating point for the algorithm was evaluated, which had a high sensitivity on the development set, reflecting an output that would be used for a screening tool. Using this operating point, on EyePACS-1, the algorithm had a sensitivity of 97.5% (95% CI, 95.8%-98.7%) and a specificity of 93.4% (95% CI, 92.8%-94.0%). In Messidor-2, the sensitivity was 96.1% (95% CI, 92.4%-98.3%) and the specificity was 93.9% (95% CI, 92.4%-95.3%). Given an approximately 8% prevalence of referable diabetic retinopathy (on a per-image basis [Table]), these findings correspond to a negative predictive value of 99.8% for EyePACS-1 and 99.6% for Messidor-2. The algorithm performance in making all-cause referable predictions, defined as moderate or worse diabetic retinopathy, referable diabetic macular edema, or ungradable images (Figure 3), was also evaluated using the EyePACS-1 data set. The Messidor-2 data set had only 3 ungradable images, so it was omitted from this analysis. For this task, the algorithm achieved an AUC of 0.974 (95% CI, 0.971-0.978). At the first (high-specificity) operating point, the algorithm had a sensitivityof90.7%(95%CI,89.2%-92.1%)andaspecificityof93.8% (95% CI, 93.2%-94.4%). At the second (high-sensitivity) operating point, the algorithm had a sensitivity of 96.7% (95% CI, 95.7%-97.5%) and a specificity of 84.0% (95% CI, 83.1%- 85.0%). Figure 2. Validation Set Performance for Referable Diabetic Retinopathy EyePACS-1: AUC, 99.1%; 95% CI, 98.8%-99.3% A 100 100 80 High-sensitivity operating point 95 High-specificity operating point 90 60 Sensitivity, % 85 40 80 75 20 70 0 5 10 15 20 25 30 0 0 20 100 80 60 40 1 – Specificity, % Performance of the algorithm (black curve) and ophthalmologists (colored circles) for the presence of referable diabetic retinopathy (moderate or worse diabetic retinopathy or referable diabetic macular edema) on A, EyePACS-1 (8788 fully gradable images) and B, Messidor-2 (1745 fully gradable images). The black diamonds on the graph correspond to the sensitivity and specificity of the algorithm at the high-sensitivity and high-specificity operating points. In A, for the high-sensitivity operating point, specificity was 93.4% (95% CI, 92.8%-94.0%) and sensitivity was 97.5% (95% CI, 95.8%-98.7%); for the Additional sensitivity analyses were conducted for severalsubcategories:(1)detectingmoderateorworsediabeticretinopathy only; (2) detecting severe or worse diabetic retinopathy only; (3) detecting referable diabetic macular edema only; (4) image quality; and (5) referable diabetic retinopathy on 2 data sets, each restricted to mydriatic and nonmydriatic images, respectively. For each subcategory 1 through 4, the algorithm achieved high sensitivity and specificity (see section on “Performance on Individual Diabetic Re",
    "output": "Google's artificial intelligence can play the ancient game of Go better than any human. It can identify faces, recognize spoken words, and pull answers to your questions from the web. But the promise is that this same kind of technology will soon handle far more serious work than playing games and feeding smartphone apps. One day, it could help care for the human body. Demonstrating this promise, Google researchers have worked with doctors to develop an AI that can automatically identify diabetic retinopathy, a leading cause blindness among adults. Using deep learning—the same breed of AI that identifies faces, animals, and objects in pictures uploaded to Google's online services—the system detects the condition by examining retinal photos. In a recent study, it succeeded at about the same rate as human opthamologists, according to a paper published today in the Journal of the American Medical Association. \"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google. But the idea behind this AI isn't to replace doctors. Blindness is often preventable if diabetic retinopathy is caught early. The hope is that the technology can screen far more people for the condition than doctors could on their own, particularly in countries where healthcare is limited, says Peng. The project began, she says, when a Google researcher realized that doctors in his native India were struggling to screen all the locals that needed to be screened. In many places, doctors are already using photos to diagnose the condition without seeing patients in person. \"This is a well validated technology that can bring screening services to remote locations where diabetic retinal eye screening is less available,\" says David McColloch, a clinical professor of medicine at the University of Washington who specializes in diabetes. That could provide a convenient on-ramp for an AI that automates the process. Peng's project is part of a much wider effort to detect disease and illness using deep neural networks, pattern recognition systems that can learn discrete tasks by analyzing vast amounts of data. Researchers at DeepMind, a Google AI lab in London, have teamed with Britain's National Health Service to build various technologies that can automatically detect when patients are at risk of disease and illness, and several other companies, including Salesforce.com and a startup called Enlitic, are exploring similar systems. At Kaggle, an internet site where data scientists compete to solve real-world problems using algorithms, groups have worked to build their own machine learning systems that can automatically identify diabetic retinopathy. Medical Brains Peng is part of Google Brain, a team inside the company that provides AI software and services for everything from search to security to Android. Within this team, she now leads a group spanning dozens of researchers that focuses solely on medical applications for AI. The work on diabetic retinopathy started as a \"20 Percent project\" about two years ago, before becoming a full-time effort. Researchers began working with hospitals in the Indian cities of Aravind and Sankara that were already collecting retinal photos for doctors to examine. Then the Google team asked more than four dozen doctors in India and the US to identify photos where mini-aneurysms, hemorrhages, and other issues indicated that diabetic patients could be at risk for blindness. At least three doctors reviewed each photo, before Peng and team fed about 128,000 of these images into their neural network. Ultimately, the system identified the condition slightly more consistently than the original group of doctors. At its most sensitive, the system avoided both false negatives and false positives more than 90 percent of the time, exceeding the National Institutes of Health's recommended standard of at least 80 percent accuracy and precision for diabetic retinopathy screens. Given the success of deep learning algorithms with other machine vision tasks, the results of the original trial aren't surprising. But Yaser Sheikh, a professor of computer science at Carnegie Mellon who is working on other forms of AI for healthcare, says that actually moving this kind of thing into the developing world can be difficult. \"It is the kind of thing that sounds good, but actually making it work has proven to be far more difficult,\" he says. \"Getting technology to actually help in the developing world---there are many, many systematic barriers.\" But Peng and her team are pushing forward. She says Google is now running additional trials with photos taken specifically to train its diagnostic AI. Preliminary results, she says, indicate that the system once again performs as well as trained doctors. The machines, it seems, are gaining new kinds of sight."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\ntinopathy Subtypes, Image Quality,” eTable 1, and eFigure 4 in the Supplement). For example, for the EyePACS-1 data set, at the first operating point for moderate or worse diabetic retinopathy, the algorithmhadasensitivityof90.1%(95%CI,87.2%-92.6%)and specificityof98.2%(95%CI,97.8%-98.5%).Forsevereorworse",
    "output": "Google's artificial intelligence can play the ancient game of Go better than any human. It can identify faces, recognize spoken words, and pull answers to your questions from the web. But the promise is that this same kind of technology will soon handle far more serious work than playing games and feeding smartphone apps. One day, it could help care for the human body. Demonstrating this promise, Google researchers have worked with doctors to develop an AI that can automatically identify diabetic retinopathy, a leading cause blindness among adults. Using deep learning—the same breed of AI that identifies faces, animals, and objects in pictures uploaded to Google's online services—the system detects the condition by examining retinal photos. In a recent study, it succeeded at about the same rate as human opthamologists, according to a paper published today in the Journal of the American Medical Association. \"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google. But the idea behind this AI isn't to replace doctors. Blindness is often preventable if diabetic retinopathy is caught early. The hope is that the technology can screen far more people for the condition than doctors could on their own, particularly in countries where healthcare is limited, says Peng. The project began, she says, when a Google researcher realized that doctors in his native India were struggling to screen all the locals that needed to be screened. In many places, doctors are already using photos to diagnose the condition without seeing patients in person. \"This is a well validated technology that can bring screening services to remote locations where diabetic retinal eye screening is less available,\" says David McColloch, a clinical professor of medicine at the University of Washington who specializes in diabetes. That could provide a convenient on-ramp for an AI that automates the process. Peng's project is part of a much wider effort to detect disease and illness using deep neural networks, pattern recognition systems that can learn discrete tasks by analyzing vast amounts of data. Researchers at DeepMind, a Google AI lab in London, have teamed with Britain's National Health Service to build various technologies that can automatically detect when patients are at risk of disease and illness, and several other companies, including Salesforce.com and a startup called Enlitic, are exploring similar systems. At Kaggle, an internet site where data scientists compete to solve real-world problems using algorithms, groups have worked to build their own machine learning systems that can automatically identify diabetic retinopathy. Medical Brains Peng is part of Google Brain, a team inside the company that provides AI software and services for everything from search to security to Android. Within this team, she now leads a group spanning dozens of researchers that focuses solely on medical applications for AI. The work on diabetic retinopathy started as a \"20 Percent project\" about two years ago, before becoming a full-time effort. Researchers began working with hospitals in the Indian cities of Aravind and Sankara that were already collecting retinal photos for doctors to examine. Then the Google team asked more than four dozen doctors in India and the US to identify photos where mini-aneurysms, hemorrhages, and other issues indicated that diabetic patients could be at risk for blindness. At least three doctors reviewed each photo, before Peng and team fed about 128,000 of these images into their neural network. Ultimately, the system identified the condition slightly more consistently than the original group of doctors. At its most sensitive, the system avoided both false negatives and false positives more than 90 percent of the time, exceeding the National Institutes of Health's recommended standard of at least 80 percent accuracy and precision for diabetic retinopathy screens. Given the success of deep learning algorithms with other machine vision tasks, the results of the original trial aren't surprising. But Yaser Sheikh, a professor of computer science at Carnegie Mellon who is working on other forms of AI for healthcare, says that actually moving this kind of thing into the developing world can be difficult. \"It is the kind of thing that sounds good, but actually making it work has proven to be far more difficult,\" he says. \"Getting technology to actually help in the developing world---there are many, many systematic barriers.\" But Peng and her team are pushing forward. She says Google is now running additional trials with photos taken specifically to train its diagnostic AI. Preliminary results, she says, indicate that the system once again performs as well as trained doctors. The machines, it seems, are gaining new kinds of sight."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Experiments / Results\ndiabetic retinopathy only at the first operating point, the algorithm had a sensitivity of 84.0% (95% CI, 75.3%-90.6%) and specificityof98.8%(95%CI,98.5%-99.0%).Fordiabeticmacular edema only, the algorithm’s sensitivity was 90.8% (95% CI, 86.1%-94.3%) and specificity was 98.7% (95% CI, 98.4%- 99.0%). The algorithm’s performance on mydriatic images was veryclosetoitsperformanceonnonmydriaticimages(andboth were similar to the overall algorithm performance; see eTable 2 in the Supplement). Multiple networks with varying number of images and grades per image were trained to determine how smaller training data sets related to the performance of the trained algorithms. In the first subsampling experiment (Figure 4A), the Messidor-2: AUC, 99.0%; 95% CI, 98.6%-99.5% B 100 100 80 High-sensitivity operating point 95 90 60 Sensitivity, % High-specificity operating point 85 40 80 75 20 70 0 5 10 15 20 25 30 0 0 20 100 80 60 40 1 – Specificity, % high-specificity operating point, specificity was 98.1% (95% CI, 97.8%-98.5%) and sensitivity was 90.3% (95% CI, 87.5%-92.7%). In B, for the high-sensitivity operating point, specificity was 93.9% (95% CI, 92.4%-95.3%) and sensitivity was 96.1% (95% CI, 92.4%-98.3%); for the high-specificity operating point, specificity was 98.5% (95% CI, 97.7%-99.1%) and sensitivity was 87.0% (95% CI, 81.1%-91.0%). There were 8 ophthalmologists who graded EyePACS-1 and 7 ophthalmologists who graded Messidor-2. AUC indicates area under the receiver operating characteristic curve. effects of data set size on algorithm performance were examined and shown to plateau at around 60 000 images (or approximately 17 000 referable images). In the second experiment (Figure 4B) on subsampling grades, 2 trends emerged: (1) increasing the number of grades per image on the training set did not yield an increase in relative performance (31.6% absolute difference) and (2) using only 1 grade per image on the tuning set led to a decline of 36% in performance compared with using all the available grades on the tuning set (an average of 4.5 grades), and that performance steadily increased as more grades were made available for the tuning set. This suggests that additional grading resources should be devoted to grading the tuning set (on which evaluation is done), which improves the quality of the reference standard and the algorithm performance.",
    "output": "Google's artificial intelligence can play the ancient game of Go better than any human. It can identify faces, recognize spoken words, and pull answers to your questions from the web. But the promise is that this same kind of technology will soon handle far more serious work than playing games and feeding smartphone apps. One day, it could help care for the human body. Demonstrating this promise, Google researchers have worked with doctors to develop an AI that can automatically identify diabetic retinopathy, a leading cause blindness among adults. Using deep learning—the same breed of AI that identifies faces, animals, and objects in pictures uploaded to Google's online services—the system detects the condition by examining retinal photos. In a recent study, it succeeded at about the same rate as human opthamologists, according to a paper published today in the Journal of the American Medical Association. \"We were able to take something core to Google---classifying cats and dogs and faces---and apply it to another sort of problem,\" says Lily Peng, the physician and biomedical engineer who oversees the project at Google. But the idea behind this AI isn't to replace doctors. Blindness is often preventable if diabetic retinopathy is caught early. The hope is that the technology can screen far more people for the condition than doctors could on their own, particularly in countries where healthcare is limited, says Peng. The project began, she says, when a Google researcher realized that doctors in his native India were struggling to screen all the locals that needed to be screened. In many places, doctors are already using photos to diagnose the condition without seeing patients in person. \"This is a well validated technology that can bring screening services to remote locations where diabetic retinal eye screening is less available,\" says David McColloch, a clinical professor of medicine at the University of Washington who specializes in diabetes. That could provide a convenient on-ramp for an AI that automates the process. Peng's project is part of a much wider effort to detect disease and illness using deep neural networks, pattern recognition systems that can learn discrete tasks by analyzing vast amounts of data. Researchers at DeepMind, a Google AI lab in London, have teamed with Britain's National Health Service to build various technologies that can automatically detect when patients are at risk of disease and illness, and several other companies, including Salesforce.com and a startup called Enlitic, are exploring similar systems. At Kaggle, an internet site where data scientists compete to solve real-world problems using algorithms, groups have worked to build their own machine learning systems that can automatically identify diabetic retinopathy. Medical Brains Peng is part of Google Brain, a team inside the company that provides AI software and services for everything from search to security to Android. Within this team, she now leads a group spanning dozens of researchers that focuses solely on medical applications for AI. The work on diabetic retinopathy started as a \"20 Percent project\" about two years ago, before becoming a full-time effort. Researchers began working with hospitals in the Indian cities of Aravind and Sankara that were already collecting retinal photos for doctors to examine. Then the Google team asked more than four dozen doctors in India and the US to identify photos where mini-aneurysms, hemorrhages, and other issues indicated that diabetic patients could be at risk for blindness. At least three doctors reviewed each photo, before Peng and team fed about 128,000 of these images into their neural network. Ultimately, the system identified the condition slightly more consistently than the original group of doctors. At its most sensitive, the system avoided both false negatives and false positives more than 90 percent of the time, exceeding the National Institutes of Health's recommended standard of at least 80 percent accuracy and precision for diabetic retinopathy screens. Given the success of deep learning algorithms with other machine vision tasks, the results of the original trial aren't surprising. But Yaser Sheikh, a professor of computer science at Carnegie Mellon who is working on other forms of AI for healthcare, says that actually moving this kind of thing into the developing world can be difficult. \"It is the kind of thing that sounds good, but actually making it work has proven to be far more difficult,\" he says. \"Getting technology to actually help in the developing world---there are many, many systematic barriers.\" But Peng and her team are pushing forward. She says Google is now running additional trials with photos taken specifically to train its diagnostic AI. Preliminary results, she says, indicate that the system once again performs as well as trained doctors. The machines, it seems, are gaining new kinds of sight."
  },
  {
    "id_paper": "45",
    "id_story": "45-02",
    "id_chunk": "45-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusions In this evaluation of retinal fundus photographs from adults with diabetes, an algorithm based on deep machine learning had high sensitivity and specificity for detecting referable diabetic retinopathy. Further research is necessary to determine the feasibility of applying this algorithm in the clinical setting and to determine whether use of the algorithm could lead to improved care and outcomes compared with current ophthalmologic assessment.",
    "output": "Google's artificial intelligence can play the ancient game of Go better than any human. It can identify faces, recognize spoken words, and pull answers to your questions from the web. But the promise is that this same kind of technology will soon handle far more serious work than playing games and feeding smartphone apps. One day, it could help care for the human body.\nDemonstrating this promise, Google researchers have worked with doctors to develop an AI that can automatically identify diabetic retinopathy, a leading cause blindness among adults. Using deep learning—the same breed of AI that identifies faces, animals, and objects in pictures uploaded to Google's online services—the system detects the condition by examining retinal photos. In a recent study, it succeeded at about the same rate as human opthamologists, according to a paper published today in the Journal of the American Medical Association.\nBut the idea behind this AI isn't to replace doctors. Blindness is often preventable if diabetic retinopathy is caught early. The hope is that the technology can screen far more people for the condition than doctors could on their own, particularly in countries where healthcare is limited, says Peng. The project began, she says, when a Google researcher realized that doctors in his native India were struggling to screen all the locals that needed to be screened.\nIn many places, doctors are already using photos to diagnose the condition without seeing patients in person. \"This is a well validated technology that can bring screening services to remote locations where diabetic retinal eye screening is less available,\" says David McColloch, a clinical professor of medicine at the University of Washington who specializes in diabetes. That could provide a convenient on-ramp for an AI that automates the process.\nPeng's project is part of a much wider effort to detect disease and illness using deep neural networks, pattern recognition systems that can learn discrete tasks by analyzing vast amounts of data. Researchers at DeepMind, a Google AI lab in London, have teamed with Britain's National Health Service to build various technologies that can automatically detect when patients are at risk of disease and illness, and several other companies, including Salesforce.com and a startup called Enlitic, are exploring similar systems. At Kaggle, an internet site where data scientists compete to solve real-world problems using algorithms, groups have worked to build their own machine learning systems that can automatically identify diabetic retinopathy.\nPeng is part of Google Brain, a team inside the company that provides AI software and services for everything from search to security to Android. Within this team, she now leads a group spanning dozens of researchers that focuses solely on medical applications for AI.\nThe work on diabetic retinopathy started as a \"20 Percent project\" about two years ago, before becoming a full-time effort. Researchers began working with hospitals in the Indian cities of Aravind and Sankara that were already collecting retinal photos for doctors to examine. Then the Google team asked more than four dozen doctors in India and the US to identify photos where mini-aneurysms, hemorrhages, and other issues indicated that diabetic patients could be at risk for blindness. At least three doctors reviewed each photo, before Peng and team fed about 128,000 of these images into their neural network.\nGiven the success of deep learning algorithms with other machine vision tasks, the results of the original trial aren't surprising. But Yaser Sheikh, a professor of computer science at Carnegie Mellon who is working on other forms of AI for healthcare, says that actually moving this kind of thing into the developing world can be difficult. \"It is the kind of thing that sounds good, but actually making it work has proven to be far more difficult,\" he says. \"Getting technology to actually help in the developing world---there are many, many systematic barriers.\"\nBut Peng and her team are pushing forward. She says Google is now running additional trials with photos taken specifically to train its diagnostic AI. Preliminary results, she says, indicate that the system once again performs as well as trained doctors. The machines, it seems, are gaining new kinds of sight. And some day, they might save yours."
  },
  {
    "id_paper": "44",
    "id_story": "44-04",
    "id_chunk": "44-04_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Abstract\nAbstract: Abstract arXiv:1711.05225v3 [cs.CV] 25 Dec 2017 We develop an algorithm that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. Our algorithm, CheXNet, is a 121-layer convolutional neural network trained on ChestX-ray14, currently the largest publicly available chest Xray dataset, containing over 100,000 frontalview X-ray images with 14 diseases. Four practicing academic radiologists annotate a test set, on which we compare the performance of CheXNet to that of radiologists. We find that CheXNet exceeds average radiologist performance on the F1 metric. We extend CheXNet to detect all 14 diseases in ChestX-ray14 and achieve state of the art results on all 14 diseases.",
    "output": "In the case of CheXnet, the research team led by Stanford adjunct professor Andrew Ng, started by training the neural network with 112,120 chest X-ray images that were previously manually labeled with up to 14 different diseases. One of them was pneumonia. After training it for a month, the software beat previous computer-based methods to detect this type of infection. The Stanford Machine Learning Group team pitted its software against four Stanford radiologists, giving each of them 420 X-ray images. This graphic shows how the radiologists–represented by the orange Xs–did compared to the program–represented by the blue curve."
  },
  {
    "id_paper": "44",
    "id_story": "44-04",
    "id_chunk": "44-04_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Introduction\nIntroduction: 1. Introduction More than 1 million adults are hospitalized with pneumonia and around 50,000 die from the disease every year in the US alone (CDC, 2017). Chest X-rays are currently the best available method for diagnosing pneumonia (WHO, 2001), playing a crucial role in clinical care (Franquet, 2001) and epidemiological studies (Cherian et al., 2005). However, detecting pneumonia in chest X-rays is a challenging task that relies on the availability of expert radiologists. In this work, we present a model that can automatically detect pneumonia from chest X-rays at a level exceeding practicing radiologists. (*Equal contribution of Medicine University of Radiology. Correspondence to: Pranav Rajpurkar pranavsr@cs.stanford.edu>, Jeremy Irvin jirvin16@cs.stanford.edu>. Project website at https://stanfordmlgroup. github.io/projects/chexnet Input Chest X-Ray Image CheXNet 121-layer CNN Output Pneumonia Positive (85%) Figure 1. CheXNet is a 121-layer convolutional neural network that takes a chest X-ray image as input, and outputs the probability of a pathology. On this example, CheXnet correctly detects pneumonia and also localizes areas in the image most indicative of the pathology.) Our model, ChexNet (shown in Figure 1), is a 121layer convolutional neural network that inputs a chest X-ray image and outputs the probability of pneumonia along with a heatmap localizing the areas of the image most indicative of pneumonia. We train CheXNet on the recently released ChestX-ray14 dataset (Wang et al., 2017), which contains 112,120 frontal-view chest X-ray images individually labeled with up to 14 different thoracic diseases, including pneumonia. We use dense connections (Huang et al., 2016) and batch normalization (Ioffe & Szegedy, 2015) to make the optimization of such a deep network tractable. Detecting pneumonia in chest radiography can be difficult for radiologists. The appearance of pneumonia in X-ray images is often vague, can overlap with other diagnoses, and can mimic many other benign abnormalities. These discrepancies cause considerable variability among radiologists in the diagnosis of pneumonia (Neuman et al., 2012; Davies et al., 1996; Hopstaken et al., 2004). To estimate radiologist performance, we collect annotations from four practicing academic radiologists on a subset of 420 images from ChestX-ray14. On these 420 images, we measure performance of individual radiologists and the model. We find that the model exceeds the average radiologist performance on the pneumonia detection task. To compare CheXNet against previous work using ChestX-ray14, we make simple modifications to CheXNet to detect all 14 diseases in ChestX-ray14, and find that we outperform best published results on all 14 diseases. Automated detection of diseases from chest X-rays at the level of expert radiologists would not only have tremendous benefit in clinical settings, it would also be invaluable in delivery of health care to populations with inadequate access to diagnostic imaging specialists.",
    "output": "New AI Can Diagnose Pneumonia Better Than Doctors\nThe software can greatly help in avoiding the misdiagnosis of pneumonia.\nScientists at Stanford University have developed a new artificial intelligence-powered imaging software designed to help doctors diagnose pneumonia with greater accuracy than ever before. It’s a great example of how AI is helping humans do their jobs better and faster.\nIn the case of CheXnet, the research team led by Stanford adjunct professor Andrew Ng, started by training the neural network with 112,120 chest X-ray images that were previously manually labeled with up to 14 different diseases. One of them was pneumonia. After training it for a month, the software beat previous computer-based methods to detect this type of infection. The Stanford Machine Learning Group team pitted its software against four Stanford radiologists, giving each of them 420 X-ray images. This graphic shows how the radiologists–represented by the orange Xs–did compared to the program–represented by the blue curve.\nImportantly, it has a simple user experience: Doctors input an X-ray of lungs, and automatically get the numeric probability of those lungs being infected with pneumonia or not. They also get a color map that highlights the level of infection throughout the tissue. Using these insights, the doctors can then make decisions about how to approach treatment.\nThe team hopes to see its research applied not only in hospitals but all around the world; an estimated two-thirds of the global population doesn’t have access to accurate radiology diagnostic tools. The researchers “hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited,” the announcement reads.\n\nAs IEEE Spectrum’s Tekla S. Perry writes, the accuracy of the CheXnet program can make a tremendous difference in emergency rooms around the world–in fact, “failure to promptly recognize and treat bacterial pneumonia may lead to significant morbidity and mortality.” She describes how her 18-year-old son went to Stanford Medical Center’s ER with “extremely high fever and cough” twice but the doctors said the X-rays didn’t show any sign of pneumonia. It was only after a routine reevaluation of X-ray images when other doctors realized that he could have pneumonia, which indeed was the case. And misdiagnosis can go both ways: In an October 2010 url, researchers discovered that “72% of patients were misdiagnosed with pneumonia upon readmission to the same hospital.” I, for one, agree with Perry: Next time I go to a hospital, I want AI fighting on my side."
  },
  {
    "id_paper": "44",
    "id_story": "44-04",
    "id_chunk": "44-04_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\nMethod: 2. CheXNet 2.1. Problem Formulation The pneumonia detection task is a binary classification problem, where the input is a frontal-view chest Xray image X and the output is a binary label y 0, 1 indicating the absence or presence of pneumonia respectively. For a single example in the training set, we optimize the weighted binary cross entropy loss F1 Score (95% CI) L(X, y) = w+ y log p(Y = 1|X) w (1 y) log p(Y = 0|X), where p(Y = i|X) is the probability that the network assigns to the label i, w+ = |N|/(|P|+|N|), and w= |P|/(|P|+|N|) with |P| and |N| the number of positive cases and negative cases of pneumonia in the training set respectively. 2.2. Model Architecture and Training CheXNet is a 121-layer Dense Convolutional Network (DenseNet) (Huang et al., 2016) trained on the ChestX-ray 14 dataset. DenseNets improve flow of information and gradients through the network, making the optimization of very deep networks tractable. We replace the final fully connected layer with one that has a single output, after which we apply a sigmoid nonlinearity. The weights of the network are initialized with weights from a model pretrained on ImageNet (Deng et al., 2009). The network is trained end-to-end using Adam with standard parameters (1 = 0.9 and 2 = 0.999) (Kingma & Ba, 2014). We train the model using minibatches of size 16. We use an initial learning rate of 0.001 that is decayed by a factor of 10 each time the validation loss plateaus after an epoch, and pick the model with the lowest validation loss. 3. Data 3.1. Training We use the ChestX-ray14 dataset released by Wang et al. (2017) which contains 112,120 frontal-view X-ray images of 30,805 unique patients. Wang et al. (2017) annotate each image with up to 14 different thoracic pathology labels using automatic extraction methods on radiology reports. We label images that have pneumonia as one of the annotated pathologies as positive examples and label all other images as negative examples. For the pneumonia detection task, we randomly split the dataset into training (28744 patients, 98637 images), validation (1672 patients, 6351 images), and test (389 patients, 420 images). There is no patient overlap between the sets. Before inputting the images into the network, we downscale the images to 224224 and normalize based on the mean and standard deviation of images in the ImageNet training set. We also augment the training data with random horizontal flipping. Pathology Wang et al. (2017) Yao et al. (2017) CheXNet (ours) Atelectasis 0.716 0.772 0.8094 Cardiomegaly 0.807 0.904 0.9248 Effusion 0.784 0.859 0.8638 Infiltration 0.609 0.695 0.7345 Mass 0.706 0.792 0.8676 Nodule 0.671 0.717 0.7802 Pneumonia 0.633 0.713 0.7680 Pneumothorax 0.806 0.841 0.8887 Consolidation 0.708 0.788 0.7901 Edema 0.835 0.882 0.8878 Emphysema 0.815 0.829 0.9371 Fibrosis 0.769 0.767 0.8047 Pleural Thickening 0.708 0.765 0.8062 Hernia 0.767 0.914 0.9164 Table 2. CheXNet outperforms the best published results on all 14 pathologies in the ChestX-ray14 dataset. In detecting Mass, Nodule, Pneumonia, and Emphysema, CheXNet has a margin of >0.05",
    "output": "New AI Can Diagnose Pneumonia Better Than Doctors\nThe software can greatly help in avoiding the misdiagnosis of pneumonia.\nScientists at Stanford University have developed a new artificial intelligence-powered imaging software designed to help doctors diagnose pneumonia with greater accuracy than ever before. It’s a great example of how AI is helping humans do their jobs better and faster.\nIn the case of CheXnet, the research team led by Stanford adjunct professor Andrew Ng, started by training the neural network with 112,120 chest X-ray images that were previously manually labeled with up to 14 different diseases. One of them was pneumonia. After training it for a month, the software beat previous computer-based methods to detect this type of infection. The Stanford Machine Learning Group team pitted its software against four Stanford radiologists, giving each of them 420 X-ray images. This graphic shows how the radiologists–represented by the orange Xs–did compared to the program–represented by the blue curve.\nImportantly, it has a simple user experience: Doctors input an X-ray of lungs, and automatically get the numeric probability of those lungs being infected with pneumonia or not. They also get a color map that highlights the level of infection throughout the tissue. Using these insights, the doctors can then make decisions about how to approach treatment.\nThe team hopes to see its research applied not only in hospitals but all around the world; an estimated two-thirds of the global population doesn’t have access to accurate radiology diagnostic tools. The researchers “hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited,” the announcement reads.\n\nAs IEEE Spectrum’s Tekla S. Perry writes, the accuracy of the CheXnet program can make a tremendous difference in emergency rooms around the world–in fact, “failure to promptly recognize and treat bacterial pneumonia may lead to significant morbidity and mortality.” She describes how her 18-year-old son went to Stanford Medical Center’s ER with “extremely high fever and cough” twice but the doctors said the X-rays didn’t show any sign of pneumonia. It was only after a routine reevaluation of X-ray images when other doctors realized that he could have pneumonia, which indeed was the case. And misdiagnosis can go both ways: In an October 2010 url, researchers discovered that “72% of patients were misdiagnosed with pneumonia upon readmission to the same hospital.” I, for one, agree with Perry: Next time I go to a hospital, I want AI fighting on my side."
  },
  {
    "id_paper": "44",
    "id_story": "44-04",
    "id_chunk": "44-04_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\nAUROC over previous state of the art results. 3.2. Test We collected a test set of 420 frontal chest X-rays. Annotations were obtained independently from four practicing radiologists at Stanford University, who were asked to label all 14 pathologies in Wang et al. (2017). The radiologists had 4, 7, 25, and 28 years of experience, and",
    "output": "New AI Can Diagnose Pneumonia Better Than Doctors\nThe software can greatly help in avoiding the misdiagnosis of pneumonia.\nScientists at Stanford University have developed a new artificial intelligence-powered imaging software designed to help doctors diagnose pneumonia with greater accuracy than ever before. It’s a great example of how AI is helping humans do their jobs better and faster.\nIn the case of CheXnet, the research team led by Stanford adjunct professor Andrew Ng, started by training the neural network with 112,120 chest X-ray images that were previously manually labeled with up to 14 different diseases. One of them was pneumonia. After training it for a month, the software beat previous computer-based methods to detect this type of infection. The Stanford Machine Learning Group team pitted its software against four Stanford radiologists, giving each of them 420 X-ray images. This graphic shows how the radiologists–represented by the orange Xs–did compared to the program–represented by the blue curve.\nImportantly, it has a simple user experience: Doctors input an X-ray of lungs, and automatically get the numeric probability of those lungs being infected with pneumonia or not. They also get a color map that highlights the level of infection throughout the tissue. Using these insights, the doctors can then make decisions about how to approach treatment.\nThe team hopes to see its research applied not only in hospitals but all around the world; an estimated two-thirds of the global population doesn’t have access to accurate radiology diagnostic tools. The researchers “hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited,” the announcement reads.\n\nAs IEEE Spectrum’s Tekla S. Perry writes, the accuracy of the CheXnet program can make a tremendous difference in emergency rooms around the world–in fact, “failure to promptly recognize and treat bacterial pneumonia may lead to significant morbidity and mortality.” She describes how her 18-year-old son went to Stanford Medical Center’s ER with “extremely high fever and cough” twice but the doctors said the X-rays didn’t show any sign of pneumonia. It was only after a routine reevaluation of X-ray images when other doctors realized that he could have pneumonia, which indeed was the case. And misdiagnosis can go both ways: In an October 2010 url, researchers discovered that “72% of patients were misdiagnosed with pneumonia upon readmission to the same hospital.” I, for one, agree with Perry: Next time I go to a hospital, I want AI fighting on my side."
  },
  {
    "id_paper": "44",
    "id_story": "44-04",
    "id_chunk": "44-04_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\none of the radiologists is a sub-specialty fellowship trained thoracic radiologist. Radiologists did not have access to any patient information or knowledge of disease prevalence in the data. Labels were entered into a standardized data entry program. 4. CheXNet vs. Radiologist Performance 4.1. Comparison We assess the performance of both radiologists and CheXNet on the test set for the pneumonia detection task. Recall that for each of the images in the test set, we have 4 labels from four practicing radiologists and 1 label from CheXNet. We compute the F1 score for each individual radiologist and for CheXNet against each of the other 4 labels as ground truth. We report the mean of the 4 resulting F1 scores for each radiologist and for CheXNet, along with the average F1 across the radiologists. We use the bootstrap to construct 95% bootstrap confidence intervals (CIs), calculating the average F1 score for both the radiologists and CheXNet on 10,000 bootstrap samples, sampled with replacement from the test set. We take the 2.5th and 97.5th percentiles of the F1 scores as the 95% bootstrap CI. We find that CheXNet achieves an F1 score of 0.435 (95% CI 0.387, 0.481), higher than the radiologist average of 0.387 (95% CI 0.330, 0.442). Table 1 summarizes the performance of each radiologist and of CheXNet. To determine whether CheXNet’s performance is statistically significantly higher than radiologist performance, we also calculate the difference between the average F1 score of CheXNet and the average F1 score of the radiologists on the same bootstrap samples. If the 95% CI on the difference does not include zero, we conclude there was a significant difference between the F1 score of CheXNet and the F1 score of the radiologists. We find that the difference in F1 scores — 0.051 (95% CI 0.005, 0.084) — does not contain 0, and therefore conclude that the performance of CheXNet is statistically significantly higher than radiologist performance. (Figure 2. CheXNet localizes pathologies it identifies using Class Activation Maps, which highlight the areas of the X-ray that are most important for making a particular pathology classification. The captions for each image are provided by one of the practicing radiologists. (Potchen et al., 1979). 5. ChexNet vs. Previous State of the Art on the ChestX-ray14 Dataset(a) Patient with multifocal community acquired pneumonia. The model correctly detects the airspace disease in the left lower and right upper lobes to arrive at the pneumonia diagnosis. (d) Patient with a right-sided pneumothroax and chest tube. The model detects the abnormal lung to correctly predict the presence of pneumothorax (collapsed lung).) We extend the algorithm to classify multiple thoracic pathologies by making three changes. First, instead of outputting one binary label, ChexNet outputs a vector t of binary labels indicating the absence or presence of each of the following 14 pathology classes: Atelectasis, Cardiomegaly, Consolidation, Edema, Effusion, Emphysema, Fibrosis, Hernia, Infiltration, Mass, Nod- (b) Patient with a left lung nodule. The model identifies the left lower lobe lung nodule and correctly classifies the pathology. (c) Patient with primary lung malignancy and two large masses, one in the left lower lobe and one in the right upper lobe adjacent to the mediastinum. The model correctly identifies both masses in the X-ray. (e) Patient with a large right pleural effusion (fluid in the pleural space). The model correctly labels the effusion and focuses on the right lower chest. (f) Patient with congestive heart failure and cardiomegaly (enlarged heart). The model correctly identifies the enlarged cardiac silhouette.",
    "output": "New AI Can Diagnose Pneumonia Better Than Doctors\nThe software can greatly help in avoiding the misdiagnosis of pneumonia.\nScientists at Stanford University have developed a new artificial intelligence-powered imaging software designed to help doctors diagnose pneumonia with greater accuracy than ever before. It’s a great example of how AI is helping humans do their jobs better and faster.\nIn the case of CheXnet, the research team led by Stanford adjunct professor Andrew Ng, started by training the neural network with 112,120 chest X-ray images that were previously manually labeled with up to 14 different diseases. One of them was pneumonia. After training it for a month, the software beat previous computer-based methods to detect this type of infection. The Stanford Machine Learning Group team pitted its software against four Stanford radiologists, giving each of them 420 X-ray images. This graphic shows how the radiologists–represented by the orange Xs–did compared to the program–represented by the blue curve.\nImportantly, it has a simple user experience: Doctors input an X-ray of lungs, and automatically get the numeric probability of those lungs being infected with pneumonia or not. They also get a color map that highlights the level of infection throughout the tissue. Using these insights, the doctors can then make decisions about how to approach treatment.\nThe team hopes to see its research applied not only in hospitals but all around the world; an estimated two-thirds of the global population doesn’t have access to accurate radiology diagnostic tools. The researchers “hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited,” the announcement reads.\n\nAs IEEE Spectrum’s Tekla S. Perry writes, the accuracy of the CheXnet program can make a tremendous difference in emergency rooms around the world–in fact, “failure to promptly recognize and treat bacterial pneumonia may lead to significant morbidity and mortality.” She describes how her 18-year-old son went to Stanford Medical Center’s ER with “extremely high fever and cough” twice but the doctors said the X-rays didn’t show any sign of pneumonia. It was only after a routine reevaluation of X-ray images when other doctors realized that he could have pneumonia, which indeed was the case. And misdiagnosis can go both ways: In an October 2010 url, researchers discovered that “72% of patients were misdiagnosed with pneumonia upon readmission to the same hospital.” I, for one, agree with Perry: Next time I go to a hospital, I want AI fighting on my side."
  },
  {
    "id_paper": "44",
    "id_story": "44-04",
    "id_chunk": "44-04_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\nule, Pleural Thickening, Pneumonia, and Pneumothorax. Second, we replace the final fully connected layer in CheXNet with a fully connected layer producing a 14-dimensional output, after which we apply an elementwise sigmoid nonlinearity. The final output is the predicted probability of the presence of each pathology class. Third, we modify the loss function to optimize the sum of unweighted binary cross",
    "output": "New AI Can Diagnose Pneumonia Better Than Doctors\nThe software can greatly help in avoiding the misdiagnosis of pneumonia.\nScientists at Stanford University have developed a new artificial intelligence-powered imaging software designed to help doctors diagnose pneumonia with greater accuracy than ever before. It’s a great example of how AI is helping humans do their jobs better and faster.\nIn the case of CheXnet, the research team led by Stanford adjunct professor Andrew Ng, started by training the neural network with 112,120 chest X-ray images that were previously manually labeled with up to 14 different diseases. One of them was pneumonia. After training it for a month, the software beat previous computer-based methods to detect this type of infection. The Stanford Machine Learning Group team pitted its software against four Stanford radiologists, giving each of them 420 X-ray images. This graphic shows how the radiologists–represented by the orange Xs–did compared to the program–represented by the blue curve.\nImportantly, it has a simple user experience: Doctors input an X-ray of lungs, and automatically get the numeric probability of those lungs being infected with pneumonia or not. They also get a color map that highlights the level of infection throughout the tissue. Using these insights, the doctors can then make decisions about how to approach treatment.\nThe team hopes to see its research applied not only in hospitals but all around the world; an estimated two-thirds of the global population doesn’t have access to accurate radiology diagnostic tools. The researchers “hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited,” the announcement reads.\n\nAs IEEE Spectrum’s Tekla S. Perry writes, the accuracy of the CheXnet program can make a tremendous difference in emergency rooms around the world–in fact, “failure to promptly recognize and treat bacterial pneumonia may lead to significant morbidity and mortality.” She describes how her 18-year-old son went to Stanford Medical Center’s ER with “extremely high fever and cough” twice but the doctors said the X-rays didn’t show any sign of pneumonia. It was only after a routine reevaluation of X-ray images when other doctors realized that he could have pneumonia, which indeed was the case. And misdiagnosis can go both ways: In an October 2010 url, researchers discovered that “72% of patients were misdiagnosed with pneumonia upon readmission to the same hospital.” I, for one, agree with Perry: Next time I go to a hospital, I want AI fighting on my side."
  },
  {
    "id_paper": "44",
    "id_story": "44-04",
    "id_chunk": "44-04_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Method\nentropy losses 14 X where p(Yc = 1|X) is the predicted probability that the image contains the pathology c and p(Yc = 0|X) is the predicted probability that the image does not contain the pathology c. We randomly split the dataset into training (70%), validation (10%), and test (20%) sets, following previous work on ChestX-ray14 (Wang et al., 2017; Yao et al., 2017). We ensure that there is no patient overlap between the splits. We compare the per-class AUROC of the model against the previous state of the art held by Yao et al. (2017) on 13 classes and Wang et al. (2017) on the remaining 1 class. We find that CheXNet achieves state of the art results on all 14 pathology classes. Table 2 illustrates the perclass AUROC comparison on the test set. On Mass, Nodule, Pneumonia, and Emphysema, we outperform previous state of the art considerably (> 0.05 increase in AUROC). 6. Model Interpretation To interpret the network predictions, we also produce heatmaps to visualize the areas of the image most indicative of the disease using class activation mappings (CAMs) (Zhou et al., 2016). To generate the CAMs, we feed an image into the fully trained network and extract the feature maps that are output by the final convolutional layer. Let fk be the kth feature map and let wc,k be the weight in the final classification layer for feature map k leading to pathology c. We obtain a map Mc of the most salient features used in classifying the image as having pathology c by taking the weighted sum of the feature maps using their associated weights. Formally, Mc = X k wc,kfk. We identify the most important features used by the model in its prediction of the pathology c by upscaling the map Mc to the dimensions of the image and overlaying the image. Figure 2 shows several examples of CAMs on the pneumonia detection task as well as the 14-class pathology classification task.",
    "output": "New AI Can Diagnose Pneumonia Better Than Doctors\nThe software can greatly help in avoiding the misdiagnosis of pneumonia.\nScientists at Stanford University have developed a new artificial intelligence-powered imaging software designed to help doctors diagnose pneumonia with greater accuracy than ever before. It’s a great example of how AI is helping humans do their jobs better and faster.\nIn the case of CheXnet, the research team led by Stanford adjunct professor Andrew Ng, started by training the neural network with 112,120 chest X-ray images that were previously manually labeled with up to 14 different diseases. One of them was pneumonia. After training it for a month, the software beat previous computer-based methods to detect this type of infection. The Stanford Machine Learning Group team pitted its software against four Stanford radiologists, giving each of them 420 X-ray images. This graphic shows how the radiologists–represented by the orange Xs–did compared to the program–represented by the blue curve.\nImportantly, it has a simple user experience: Doctors input an X-ray of lungs, and automatically get the numeric probability of those lungs being infected with pneumonia or not. They also get a color map that highlights the level of infection throughout the tissue. Using these insights, the doctors can then make decisions about how to approach treatment.\nThe team hopes to see its research applied not only in hospitals but all around the world; an estimated two-thirds of the global population doesn’t have access to accurate radiology diagnostic tools. The researchers “hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited,” the announcement reads.\n\nAs IEEE Spectrum’s Tekla S. Perry writes, the accuracy of the CheXnet program can make a tremendous difference in emergency rooms around the world–in fact, “failure to promptly recognize and treat bacterial pneumonia may lead to significant morbidity and mortality.” She describes how her 18-year-old son went to Stanford Medical Center’s ER with “extremely high fever and cough” twice but the doctors said the X-rays didn’t show any sign of pneumonia. It was only after a routine reevaluation of X-ray images when other doctors realized that he could have pneumonia, which indeed was the case. And misdiagnosis can go both ways: In an October 2010 url, researchers discovered that “72% of patients were misdiagnosed with pneumonia upon readmission to the same hospital.” I, for one, agree with Perry: Next time I go to a hospital, I want AI fighting on my side."
  },
  {
    "id_paper": "44",
    "id_story": "44-04",
    "id_chunk": "44-04_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Investor, Journalist\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 8. Conclusion Pneumonia accounts for a significant proportion of patient morbidity and mortality (Gon calves-Pereira et al., 2013). Early diagnosis and treatment of pneumonia is critical to preventing complications including death (Aydogdu et al., 2010). With approximately 2 billion procedures per year, chest X-rays are the most common imaging examination tool used in practice, critical for screening, diagnosis, and management of a variety of diseases including pneumonia (Raoof et al., 2012). However, two thirds of the global population lacks access to radiology diagnostics, according to an estimate by the World Health Organization (Mollura et al., 2010). There is a shortage of experts who can interpret X-rays, even when imaging equipment is available, leading to increased mortality from treatable diseases (Kesselman et al., 2016). We develop an algorithm which detects pneumonia from frontal-view chest X-ray images at a level exceeding practicing radiologists. We also show that a simple extension of our algorithm to detect multiple diseases outperforms previous state of the art on ChestX-ray14, the largest publicly available chest Xray dataset. With automation at the level of experts, we hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited.",
    "output": "New AI Can Diagnose Pneumonia Better Than Doctors\nScientists at Stanford University have developed a new artificial intelligence-powered imaging software designed to help doctors diagnose pneumonia with greater accuracy than ever before. It’s a great example of how AI is helping humans do their jobs better and faster.\nImportantly, it has a simple user experience: Doctors input an X-ray of lungs, and automatically get the numeric probability of those lungs being infected with pneumonia or not. They also get a color map that highlights the level of infection throughout the tissue. Using these insights, the doctors can then make decisions about how to approach treatment.\nThe team hopes to see its research applied not only in hospitals but all around the world; an estimated two-thirds of the global population doesn’t have access to accurate radiology diagnostic tools. The researchers “hope that this technology can improve healthcare delivery and increase access to medical imaging expertise in parts of the world where access to skilled radiologists is limited,” the announcement reads."
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Abstract\nAbstract: Abstract Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. To achieve this, we decouple appearance and motion information using a self-supervised formulation. To support complex motions, we use a representation consisting of a set of learned keypoints along with their local affine transformations. A generator network models occlusions arising during target motions and combines the appearance extracted from the source image and the motion derived from the driving video. Our framework scores best on diverse benchmarks and on a variety of object categories. Our source code is publicly available1. 1",
    "output": "Image animation consists of generating a video sequence so that an object in a source image is animated according to the motion of a driving video. Our framework addresses this problem without using any annotation or prior information about the specific object to animate. Once trained on a set of videos depicting objects of the same category (e.g. faces, human bodies), our method can be applied to any object of this class. (Source, my emphasis)\nThe requirement is the same object category.\nYou can try it out either using the GitHub repository or Colab Notebook."
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Introduction\nIntroduction: Introduction Generating videos by animating objects in still images has countless applications across areas of interest including movie production, photography, and e-commerce. More precisely, image animation refers to the task of automatically synthesizing videos by combining the appearance extracted from a source image with motion patterns derived from a driving video. For instance, a face image of a certain person can be animated following the facial expressions of another individual (see Fig. 1). In the literature, most methods tackle this problem by assuming strong priors on the object representation (e.g. 3D model) [4] and resorting to computer graphics techniques [6, 34]. These approaches can be referred to as object-specific methods, as they assume knowledge about the model of the specific object to animate. Recently, deep generative models have emerged as effective techniques for image animation and video retargeting [2, 42, 3, 43, 28, 29, 38, 41, 32, 22]. In particular, Generative Adversarial Networks (GANs) [14] and Variational Auto-Encoders (VAEs) [21] have been used to transfer facial expressions [38] or motion patterns [3] between human subjects in videos. Nevertheless, these approaches usually rely on pre-trained models in order to extract object-specific representations such as keypoint locations. Unfortunately, these pre-trained models are built using costly ground-truth data annotations [2, 28, 32] and are not available in general for an arbitrary object category. To address this issues, recently Siarohin et al. [29] introduced Monkey-Net, the first object-agnostic deep model for image Nicu Sebe DISI, University of Trento Huawei Technologies Ireland niculae.sebe@unitn.it Driving video image Source Driving video image Source Figure 1: Example animations produced by our method trained on different datasets: VoxCeleb [23] (top left), Tai-Chi-HD (top right), Fashion-Videos [42] (bottom left) and MGif [29] (bottom right). We use relative motion transfer for VoxCeleb and Fashion-Videos and absolute transfer for MGif and Tai-Chi-HD see Sec. 3.4. Check our project page for more qualitative results2. animation. Monkey-Net encodes motion information via keypoints learned in a self-supervised fashion. At test time, the source image is animated according to the corresponding keypoint trajectories estimated in the driving video. The major weakness of Monkey-Net is that it poorly models object appearance transformations in the keypoint neighborhoods assuming a zeroth order model (as we show in Sec. 3.1). This leads to poor generation quality in the case of large object pose changes (see Fig. 4). To tackle this issue, we propose to use a set of self-learned keypoints together with local affine transformations to model complex motions. We therefore call our method a first-order motion model. Second, we introduce an occlusion-aware generator, which adopts an occlusion mask automatically estimated to indicate object parts that are not visible in the source image and that should be inferred from the context. This is especially needed when the driving video contains large motion patterns and occlusions are typical. Third, we extend the equivariance loss commonly used for keypoints detector training [18, 45], to improve the estimation of local affine transformations. Fourth, we experimentally show that our method significantly outperforms state-of-the-art image animation methods and can handle high-resolution datasets where other approaches generally fail. Finally, we release a new high resolution dataset, Thai-Chi-HD, which we believe could become a reference benchmark for evaluating frameworks for image animation and video generation. 2",
    "output": "Seeing used to be believing. Thank AI, we finally have to farewell this cute and naive, but dangerous faith. Because it actually never was. In the XXth century, photos were retouched by repressive regimes. With Deep Learning, we experience new ways to re-illustrate reality. It is not a danger; it's a chance.\nUsing a painting by Arcimboldo, we can see how the motion assignment works — some of the graphical features are still detected as background and are not animated.\nThe most interesting effect happens if we use an image without physiognomical patterns — or even abstract art. I used work by Pollock:\nProbably if we combine this method with StyleTransfer — or even with Deep Painterly Harmonization, we will achieve even more coherent results.\nCan we still believe in visuals? Can we distinguish between real and fake in the Digital Age? The answer is: \"we cannot anymore\". But: the answer still does not end on that point.\nThis naive belief in the truth behind an image — so seductive because so simple — is a dangerous deception. There are zillions of truths behind an image. Even a raw image without any traces of post-production is not a truth, but a particularly chosen perspective. A glimpse of the unknown ontology. Pars pro toto — and we should take care of its semantical fragility.\nWe have to re-think our relationship with concepts of truth. Because there are so many truths like people. Sure, it's easy to deceive somebody, faking an image or video footage and presenting it as a real thing. And people do it already — from friendly pranks till presidential uses.\nBelieving in images is a comfortable self-deception. And I am thankful to all AI efforts to disrupt this belief."
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\nMethod: Method We are interested in animating an object depicted in a source image S based on the motion of a similar object in a driving video D. Since direct supervision is not available (pairs of videos in which objects move similarly), we follow a self-supervised strategy inspired from Monkey-Net [29]. For training, we employ a large collection of video sequences containing objects of the same object category. Our model is trained to reconstruct the training videos by combining a single frame and a learned latent representation of the motion in the video. Observing frame pairs, each extracted from the same video, it learns to encode motion as a combination of motion-specific keypoint displacements and local affine transformations. At test time we apply our model to pairs composed of the source image and of each frame of the driving video and perform image animation of the source object. An overview of our approach is presented in Fig. 2. Our framework is composed of two main modules: the motion estimation module and the image generation module. The purpose of the motion estimation module is to predict a dense motion field from a frame D R3HW of dimension H  W of the driving video D to the source frame S R3HW . The dense motion field is later used to align the feature maps computed from S with the object pose in D. The motion field is modeled by a function TSD : R2 R2 that maps each pixel location in D with its corresponding location in S. TSD is often referred to as backward optical flow. We employ backward optical flow, rather than forward optical flow, since back-warping can be implemented efficiently in a differentiable manner using bilinear sampling [17]. We assume there exists an abstract reference frame R. We independently estimate two transformations: from R to S (TSR) and from R to D (TDR). Note that unlike X2Face [41] the reference frame is an abstract concept that cancels out in our derivations later. Therefore it is never explicitly computed and cannot be visualized. This choice allows us to independently process D and S. This is desired since, at test time the model receives pairs of the source image and driving frames sampled from a different video, which can be very different visually. Instead of directly predicting TDR and TSR, the motion estimator module proceeds in two steps. In the first step, we approximate both transformations from sets of sparse trajectories, obtained by using keypoints learned in a self-supervised way. The locations of the keypoints in D and S are separately predicted by an encoder-decoder network. The keypoint representation acts as a bottleneck resulting in a compact motion representation. As shown by Siarohin et al. [29], such sparse motion representation is well-suited for animation as at test time, the keypoints of the source image can be moved using the keypoints trajectories in the driving video. We model motion in the neighbourhood of each keypoint using local affine transformations. Compared to using keypoint displacements only, the local affine transformations allow us to model a larger family of transformations. We use Taylor expansion to represent TDR by a set of keypoint locations and affine transformations. To this end, the keypoint detector network outputs keypoint locations as well as the parameters of each affine transformation. During the second step, a dense motion network combines the local approximations to obtain the resulting dense motion field TSD. Furthermore, in addition to the dense motion field, this network outputs an occlusion mask OSD that indicates which image parts of D can be reconstructed by warping of the source image and which parts should be inpainted, i.e.inferred from the context. Finally, the generation module renders an image of the source object moving as provided in the driving video. Here, we use a generator network G that warps the source image according to TSD and inpaints the image parts that are occluded in the source image. In the following sections we detail each of these step and the training procedure. 3.1 Local Affine Transformations for Approximate Motion Description The motion estimation module estimates the backward optical flow TSD from a driving frame D to the source frame S. As discussed above, we propose to approximate TSD by its first order Taylor expansion in a neighborhood of the keypoint locations. In the",
    "output": "The key points are set along with transformations (similar to a puppet tool in Photoshop, or like sensors on motion capture suit) — and so the trained movement can be transferred to a target image.\nShortly put, the unsupervised learning approach analyzes the motion data in source footage, universalizes it and, applies to target footage."
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\nrest of this section, we describe the motivation behind this choice, and detail the proposed approximation of TSD. We assume there exist an abstract reference frame R. Therefore, estimating TSD consists in estimating TSR and TRD. Furthermore, given a frame X, we estimate each transformation TXR in the neighbourhood of the learned keypoints. Formally, given a transformation TXR, we consider its first order Taylor expansions in K keypoints p1, . . . pK. Here, p1, . . . pK denote the coordinates of the keypoints in the reference frame R. Note that for the sake of simplicity in the following the point locations in the reference pose space are all denoted by p while the point locations in the X, S or D pose spaces are denoted by z. We obtain: TXR(p) = TXR(pk) + d dpTXR(p) p=pk In this formulation, the motion function TXR is represented by its values in each keypoint pk and its Jacobians computed in each pk location: (p pk) + o(p pk), (1) Furthermore, in order to estimate TRX = T 1 XR, we assume that TXR is locally bijective in the neighbourhood of each keypoint. We need to estimate TSD near the keypoint zk in D, given that zk is the pixel location corresponding to the keypoint location pk in R. To do so, we first estimate the transformation TRD near the point zk in the driving frame D, e.g. pk = TRD(zk). Then we estimate the transformation TSR near pk in the reference R. Finally TSD is obtained as follows: TSD = TSR TRD = TSR T 1 DR, (3) After computing again the first order Taylor expansion of Eq. (3) (see Sup. Mat.), we obtain: TSD(z) TSR(pk) + Jk(z TDR(pk)) (4) with: Jk = d d dpTSR(p) p=pk In practice, TSR(pk) and TDR(pk) in Eq. (4) are predicted by the keypoint predictor. More precisely, we employ the standard U-Net architecture that estimates K heatmaps, one for each keypoint. The last layer of the decoder uses softmax activations in order to predict heatmaps that can be interpreted as keypoint detection confidence map. Each expected keypoint location is estimated using the average operation as in [29, 25]. Note if we set Jk = 1 (1 is 2  2 identity matrix), we get the motion model of Monkey-Net. Therefore Monkey-Net uses a zeroth-order approximation of TSD(z) z. For both frames S and D, the keypoint predictor network also outputs four additional channels for each keypoint. From these channels, we obtain the coefficients of the matrices d dpTSR(p)|p=pk and d dpTSR(p)|p=pk in Eq. (5) by computing spatial weighted average using as weights the corresponding keypoint confidence map. Combining Local Motions. We employ a convolutional network P to estimate TSD from the set of Taylor approximations of TSD(z) in the keypoints and the original source frame S. Importantly, since TSD maps each pixel location in D with its corresponding location in S, the local patterns in TSD, such as edges or texture, are pixel-to-pixel aligned with D but not with S. This misalignment issue makes the task harder for the network to predict TSD from S. In order to provide inputs already roughly aligned with TSD, we warp the source frame S according to local transformations estimated in Eq. (4). Thus, we obtain K transformed images S1, . . . SK that are each aligned with TSD in the neighbourhood of a keypoint.",
    "output": "The key points are set along with transformations (similar to a puppet tool in Photoshop, or like sensors on motion capture suit) — and so the trained movement can be transferred to a target image.\nShortly put, the unsupervised learning approach analyzes the motion data in source footage, universalizes it and, applies to target footage."
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\nImportantly, we also consider an additional image S0 = S for the background. For each keypoint pk we additionally compute heatmaps Hk indicating to the dense motion network where each transformation happens. Each Hk(z) is implemented as the difference of two heatmaps centered in TDR(pk) and TSR(pk): (TDR(pk) z)2 Hk(z) = exp  In all our experiments, we employ  = 0.01 following Jakab et al. [18]. The heatmaps Hk and the transformed images S0, . . . SK are concatenated and processed by a UNet [26]. TSD is estimated using a part-based model inspired by Monkey-Net [29]. We assume that an object is composed of K rigid parts and that each part is moved according to Eq. (4). Therefore we estimate K+1 masks Mk, k = 0, . . . K that indicate where each local transformation holds. The final dense motion prediction TSD(z) is given by: K X 1 (5) dpTDR(p) p=pk ! (TSR(pk) z)2 ! exp . (6)  3.2 Occlusion-aware Image Generation As mentioned in Sec.3, the source image S is not pixel-to-pixel aligned with the image to be generated D. In order to handle this misalignment, we use a feature warping strategy similar to [30, 29, 15]. More precisely, after two down-sampling convolutional blocks, we obtain a feature map  RH′W ′ of dimension H′  W ′. We then warp  according to TSD. In the presence of occlusions in S, optical flow may not be sufficient to generate D. Indeed, the occluded parts in S cannot be recovered by image-warping and thus should be inpainted. Consequently, we introduce an occlusion map OSD [0, 1]H′W ′ to mask out the feature map regions that should be inpainted. Thus, the occlusion mask diminishes the impact of the features corresponding to the occluded parts. The transformed feature map is written as: ′ = OSD fw(, TSD) (8) where fw(, ) denotes the back-warping operation and denotes the Hadamard product. We estimate the occlusion mask from our sparse keypoint representation, by adding a channel to the final layer of the dense motion network. Finally, the transformed feature map ′ is fed to subsequent network layers of the generation module (see Sup. Mat.) to render the sought image. 3.3 Training Losses We train our system in an end-to-end fashion combining several losses. First, we use the reconstruction loss based on the perceptual loss of Johnson et al. [19] using the pre-trained VGG-19 network as our main driving loss. The loss is based on implementation of Wang et al. [38]. With the input driving frame D and the corresponding reconstructed frame D, the reconstruction loss is written as: I X Ni( D) Ni(D) , (9) Lrec( D, D) = i=1 where Ni() is the ith channel feature extracted from a specific VGG-19 layer and I is the number of feature channels in this layer. Additionally we propose to use this loss on a number of resolutions, forming a pyramid obtained by down-sampling D and D, similarly to MS-SSIM [40, 33]. The resolutions are 256  256, 128  128, 64  64 and 32  32. There are 20 loss terms in total. Imposing Equivariance Constraint. Our keypoint predictor does not require any keypoint annotations during training. This may lead to unstable performance. Equivariance constraint is one of the most important factors driving the discovery of unsupervised keypoints [18, 44]. It forces the model to predict consistent keypoints with respect to known geometric transformations. We use thin plate splines deformations as they were previously used in unsupervised keypoint detection",
    "output": "The key points are set along with transformations (similar to a puppet tool in Photoshop, or like sensors on motion capture suit) — and so the trained movement can be transferred to a target image.\nShortly put, the unsupervised learning approach analyzes the motion data in source footage, universalizes it and, applies to target footage."
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\n[18, 44] and are similar to natural image deformations. Since our motion estimator does not only predict the keypoints, but also the Jacobians, we extend the well-known equivariance loss to additionally include constraints on the Jacobians. We assume that an image X undergoes a known spatial deformation TXY. In this case TXY can be an affine transformation or a thin plane spline deformation. After this deformation we obtain a new image Y. Now by applying our extended motion estimator to both images, we obtain a set of local approximations for TXR and TYR. The standard equivariance constraint writes as: TXR TXY TYR (10) After computing the first order Taylor expansions of both sides, we obtain the following constraints (see derivation details in Sup. Mat.): TXR(pk) TXY TYR(pk), (11) d  d dpTXR(p) p=pk dpTXY(p) p=TYR(pk) d dpTYR(p) p=pk , (12) L1 would force the magnitude of the Jacobians to zero and would lead to numerical problems. To this end, we reformulate this constraint in the following way: 1  d 1 d dpTXR(p) p=pk dpTXY(p) p=TYR(pk) where 1 is 2  2 identity matrix. Then, L1 loss is employed similarly to the keypoint location constraint. Finally, in our preliminary experiments, we observed that our model shows low sensitivity to the relative weights of the reconstruction and the two equivariance losses. Therefore, we use equal loss weights in all our experiments. 3.4 Testing Stage: Relative Motion Transfer At this stage our goal is to animate an object in a source frame S1 using the driving video D1, . . . DT . Each frame Dt is independently processed to obtain St. Rather than transferring the motion encoded in TS1Dt(pk) to S, we transfer the relative motion between D1 and Dt to S1. In other words, we apply a transformation TDtD1(p) to the neighbourhood of each keypoint pk: TS1St(z) TS1R(pk) + Jk(z TSR(pk) + TD1R(pk) TDtR(pk)) (14) with Jk = d d dpTD1R(p) p=pk Detailed mathematical derivations are provided in Sup. Mat.. Intuitively, we transform the neighbourhood of each keypoint pk in S1 according to its local deformation in the driving video. Indeed, transferring relative motion over absolute coordinates allows to transfer only relevant motion patterns, while preserving global object geometry. Conversely, when transferring absolute coordinates, as in X2Face [41], the generated frame inherits the object proportions of the driving video. It’s important to note that one limitation of transferring relative motion is that we need to assume that the objects in S1 and D1 have similar poses (see [29]). Without initial rough alignment, Eq. (14) may lead to absolute keypoint locations physically impossible for the object of interest. 4",
    "output": "The key points are set along with transformations (similar to a puppet tool in Photoshop, or like sensors on motion capture suit) — and so the trained movement can be transferred to a target image.\nShortly put, the unsupervised learning approach analyzes the motion data in source footage, universalizes it and, applies to target footage."
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\nExperiments / Results: Experiments Datasets. We train and test our method on four different datasets containing various objects. Our model is capable of rendering videos of much higher resolution compared to [29] in all our experiments. • The VoxCeleb dataset [23] is a face dataset of 22496 videos, extracted from YouTube videos. For pre-processing, we extract an initial bounding box in the first video frame. We track this face until it is too far away from the initial position. Then, we crop the video frames using the smallest crop containing all the bounding boxes. The process is repeated until the end of the sequence. We filter out sequences that have resolution lower than 256 256 and the remaining videos are resized to 256 256 preserving the aspect ratio. It’s important to note that compared to X2Face [41], we obtain more natural videos where faces move freely within the bounding box. Overall, we obtain 19522 training videos and 525 test videos, with lengths varying from 64 to 1024 frames. • The UvA-Nemo dataset [9] is a facial analysis dataset that consists of 1240 videos. We apply the exact same pre-processing as for VoxCeleb. Each video starts with a neutral expression. Similar to Wang et al. [39], we use 1116 videos for training and 124 for evaluation. • The BAIR robot pushing dataset [10] contains videos collected by a Sawyer robotic arm pushing diverse objects over a table. It consists of 42880 training and 128 test videos. Each video is 30 frame long and has a 256 256 resolution. • Following Tulyakov et al. [35], we collected 280 tai-chi videos from YouTube. We use 252 videos for training and 28 for testing. Each video is split in short clips as described in pre-processing of VoxCeleb dataset. We retain only high quality videos and resized all the clips to 256 256 pixels (instead of 64 64 pixels in [35]). Finally, we obtain 3049 and 285 video chunks for training and testing respectively with video length varying from 128 to 1024 frames. This dataset is referred to as the Tai-Chi-HD dataset. The dataset will be made publicly available. d dpTYR(p) p=pk , (13) 1 (15) dpTDtR(p) p=pk Table 1: Quantitative ablation study for video reconstruction on Tai-Chi-HD. Tai-Chi-HD L1 (AKD, MKR) AED Baseline 0.073 (8.945, 0.099) 0.235 Pyr. 0.069 (9.407, 0.065) 0.213 Pyr.+OSD 0.069 (8.773, 0.050) 0.205 Jac. w/o Eq. (12) 0.073 (9.887, 0.052) 0.220 Full 0.063 (6.862, 0.036) 0.179 Table 2: Paired user study: user preferences in favour of our approach. X2Face [41] Monkey-Net [29] Tai-Chi-HD 92.0% 80.6% VoxCeleb 95.8% 68.4% Nemo 79.8% 60.6% Bair 95.0% 67.0% quantitatively evaluate each method on the \"proxy\" task of video reconstruction. This task consists of reconstructing the input video from a representation in which appearance and motion are decoupled. In our case, we reconstruct the input video by combining the sparse motion representation in (2) of each frame and the first video frame. Second, we evaluate our model on image animation according to a user-study. In all experiments we use K=10 as in [29]. Other implementation details are given in Sup. Mat. Metrics. To evaluate video reconstruction, we adopt the metrics proposed in Monkey-Net [29]: • L1. We report the average L1 distance between the generated and the ground-truth videos. • Average Keypoint Distance (AKD",
    "output": "None\nNone\nAnd it works astonishingly well.\nI tried my luck on Nefertiti using the footage of AI pioneer Geoffrey Hinton. This footage is delivered with Notebook. You can use another video material. It has to fit into specific requirements and sizes.\nNone\nNone\nNone\nNone\nNone\nAge of DeepFakes: a chance?"
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\n). For the Tai-Chi-HD, VoxCeleb and Nemo datasets, we use 3rd-party pre-trained keypoint detectors in order to evaluate whether the motion of the input video is preserved. For the VoxCeleb and Nemo datasets we use the facial landmark detector of Bulat et al. [5]. For the Tai-",
    "output": "None\nNone\nAnd it works astonishingly well.\nI tried my luck on Nefertiti using the footage of AI pioneer Geoffrey Hinton. This footage is delivered with Notebook. You can use another video material. It has to fit into specific requirements and sizes.\nNone\nNone\nNone\nNone\nNone\nAge of DeepFakes: a chance?"
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\nChi-HD dataset, we employ the human-pose estimator of Cao et al. [7]. These keypoints are independently computed for each frame. AKD is obtained by computing the average distance between the detected keypoints of the ground truth and of the generated video. • Missing Keypoint Rate (MKR). In the case of Tai-Chi-HD, the human-pose estimator returns an additional binary label for each keypoint indicating whether or not the keypoints were successfully detected. Therefore, we also report the MKR defined as the percentage of keypoints that are detected in the ground truth frame but not in the generated one. This metric assesses the appearance quality of each generated frame. • Average Euclidean Distance (AED). Considering an externally trained image representation, we report the average euclidean distance between the ground truth and generated frame representation, similarly to Esser et al. [11]. We employ the feature embedding used in Monkey-Net [29]. Ablation Study. We compare the following variants of our model. Baseline: the simplest model trained without using the occlusion mask (OSD=1 in Eq. (8)), jacobians (Jk = 1 in Eq. (4)) and is supervised with Lrec at the highest resolution only; Pyr.: the pyramid loss is added to Baseline; Pyr.+OSD: with respect to Pyr., we replace the generator network with the occlusion-aware network; Jac. w/o Eq. (12) our model with local affine transformations but without equivariance constraints on jacobians Eq. (12); Full: the full model including local affine transformations described in Sec. 3.1. In Fig. 3, we report the qualitative ablation. First, the pyramid loss leads to better results according to all the metrics except AKD. Second, adding OSD to the model consistently improves all the metrics with respect to Pyr.. This illustrates the benefit of explicitly modeling occlusions. We found that without equivariance constraint over the jacobians, Jk becomes unstable which leads to poor motion estimations. Finally, our Full model further improves all the metrics. In particular, we note that, with respect to the Baseline model, the MKR of the full model is smaller by the factor of 2.75. It shows that our rich motion representation helps generate more realistic images. These results are confirmed by our qualitative evaluation in Tab. 1 where we compare the Baseline and the Full models. In these experiments, each frame D of the input video is reconstructed from its first frame (first column) and the estimated keypoint trajectories. We note that the Baseline model does not locate any Input D Baseline Pyr. Pyr.+OSD Jac. w/o Eq. (12) Full Figure 3: Qualitative ablation on Tai-Chi-HD. Table 3: Video reconstruction: comparison with the state of the art on four different datasets. Tai-Chi-HD VoxCeleb Nemo Bair L1 (AKD, MKR) AED L1 AKD AED L1 AKD AED L1 X2Face [41] 0.080 (17.654, 0.109) 0.272 0.078 7.687 0.405 0.031 3.539 0.221 0.065 Monkey-Net [29] 0.077 (10.798, 0.059) 0.228 0.049 1.878 0.199 0.018 1.285 0.077 0.034 Ours 0.063 (6.862, 0.036) 0.179 0.043 1.294 0.140 0.016 1.119 0.048 0.027 Driving video image Source X2Face [41] MonkeyNet [29] Ours Figure 4: Qualitative comparison with state of the art for the task of image animation on two sequences and two source images from the Tai-Chi-HD dataset. keypoints in the arms area. Consequently, when the pose difference with the initial pose increases, the model",
    "output": "None\nNone\nAnd it works astonishingly well.\nI tried my luck on Nefertiti using the footage of AI pioneer Geoffrey Hinton. This footage is delivered with Notebook. You can use another video material. It has to fit into specific requirements and sizes.\nNone\nNone\nNone\nNone\nNone\nAge of DeepFakes: a chance?"
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\ncannot reconstruct the video (columns 3,4 and 5). In contrast, the Full model learns to detect a keypoint on each arm, and therefore, to more accurately reconstruct the input video even in the case of complex motion. Comparison with State of the Art. We now compare our method with state of the art for the video reconstruction task as in [29]. To the best of our knowledge, X2Face [41] and Monkey",
    "output": "None\nNone\nAnd it works astonishingly well.\nI tried my luck on Nefertiti using the footage of AI pioneer Geoffrey Hinton. This footage is delivered with Notebook. You can use another video material. It has to fit into specific requirements and sizes.\nNone\nNone\nNone\nNone\nNone\nAge of DeepFakes: a chance?"
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\n-Net [29] are the only previous approaches for model-free image animation. Quantitative results are reported in Tab. 3. We observe that our approach consistently improves every single metric for each of the four different datasets. Even on the two face datasets, VoxCeleb and Nemo datasets, our approach clearly outperforms X2Face that was originally proposed for face generation. The better performance of our approach compared to X2Face is especially impressive X2Face exploits a larger motion embedding (128 floats) than our approach (60=K*(2+4) floats). Compared to Monkey-Net that uses a motion representation with a similar dimension (50=K*(2+3)), the advantages of our approach are clearly visible on the Tai-Chi-HD dataset that contains highly non-rigid objects (i.e.human body). We now report a qualitative comparison for image animation. Generated sequences are reported in Fig. 4. The results are well in line with the quantitative evaluation in Tab. 3. Indeed, in both examples, X2Face and Monkey-Net are not able to correctly transfer the body notion in the driving video, instead warping the human body in the source image as a blob. Conversely, our approach is able to generate significantly better looking videos in which each body part is independently animated. This qualitative evaluation illustrates the potential of our rich motion description. We complete our evaluation with a user study. We ask users to select the most realistic image animation. Each question consists of the source image, the driving video, and the corresponding results of our method and a competitive method. We require each question to be answered by 10 AMT worker. This evaluation is repeated on 50 different input pairs. Results are reported in Tab. 2. We observe that our method is clearly preferred over the competitor methods. Interestingly, the largest difference with the state of the art is obtained on Tai-Chi-HD: the most challenging dataset in our evaluation due to its rich motions.",
    "output": "None\nNone\nAnd it works astonishingly well.\nI tried my luck on Nefertiti using the footage of AI pioneer Geoffrey Hinton. This footage is delivered with Notebook. You can use another video material. It has to fit into specific requirements and sizes.\nNone\nNone\nNone\nNone\nNone\nAge of DeepFakes: a chance?"
  },
  {
    "id_paper": "16",
    "id_story": "16-01",
    "id_chunk": "16-01_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 5 Conclusions We presented a novel approach for image animation based on keypoints and local affine transformations. Our novel mathematical formulation describes the motion field between two frames and is efficiently computed by deriving a first order Taylor expansion approximation. In this way, motion is described as a set of keypoints displacements and local affine transformations. A generator network combines the appearance of the source image and the motion representation of the driving video. In addition, we proposed to explicitly model occlusions in order to indicate to the generator network which image parts should be inpainted. We evaluated the proposed method both quantitatively and qualitatively and showed that our approach clearly outperforms state of the art on all the benchmarks.",
    "output": "Among various methods, the framework and paper \"First Order Motion Model for Image Animation\" by Aliaksandr Siarohin et al. captivates through its brilliant idea:\nMotion is described as a set of keypoints displacements and local affine transformations. A generator network combines the appearance of the source image and the motion representation of the driving video. In addition, we proposed to explicitly model occlusions in order to indicate to the generator network which image parts should be inpainted (source).\nThe result was more than convincing:\nIn opposite, applying my own photo delivers some glitches, especially regarding glasses. I suppose, these patterns are missing in the footage and so the allocation of keypoints sometimes fails:"
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Abstract\nAbstract: Artificial intelligence-supported screen reading versus standard double reading in the Mammography Screening with Artificial Intelligence trial (MASAI): a clinical safety analysis of a randomised, controlled, non-inferiority, singleblinded, screening accuracy study Kristina Lng, Viktoria Josefsson, Anna-Maria Larsson, Stefan Larsson, Charlotte Högberg, Hanna Sartor, Solveig Hofvind, Ingvar Andersson, Aldana Rosso Summary Background Retrospective studies have shown promising results using artificial intelligence (AI) to improve mammography screening accuracy and reduce screen-reading workload; however, to our knowledge, a randomised trial has not yet been conducted. We aimed to assess the clinical safety of an AI-supported screen-reading protocol compared with standard screen reading by radiologists following mammography. Lancet Oncol 2023; 24: 936–44 See Comment page 830 Division of Diagnostic Translational Medicine, Lund University, Malmö, Sweden (K Lng PhD, V Josefsson MD, H Sartor PhD, I Andersson PhD, Methods In this randomised, controlled, population-based trial, women aged 40–80 years eligible for mammography screening (including general screening with 15–2-year intervals and annual screening for those with moderate hereditary risk of breast cancer or a history of breast cancer) at four screening sites in Sweden were informed about the study as part of the screening invitation. Those who did not opt out were randomly allocated (1:1) to AI-supported screening (intervention group) or standard double reading without AI (control group). Screening examinations were automatically randomised by the Picture Archive and Communications System with a pseudo-random number generator after image acquisition. The participants and the radiographers acquiring the screening examinations, but not the radiologists reading the screening examinations, were masked to study group allocation. The AI system (Transpara version 1.7.0) provided an examination-based malignancy risk score on a 10-level scale that was used to triage screening examinations to single reading (score 1–9) or double reading (score 10), with AI risk scores (for all examinations) and computer-aided detection marks (for examinations with risk score 8–10) available to the radiologists doing the screen reading. Here we report the prespecified clinical safety analysis, to be done after 80 000 women were enrolled, to assess the secondary outcome measures of early screening performance (cancer detection rate, recall rate, false positive rate, positive predictive value [PPV] of recall, and type of cancer detected [invasive or in situ]) and screen-reading workload. Analyses were done in the modified intention-to-treat population (ie, all women randomly assigned to a group with one complete screening examination, excluding women recalled due to enlarged lymph nodes diagnosed with lymphoma). The lowest acceptable limit for safety in the intervention group was a cancer detection rate of more than 3 per 1000 participants screened. The trial is registered with ClinicalTrials.gov, NCT04838756, and is closed to accrual; follow-up is ongoing to assess the primary endpoint of the trial, interval cancer rate. Findings Between April 12, 2021, and July 28, 2022, 80 033 women were randomly assigned to AI-supported screening (n=40 003) or double reading without AI (n=40 030). 13 women were excluded from the analysis. The median age was 540 years (IQR 467–639). Race and ethnicity data were not collected. AI-supported screening among 39 996 participants resulted in 244 screen-detected cancers, 861 recalls, and a total of 46 345 screen readings. Standard screening among 40 024 participants resulted in 203 screen-detected cancers, 817 recalls, and a total of 83 231 screen readings. Cancer detection rates were 61 (95% CI 54–69) per 1000 screened participants in the intervention group, above the lowest acceptable limit for safety, and 51 (44–58) per 1000 in the control group—a ratio of 12 (95% CI 10–15; p=0052). Recall rates were 22% (95% CI 20–23) in the intervention group and 20% (19–22) in the control group. The false positive rate was 15% (95% CI 14–17) in both groups. The PPV of recall was",
    "output": "An interim safety analysis of the first randomized controlled trial of its kind involving over 80,000 Swedish women published in The Lancet Oncology journal, finds artificial intelligence (AI)-supported mammography analysis is as good as two breast radiologists working together to detect breast cancer, without increasing false positives and almost halving the screen-reading workload.\nHowever, the final trial results looking at whether the use of AI in interpreting mammography images translates into a reduction in interval cancers (cancers detected between screenings that generally have a poorer prognosis than screen-detected cancers) in 100,000 women followed over two years—and ultimately whether AI's use in mammography screening is justified—are not expected for several years.\n\"These promising interim safety results should be used to inform new trials and program-based evaluations to address the pronounced radiologist shortage in many countries. But they are not enough on their own to confirm that AI is ready to be implemented in mammography screening,\" cautions lead author Dr. Kristina Lång from Lund University, Sweden.\nIn the AI-supported analysis, the AI system first analyzed the mammography image and predicted the risk of cancer on a scale of one to 10, with one representing the lowest risk and 10 the highest. If the risk score was less than 10 the image was further analyzed by one radiologist, whereas if the AI system predicted a risk score of 10 then two radiologists analyzed the image.\nImportantly, there were 36,886 fewer screen readings by radiologists in the AI-supported group than in the control group (46,345 vs. 83,231), resulting in a 44% reduction in the screen-reading workload of radiologists.\nAlthough the actual time saved by using AI was not measured in the trial, the researchers calculate that if a radiologist reads on average 50 mammograms an hour, it would have taken one radiologist 4.6 months less to read the roughly 40,000 screening examinations with the help of AI compared with the roughly 40,000 in the control arm that were double read.\n\"The greatest potential of AI right now is that it could allow radiologists to be less burdened by the excessive amount of reading,\" says Lång. \"While our AI-supported screening system requires at least one radiologist in charge of detection, it could potentially do away with the need for double reading of the majority of mammograms easing the pressure on workloads and enabling radiologists to focus on more advanced diagnostics while shortening waiting times for patients.\"\nDespite the promising findings, the authors note several limitations including that the analysis was conducted at a single center and was limited to one type of mammography device and one AI system which might limit the generalizability of the results. They also note that while technical factors will affect the performance and processing of the AI system, these will likely be less important than the experience of radiologists.\nBecause the AI-supported system places the final decision on whether to recall women on radiologists, the results are dependent on their performance. In this trial, radiologists were moderately to highly experienced, which could limit the generalizability of the findings to less experienced readers. Lastly, information on race and ethnicity was not collected.\nMore information: Kristina Lång et al, Artificial intelligence-supported screen reading versus standard double reading in the Mammography Screening with Artificial Intelligence trial (MASAI): a clinical safety analysis of a randomised, controlled, non-inferiority, single blinded, screening accuracy study, The Lancet Oncology (2023). DOI: 10.1016/S1470-2045(23)00298-X , www.thelancet.com/journals/lan … (23)00298-X/fulltext Journal information: Lancet Oncology"
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Abstract_chunk2",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Abstract\n283% (95% CI 253–315) in the intervention group and 248% (219–280) in the control group. In the intervention group, 184 (75%) of 244 cancers detected were invasive and 60 (25%) were in situ; in the control group, 165 (81%) of 203 cancers were invasive and 38 (19%) were in situ. The screen-reading workload was reduced by 443% using AI. Interpretation AI-supported mammography screening resulted in a similar cancer detection rate compared with standard double reading, with a substantially lower screen-reading workload, indicating that the use of AI in mammography screening is safe. The trial was thus not halted and the primary endpoint of interval cancer rate will be assessed in 100 000 enrolled participants after 2-years of follow up.",
    "output": "An interim safety analysis of the first randomized controlled trial of its kind involving over 80,000 Swedish women published in The Lancet Oncology journal, finds artificial intelligence (AI)-supported mammography analysis is as good as two breast radiologists working together to detect breast cancer, without increasing false positives and almost halving the screen-reading workload.\nHowever, the final trial results looking at whether the use of AI in interpreting mammography images translates into a reduction in interval cancers (cancers detected between screenings that generally have a poorer prognosis than screen-detected cancers) in 100,000 women followed over two years—and ultimately whether AI's use in mammography screening is justified—are not expected for several years.\n\"These promising interim safety results should be used to inform new trials and program-based evaluations to address the pronounced radiologist shortage in many countries. But they are not enough on their own to confirm that AI is ready to be implemented in mammography screening,\" cautions lead author Dr. Kristina Lång from Lund University, Sweden.\nIn the AI-supported analysis, the AI system first analyzed the mammography image and predicted the risk of cancer on a scale of one to 10, with one representing the lowest risk and 10 the highest. If the risk score was less than 10 the image was further analyzed by one radiologist, whereas if the AI system predicted a risk score of 10 then two radiologists analyzed the image.\nImportantly, there were 36,886 fewer screen readings by radiologists in the AI-supported group than in the control group (46,345 vs. 83,231), resulting in a 44% reduction in the screen-reading workload of radiologists.\nAlthough the actual time saved by using AI was not measured in the trial, the researchers calculate that if a radiologist reads on average 50 mammograms an hour, it would have taken one radiologist 4.6 months less to read the roughly 40,000 screening examinations with the help of AI compared with the roughly 40,000 in the control arm that were double read.\n\"The greatest potential of AI right now is that it could allow radiologists to be less burdened by the excessive amount of reading,\" says Lång. \"While our AI-supported screening system requires at least one radiologist in charge of detection, it could potentially do away with the need for double reading of the majority of mammograms easing the pressure on workloads and enabling radiologists to focus on more advanced diagnostics while shortening waiting times for patients.\"\nDespite the promising findings, the authors note several limitations including that the analysis was conducted at a single center and was limited to one type of mammography device and one AI system which might limit the generalizability of the results. They also note that while technical factors will affect the performance and processing of the AI system, these will likely be less important than the experience of radiologists.\nBecause the AI-supported system places the final decision on whether to recall women on radiologists, the results are dependent on their performance. In this trial, radiologists were moderately to highly experienced, which could limit the generalizability of the findings to less experienced readers. Lastly, information on race and ethnicity was not collected.\nMore information: Kristina Lång et al, Artificial intelligence-supported screen reading versus standard double reading in the Mammography Screening with Artificial Intelligence trial (MASAI): a clinical safety analysis of a randomised, controlled, non-inferiority, single blinded, screening accuracy study, The Lancet Oncology (2023). DOI: 10.1016/S1470-2045(23)00298-X , www.thelancet.com/journals/lan … (23)00298-X/fulltext Journal information: Lancet Oncology"
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Introduction\nIntroduction: Introduction European guidelines recommend double reading of screening mammograms to ensure high sensitivity.1 A meta-analysis suggested that double reading resulted in 044 more cancers being detected per 1000 people screened than with single reading;2 however, this comes at the expense of a large screen-reading workload and can potentially increase false positives.3,4 Double reading can also be difficult to sustain because of a shortage of breast radiologists in many countries.5 In addition, despite double reading, some cancers might be missed and diagnosed as interval cancers.6 Interval cancers generally have a worse prognosis than screen-detected cancers, and the interval cancer rate is therefore an important indicator of screening efficacy.1,6 In retrospective studies, about 20–30% of interval cancers have been shown to display highly suspicious signs of malignancy at the preceding screening mammogram,6–8 suggesting that mammography alone could have been sufficient for detection—ie, without the need for supplementary imaging methods. Establishing a more efficient and effective mammography screening programme is therefore warranted. Recently developed image analysis tools based on artificial intelligence (AI) have promising applications in mammography screening, such as facilitating triage of screening examinations according to risk of malignancy or supporting detection with computer-aided detection Research in context Evidence before this study We searched MEDLINE for studies published in English between Jan 1, 2015, and Dec 31, 2020, that included “breast cancer screening” or “mammography screening”, and “artificial intelligence” or “machine learning” in the title or abstract. No prospective trials were identified. There were several retrospective accuracy studies using screening data or enriched datasets. We found no systematic reviews on test accuracy. The retrospective studies, using different artificial intelligence (AI) software and mammography devices, indicated that AI could be used to differentiate between screening examinations with low and high probability of malignancy, which could potentially be used to improve the efficacy of screening and reduce the workload, especially the requirement for double reading. (CAD) marks highlighting suspicious findings.9 Retrospective studies suggest that the accuracy of AI is similar to or better than that of breast radiologists.10–13 AI has also been shown to be able to identify examinations that were normal (ie, true negatives), and, since the vast majority of women who attend screening do not have breast cancer, adapting single and double reading to AI risk scores could allow more efficient screen reading.14–17 Additionally, AI has been shown to retrospectively classify screening examinations as high risk before a diagnosis of interval cancer, and could, therefore, help radiologists to reduce false negative screening results when used as detection support.16,18,19 Taken together, the evidence suggests that use of AI could potentially benefit mammography screening by reducing the screenreading workload and the number of interval cancers, but randomised trials are needed to assess the efficacy of AI-supported screening.13 In the randomised, controlled Mammography Screening with Artificial Intelligence trial (MASAI), we investigate an AI-supported screen-reading procedure involving triage of screening examinations to single or double reading, along with detection support. Here we report a prespecified safety analysis, the objective of which was to assess the safety of using AI-supported screening compared with standard double reading by determining the effect on cancer detection, which could be used to inform new trials or programme-based more cancers being detected and exceeded the lowest acceptable limit for safety compared with standard double reading without AI, without affecting the false positive rate. The AI supported screen-reading procedure enabled a 443% reduction in the screen-reading workload. The results indicate that the proposed screening strategy is safe. Implications of all the available evidence The results from this randomised trial support the findings of earlier retrospective studies, indicating a general potential of AI to improve screening efficacy and reduce workload. The clinical safety analysis concludes that the AI-supported screen-reading procedure can be considered safe. Implementation of AI in clinical practice to reduce the screen-reading workload could therefore be considered to help address workforce shortages. The assessment of the primary endpoint of interval cancer rate, together with a characterisation of detected cancers in the entire study population,",
    "output": "Breast cancer screening with mammography has been shown to improve prognosis and reduce mortality by detecting breast cancer at an earlier, more treatable stage. However, estimates suggest that 20–30% of interval cancers that should have been spotted at the preceding screening mammogram are missed, and suspicious findings often turn out to be benign.\nAI has been proposed as an automated second reader for mammograms that might help reduce this workload and improve screening accuracy. The technology has shown encouraging results in retrospective studies using AI to triage examinations to either single or double reading and by providing radiologists with computer-aided detection (CAD) marks highlighting suspicious features to reduce false negative results. But robust evidence from prospective randomized trials has been lacking."
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Introduction\nwill provide further insight into the efficacy of screening, possible side-effects such as overdiagnosis, and the prognostic implications of using AI in mammography screening, taking cost-effectiveness into account. evaluations. In addition, we compared recalls, false positives, positive predictive value of recalls, and screenreading workload for the two screen-reading procedures.",
    "output": "Breast cancer screening with mammography has been shown to improve prognosis and reduce mortality by detecting breast cancer at an earlier, more treatable stage. However, estimates suggest that 20–30% of interval cancers that should have been spotted at the preceding screening mammogram are missed, and suspicious findings often turn out to be benign.\nAI has been proposed as an automated second reader for mammograms that might help reduce this workload and improve screening accuracy. The technology has shown encouraging results in retrospective studies using AI to triage examinations to either single or double reading and by providing radiologists with computer-aided detection (CAD) marks highlighting suspicious features to reduce false negative results. But robust evidence from prospective randomized trials has been lacking."
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\nMethod: Methods Study design and participants The MASAI trial was designed as a randomised, parallel, non-inferiority, single-blinded, controlled, screening accuracy study to compare AI-supported mammography screening with standard double reading without AI. The study was done within the Swedish national screening programme and participants were recruited at four screening sites in southwest Sweden (Malmö, Lund, Landskrona, and Trelleborg). Screen reading and further assessment of recalled participants were done at a single site, the Unilabs Mammography Unit at Skne University Hospital (Malmö, Sweden). The inclusion criterion was women (defined here as people registered with a female Swedish personal identity number indicating female gender, which can include trans women who have changed their legal gender) eligible to participate in population-based mammography screening, which also includes those with moderate hereditary risk of breast cancer and those with a history of breast cancer. No exclusion criteria were applied. The Swedish population-based mammography screening programme invites women aged 40–74 years for screening at intervals of 15–2 years. Those younger than 55 years are first screened at 15-year intervals, and those aged 55 years or older are screened at 2-year intervals. Annual screening is done for people considered to have a moderate hereditary risk of breast cancer (lifetime risk 18–29%) and for those with a history of breast cancer (for 10 years after surgery, with an upper age limit of 80 years). Information about the study was included in screening invitation letters and in SMS text message reminders before scheduled appointments, with a link to a website containing detailed study information in Swedish and English. People eligible for screening who did not wish to participate in the trial were asked to opt out at the time of the screening visit and received standard of care. Information about the race or ethnicity of participants was not collected. For more on Transpara see https://screenpoint-medical.com The study was approved by the Swedish Ethical Review Authority (2020-04936), which also waived the need for written informed consent. The study protocol (versions 1.1 and 1.2) and the statistical analysis plan are available at the Lund University website. The protocol was updated to improve clarity; there were no changes in the trial procedures nor analyses in the statistical analysis plan from those described in the first and updated protocol versions. For the study protocol and statistical analysis plan see https://portal.research.lu.se/en/ projects/mammography- randomised in the Picture Archive and Communications System (PACS; Sectra, Linköping, Sweden) to AIsupported screening (intervention group) or standard double reading without AI (control group) with a pseudorandom number generator. The people screened and the radiographers acquiring the screening examinations were masked to study group allocation, since the automatic randomisation was not visible on the radiographer’s PACS interface. The screen readers were not masked to the results of the allocation. Procedures A single-vendor mammography system was used for the screening examinations (Senographe Pristina, GE Healthcare, Freiburg, Germany). Standard screening examination included two views per breast with the addition of implant-displacement views for people with breast implants. The examinations randomised to the intervention group were analysed using Transpara version 1.7.0 (ScreenPoint Medical, Nijmegen, Netherlands). This system uses deep learning to identify and interpret mammographic regions suspicious for cancer. It was developed with more than 200 000 examinations for training and testing, which were obtained from multiple institutions in more than ten countries covering a range of populations, modality manufacturers, and variations in screening and diagnostic workflows. Annotations of more than 10 000 cancers in the database are based on biopsy results and include regions marked in previous mammograms in which cancers were visible but not detected by radiologists. The AI system provided an examination-based malignancy risk score on a continuous scale ranging from 1 to 10. The risk scores were also presented on a discrete 10-level scale, calibrated to assign approximately a tenth of screening examinations to each risk score. Examinations were considered to",
    "output": "European guidelines recommend double reading of screening mammograms by two radiologists to ensure high sensitivity (to correctly identify those with disease). But there is a shortage of breast radiologists in many countries, including a shortfall of around 41 (8%) in the UK in 2020 and about 50 in Sweden, and it takes over a decade to train a radiologist capable of interpreting mammograms.\nBetween April 2021 and July 2022, 80,033 women aged 40-80 years who had undergone mammogram screening at four sites in southwest Sweden were randomly assigned in a 1:1 ratio to either AI-supported analysis, where a commercially available AI-supported mammogram reading system analyzed the mammograms before they were also read by one or two radiologists (intervention arm), or standard analysis performed by two radiologists without AI (control arm).\nThe system also provided CAD marks to assist radiologists in accurately interpreting mammography images. Women were recalled for additional testing based on suspicious findings. Radiologists had the final decision to recall women and were instructed to recall cases with the highest 1% risk, except for obvious false positives.\nWriting in a urled Comment, Dr. Nereo Segnan, former Head of the Unit of Cancer Epidemiology and past Director of Department of Screening at CPO Piemonte in Italy (who was not involved in the study) notes that the AI risk score for breast cancer seems very accurate at being able to separate high risk from low-risk women, adding that, \"In risk stratified screening protocols, the potential for appropriately modulating the criteria for recall in low-risk and high-risk groups is remarkable.\""
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\nbe low risk (risk score 1–7), intermediate risk (risk scores 8 and 9), or high risk (risk score 10). Cancer prevalence increases sharply in the group with a risk score of 10, and retrospective studies using the same AI version as in this trial have reported 87–90% of screen-detected cancers and 45% of interval cancers to be in this group.16,17 The AI system also provided CAD marks at suspicious",
    "output": "European guidelines recommend double reading of screening mammograms by two radiologists to ensure high sensitivity (to correctly identify those with disease). But there is a shortage of breast radiologists in many countries, including a shortfall of around 41 (8%) in the UK in 2020 and about 50 in Sweden, and it takes over a decade to train a radiologist capable of interpreting mammograms.\nBetween April 2021 and July 2022, 80,033 women aged 40-80 years who had undergone mammogram screening at four sites in southwest Sweden were randomly assigned in a 1:1 ratio to either AI-supported analysis, where a commercially available AI-supported mammogram reading system analyzed the mammograms before they were also read by one or two radiologists (intervention arm), or standard analysis performed by two radiologists without AI (control arm).\nThe system also provided CAD marks to assist radiologists in accurately interpreting mammography images. Women were recalled for additional testing based on suspicious findings. Radiologists had the final decision to recall women and were instructed to recall cases with the highest 1% risk, except for obvious false positives.\nWriting in a urled Comment, Dr. Nereo Segnan, former Head of the Unit of Cancer Epidemiology and past Director of Department of Screening at CPO Piemonte in Italy (who was not involved in the study) notes that the AI risk score for breast cancer seems very accurate at being able to separate high risk from low-risk women, adding that, \"In risk stratified screening protocols, the potential for appropriately modulating the criteria for recall in low-risk and high-risk groups is remarkable.\""
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\nregional findings of calcifications and soft-tissue lesions, with a regional risk score on a discrete scale from 1 to 98. To limit the number of CAD marks that could potentially disturb the screen reading or lead to an increase in false positives, the AI system was preconfigured for CAD marks to be available only for examinations with risk scores of 8, 9, and 10, accounting for approximately 30% of all examinations (regional risk score threshold >42). The AI system was also configured to analyse implant-displacement views in screening examinations of people with breast implants. The PACS was customised with separate worklists for single and double reading. Examinations with the highest 1% risk, classified as extra high risk, were flagged in the high-risk worklist as 10H. A risk score threshold of 98, which was determined from the observed risk score distribution in the screening population, was used to select this group. Screening examinations in the control group were not analysed with AI at any timepoint. In the intervention group, examinations with risk scores of 1–9 (low and intermediate risk) underwent single reading and examinations with risk scores of 10 (high risk) underwent double reading (figure 1). Double reading was done by two different breast radiologists. The second reader had access to the first reader’s assessment (unblinded double reading), which is the standard of care in the regional screening programme in the Skne region. Readers were aware of the examination risk score (for all examinations), presented both in the PACS worklists and on the image monitor. Readers first read the examination without CAD marks and then with CAD marks, if available (ie, for examinations with risk scores of 8–10). The readers were instructed to recall cases with the highest 1% risk, except for obvious false positives. In the control group, screening examinations were read with standard unblinded double reading without AI. The outcomes of the screen reading were either no suspicion of malignancy or recall. Participants could be recalled due to mammographic findings or selfreported symptoms. Current practice in the screening programme is to recall participants with self-reported symptoms, such as a lump, when the mammogram cannot safely be classified as normal. Before the final decision, readers had the option of referral to a consensus meeting or to a technical recall (eg, due to poor image quality), or both. Consensus meetings are common practice in screening programmes with double reading; in these meetings, difficult or equivocal findings are reassessed by two radiologists, with a joint decision made to recall or clear of suspicion of malignancy.3 The images acquired at technical recall were by default randomised de novo due to the technical setup; however, participants were assessed according to their originally assigned group. Screening examinations allocated to the intervention group that failed to be processed by AI underwent standard-of-care reading. 16 breast radiologists at the Unilabs Mammography Unit at Skne University Hospital were involved in the screen reading, of whom 15 had more than 2 years of experience in breast imaging and 14 had more than 5 years of experience. 12 of the radiologists had a yearly reading volume of at least 5000 cases. Three radiologists had a yearly reading volume of 1000–3000 cases, and one radiologist read on average 700 cases per year. Based on the group composition, only readers with more than 2 years of experience were allowed to read from the single-reading worklist. Before each screen-reading session, the radiologist rolled a six-sided die to randomly allocate themselves to either of the two groups: numbers 1–3 allocated them to the control group and 4–6 to the intervention group. Participants could withdraw from the study at any time, at which point all personal data would be removed and they would be excluded from analyses. True positive cases were initially identified through linkage with the Regional Cancer Registry (on Sept 12, 2022); to compensate for a delay in registry reporting, all recalled participants were manually assessed with use of patient records, and true positives were validated by histopathology reports on surgical samples or core-needle biopsies. Outcomes The primary outcome measure of the MASAI trial is",
    "output": "European guidelines recommend double reading of screening mammograms by two radiologists to ensure high sensitivity (to correctly identify those with disease). But there is a shortage of breast radiologists in many countries, including a shortfall of around 41 (8%) in the UK in 2020 and about 50 in Sweden, and it takes over a decade to train a radiologist capable of interpreting mammograms.\nBetween April 2021 and July 2022, 80,033 women aged 40-80 years who had undergone mammogram screening at four sites in southwest Sweden were randomly assigned in a 1:1 ratio to either AI-supported analysis, where a commercially available AI-supported mammogram reading system analyzed the mammograms before they were also read by one or two radiologists (intervention arm), or standard analysis performed by two radiologists without AI (control arm).\nThe system also provided CAD marks to assist radiologists in accurately interpreting mammography images. Women were recalled for additional testing based on suspicious findings. Radiologists had the final decision to recall women and were instructed to recall cases with the highest 1% risk, except for obvious false positives.\nWriting in a urled Comment, Dr. Nereo Segnan, former Head of the Unit of Cancer Epidemiology and past Director of Department of Screening at CPO Piemonte in Italy (who was not involved in the study) notes that the AI risk score for breast cancer seems very accurate at being able to separate high risk from low-risk women, adding that, \"In risk stratified screening protocols, the potential for appropriately modulating the criteria for recall in low-risk and high-risk groups is remarkable.\""
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\ninterval cancer rate, which will be assessed after the full study population of 100 000 screened participants have had at least a 2-year follow-up (estimated December, 2024). Secondary outcome measures are early screening performance (cancer detection rate, recall rate, false positive rate, and positive predictive value [PPV] of recall), screen-reading workload (number of screen-readings and consensus meetings), detection in relation to tumour type and stage, proportion of interval",
    "output": "European guidelines recommend double reading of screening mammograms by two radiologists to ensure high sensitivity (to correctly identify those with disease). But there is a shortage of breast radiologists in many countries, including a shortfall of around 41 (8%) in the UK in 2020 and about 50 in Sweden, and it takes over a decade to train a radiologist capable of interpreting mammograms.\nBetween April 2021 and July 2022, 80,033 women aged 40-80 years who had undergone mammogram screening at four sites in southwest Sweden were randomly assigned in a 1:1 ratio to either AI-supported analysis, where a commercially available AI-supported mammogram reading system analyzed the mammograms before they were also read by one or two radiologists (intervention arm), or standard analysis performed by two radiologists without AI (control arm).\nThe system also provided CAD marks to assist radiologists in accurately interpreting mammography images. Women were recalled for additional testing based on suspicious findings. Radiologists had the final decision to recall women and were instructed to recall cases with the highest 1% risk, except for obvious false positives.\nWriting in a urled Comment, Dr. Nereo Segnan, former Head of the Unit of Cancer Epidemiology and past Director of Department of Screening at CPO Piemonte in Italy (who was not involved in the study) notes that the AI risk score for breast cancer seems very accurate at being able to separate high risk from low-risk women, adding that, \"In risk stratified screening protocols, the potential for appropriately modulating the criteria for recall in low-risk and high-risk groups is remarkable.\""
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\ncancers by cancer type and stage, sensitivity and specificity, and incremental cost-effectiveness ratio. In the current clinical safety analysis, the secondary outcome measures of early screening performance of cancer detection rate (number of cancers detected per 1000 participants screened), recall rate (proportion of screened participants who were recalled), false positive rate, PPV of recall, type of detected cancer (invasive or in situ), and screen-reading workload were assessed. The screen-reading workload was reported as the sum of all screen readings, including those made at consensus meetings. The number and proportion of screenings that resulted in a consensus meetings (consensus meeting rate) were also reported separately. Statistical analysis The intention-to-treat population comprised all participants who underwent breast screening. The modified intentionto-treat (mITT) population comprised participants with a complete screening examination, excluding those who were asked to attend a technical recall but did not attend. Participants recalled due to bilateral enlarged lymph nodes and diagnosed with lymphoma were also excluded from the mITT population, since they were not recalled due to AI triage Risk score 10 Double reading plus AI Intervention group Risk score 1–9 suspicion of breast cancer. Participants were analysed in their allocated group regardless of the actual reading procedure (treatment policy strategy). The hypothesis for the primary analysis was the noninferiority of AI-supported mammography screening compared with standard double reading, in terms of interval cancer rate, with a secondary hypothesis of superiority. Considering the interplay of screen-reading workload and the number of interval cancers, the noninferiority margin for the primary endpoint was set at the intervention yielding at most 20% more interval cancers than in the control group. The sample size calculations were done with use of Fisher’s exact test to compare the risk ratio based on the observed interval cancer rate in the current screening programme. A total sample size of 100 000 (intention-to-treat population) was expected to have at least 80% statistical power to show that the ratio of the interval cancer rate is at most 12 in the intervention group compared with the control group. The mITT population among the 100 000 enrolled participants will be used in the primary analysis. The sample size calculation for the clinical safety analysis was based on a worst case scenario of the intervention yielding a cancer detection rate of 3 per 1000 screened participants (based on the assumption that single reading could lead to reduced detection), at which rate the study could be halted, compared with a detection rate of 5 per 1000 screened participants in the control group (reflecting the observed rate in the current screening programme). According to Fisher’s exact test, a sample size of 80 000 (intention-to-treat population) was needed to show with a power greater than 80% that the proportion of detected cancer did not reach the worst case scenario. The mITT population among the 80 000 enrolled participants was used in the clinical safety analysis. Throughout the study, the overall recall rate was monitored as part of the institutional quality assurance reports to ensure that the recall rate did not drop below what was observed in the clinic 6 months before the start of the trial (average recall rate 21%), which could indicate a reduction in cancer detection. The number of enrolled participants was monitored monthly. Trial data were extracted from PACS on Sept 12, 2022, which was 15 months after roughly 80 000 participants had been enrolled, to ensure sufficient time for the screen reading and initial investigations of recalled participants. Participants were matched with the Regional Cancer Registry on the same day as data extraction and a rapid preliminary analysis of cancer detection was available 1 week later, which was used to inform the decision to continue the trial. Descriptive statistics were used to summarise study population characteristics. Frequencies and percentages were calculated for categorical data. 95% CIs were calculated with the Clopper-Pearson method. The cancer detection rate, recall rate, false positive rate, and PPV of recall were calculated separately for the intervention and control groups. The cancer detection rate was compared with Fisher’s exact test and the ratio of the proportions with corresponding 95% CIs were reported. A two-sided p value of less than",
    "output": "European guidelines recommend double reading of screening mammograms by two radiologists to ensure high sensitivity (to correctly identify those with disease). But there is a shortage of breast radiologists in many countries, including a shortfall of around 41 (8%) in the UK in 2020 and about 50 in Sweden, and it takes over a decade to train a radiologist capable of interpreting mammograms.\nBetween April 2021 and July 2022, 80,033 women aged 40-80 years who had undergone mammogram screening at four sites in southwest Sweden were randomly assigned in a 1:1 ratio to either AI-supported analysis, where a commercially available AI-supported mammogram reading system analyzed the mammograms before they were also read by one or two radiologists (intervention arm), or standard analysis performed by two radiologists without AI (control arm).\nThe system also provided CAD marks to assist radiologists in accurately interpreting mammography images. Women were recalled for additional testing based on suspicious findings. Radiologists had the final decision to recall women and were instructed to recall cases with the highest 1% risk, except for obvious false positives.\nWriting in a urled Comment, Dr. Nereo Segnan, former Head of the Unit of Cancer Epidemiology and past Director of Department of Screening at CPO Piemonte in Italy (who was not involved in the study) notes that the AI risk score for breast cancer seems very accurate at being able to separate high risk from low-risk women, adding that, \"In risk stratified screening protocols, the potential for appropriately modulating the criteria for recall in low-risk and high-risk groups is remarkable.\""
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\n005 was considered to indicate statistical significance. The numbers of screen readings and consensus meetings were calculated separately for the intervention and the control groups. In a post-hoc analysis, the distribution of AI risk scores by screening examinations, screen-detected cancers, recalls, and PPV of recall were reported with descriptive statistics. This analysis was included to describe AI performance and no inferential statistical analyses were done. All statistical analyses were done",
    "output": "European guidelines recommend double reading of screening mammograms by two radiologists to ensure high sensitivity (to correctly identify those with disease). But there is a shortage of breast radiologists in many countries, including a shortfall of around 41 (8%) in the UK in 2020 and about 50 in Sweden, and it takes over a decade to train a radiologist capable of interpreting mammograms.\nBetween April 2021 and July 2022, 80,033 women aged 40-80 years who had undergone mammogram screening at four sites in southwest Sweden were randomly assigned in a 1:1 ratio to either AI-supported analysis, where a commercially available AI-supported mammogram reading system analyzed the mammograms before they were also read by one or two radiologists (intervention arm), or standard analysis performed by two radiologists without AI (control arm).\nThe system also provided CAD marks to assist radiologists in accurately interpreting mammography images. Women were recalled for additional testing based on suspicious findings. Radiologists had the final decision to recall women and were instructed to recall cases with the highest 1% risk, except for obvious false positives.\nWriting in a urled Comment, Dr. Nereo Segnan, former Head of the Unit of Cancer Epidemiology and past Director of Department of Screening at CPO Piemonte in Italy (who was not involved in the study) notes that the AI risk score for breast cancer seems very accurate at being able to separate high risk from low-risk women, adding that, \"In risk stratified screening protocols, the potential for appropriately modulating the criteria for recall in low-risk and high-risk groups is remarkable.\""
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Method\nwith Stata IC 17.0 software and Python 3.8.5. The trial is registered with ClincialTrials.gov, NCT04838756. Role of the funding source The funders of the study had no role in study design, data collection, data analysis, data interpretation, or writing of the report.",
    "output": "European guidelines recommend double reading of screening mammograms by two radiologists to ensure high sensitivity (to correctly identify those with disease). But there is a shortage of breast radiologists in many countries, including a shortfall of around 41 (8%) in the UK in 2020 and about 50 in Sweden, and it takes over a decade to train a radiologist capable of interpreting mammograms.\nBetween April 2021 and July 2022, 80,033 women aged 40-80 years who had undergone mammogram screening at four sites in southwest Sweden were randomly assigned in a 1:1 ratio to either AI-supported analysis, where a commercially available AI-supported mammogram reading system analyzed the mammograms before they were also read by one or two radiologists (intervention arm), or standard analysis performed by two radiologists without AI (control arm).\nThe system also provided CAD marks to assist radiologists in accurately interpreting mammography images. Women were recalled for additional testing based on suspicious findings. Radiologists had the final decision to recall women and were instructed to recall cases with the highest 1% risk, except for obvious false positives.\nWriting in a urled Comment, Dr. Nereo Segnan, former Head of the Unit of Cancer Epidemiology and past Director of Department of Screening at CPO Piemonte in Italy (who was not involved in the study) notes that the AI risk score for breast cancer seems very accurate at being able to separate high risk from low-risk women, adding that, \"In risk stratified screening protocols, the potential for appropriately modulating the criteria for recall in low-risk and high-risk groups is remarkable.\""
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Experiments / Results\nExperiments / Results: Results Between April 12, 2021, and July 28, 2022, 80 160 women presented for screening and 127 (02%) opted out of the trial. 80 033 participants were randomly assigned: 40 003 (500%) to undergo AI-supported screening (intervention group) and 40 030 (500%) to undergo double reading without AI (control group). 39 996 partici­ pants in the intervention group and 40 024 in the control group were included in the clinical safety analysis (mITT Figure 2: Trial profile AI=artificial intelligence. population; figure 2). The median age of participants in the mITT population was 540 years (IQR 467–639). The age distribution and indication for screening was similar between groups (table 1). The AI system did not to provide a risk score for 306 (08%) of 39 996 participants in the intervention group. There were 38 (01%) technical recalls among 40 003 participants in the intervention group and 46 (01%) among 40 030 participants in the control group. Early screening performance and workload measures are presented in table 2. Based on the rapid preliminary analysis of cancer detection internally reported on Sept 20, 2022, among the 39 996 participants screened with AI, 244 cancers were detected and 861 participants were recalled. Among the 40 024 participants in the control group, 203 cancers were detected and 817 partici­ pants were recalled. The cancer detection rate was 61 (95% CI 54–69) per 1000 participants for AI-supported screening (ie, above the lower safety limit) and 51 (44–58) per 1000 for double reading without AI, a ratio of 12 (95% CI 10–15; p=0052). The absolute difference in cancer detection per 1000 screened participants was 10 (95% CI 00–21). The false positive rate was the same in both groups. 36 886 fewer screen readings were done in the intervention group than in the control group, representing in a 443% reduction in the screen-reading workload. Of the 244 cancers detected in the intervention group, 184 (75%) were invasive, among which 152 (83%) were stage T1 (tumour diameter 20 mm). In the control group, 165 (81%) of 203 cancers were invasive, of which 129 (78%) were stage T1. In situ cancers constituted 60 (25%) detected cancers in the intervention group and 38 (19%) in the control group. The distribution of AI risk scores in the intervention group and early screening performance measures per risk score are presented in table 3 (post hoc). The cancer detection rate in the high-risk group (ie, those with a risk score of 10 that underwent double reading) was 723 per 1000 participants screened (208 of 2875 participants), a frequency of one cancer per 14 screening examinations. In the high-risk group, 11 (26%) of 416 recalls were due to self-reported symptoms. Of the 490 screening examinations flagged as extra high risk by AI (highest 1% risk), 189 (386%) were recalled—ie, 220% of all 861 recalls in the intervention group. Of the 189 recalled participants classified as being extra high risk, 136 had cancer (PPV of recall 720%), resulting in a cancer detection rate of 2776 per 1000 screening examinations in the extrahigh-risk category. Thus, the 12% of screening examinations flagged as extra high risk contained 557% of all screen-detected cancers in this group. 36 815 (920%) of 39 996 screening examinations were those with risk scores of 1–9 (which underwent single reading), among which there were 440 (12%) recalls (511% of all 861 recalls), including 114 (259%) recalls Intervention group (n=39 996) Control group (n=40 024) Age, years Mean (SD) 553 (102) 553 (102) Range 396–801 395–799 45 7568 (189%) 7607 (190%) 45–49 7155 (179%) 7209 (180%) 50–54 6505 (163%) 6559 (164%) 55–59 5021 (126%) 4822 (120%) 60–64 5007 (125%) 5214 (130%) 65–69 4345 (109%)",
    "output": "AI failed to provide a risk score in 0.8% of cases (306/39,996) that were referred to standard care (double reading).\nThe recall rates averaged 2.2% (861 women) for AI-supported screening and 2.0% (817 women) for standard double reading without AI. These were similar to the average 2.1% recall rate in the clinic six months prior to the trial starting, indicating that cancer detection rates had not fallen.\nIn total, 244 women (28%) recalled from AI-supported screening were found to have cancer compared with 203 women (25%) recalled from standard screening—resulting in 41 more cancers detected with the support of AI (of which 19 were invasive and 22 were in situ cancers). The false-positive rate was 1.5% in both arms."
  },
  {
    "id_paper": "54",
    "id_story": "54-01",
    "id_chunk": "54-01_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Researcher, Teacher\n\nSection: Experiments / Results\n4265 (107%) 70 4395 (110%) 4348 (109%) Screening indication General screening 38 969 (974%) 38 951 (973%) History of breast cancer 984 (25%) 1017 (25%) Moderate hereditary risk 43 (01%) 56 (01%) Data are mean (SD), range, or n (%). Table 1: Baseline population characteristics, modified intention-to-treat population Intervention group (n=39 996) Control group (n=40 024) Early screening performance Number of recalls 861 817 Recall rate, % 22% (20–23) 20% (19–22) Number of screendetected cancers 244 203 61 (54–69) 51 (44–58) Cancer-detection rate, per 1000 participants screened False positive rate, % 15% (14–17) 15% (14–17) Positive predictive value of recall, % 283% (253–315) 248% (219–280) Workload Number of screen readings 46 345 83 231 Number of consensus meetings 1584 1576 Consensus meeting rate 40% (38–42) 39% (38–41) Data are n or point estimate (95% CI). Table 2: Early screening performance and workload measures, modified intention-to-treat population based on self-reported symptoms. 36 cancers were detected by screening in the single-reading with AI group (148% of all 244 screen-detected cancers), with an overall cancer detection rate of 10 per 1000 participants screened. There was a considerable difference in cancer detection rate between those with risk scores of 1–7 (02 per 1000 participants screened; six cancers detected among 30 464 participants) and those with risk scores of 8–9 (47 per 1000; 30 cancers detected among 6351 partici­pants), meaning that, to detect one cancer, radiologists had to read Positive predictive value of recalls Screendetected cancers (n=244) Participants screened (N=39 996) Participants recalled (n=861) 10 2875 (72%) 416 (483%) 208 (852%) 500% 9 3212 (80%) 116 (135%) 23 (94%) 198% 8 3139 (78%) 65 (75%) 7 (29%) 108% 7 3075 (77%) 36 (42%) 1 (04%) 28% 6 3193 (80%) 41 (48%) 1 (04%) 24% 5 3503 (88%) 52 (60%) 0 0% 4 3697 (92%) 35 (41%) 1 (04%) 29% 3 4247 (106%) 30 (35%) 1 (04%) 33% 2 4368 (109%) 31 (36%) 1 (04%) 32% 1 8381 (210%) 34 (39%) 1 (04%) 29% Missing score 306 (08%) 5 (06%) 0 0% Data are n (%) or %. Table 3: Distribution of artificial intelligence examination risk scores and early screening performance measures, intervention group (post-hoc analysis) 5000 examinations in the group with scores of 1–7 and 212 examinations in the group with scores of 8–9.",
    "output": "AI failed to provide a risk score in 0.8% of cases (306/39,996) that were referred to standard care (double reading).\nThe recall rates averaged 2.2% (861 women) for AI-supported screening and 2.0% (817 women) for standard double reading without AI. These were similar to the average 2.1% recall rate in the clinic six months prior to the trial starting, indicating that cancer detection rates had not fallen.\nIn total, 244 women (28%) recalled from AI-supported screening were found to have cancer compared with 203 women (25%) recalled from standard screening—resulting in 41 more cancers detected with the support of AI (of which 19 were invasive and 22 were in situ cancers). The false-positive rate was 1.5% in both arms."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: Abstract We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.1 1",
    "output": "This opens the door for speech recognition models in many more languages, dialects, and domains that previously required much more transcribed audio data to provide acceptable accuracy.\nSimilar to the Bidirectional Encoder Representations from Transformers (BERT), our model is trained by predicting speech units for masked parts of the audio. A major difference is that speech audio is a continuous signal that captures many aspects of the recording with no clear segmentation into words or other units. Wav2vec 2.0 tackles this issue by learning basic units that are 25ms long to enable learning of high-level contextualized representations. These units are then used to describe many different speech audio recordings and make wav2vec more robust. This enables us to build speech recognition systems that can outperform the best semisupervised methods, even with 100x less labeled training data.\nLearning discrete latent speech units\nWhat happens if we increase the amount of unannotated data? To answer this question, we trained on 53K hours of unannotated data from the LibriVox dataset (a large collection of public audiobooks) and fine-tuned with only 10 minutes of labeled data. The result was a model that still achieved a WER of 8.6 percent. This demonstrates that wav2vec 2.0 can enable speech recognition models for settings where there is very little labeled training data."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: Introduction Neural networks benefit from large quantities of labeled training data. However, in many settings labeled data is much harder to come by than unlabeled data: current speech recognition systems require thousands of hours of transcribed speech to reach acceptable performance which is not available for the vast majority of the nearly 7,000 languages spoken worldwide [31]. Learning purely from labeled examples does not resemble language acquisition in humans: infants learn language by listening to adults around them - a process that requires learning good representations of speech. In machine learning, self-supervised learning has emerged as a paradigm to learn general data representations from unlabeled examples and to fine-tune the model on labeled data. This has been particularly successful for natural language processing [43, 45, 9] and is an active research area for computer vision [20, 2, 36, 19, 6]. In this paper, we present a framework for self-supervised learning of representations from raw audio data. Our approach encodes speech audio via a multi-layer convolutional neural network and then masks spans of the resulting latent speech representations [26, 56], similar to masked language modeling [9]. The latent representations are fed to a Transformer network to build contextualized representations and the model is trained via a contrastive task where the true latent is to be distinguished from distractors [54, 49, 48, 28] ( 2). As part of training, we learn discrete speech units [53, 32, 7, 18] via a gumbel softmax [24, 5] to represent the latent representations in the contrastive task (Figure 1) which we find to be more effective than non-quantized targets. After pre-training on unlabeled speech, the model is fine-tuned Context representations C ... ... Quantized representations Q q q q q q Latent speech representations Z X raw waveform Figure 1: Illustration of our framework which jointly learns contextualized speech representations and an inventory of discretized speech units. on labeled data with a Connectionist Temporal Classification (CTC) loss [14, 4] to be used for downstream speech recognition tasks ( 3) Previous work learned a quantization of the data followed by a contextualized representations with a self-attention model [5, 4], whereas our approach solves both problems end-to-end. Masking parts of the input with Transformer networks for speech has been explored [4, 26], but prior work relies either on a two-step pipeline or their model is trained by reconstructing the filter bank input features. Other related work includes learning representations from auto-encoding the input data [52, 11] or directly predicting future timesteps [8]. Our results show that jointly learning discrete speech units with contextualized representations achieves substantially better results than fixed units learned in a prior step [4]. We also demonstrate the feasibility of ultra-low resource speech recognition: when using only 10 minutes of labeled data, our approach achieves word error rate (WER) 4.8/8.2 on the clean/other test sets of Librispeech. We set a new state of the art on TIMIT phoneme recognition as well as the 100 hour clean subset of Librispeech. Moreover, when we lower the amount of labeled data to just one hour, we still outperform the previous state of the art self-training method of [42] while using 100 times less labeled data and the same amount of unlabeled data. When we use all 960 hours of labeled data from Librispeech, then our model achieves 1.8/3.3 WER ( 4,  5).",
    "output": "Traditional speech recognition models are primarily trained on annotated speech audio with transcriptions. Good systems require large amounts of annotated data, which is only available for a small number of languages. Self-supervision provides a way to leverage unannotated data to build better systems.\nOther self-supervised approaches for speech try to reconstruct the audio signal, which requires the model to capture every aspect of the speech, including recording environment, channel noise, and speaker traits. Another common approach is to train the model by asking it to predict what the speaker said next by contrasting several options."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 2 Model Our model is composed of a multi-layer convolutional feature encoder f : X 7Z which takes as input raw audio X and outputs latent speech representations z1, . . . , zT for T time-steps. They are then fed to a Transformer g : Z 7C to build representations c1, . . . , cT capturing information from the entire sequence [9, 5, 4]. The output of the feature encoder is discretized to qt with a quantization module Z 7Q to represent the targets (Figure 1) in the self-supervised objective ( 3.2). Compared to vq-wav2vec [5], our model builds context representations over continuous speech representations and self-attention captures dependencies over the entire sequence of latent representations end-to-end. Feature encoder. The encoder consists of several blocks containing a temporal convolution followed by layer normalization [1] and a GELU activation function [21]. The raw waveform input to the encoder is normalized to zero mean and unit variance. The total stride of the encoder determines the number of time-steps T which are input to the Transformer ( 4.2). Contextualized representations with Transformers. The output of the feature encoder is fed to a context network which follows the Transformer architecture [55, 9, 33]. Instead of fixed positional embeddings which encode absolute positional information, we use a convolutional layer similar to [37, 4, 57] which acts as relative positional embedding. We add the output of the convolution followed by a GELU to the inputs and then apply layer normalization.  Contrastive loss L Transformer Masked CNN results in prior work which learned discrete units in a first step followed by learning contextualized representations [5]. Product quantization amounts to choosing quantized representations from multiple codebooks and concatenating them. Given G codebooks, or groups, with V entries e  RV d/G, we choose one entry from each codebook and concatenate the resulting vectors e1, . . . , eG and apply a linear transformation Rd 7Rf to obtain q Rf. The Gumbel softmax enables choosing discrete codebook entries in a fully differentiable way [16, 24, 35]. We use the straight-through estimator [26] and setup G hard Gumbel softmax operations [24]. The feature encoder output z is mapped to l RGV logits and the probabilities for choosing the v-th codebook entry for group g are pg,v = exp(lg,v + nv)/ PV k=1 exp(lg,k + nk)/ , (1) where  is a non-negative temperature, n = log(log(u)) and u are uniform samples from U(0, 1). During the forward pass, codeword i is chosen by i = argmaxjpg,j and in the backward pass, the true gradient of the Gumbel softmax outputs is used. 3 Training To pre-train the model we mask a certain proportion of time steps in the latent feature encoder space ( 3.1), similar to masked language modeling in BERT [9]. The training objective requires identifying the correct quantized latent audio representation in a set of distractors for each masked time step ( 3.2) and the final model is fine-tuned on the labeled data ( 3.3). 3.1 Masking We mask a proportion of the feature encoder outputs, or time steps before feeding them to the context network and replace them with a trained feature vector shared between all masked time steps; we do not mask inputs to the quantization module. To mask the latent speech representations output by the encoder, we randomly sample without replacement a certain proportion p of all time steps to be starting indices and then mask the subsequent M consecutive time steps from every sampled index; spans may overlap. 3.2 Objective During pre-training, we learn representations of speech audio by solving a contrastive task Lm which requires to identify the true quantized latent speech representation for a masked time step within a set of distractors. This is augmented by a codebook diversity loss Ld to encourage the model to use the code",
    "output": "The model first processes the raw waveform of the speech audio with a multilayer convolutional neural network to get latent audio representations of 25ms each. These representations are then fed into a quantizer as well as a transformer. The quantizer chooses a speech unit for the latent audio representation from an inventory of learned units. About half the audio representations are masked before being fed into the transformer. The transformer adds information from the entire audio sequence. Finally, the output of the transformer is used to solve a contrastive task. This task requires the model to identify the correct quantized speech units for the masked positions."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nbook entries equally often. L = Lm + Ld (2) where  is a tuned hyperparameter. Contrastive Loss. Given context network output ct centered over masked time step t, the model needs to identify the true quantized latent speech representation qt in a set of K + 1 quantized candidate representations q Qt which includes qt and K distractors [23, 54]. Distractors are uniformly sampled from other masked time steps of the same utterance. The loss is defined as Lm = log exp(sim(ct, qt)/) P qQt exp(sim(ct, q)/) (3) where we compute the cosine similarity sim(a, b) = aT b/abbetween context representations and quantized latent speech representations [19, 6]. codebook pg across a batch of utterances; the softmax disribution does not contain the gumbel noise nor a temperature:2 G X Ld = 1 GV g=1 H( pg) = 1 GV 3.3 Fine-tuning Pre-trained models are fine-tuned for speech recognition by adding a randomly initialized linear projection on top of the context network into C classes representing the vocabulary of the task [4]. For Librispeech, we have 29 tokens for character targets plus a word boundary token. Models are optimized by minimizing a CTC loss [14] and we apply a modified version of SpecAugment [41] by masking to time-steps and channels during training which delays overfitting and significantly improves the final error rates, especially on the Libri-light subsets with few labeled examples. 4 Experimental Setup 4.1 Datasets As unlabeled data we consider the Librispeech corpus [40] without transcriptions containing 960 hours of audio (LS-960) or the audio data from LibriVox (LV-60k). For the latter we follow the preprocessing of [27] resulting in 53.2k hours of audio. We fine-tune on five labeled data settings: 960 hours of transcribed Librispeech, the train-clean-100 subset comprising 100 hours (100 hours labeled), as well as the Libri-light limited resource training subsets originally extracted from Librispeech, these are train-10h (10 hours labeled), train-1h (1 hour labeled), train-10min (10 min labeled). We follow the evaluation protocol of Libri-light for these splits and evaluate on the standard Librispech dev-other/clean and test-clean/other sets. We fine-tune the pre-trained models for phoneme recognition on the TIMIT dataset [13]. It contains five hours of audio recordings with detailed phoneme labels. We use the standard train, dev and test split and follow the standard protocol of collapsing phone labels to 39 classes. 4.2 Pre-training Models are implemented in fairseq [39]. For masking, we sample p = 0.065 of all time-steps to be starting indices and mask the subsequent M = 10 time-steps. This results in approximately 49% of all time steps to be masked with a mean span length of 14.7, or 299ms (see Appendix A for more details on masking). The feature encoder contains seven blocks and the temporal convolutions in each block have 512 channels with strides (5,2,2,2,2,2,2) and kernel widths (10,3,3,3,3,2,2). This results in an encoder output frequency of 49 hz with a stride of about 20ms between each sample, and a receptive field of 400 input samples or 25ms of audio. The convolutional layer modeling relative positional embeddings has kernel size 128 and 16 groups. We experiment with two model configurations which use the same encoder architecture but differ in the Transformer setup: BASE contains 12 transformer blocks, model dimension 768, inner dimension (FFN) 3,072 and 8 attention heads. Batches are built by cropping 250k audio samples, or 15.6sec, from each example. Crops are batched together to not exceed 1.4m samples per GPU and we train on a total of 64 V100 GPUs for 1.6 days [38]; the total batch size is 1.6h",
    "output": "The model first processes the raw waveform of the speech audio with a multilayer convolutional neural network to get latent audio representations of 25ms each. These representations are then fed into a quantizer as well as a transformer. The quantizer chooses a speech unit for the latent audio representation from an inventory of learned units. About half the audio representations are masked before being fed into the transformer. The transformer adds information from the entire audio sequence. Finally, the output of the transformer is used to solve a contrastive task. This task requires the model to identify the correct quantized speech units for the masked positions."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n. The LARGE model contains 24 transformer blocks with model dimension 1,024, inner dimension 4,096 and 16 attention heads. We crop 320k audio samples, or 20sec, with a limit of 1.2m samples per GPU and train on 128 V100 GPUs over 2.3 days for Librispeech and 5.2 days for LibriVox; the total batch size is 2.7h. We use dropout 0.1 in the Transformer, at the output of the feature encoder and the input to the quantization module. Layers are dropped at a rate of 0.05 for BASE and 0.2 for LARGE [22, 12]; there is no layer drop for LV-60k. G X V X v=1 pg,v log pg,v (4) g=1 We optimize with Adam [29], warming up the learning rate for the first 8% of updates to a peak of 5 104 for BASE and 3 104 for LARGE, and then linearly decay it. LARGE trains for 250k updates, BASE for 400k updates, and LARGE on LV-60k for 600k updates. We use weight = 0.1 for the diversity loss Equation 2. For the quantization module we use G = 2 and V = 320 for both models, resulting in a theoretical maximum of 102.4k codewords. Entries are of size d/G = 128 for BASE amd d/G = 384 for LARGE. The Gumbel softmax temperature is annealed from 2 to a minimum of 0.5 for BASE and 0.1 for LARGE by a factor of 0.999995 at every update. The temperature in the contrastive loss (Equation 3) is set to = 0.1. For the smaller Librispeech dataset, we regularize the model by applying an L2 penalty to the activations of the final layer of the feature encoder and scale down the gradients for the encoder by a factor of 10. We also use a slightly different encoder architecture where we do not use layer normalization, and instead of normalizing the raw waveform, the output of the first encoder layer is normalized. In the contrastive loss we use K = 100 distractors. We choose the training checkpoint with the lowest Lm on the validation set. 4.3 Fine-tuning After pre-training we fine-tune the learned representations on labeled data and add a randomly initialized output layer on top of the Transformer to predict characters (Librispeech/Libri-light) or phonemes (TIMIT). For Libri-light, we train three seeds with two different learning rates (2e-5 and 3e-5) for all subsets and choose the configuration with lowest WER on dev-other subset decoded with the official 4-gram language model (LM) with beam 50 and fixed model weights (LM weight 2, word insertion penalty -1). For BASE on the labeled 960h subset we use a learning rate of 1e-4. We optimize with Adam and a tri-state rate schedule where the learning rate is warmed up for the first 10% of updates, held constant for the next 40% and then linearly decayed for the remainder. BASE uses a batch size of 3.2m samples per GPU and we fine-tune on 8 GPUs, giving a total batch size of 1,600sec. LARGE batches 1.28m samples on each GPU and we fine-tune on 24 GPUs, resulting in an effective batch size of 1,920sec. For the first 10k updates only the output classifier is trained, after which the Transformer is also updated. The feature encoder is not trained during fine-tuning. We mask the feature encoder representations with a strategy similar to SpecAugment [41] detailed in Appendix B. 4.4 Language Models and Decoding We consider two types of language models (LM): a 4-gram model and a Transformer [3] trained on the Librispeech LM corpus. The Transformer LM is identical to [51] and contains 20 blocks, model dimension 1,280, inner dimension",
    "output": "The model first processes the raw waveform of the speech audio with a multilayer convolutional neural network to get latent audio representations of 25ms each. These representations are then fed into a quantizer as well as a transformer. The quantizer chooses a speech unit for the latent audio representation from an inventory of learned units. About half the audio representations are masked before being fed into the transformer. The transformer adds information from the entire audio sequence. Finally, the output of the transformer is used to solve a contrastive task. This task requires the model to identify the correct quantized speech units for the masked positions."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n6,144 and 16 attention heads. We tune the weights of the language model (interval [0, 5]) and a word insertion penalty ([5, 5]) via Bayesian optimization3: we run 128 trials with beam 500 for the 4-gram LM and beam 50 for the Transformer LM and choose the best set of weights according to performance on dev-other. Test performance",
    "output": "The model first processes the raw waveform of the speech audio with a multilayer convolutional neural network to get latent audio representations of 25ms each. These representations are then fed into a quantizer as well as a transformer. The quantizer chooses a speech unit for the latent audio representation from an inventory of learned units. About half the audio representations are masked before being fed into the transformer. The transformer adds information from the entire audio sequence. Finally, the output of the transformer is used to solve a contrastive task. This task requires the model to identify the correct quantized speech units for the masked positions."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nis measured with beam 1,500 for the n-gram LM and beam 500 for the Transformer LM. We use the beam search decoder of [44]. 5",
    "output": "The model first processes the raw waveform of the speech audio with a multilayer convolutional neural network to get latent audio representations of 25ms each. These representations are then fed into a quantizer as well as a transformer. The quantizer chooses a speech unit for the latent audio representation from an inventory of learned units. About half the audio representations are masked before being fed into the transformer. The transformer adds information from the entire audio sequence. Finally, the output of the transformer is used to solve a contrastive task. This task requires the model to identify the correct quantized speech units for the masked positions."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: Results 5.1 Low-Resource Labeled Data Evaluation We first evaluate our pre-trained models in settings where the amount of labeled data is limited to get a sense of how the representations learned on unlabeled data can improve low resource settings. If a pre-trained model captures the structure of speech, then it should require few labeled examples to fine-tune it for speech recognition. The models are pre-trained on the audio data of either Librispeech (LS-960) or LibriVox (LV-60k) and most results are obtained by decoding with a Transformer language model (Transf.); Appendix C shows results with no language model at all as well as with an n-gram language model. The LARGE model pre-trained on LV-60k and fine-tuned on only 10 minutes of labeled data achieves a word error rate of 5.2/8.6 on the Librispeech clean/other test sets. Ten minutes of labeled data corresponds to just 48 recordings with an average length of 12.5 seconds. This demonstrates that ultra-low resource speech recognition is possible with self-supervised learning on unlabeled data. Table 1: WER on the Librispeech dev/test sets when training on the Libri-light low-resource labeled data setups of 10 min, 1 hour, 10 hours and the clean 100h subset of Librispeech. Models use either the audio of Librispeech (LS-960) or the larger LibriVox (LV-60k) as unlabeled data. We consider two model sizes: BASE (95m parameters) and LARGE (317m parameters). Prior work used 860 unlabeled hours (LS-860) but the total with labeled data is 960 hours and comparable to our setup. Model Unlabeled LM dev test data clean other clean other 10 min labeled Discrete BERT [4] LS-960 4-gram 15.7 24.1 16.3 25.2 BASE LS-960 4-gram 8.9 15.7 9.1 15.6 Transf. 6.6 13.2 6.9 12.9 LARGE LS-960 Transf. 6.6 10.6 6.8 10.8 LV-60k Transf. 4.6 7.9 4.8 8.2 1h labeled Discrete BERT [4] LS-960 4-gram 8.5 16.4 9.0 17.6 BASE LS-960 4-gram 5.0 10.8 5.5 11.3 Transf. 3.8 9.0 4.0 9.3 LARGE LS-960 Transf. 3.8 7.1 3.9 7.6 LV-60k Transf. 2.9 5.4 2.9 5.8 10h labeled Discrete BERT [4] LS-960 4-gram 5.3 13.2 5.9 14.1 Iter. pseudo-labeling [58] LS-960 4-gram+Transf. 23.51 25.48 24.37 26.02 LV-60k 4-gram+Transf. 17.00 19.34 18.03 19.92 BASE LS-960 4-gram 3.8 9.1 4.3 9.5 Transf. 2.9 7.4 3.2 7.8 LARGE LS-960 Transf. 2.9 5.7 3.2 6.1 LV-60k Transf. 2.4 4.8 2.6 4.9 100h labeled Hybrid DNN/HMM [34] - 4-gram 5.0 19.5 5.8 18.6 TTS data augm. [30] - LSTM 4.3 13.5 Discrete BERT [4] LS-960 4-gram 4.0 10.9 4.5 12.1 Iter. pseudo-labeling [58] LS-860 4-gram+Transf. 4.98 7.97 5.59 8.95 LV-60k 4-gram+Transf. 3.19 6.14 3.72 7.11 Noisy student [42] LS-860 LSTM 3.9 8.8 4.2 8.6 BASE LS-960 4-gram 2.7 7.9 3.4 8.0 Transf. 2.2 6.3 2.6 6.3 LARGE LS-960",
    "output": "We also developed a cross-lingual approach, dubbed XLSR, that can learn speech units common to several languages. This approach helps when we have even small amounts of unlabeled speech, since languages for which we have little data can benefit from languages for which more data is available.\nUne erreur s’est produite Nous rencontrons des difficultés pour lire cette vidéo. En savoir plus\nCross-lingual training\nFor some languages, even unannotated data is limited. To address this issue, we explore the idea of cross-lingual training. The idea is to pretrain a single model on multiple languages at the same time, which results in representations that are better than training on a single language. This has worked particularly well for natural language processing with XLM-R. Performance for low-resource languages can improve significantly with this method, since they benefit from related languages.\nResults on the Common Voice benchmark in terms of phoneme error rate (PER), comparing training on each language individually (XLSR-Mono) with training on all 10 languages simultaneously (XLSR-10)."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nTransf. 2.1 4.8 2.3 5.0 LV-60k Transf. 1.9 4.0 2.0 4.0 Our approach of jointly learning discrete units and contextualized representations clearly improves over previous work which learned quantized audio units in a separate step [4], reducing WER by a about a third. A recent iterative self-training approach [42] represents the state of the art on the clean 100 hour subset of Librispe",
    "output": "We also developed a cross-lingual approach, dubbed XLSR, that can learn speech units common to several languages. This approach helps when we have even small amounts of unlabeled speech, since languages for which we have little data can benefit from languages for which more data is available.\nUne erreur s’est produite Nous rencontrons des difficultés pour lire cette vidéo. En savoir plus\nCross-lingual training\nFor some languages, even unannotated data is limited. To address this issue, we explore the idea of cross-lingual training. The idea is to pretrain a single model on multiple languages at the same time, which results in representations that are better than training on a single language. This has worked particularly well for natural language processing with XLM-R. Performance for low-resource languages can improve significantly with this method, since they benefit from related languages.\nResults on the Common Voice benchmark in terms of phoneme error rate (PER), comparing training on each language individually (XLSR-Mono) with training on all 10 languages simultaneously (XLSR-10)."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nech but it requires multiple iterations of labeling, filtering, and re-training. Our approach is simpler: we pre-train on the unlabeled data and fine-tune on the labeled data. On the 100 hour subset of Librispeech, their method achieves WER 4.2/8.6 on test-clean/other which compares to WER 2.3/5.0 with the LARGE model in a like for like setup, a relative WER reduction of 45%/42%. Table 2: WER on Librispeech when using all 960 hours of labeled data (cf. Table 1). Model Unlabeled LM dev test data clean other clean other Supervised CTC Transf [51] - CLM+Transf. 2.20 4.94 2.47 5.45 S2S Transf. [51] - CLM+Transf. 2.10 4.79 2.33 5.17 Transf. Transducer [60] - Transf. - - 2.0 4.6 ContextNet [17] - LSTM 1.9 3.9 1.9 4.1 Conformer [15] - LSTM 2.1 4.3 1.9 3.9 Semi-supervised CTC Transf. + PL [51] LV-60k CLM+Transf. 2.10 4.79 2.33 4.54 S2S Transf. + PL [51] LV-60k CLM+Transf. 2.00 3.65 2.09 4.11 Iter. pseudo-labeling [58] LV-60k 4-gram+Transf. 1.85 3.26 2.10 4.01 Noisy student [42] LV-60k LSTM 1.6 3.4 1.7 3.4 This work LARGE - from scratch - Transf. 1.7 4.3 2.1 4.6 BASE LS-960 Transf. 1.8 4.7 2.1 4.8 LARGE LS-960 Transf. 1.7 3.9 2.0 4.1 LV-60k Transf. 1.6 3.0 1.8 3.3 light data splits contain both clean and noisy data leading to better accuracy on test-other compared to test-clean. Increasing model size reduces WER on all setups with the largest improvements on test-other (BASE vs. LARGE both on LS-960) and increasing the amount of unlabeled training data also leads to large improvements (LARGE LS-960 vs. LV-60k). 5.2 High-Resource Labeled Data Evaluation on Librispeech In this section we evaluate the performance when large quantities of labeled speech are available to assess the effectiveness of our approach in a high resource setup. Specifically, we fine-tune the same models as before on the full 960 hours of labeled Librispeech: BASE and LARGE pre-trained on LS-960 as well as LARGE pre-trained on LV-60k. Table 2 shows that our approach achieves WER 1.8/3.3 on test-clean/other on the full Librispeech benchmark. This is despite a weaker baseline architecture: supervised training of our architecture achieves WER 2.1/4.6 (LARGE - from scratch) compared to WER 1.9/4.1 for ContextNet [17], the baseline architecture of the state of the art [42]. We use a simple Transformer with CTC which does not perform as well as seq2seq models [51]. Note that the vocabulary of our acoustic model (characters) does not match the vocabulary of the LM (words) which delays feedback from the LM and is likely to be detrimental. Most recent work [51, 58, 17, 42] uses the better performing word pieces [50] for both models. Moreover, our result is achieved without any data balancing such as [42]. Finally, self-training is likely complimentary to pre-training and their combination may yield even better results. Appendix E presents a detailed error analysis of our pre-trained models in various labeled data setups. 5.3 Phoneme Recognition on TIMIT Next, we evaluate accuracy on TIMIT phone",
    "output": "We also developed a cross-lingual approach, dubbed XLSR, that can learn speech units common to several languages. This approach helps when we have even small amounts of unlabeled speech, since languages for which we have little data can benefit from languages for which more data is available.\nUne erreur s’est produite Nous rencontrons des difficultés pour lire cette vidéo. En savoir plus\nCross-lingual training\nFor some languages, even unannotated data is limited. To address this issue, we explore the idea of cross-lingual training. The idea is to pretrain a single model on multiple languages at the same time, which results in representations that are better than training on a single language. This has worked particularly well for natural language processing with XLM-R. Performance for low-resource languages can improve significantly with this method, since they benefit from related languages.\nResults on the Common Voice benchmark in terms of phoneme error rate (PER), comparing training on each language individually (XLSR-Mono) with training on all 10 languages simultaneously (XLSR-10)."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nme recognition by fine-tuning the pre-trained models on the labeled TIMIT training data. We fine-tune as for the 10 hour subset of Libri-light but do not use a language model. Table 3 shows that our approach can achieve a new state of the art on this dataset, reducing PER by a relative 23%/29% over the next best result on the dev/test sets. Appendix D shows an",
    "output": "We also developed a cross-lingual approach, dubbed XLSR, that can learn speech units common to several languages. This approach helps when we have even small amounts of unlabeled speech, since languages for which we have little data can benefit from languages for which more data is available.\nUne erreur s’est produite Nous rencontrons des difficultés pour lire cette vidéo. En savoir plus\nCross-lingual training\nFor some languages, even unannotated data is limited. To address this issue, we explore the idea of cross-lingual training. The idea is to pretrain a single model on multiple languages at the same time, which results in representations that are better than training on a single language. This has worked particularly well for natural language processing with XLM-R. Performance for low-resource languages can improve significantly with this method, since they benefit from related languages.\nResults on the Common Voice benchmark in terms of phoneme error rate (PER), comparing training on each language individually (XLSR-Mono) with training on all 10 languages simultaneously (XLSR-10)."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nanalysis of how the discrete latent speech representations related to phonemes. Other recent work on pre-training which evaluates on TIMIT includes [47] who solve multiple tasks to learn good representations of speech. Table 3: TIMIT phoneme recognition accuracy in terms of phoneme error rate (PER). CNN + TD-filterbanks [59] 15.6 18.0 PASE+ [47] - 17.2 Li-GRU + fMLLR [46] – 14.9 wav2vec [49] 12.9 14.7 vq-wav2vec [5] 9.6 11.6 This work (no LM) LARGE (LS-960) 7.4 8.3 Table 4: Average WER and standard deviation on combined dev-clean/other of Librispeech for three training seeds. We ablate quantizing the context network input and the targets in the contrastive loss. Continuous inputs, quantized targets (Baseline) 7.97 0.02 Quantized inputs, quantized targets 12.18 0.41 Quantized inputs, continuous targets 11.18 0.16 Continuous inputs, continuous targets 8.58 0.08 5.4 Ablations A difference to previous work [5, 4] is that we quantize the latent audio representations only for the contrastive loss, i.e., when latents are used as targets, but not when the latents are input to the Transformer network. We motivate this choice by an ablating for which we adopt a reduced training setup to increase experimental turn around: we pre-train BASE on LS-960 for 250k updates with masking probability p = 0.075, fine-tune on train-10h for 60k updates on a single GPU with 640k samples per batch, or 40 sec of speech audio. We report the average WER and standard deviation on the concatenation of dev-clean and dev-other (dev PER) for three seeds of fine-tuning. Table 4 shows that our strategy of continuous inputs with quantized targets (Baseline) performs best. Continuous latent speech representations retain more information to enable better context representations and quantizing the target representations leads to more robust training. Quantizing the latents both in the input and the targets performs least well, and explains the lower performance of prior work [5, 4]. Continuous targets reduce the effectiveness of self-supervised training since targets can capture detailed artifacts of the current sequence, e.g. speaker and background information, which make the task easier and prevent the model from learning general representations beneficial to speech recognition. The training accuracy of identifying the correct latent audio representation increases from 62% to 78.0% when switching from quantized to continuous targets. Continuous inputs and continuous targets perform second best but various attempts to improve it did not lead to better results (see Appendix F for this experiment and other ablations on various hyperparameters). 6",
    "output": "We also developed a cross-lingual approach, dubbed XLSR, that can learn speech units common to several languages. This approach helps when we have even small amounts of unlabeled speech, since languages for which we have little data can benefit from languages for which more data is available.\nUne erreur s’est produite Nous rencontrons des difficultés pour lire cette vidéo. En savoir plus\nCross-lingual training\nFor some languages, even unannotated data is limited. To address this issue, we explore the idea of cross-lingual training. The idea is to pretrain a single model on multiple languages at the same time, which results in representations that are better than training on a single language. This has worked particularly well for natural language processing with XLM-R. Performance for low-resource languages can improve significantly with this method, since they benefit from related languages.\nResults on the Common Voice benchmark in terms of phoneme error rate (PER), comparing training on each language individually (XLSR-Mono) with training on all 10 languages simultaneously (XLSR-10)."
  },
  {
    "id_paper": "24",
    "id_story": "24-03",
    "id_chunk": "24-03_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion We presented wav2vec 2.0, a framework for self-supervised learning of speech representations which masks latent representations of the raw waveform and solves a contrastive task over quantized speech representations. Our experiments show the large potential of pre-training on unlabeled data for speech processing: when using only 10 minutes of labeled training data, or 48 recordings of 12.5 seconds on average, we achieve a WER of 4.8/8.2 on test-clean/other of Librispeech. Our model achieves results which achieve a new state of the art on the full Librispeech benchmark for noisy speech. On the clean 100 hour Librispeech setup, wav2vec 2.0 outperforms the previous best result while using 100 times less labeled data. The approach is also effective when large amounts of labeled data are available. We expect performance gains by switching to a seq2seq architecture and a word piece vocabulary. dev PER test PER avg. WER std. Broader Impact There are around 7,000 languages in the world and many more dialects. However, for most of them no speech recognition technology exists since current systems require hundreds or thousands of hours of labeled data which is hard to collect for most languages. We have shown that speech recognition models can be built with very small amounts of annotated data at very good accuracy. We hope our work will make speech recognition technology more broadly available to many more languages and dialects.",
    "output": "We are releasing pretrained models and code for wav2vec 2.0, the successor to wav2vec. This new model learns basic speech units used to tackle a self-supervised task. The model is trained to predict the correct speech unit for masked parts of the audio, while at the same time learning what the speech units should be. With just 10 minutes of transcribed speech and 53K hours of unlabeled speech, wav2vec 2.0 enables speech recognition models at a word error rate (WER) of 8.6 percent on noisy speech and 5.2 percent on clean speech on the standard LibriSpeech benchmark. There are thousands of languages spoken around the world, many with several different dialects, which presents a huge challenge for building high-quality speech recognition technology. It’s simply not feasible to obtain resources for each dialect and every language across the many possible domains (read speech, telephone speech, etc.). Our new model, wav2vec 2.0, uses self-supervision to push the boundaries by learning from unlabeled training data to enable speech recognition systems for many more languages, dialects, and domains. With just one hour of labeled training data, wav2vec 2.0 outperforms the previous state of the art on the 100-hour subset of the LibriSpeech benchmark — using 100 times less labeled data. Wav2vec 2.0 is part of our vision for machine learning models that rely less on labeled data, thanks to self-supervised learning. Self-supervision has helped us advance image classification, video understanding, and our content understanding systems. We hope that the algorithm will enable improved speech technology for many more languages, dialects, and domains, and lead to improvements for existing systems. Our approach learns a set of speech units, which are shorter than phonemes, to describe the speech audio sequence. Since this set is finite, the model cannot represent all variations, such as background noise. Instead, the units encourage the model to focus on the most important factors to represent the speech audio. In our experiments, we find that this works better than alternative approaches on the LibriSpeech benchmark. With cross-lingual training, wav2vec 2.0 learns speech units that are used in multiple languages. With wav2vec 2.0, we can also learn speech units that are used across languages. We find that some units are used for only a particular language, whereas others are used in similar languages and sometimes even in languages that aren’t very similar. Performance on public speech benchmarks We trained wav2vec on 960 hours of unannotated speech data from the LibriSpeech benchmark, which contains public audiobooks. After pretraining, we fine-tuned the model on 100 hours, 1 hour, or just 10 minutes of annotated data from Libri-light to perform speech recognition. The result shows a large improvement over the previous state of the art on 100 hours of annotated data (Noisy Student training) when wav2vec 2.0 uses the same amount of annotated data. Moreover, it still shows improvement over the previous best result even when using 100x less annotated data, or just one hour. WER for Noisy Student self-training with 100 hours of labeled data. Wav2vec 2.0 with 100 hours, 1 hour, and only 10 minutes of labeled data. All models use the remainder of the LibriSpeech corpus (total 960 hours) as unannotated data, except for the last result, which uses 53K hours from LibriVox. To evaluate cross-linguality, we trained wav2vec 2.0 on unannotated speech audio of 12 languages from the Common Voice benchmark. The resulting approach, called XLSR, shows that cross-lingual training dramatically improves performance on low-resource languages, compared with training only on a single language. We also measured how often the learned speech units are used in each language and visualized the result in a 2D plot. This illustration shows that related languages tend to use similar units, which confirms that our model learns cross-lingual units. Wav2vec 2.0 enables us to build better speech recognition systems for many more languages and domains with much less annotated data. We’ve open-sourced the code and pretrained models to enable other researchers to do exactly this. The code is part of fairseq, Facebook AI Research’s sequence modeling toolkit, which provides implementations for many of our research urls. A few commands enable training and fine-tuning of the provided models. We are excited about the potential of powerful speech representations for other applications,"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Abstract\nAbstract: Large language models (LLMs) have shown promise in medical question answering, with Med-PaLM being the frst to exceed a ‘passing’ score in United States Medical Licensing Examination style questions. However, challenges remain in long-form medical question answering and handling real-world workfows. Here, we present Med-PaLM 2, which bridges these gaps with a combination of base LLM improvements, medical domain fne-tuning and new strategies for improving reasoning and grounding through ensemble refnement and chain of retrieval. Med-PaLM 2 scores up to 86.5% on the MedQA dataset, improving upon Med-PaLM by over 19%, and demonstrates dramatic performance increases across MedMCQA, PubMedQA and MMLU clinical topics datasets. Our detailed human evaluations framework shows that physicians prefer Med-PaLM 2 answers to those from other physicians on eight of nine clinical axes. Med-PaLM 2 also demonstrates signifcant improvements over its predecessor across all evaluation metrics, particularly on new adversarial datasets designed to probe LLM limitations (P  0.001). In a pilot study using real-world medical questions, specialists preferred Med-PaLM 2 answers to generalist physician answers 65% of the time. While specialist answers were still preferred overall, both specialists and generalists rated Med-PaLM 2 to be as safe as physician answers, demonstrating its growing potential in real-world medical applications.",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One might argue that generative models like those in Google’s MedLM family are much more sophisticated than what came before them. But research has shown that generative models aren’t particularly accurate when it comes to answering healthcare-related questions, even fairly basic ones. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Introduction\nIntroduction: Language is at the heart of health and medicine, underpinning interactions between people and care providers. Progress in LLMs has enabled the exploration of medical domain capabilities in artificial intelligence (AI) systems that can understand and communicate using language, promising richer human–AI interaction and collaboration. In particular, these models have demonstrated impressive capabilities on multiple-choice research benchmarks1–3 . The advent of transformers4 and LLMs5,6 has renewed interest in the possibilities of AI for medical question-answering tasks—a long-standing ‘grand challenge’7–9 . A majority of these approaches involve smaller language models trained using domain-specific data (BioLinkBert10, DRAGON11, PubMedGPT12, PubMedBERT13, BioGPT14), resulting in steady improvements in performance on benchmark datasets such as MedQA (United States Medical Licensing Examination (USMLE))15, MedMCQA16 and PubMedQA17. The rise of larger general-purpose LLMs such as GPT-3 (ref. 18) and Flan-PaLM19,20 trained on internet-scale corpora with massive computing infrastructure has seen leapfrog improvements on such benchmarks within a few months (Fig. 1). In particular, GPT-3.5 (ref. 3) reached an accuracy of 60.2% on the MedQA (USMLE) dataset, Flan-PaLM reached an accuracy of 67.6% and GPT-4-base2 achieved 86.1%. In parallel, application protocol interface (API) access to the GPT family of models spurred several studies evaluating the specialized clinical knowledge in these models, without specific alignment to the medical domain. Levine et al.21 evaluated the diagnostic and triage accuracies of GPT-3 for 48 validated case vignettes of both common and severe conditions and compared to laypeople and physicians. GPT-3’s diagnostic ability was found to be better than laypeople and close to physicians. On triage, performance was less impressive and closer to laypeople. Similarly, GPT-3 performance in genetics, surgery and ophthalmology was studied in refs. 22–24, respectively. Ayers et al.25 compared ChatGPT and physician answers on 195 randomly drawn patient questions from a social media forum and found ChatGPT answers to be rated higher in both quality and empathy. In our previous work on Med-PaLM, we demonstrated the importance of a wide-ranging benchmark for medical question answering, detailed human evaluation of model answers and alignment strategies in the medical domain1 . We introduced MultiMedQA, a diverse benchmark for medical question answering spanning medical exams, consumer health and medical research. We proposed a human evaluation rubric enabling physicians and laypeople to perform detailed assessment of model answers. Our initial model, Flan-PaLM, achieved strong performance across multiple-choice benchmarks. However, human evaluation revealed further work was necessary to ensure factual long-form answers aligned with human values and expectations in this safety-critical domain (a process generally referred to as ‘alignment’). We developed Med-PaLM, resulting in substantially improved physician evaluations over Flan-PaLM. However, evaluation on these benchmarks was limited as a measure of practical utility in real-world workflows, and key shortfalls remained compared to physician answers. Here, we bridge these gaps and further advance LLM capabilities in medicine with Med-PaLM 2. We developed this model using a combination of an improved base LLM (PaLM 2; ref. 26), medical domain-specific fine-tuning and new prompting strategies to improve reasoning and grounding, including ensemble refinement and chain of retrieval. Med-PaLM 2 improves upon Med-PaLM by over 19% on MedQA, as depicted in Fig. 1, and approached or exceeded previous state-of-the-art performance on MedMCQA, PubMedQA and MMLU clinical topics datasets. While these benchmarks are a useful measure of the knowledge encoded in LLMs, they do not capture a model’s ability to generate factual, safe",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being exercised consistently with [generative AI],” the WHO said in a statement. “Precipitous adoption of untested systems could lead to errors by healthcare workers, cause harm to patients, erode trust in AI and thereby undermine or delay the potential long-"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Introduction\nanswers to questions that require nuanced answers, typical in real-world medical question answering. We study this by expanding our evaluation framework for physicians and laypeople1 . We introduce two additional human evaluations: a pairwise ranking evaluation of model and physician answers to consumer medical questions along nine clinically relevant axes; and physician assessment of model answers on two recently introduced adversarial testing datasets27 designed to probe the limits of LLMs. Finally, we study the practical",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being exercised consistently with [generative AI],” the WHO said in a statement. “Precipitous adoption of untested systems could lead to errors by healthcare workers, cause harm to patients, erode trust in AI and thereby undermine or delay the potential long-"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Introduction\nutility of Med-PaLM 2 for bedside consultations. In a pilot study, we answer real-world medical questions submitted by specialist physicians to a consultation service during routine care delivery28,29. Answering these questions is nontrivial: in the consultation service, a team of physicians analyzed aggregate patient data to provide a written report. Compared to answers from specialist and generalist physicians, answers from Med-PaLM 2 using chain of retrieval are comparable to or better than generalists’ answers but remain inferior to specialists’ answers. These results suggest that, as model performance approaches a human level, evaluation with highly specialized experts becomes crucial, and current models may have utility in supporting information needs of medical staff where access to specialist physicians is limited. Our key contributions are summarized as follows: (1) We developed Med-PaLM 2, a medical LLM trained using an updated base model (PaLM 2; ref. 26) and targeted medical domain-specific fine-tuning. (2) We introduced ‘ensemble refinement’ as a prompting strategy to improve LLM reasoning. (3) We described ‘chain of retrieval’, a step-by-step pipeline using search as a tool that enables Med-PaLM 2 to answer difficult medical research questions by grounding its claims in relevant sources. (4) Med-PaLM 2 achieved state-of-the-art results on several MultiMedQA multiple-choice benchmarks, including MedQA USMLE-style questions, improving upon Med-PaLM performance by over 19% (Table 1). (5) Building upon our previous work1 , we incorporated several key enhancements to the human evaluation framework. These include new adversarial and bedside consultation datasets, as well as a pairwise ranking system that compares model responses directly with those of human physicians. (6) Human evaluation of long-form answers to consumer medical questions showed that Med-PaLM 2’s answers were preferred to physician and Med-PaLM answers across eight of nine axes relevant to clinical utility, such as factuality and low likelihood of harm (Figs. 2 and 3). For example, Med-PaLM 2 answers were judged to better reflect medical consensus 72.9% of the time compared to physician answers (Fig. 1). (7) We introduced two adversarial question datasets to probe the safety and limitations of these models. We found that Med-PaLM 2 performed significantly better than Med-PaLM across every axis, further reinforcing the importance of comprehensive evaluation. For instance, answers had low risk of harm for 90.6% of Med-PaLM 2 answers, compared to 79.4% for Med-PaLM (Fig. 2 and Supplementary Table 4). (8) For real-world questions that arose during care delivery, specialists preferred Med-PaLM 2 answers over generalist physician answers 65% of the time, while generalists preferred them equally. Model answers remained inferior to specialist answers; both specialists and generalists preferred specialist answers about 60% of the time. Specialists and generalists viewed Med-PaLM 2 answers to be as safe as physician answers (Fig. 4).",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being exercised consistently with [generative AI],” the WHO said in a statement. “Precipitous adoption of untested systems could lead to errors by healthcare workers, cause harm to patients, erode trust in AI and thereby undermine or delay the potential long-"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Experiments / Results\nExperiments / Results: Results Table 1 and Supplementary Table 1 summarize Med-PaLM 2 results on MultiMedQA multiple-choice benchmarks. Unless specified otherwise, Med-PaLM 2 refers to the unified model trained on the mixture in Extended Data Table 1. We also include comparisons to GPT-4 (refs. 2,30). We note that comparisons to GPT-4 are not straightforward because it is a proprietary system and we are not able to measure overlap of the evaluation data with the model’s training data as we did for Med-PaLM 2 in Table 2. MedQA Our unified Med-PaLM 2 model reaches an accuracy of 85.4% using ER as a prompting strategy. Our best result on this dataset is 86.5%, obtained from a version of Med-PaLM 2 not aligned for consumer medical question answering, but instead instruction fine-tuned only on MedQA. MedMCQA On MedMCQA, Med-PaLM 2 obtains a score of 72.3%, exceeding Flan-PaLM performance by over 14% but slightly short of previous state-of-the-art performance (73.66 from GPT-4-base30). PubMedQA On PubMedQA, Med-PaLM 2 obtains a score of 75.0%. This is below the state-of-the-art performance (81.0 from BioGPT-Large14) and is likely because no data were included for this dataset for instruction fine-tuning. However, after further exploring prompting strategies for PubMedQA on the development set, the unified model reached an accuracy of 79.8% with a single run and 81.8% using self-consistency (11). The latter result was state of the art, although we caution that PubMedQA’s test set is small (500 examples), and remaining failures of Med-PaLM 2 and other strong models appear to be largely attributable to label noise intrinsic in the dataset (especially given human performance is 78.0%17). MMLU clinical topics On MMLU clinical topics, Med-PaLM 2 significantly improves over previously reported results in Med-PaLM1 and exceeds previous state-of-the-art performance on three out six topics, with GPT-4-base reporting better numbers in the other three. We note that the test set for each of these topics is small, as reported in Extended Data Table 1. We see a drop in performance between GPT-4-base and the aligned (production) GPT-4 model on these multiple-choice benchmarks (Table 1). Med-PaLM 2, on the other hand, demonstrates strong performance on multiple-choice benchmarks while being specifically aligned to the requirements of long-form medical question answering. While multiple-choice benchmarks are a useful measure of the knowledge encoded in these models, we believe human evaluations of model answers along clinically relevant axes are necessary to assess their utility in real-world clinical applications. We also see in Supplementary Table 1 that ensemble refinement improves on few-shot and self-consistency prompting strategies in eliciting strong model performance across these benchmarks. Overlap analysis Overlap percentages ranged from 0.9% for MedQA to 48.0% on MMLU Medical Genetics. Performance of Med-PaLM 2 was slightly higher on questions with overlap for six out of nine datasets, though the difference was only statistically significant for MedMCQA (accuracy difference 4.6%, [1.3, 7.7]) due to the relatively small number of questions with overlap in most datasets (Table 2). When we reduced the overlap segment length from 512 to 120 characters (Methods), overlap percentages increased (11.15% for MedQA to 56.00% on MMLU Medical Genetics), but performance differences on questions with overlap were similar (Supplementary Table 2), and the difference was still statistically significant for just one dataset. These results are similar to those observed in ref. 19, which also saw minimal performance difference from testing on overlapping data. A limitation of this analysis is that we were not able to exhaustively identify the subset of overlapping questions where the correct answer is also explicitly provided due",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One might argue that generative models like those in Google’s MedLM family are much more sophisticated than what came before them. But research has shown that generative models aren’t particularly accurate when it comes to answering healthcare-related questions, even fairly basic ones. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Experiments / Results\nto heterogeneity in how correct answers can be presented across different documents. Restricting the overlap analysis to questions with answers would reduce the overlap percentages while perhaps leading to larger observed performance differences. Independent evaluation On the MultiMedQA 140 dataset, physicians rated Med-PaLM 2 answers as generally comparable to physician-generated and Med-PaLM-generated answers along the axes we evaluated (Fig. 2 and Supplementary Table 3). This analysis was",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One might argue that generative models like those in Google’s MedLM family are much more sophisticated than what came before them. But research has shown that generative models aren’t particularly accurate when it comes to answering healthcare-related questions, even fairly basic ones. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Experiments / Results\nlargely underpowered for the effect sizes (differences) observed, without significant differences when applying Bonferroni correction for multiple comparisons. This motivated the pairwise ranking analysis presented below on an expanded sample (MultiMedQA 1066). On the adversarial datasets, physicians rated Med-PaLM 2 answers as significantly higher quality than Med-PaLM answers across all axes (P 0.001 for all axes; Supplementary Table 4). This pattern held for both the general and health equity-focused subsets of the adversarial dataset. Finally, laypeople rated Med-PaLM 2 answers to questions in the MultiMedQA 140 dataset as more helpful and relevant than Med-PaLM answers (P 0.002 for both dimensions; Supplementary Fig. 3 and Supplementary Table 5). Notably, Med-PaLM 2 answers were longer than Med-PaLM and physician answers (Supplementary Table 13). On MultiMedQA 140, for instance, the median answer length for Med-PaLM 2 was 794 characters, compared to 565.5 for Med-PaLM and 337.5 for physicians. Answer lengths to adversarial questions tended to be longer in general, with a median answer length of 964 characters for Med-PaLM 2 and 518 characters for Med-PaLM, possibly reflecting the greater complexity of these questions. Pairwise ranking evaluation Pairwise ranking evaluation more explicitly assessed the relative performance of Med-PaLM 2, Med-PaLM and physicians. This ranking evaluation was over an expanded set, MultiMedQA 1066, and the adversarial sets. Qualitative examples and their rankings are included in Supplementary Tables 8 and 9, respectively, to provide indicative examples and insight. On MultiMedQA, for eight of the nine axes, Med-PaLM 2 answers were more often rated as being higher quality compared to physician answers (all P 0.001 for each of the separate comparisons; Fig. 1 and Supplementary Table 6). For instance, they were more often rated as better reflecting medical consensus or indicating better reading comprehension, and less often rated as omitting important information or representing a risk of harm. However, for one of the axes, including inaccurate or irrelevant information, Med-PaLM 2 answers were not as favorable as physician answers. Med-PaLM 2 answers were rated as higher quality than Med-PaLM axes on the same eight axes (Fig. 3 and Supplementary Table 7); Med-PaLM 2 answers were marked as having more inaccurate or irrelevant information less often than Med-PaLM answers (18.4% Med-PaLM 2 versus 21.5% Med-PaLM), but the difference was not significant (P = 0.12). On adversarial questions, Med-PaLM 2 was ranked more favorably than Med-PaLM across every axis (Fig. 3), often by substantial margins. Three-way utility ranking We present results for three-way ranking of model, generalist and specialist answers in Fig. 4a. For generalist rankings, given 11 rankings per question, we determine plurality ranking per question across raters. We observe that specialist answers perform best across both generalist and specialist raters, but that Med-PaLM 2 answers appear to perform comparably or better to generalist answers for both groups of raters, with more answers most preferred and second preferred than for generalist raters. In Fig. 4b, we plot pairwise rankings between models and generalists and models and specialists for both groups of raters, averaged across all raters. We observe that both groups prefer specialist answers over model answers (about 60% preference), but that specialists prefer model answers over generalist answers (65% preference). Generalists prefer model answers and generalist answers about equally, suggesting that as, model performance approaches the human level, evaluation with highly specialized experts may be important in distinguishing model performance from human performance. Individual evaluation of harm In Supplementary Tables 14 and 15, we present results for harm evaluation for each answer from the model, generalists and specialists. We observe that a majority of generalist physicians find that answers across all three answer sources are not harmful, but at an",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One might argue that generative models like those in Google’s MedLM family are much more sophisticated than what came before them. But research has shown that generative models aren’t particularly accurate when it comes to answering healthcare-related questions, even fairly basic ones. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Experiments / Results\n80% agreement threshold for harmlessness, a few questions from each source are flagged. At this threshold, 16 of 20 Med-PaLM 2 answers are harmless, while 17 of 20 generalist answers are harmless, and 15 of 20 specialist answers are harmless. For specialist physicians (one rater per answer), 17 of 20 model answers were harmless, 19 of 20 generalist answers and 18 of 20 specialist answers. Interestingly, across both rating groups,",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One might argue that generative models like those in Google’s MedLM family are much more sophisticated than what came before them. But research has shown that generative models aren’t particularly accurate when it comes to answering healthcare-related questions, even fairly basic ones. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being"
  },
  {
    "id_paper": "49",
    "id_story": "49-03",
    "id_chunk": "49-03_Experiments_/_Results_chunk3",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Experiments / Results\na few physician answers were flagged as potentially harmful, indicating the challenging and subjective nature of evaluating harm. Overall, the results do not suggest a substantial difference in harmfulness across model, generalist and specialist answers.",
    "output": "Google thinks that there’s an opportunity to offload more healthcare tasks to generative AI models — or at least, an opportunity to recruit those models to aid healthcare workers in completing their tasks. Today, the company announced MedLM, a family of models fine-tuned for the medical industries. Based on Med-PaLM 2, a Google-developed model that performs at an “expert level” on dozens of medical exam questions, MedLM is available to Google Cloud customers in the U.S. (it’s in preview in certain other markets) who’ve been whitelisted through Vertex AI, Google’s fully managed AI dev platform. There are two MedLM models available currently: a larger model designed for what Google describes as “complex tasks” and a smaller, fine-tunable model best for “scaling across tasks.” “Through piloting our tools with different organizations, we’ve learned that the most effective model for a given task varies depending on the use case,” reads a blog post penned by Yossi Matias, VP of engineering and research at Google, provided to TechCrunch ahead of today’s announcement. “For example, summarizing conversations might be best handled by one model, and searching through medications might be better handled by another.” Google says that one early MedLM user, the for-profit facility operator HCA Healthcare, has been piloting the models with physicians to help draft patient notes at emergency department hospital sites. Another tester, BenchSci, has built MedLM into its “evidence engine” for identifying, classifying and ranking novel biomarkers. “We’re working in close collaboration with practitioners, researchers, health and life science organizations and the individuals at the forefront of healthcare every day,” writes Matias. Google — along with chief rivals Microsoft and Amazon — are racing desperately to corner a healthcare AI market that could be worth tens of billions of dollars by 2032. Recently, Amazon launched AWS HealthScribe, which uses generative AI to transcribe, summarize and analyze notes from patient-doctor conversations. Microsoft is piloting various AI-powered healthcare products, including medical “assistant” apps underpinned by large language models. Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW But there’s reason to be wary of such tech. AI in healthcare, historically, has been met with mixed success. Babylon Health, an AI startup backed by the U.K.’s National Health Service, has found itself under repeated scrutiny for making claims that its disease-diagnosing tech can perform better than doctors. And IBM was forced to sell its AI-focused Watson Health division at a loss after technical problems led customer partnerships to deteriorate. One might argue that generative models like those in Google’s MedLM family are much more sophisticated than what came before them. But research has shown that generative models aren’t particularly accurate when it comes to answering healthcare-related questions, even fairly basic ones. One study co-authored by a group of ophthalmologists asked ChatGPT and Google’s Bard chatbot questions about eye conditions and diseases, and found that the majority of responses from all three tools were wildly off the mark. ChatGPT generates cancer treatment plans full of potentially deadly errors. And models including ChatGPT and Bard spew racist, debunked medical ideas in response to queries about kidney function, lung capacity and skin. In October, the World Health Organization (WHO) warned of the risks from using generative AI in healthcare, noting the potential for models to generate harmful wrong answers, propagate disinformation about health issues and reveal health data or other sensitive info. (Because models occasionally memorize training data and return portions of this data given the right prompt, it’s not out of the question that models trained on medical records could inadvertently leak those records.) “While WHO is enthusiastic about the appropriate use of technologies, including [generative AI], to support healthcare professionals, patients, researchers and scientists, there’s concern that caution that would normally be exercised for any new technology is not being"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Abstract\nAbstract: Proteins are essential to life, and understanding their structure can facilitate a mechanistic understanding of their function. Through an enormous experimental effort1–4, the structures of around 100,000 unique proteins have been determined5, but this represents a small fraction of the billions of known protein sequences6,7. Structural coverage is bottlenecked by the months to years of painstaking effort required to determine a single protein structure. Accurate computational approaches are needed to address this gap and to enable large-scale structural bioinformatics. Predicting the three-dimensional structure that a protein will adopt based solely on its amino acid sequence—the structure prediction component of the ‘protein folding problem’8—has been an important open research problem for more than 50 years9. Despite recent progress10–14, existing methods fall far short of atomic accuracy, especially when no homologous structure is available. Here we provide the first computational method that can regularly predict protein structures with atomic accuracy even in cases in which no similar structure is known. We validated an entirely redesigned version of our neural network-based model, AlphaFold, in the challenging 14th Critical Assessment of protein Structure Prediction (CASP14)15, demonstrating accuracy competitive with experimental structures in a majority of cases and greatly outperforming other methods. Underpinning the latest version of AlphaFold is a novel machine learning approach that incorporates physical and biological knowledge about protein structure, leveraging multi-sequence alignments, into the design of the deep learning algorithm.",
    "output": "DeepMind and several research partners have released a database containing the 3D structures of nearly every protein in the human body, as computationally determined by the breakthrough protein folding system demonstrated last year, AlphaFold. The freely available database represents an enormous advance and convenience for scientists across hundreds of disciplines and domains, and may very well form the foundation of a new phase in biology and medicine. The problem with the proteome is that it’s much, much more complex. Proteins, like DNA, are sequences of known molecules; in DNA these are the handful of familiar bases (adenine, guanine, etc.), but in proteins they are the 20 amino acids (each of which is coded by multiple bases in genes). This in itself creates a great deal more complexity, but it’s only the start. The sequences aren’t simply “code” but actually twist and fold into tiny molecular origami machines that accomplish all kinds of tasks within our body. It’s like going from binary code to a complex language that manifests objects in the real world. Practically speaking this means that the proteome is made up of not just 20,000 sequences of hundreds of acids each, but that each one of those sequences has a physical structure and function. And one of the hardest parts of understanding them is figuring out what shape is made from a given sequence. This is generally done experimentally using something like x-ray crystallography, a long, complex process that may take months or longer to figure out a single protein — if you happen to have the best labs and techniques at your disposal. The structure can also be predicted computationally, though the process has never been good enough to actually rely on — until AlphaFold came along. Without going into the whole history of computational proteomics (as much as I’d like to), we essentially went from distributed brute-force tactics 15 years ago — remember Folding@home? — to more honed processes in the last decade. Then AI-based approaches came on the scene, making a splash in 2019 when DeepMind’s AlphaFold leapfrogged every other system in the world — then made another jump in 2020, achieving accuracy levels high enough and reliable enough that it prompted some experts to declare the problem of turning an arbitrary sequence into a 3D structure solved. I’m only compressing this long history into one paragraph because it was extensively covered at the time, but it’s hard to overstate how sudden and complete this advance was. This was a problem that stumped the best minds in the world for decades, and it went from “we maybe have an approach that kind of works, but extremely slowly and at great cost” to “accurate, reliable, and can be done with off the shelf computers” in the space of a year. The result is that 98.5% of the human proteome is now “folded,” as they say, meaning there is a predicted structure that the AI model is confident enough (and importantly, we are confident enough in its confidence) represents the real thing. Oh, and they also folded the proteome for 20 other organisms, like yeast and E. coli, amounting to about 350,000 protein structures total. It’s by far — by orders of magnitude — the largest and best collection of this absolutely crucial information. Ordinarily examining the proteins suspected of being at the root of a given problem would be expensive and time-consuming, and for diseases that affect relatively few people, money and time are in short supply when they can be applied to more common problems like cancers or dementia-related diseases. But being able to simply call up the structures of 10 healthy proteins and 10 mutated versions of the same, insights may appear in seconds that might otherwise have taken years of painstaking experimental work. The drug discovery and testing process still takes years, but maybe now it can start tomorrow for a currently untreatable disease instead of in 2025. Lest you think too much is resting on a computer’s prediction of experimentally unverified results, in another, totally different case, some of the painstaking work had already been done. John McGeehan of the University of Portsmouth, with whom DeepMind partnered for another potential use case, explained how this affected his team’s work on plastic decomposition. The plan is to, over the next year or two, make predictions for every single known and sequenced protein — somewhere in the neighborhood of a hundred million. And for the most part (the few structures not susceptible to this approach seem to make themselves known quickly) biologists should be able to have great confidence in the results. “This was by far the hardest"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\nIntroduction: The development of computational methods to predict three-dimensional (3D) protein structures from the protein sequence has proceeded along two complementary paths that focus on either the physical interactions or the evolutionary history. The physical interaction programme heavily integrates our understanding of molecular driving forces into either thermodynamic or kinetic simulation of protein physics16 or statistical approximations thereof17. Although theoretically very appealing, this approach has proved highly challenging for even moderate-sized proteins due to the computational intractability of molecular simulation, the context dependence of protein stability and the difficulty of producing sufficiently accurate models of protein physics. The evolutionary programme has provided an alternative in recent years, in which the constraints on protein structure are derived from bioinformatics analysis of the evolutionary history of proteins, homology to solved structures18,19 and pairwise evolutionary correlations20–24. This bioinformatics approach has benefited greatly from the steady growth of experimental protein structures deposited in the Protein Data Bank (PDB)5, the explosion of genomic sequencing and the rapid development of deep learning techniques to interpret these correlations. Despite these advances, contemporary physical and evolutionary-history-based approaches produce predictions that are far short of experimental accuracy in the majority of cases in which a close homologue has not been solved experimentally and this has limited their utility for many biological applications. In this study, we develop the first, to our knowledge, computational approach capable of predicting protein structures to near experimental accuracy in a majority of cases. The neural network AlphaFold that we developed was entered into the CASP14 assessment (May–July 2020; entered under the team name ‘AlphaFold2’ and a completely different model from our CASP13 AlphaFold system10). The CASP assessment is carried out biennially using recently solved structures that have not been deposited in the PDB or publicly disclosed so that it is a blind test for the participating methods, and has long served as the gold-standard assessment for the accuracy of structure prediction25,26. In CASP14, AlphaFold structures were vastly more accurate than competing methods. AlphaFold structures had a median backbone accuracy of 0.96 r.m.s.d.95 (C root-mean-square deviation at 95% residue coverage) (95% confidence interval = 0.85–1.16 ) whereas the next best performing method had a median backbone accuracy of 2.8 r.m.s.d.95 (95% confidence interval = 2.7–4.0 ) (measured on CASP domains; see Fig. 1a for backbone accuracy and Supplementary Fig. 14 for all-atom accuracy). As a comparison point for this accuracy, the width of a carbon atom is approximately 1.4 . In addition to very accurate domain structures (Fig. 1b), AlphaFold is able to produce highly accurate side chains (Fig. 1c) when the backbone is highly accurate and considerably improves over template-based methods even when strong templates are available. The all-atom accuracy of AlphaFold was 1.5 r.m.s.d.95 (95% confidence interval = 1.2–1.6 ) compared with the 3.5 r.m.s.d.95 (95% confidence interval = 3.1–4.2 ) of the best alternative method. Our methods are scalable to very long proteins with accurate domains and domain-packing (see Fig. 1d for the prediction of a 2,180-residue protein with no structural homologues). Finally, the model is able to provide precise, per-residue estimates of its reliability that should enable the confident use of these predictions. We demonstrate in Fig. 2a that the high accuracy that AlphaFold demonstrated in CASP14 extends to a large sample of recently released PDB structures; in this dataset, all structures were deposited in the PDB after our training data cut-off and are analysed as full chains (see Methods, Supplementary Fig. 15 and Supplementary Table 6 for more details). Furthermore, we observe high side-chain accuracy when the backbone prediction is accurate (Fig. 2b) and we show that our confidence measure, the predicted local-distance difference test (pLDDT), reliably predicts the C local-",
    "output": "It was, however, just the beginning of the work in that field — like finishing all the edge pieces of a giant puzzle. And one of the next big projects everyone turned their eyes toward in those years was understanding the human proteome — which is to say all the proteins used by the human body and encoded into the genome.\nAll that will be made available as a freely browsable database that any researcher can simply plug a sequence or protein name into and immediately be provided the 3D structure. The details of the process and database can be found in a url published today in the journal Nature.\n“As a scientist myself, who works on an almost unfathomable protein,” said EMBL-EBI’s Edith Heard (she didn’t specify which protein), “it’s really exciting to know that you can find out what the business end of a protein is now, in such a short time — it would have taken years. So being able to access the structure and say ‘aha, this is the business end,’ you can then focus on trying to work out what that business end does. And I think this is accelerating science by steps of years, a bit like being able to sequence genomes did decades ago.”\n“When we first sent our seven sequences to the DeepMind team, for two of those we already had experimental structures. So we were able to test those when they came back, and it was one of those moments, to be honest, when the hairs stood up on the back of my neck,” said McGeehan. “Because the structures that they produced were identical to our crystal structures. In fact, they contained even more information than the crystal structures were able to provide in certain cases. We were able to use that information directly to develop faster enzymes for breaking down plastics. And those experiments are already underway, immediately. So the acceleration to our project here is, I would say, multiple years.”"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\ndistance difference test (lDDT-C) accuracy of the corresponding prediction (Fig. 2c). We also find that the global superposition metric template modelling score (TM-score)27 can be accurately estimated (Fig. 2d). Overall, these analyses validate that the high accuracy and reliability of AlphaFold on CASP14 proteins also transfers to an uncurated collection of recent P",
    "output": "It was, however, just the beginning of the work in that field — like finishing all the edge pieces of a giant puzzle. And one of the next big projects everyone turned their eyes toward in those years was understanding the human proteome — which is to say all the proteins used by the human body and encoded into the genome.\nAll that will be made available as a freely browsable database that any researcher can simply plug a sequence or protein name into and immediately be provided the 3D structure. The details of the process and database can be found in a url published today in the journal Nature.\n“As a scientist myself, who works on an almost unfathomable protein,” said EMBL-EBI’s Edith Heard (she didn’t specify which protein), “it’s really exciting to know that you can find out what the business end of a protein is now, in such a short time — it would have taken years. So being able to access the structure and say ‘aha, this is the business end,’ you can then focus on trying to work out what that business end does. And I think this is accelerating science by steps of years, a bit like being able to sequence genomes did decades ago.”\n“When we first sent our seven sequences to the DeepMind team, for two of those we already had experimental structures. So we were able to test those when they came back, and it was one of those moments, to be honest, when the hairs stood up on the back of my neck,” said McGeehan. “Because the structures that they produced were identical to our crystal structures. In fact, they contained even more information than the crystal structures were able to provide in certain cases. We were able to use that information directly to develop faster enzymes for breaking down plastics. And those experiments are already underway, immediately. So the acceleration to our project here is, I would say, multiple years.”"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\nDB submissions, as would be expected (see Supplementary Methods 1.15 and Supplementary Fig. 11 for confirmation that this high accuracy extends to new folds).",
    "output": "It was, however, just the beginning of the work in that field — like finishing all the edge pieces of a giant puzzle. And one of the next big projects everyone turned their eyes toward in those years was understanding the human proteome — which is to say all the proteins used by the human body and encoded into the genome.\nAll that will be made available as a freely browsable database that any researcher can simply plug a sequence or protein name into and immediately be provided the 3D structure. The details of the process and database can be found in a url published today in the journal Nature.\n“As a scientist myself, who works on an almost unfathomable protein,” said EMBL-EBI’s Edith Heard (she didn’t specify which protein), “it’s really exciting to know that you can find out what the business end of a protein is now, in such a short time — it would have taken years. So being able to access the structure and say ‘aha, this is the business end,’ you can then focus on trying to work out what that business end does. And I think this is accelerating science by steps of years, a bit like being able to sequence genomes did decades ago.”\n“When we first sent our seven sequences to the DeepMind team, for two of those we already had experimental structures. So we were able to test those when they came back, and it was one of those moments, to be honest, when the hairs stood up on the back of my neck,” said McGeehan. “Because the structures that they produced were identical to our crystal structures. In fact, they contained even more information than the crystal structures were able to provide in certain cases. We were able to use that information directly to develop faster enzymes for breaking down plastics. And those experiments are already underway, immediately. So the acceleration to our project here is, I would say, multiple years.”"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nMethod: The AlphaFold network AlphaFold greatly improves the accuracy of structure prediction by incorporating novel neural network architectures and training procedures based on the evolutionary, physical and geometric constraints of protein structures. In particular, we demonstrate a new architecture to jointly embed multiple sequence alignments (MSAs) and pairwise features, a new output representation and associated loss that enable accurate end-to-end structure prediction, a new equivariant attention architecture, use of intermediate losses to achieve iterative refinement of predictions, masked MSA loss to jointly train with the structure, learning from unlabelled protein sequences using self-distillation and self-estimates of accuracy. The AlphaFold network directly predicts the 3D coordinates of all heavy atoms for a given protein using the primary amino acid sequence and aligned sequences of homologues as inputs (Fig. 1e; see Methods for details of inputs including databases, MSA construction and use of templates). A description of the most important ideas and components is provided below. The full network architecture and training procedure are provided in the Supplementary Methods. The network comprises two main stages. First, the trunk of the network processes the inputs through repeated layers of a novel neural network block that we term Evoformer to produce an Nseq Nres array (Nseq, number of sequences; Nres, number of residues) that represents a processed MSA and an Nres Nres array that represents residue pairs. The MSA representation is initialized with the raw MSA (although see Supplementary Methods 1.2.7 for details of handling very deep MSAs). The Evoformer blocks contain a number of attention-based and non-attention-based components. We show evidence in ‘Interpreting the neural network’ that a concrete structural hypothesis arises early within the Evoformer blocks and is continuously refined. The key innovations in the Evoformer block are new mechanisms to exchange information within the MSA and pair representations that enable direct reasoning about the spatial and evolutionary relationships. The trunk of the network is followed by the structure module that introduces an explicit 3D structure in the form of a rotation and translation for each residue of the protein (global rigid body frames). These representations are initialized in a trivial state with all rotations set to the identity and all positions set to the origin, but rapidly develop and refine a highly accurate protein structure with precise atomic details. Key innovations in this section of the network include breaking the chain structure to allow simultaneous local refinement of all parts of the structure, a novel equivariant transformer to allow the network to implicitly reason about the unrepresented side-chain atoms and a loss term that places substantial weight on the orientational correctness of the residues. Both within the structure module and throughout the whole network, we reinforce the notion of iterative refinement by repeatedly applying the final loss to outputs and then feeding the outputs recursively into the same modules. The iterative refinement using the whole network (which we term ‘recycling’ and is related to approaches in computer vision28,29) contributes markedly to accuracy with minor extra training time (see Supplementary Methods 1.8 for details). Fraction of correct F1 rotamers 0.25 0.9 Fraction of proteins 0.20 0.8 0.15 0.7 0.10 0.6 0.05 0.5 0 0–0.5 0.5–1 1–2 2–4 4–8 >8 20 40 60 80 100 lDDT-C of a residue Full chain C r.m.s.d.95 () c 100 100 80 60 lDDT-C 90 40 20 80 90 100 80 0 0 20 40 60 80 100 Average pLDDT on the resolved region d 1.0 1.0 0.8 0.6 TM-score 0.9 0.4 0.2 0.8 0.9 1.0 0.8 0 0 0.2 0.4 0.6 0.8 1.0 pTM on the resolved region Fig. 2 | Accuracy of AlphaFold on recent PDB structures. The analysed structures are newer than any structure in the training set. Further filtering is applied to reduce redund",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nancy (see Methods). a, Histogram of backbone r.m.s.d. for full chains (C r.m.s.d. at 95% coverage). Error bars are 95% confidence intervals (Poisson). This dataset excludes proteins with a template (identified by hmmsearch) from the training set with more than 40% sequence identity covering more than 1%",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nof the chain (n = 3,144 protein chains). The overall median is 1.46 (95% confidence interval = 1.40–1.56 ). Note that this measure will be highly sensitive to domain packing and domain accuracy; a high r.m.s.d. is expected for some chains with uncertain packing or packing errors. b, Correlation between backbone accuracy and side-chain accuracy. Filtered to structures with any observed side chains and resolution better than 2.5 (n = 5,317 protein chains); side chains were further filtered to B-factor 30 2. A rotamer is classified as correct if the predicted torsion angle is within 40°. Each point aggregates a range of lDDT-C, with a bin size of 2 units above 70 lDDT-C and 5 units otherwise. Points correspond to the mean accuracy; error bars are 95% confidence intervals (Student t-test) of the mean on a per-residue basis. c, Confidence score compared to the true accuracy on chains. Least-squares linear fit lDDT-C = 0.997 pLDDT 1.17 (Pearson’s r = 0.76). n = 10,795 protein chains. The shaded region of the linear fit represents a 95% confidence interval estimated from 10,000 bootstrap samples. In the companion paper39, additional quantification of the reliability of pLDDT as a confidence measure is provided. d, Correlation between pTM and full chain TM-score. Least-squares linear fit TM-score = 0.98 pTM + 0.07 (Pearson’s r = 0.85). n = 10,795 protein chains. The shaded region of the linear fit represents a 95% confidence interval estimated from 10,000 bootstrap samples. Evoformer The key principle of the building block of the network—named Evoformer (Figs. 1e, 3a)—is to view the prediction of protein structures as a graph inference problem in 3D space in which the edges of the graph are defined by residues in proximity. The elements of the pair representation encode information about the relation between the residues (Fig. 3b). The columns of the MSA representation encode the individual residues of the input sequence while the rows represent the sequences in which those residues appear. Within this framework, we define a number of update operations that are applied in each block in which the different update operations are applied in series. The MSA representation updates the pair representation through an element-wise outer product that is summed over the MSA sequence dimension. In contrast to previous work30, this operation is applied within every block rather than once in the network, which enables the continuous communication from the evolving MSA representation to the pair representation. Within the pair representation, there are two different update patterns. Both are inspired by the necessity of consistency of the pair representation—for a pairwise description of amino acids to be representable as a single 3D structure, many constraints must be satisfied including the triangle inequality on distances. On the basis of this intuition, we arrange the update operations on the pair representation in terms of triangles of edges involving three different nodes (Fig. 3c). In particular, we add an extra logit bias to axial attention31 to include the ‘missing edge’ of the triangle and we define a non-attention update operation ‘triangle multiplicative update’ that uses two edges to update the missing third edge (see Supplementary Methods 1.6.5 for details). The triangle multiplicative update was developed originally as a more symmetric and cheaper replacement for the attention, and networks that use only the attention or multiplicative update are both able to produce high-accuracy structures. However, the combination of the two updates is more accurate. We also use a variant of axial attention within the MSA representation. During the per-sequence attention in the MSA, we project additional logits from the pair stack to bias the MSA attention. This closes the loop by providing information flow from the pair representation back into the MSA representation, ensuring that the overall Evoformer block is able to fully mix information between the pair and",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nMSA representations and prepare for structure generation within the structure module. End-to-end structure prediction The structure module (Fig. 3d) operates on a concrete 3D backbone structure using the pair representation and the original sequence row (single representation) of the MSA representation from the trunk. The and translations, each with respect to the global frame (residue",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\ngas) (Fig. 3e). These rotations and translations—representing the geometry of the N-C-C atoms—prioritize the orientation of the protein backbone so that the location of the side chain of each residue is highly constrained within that frame. Conversely, the peptide bond geometry is completely unconstrained and the network is observed to frequently violate the chain constraint during the application of the structure module as breaking this constraint enables the local refinement of all parts of the chain without solving complex loop closure problems. Satisfaction of the peptide bond geometry is encouraged during fine-tuning by a violation loss term. Exact enforcement of peptide bond geometry is only achieved in the post-prediction relaxation of the structure by gradient descent in the Amber32 force field. Empirically, this final relaxation does not improve the accuracy of the model as measured by the global distance test (GDT)33 or lDDT-C34 but does remove distracting stereochemical violations without the loss of accuracy. The residue gas representation is updated iteratively in two stages (Fig. 3d). First, a geometry-aware attention operation that we term ‘invariant point attention’ (IPA) is used to update an Nres set of neural activations (single representation) without changing the 3D positions, then an equivariant update operation is performed on the residue gas using the updated activations. The IPA augments each of the usual attention queries, keys and values with 3D points that are produced in the local frame of each residue such that the final value is invariant to global rotations and translations (see Methods ‘IPA’ for details). The 3D queries and keys also impose a strong spatial/locality bias on the attention, which is well-suited to the iterative refinement of the protein structure. After each attention operation and element-wise transition block, the module computes an update to the rotation and translation of each backbone frame. The application of these updates within the local frame of each residue makes the overall attention and update block an equivariant operation on the residue gas. Predictions of side-chain angles as well as the final, per-residue accuracy of the structure (pLDDT) are computed with small per-residue networks on the final activations at the end of the network. The estimate of the TM-score (pTM) is obtained from a pairwise error prediction that is computed as a linear projection from the final pair representation. The final loss (which we term the frame-aligned point error (FAPE) (Fig. 3f)) compares the predicted atom positions to the true positions under many different alignments. For each alignment, defined by aligning the predicted frame (Rk, tk) to the corresponding true frame, we compute the distance of all predicted atom positions xi from the true atom positions. The resulting Nframes Natoms distances are penalized with a clamped L1 loss. This creates a strong bias for atoms to be correct relative to the local frame of each residue and hence correct with respect to its side-chain interactions, as well as providing the main source of chirality for AlphaFold (Supplementary Methods 1.9.3 and Supplementary Fig. 9). Training with labelled and unlabelled data The AlphaFold architecture is able to train to high accuracy using only supervised learning on PDB data, but we are able to enhance accuracy (Fig. 4a) using an approach similar to noisy student self-distillation35. In this procedure, we use a trained network to predict the structure of around 350,000 diverse sequences from Uniclust3036 and make a new dataset of predicted structures filtered to a high-confidence subset. We then train the same architecture again from scratch using a mixture of PDB data and this new dataset of predicted structures as the training data, in which the various training data augmentations such as cropping and MSA subsampling make it challenging for the network to recapitulate the previously predicted structures. This self-distillation procedure makes effective use of the un",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nlabelled sequence data and considerably improves the accuracy of the resulting network. Additionally, we randomly mask out or mutate individual residues within the MSA and have a Bidirectional Encoder Representations from Transformers (BERT)-style37 objective to predict the masked elements of the MSA sequences. This objective encourages the network to learn to interpret phylogenetic and covariation relationships without hardcoding a",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk4_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nparticular correlation statistic into the features. The BERT objective is trained jointly with the normal PDB structure loss on the same training examples and is not pre-trained, in contrast to recent independent work38. (Fig. 4 | Interpreting the neural network. a, Ablation results on two target sets: the CASP14 set of domains (n = 87 protein domains) and the PDB test set of chains with template coverage of 30% at 30% identity (n = 2,261 protein chains). Domains are scored with GDT and chains are scored with lDDT-C. The ablations are reported as a difference compared with the average of the three baseline seeds. Means (points) and 95% bootstrap percentile intervals (error bars) are computed using bootstrap estimates of 10,000 samples. b, Domain GDT trajectory over 4 recycling iterations and 48 Evoformer blocks on CASP14 targets LmrP (T1024) and Orf8 (T1064) where D1 and D2 refer to the individual domains as defined by the CASP assessment. Both T1024 domains obtain the correct structure early in the network, whereas the structure of T1064 changes multiple times and requires nearly the full depth of the network to reach the final structure. Note, 48 Evoformer blocks comprise one recycling iteration.) Evoformer block—in which each intermediate represents the belief of the network of the most likely structure at that block. The resulting trajectories are surprisingly smooth after the first few blocks, showing that AlphaFold makes constant incremental improvements to the structure until it can no longer improve (see Fig. 4b for a trajectory of accuracy). These trajectories also illustrate the role of network depth. For very challenging proteins such as ORF8 of SARS-CoV-2 (T1064), the network searches and rearranges secondary structure elements for many layers before settling on a good structure. For other proteins such as LmrP (T1024), the network finds the final structure within the first few layers. Structure trajectories of CASP14 targets T1024, T1044, T1064 and T1091 that demonstrate a clear iterative building process for a range of protein sizes and difficulties are shown in Supplementary Videos 1–4. In Supplementary Methods 1.16 and Supplementary Figs. 12, 13, we interpret the attention maps produced by AlphaFold layers. Figure 4a contains detailed ablations of the components of AlphaFold that demonstrate that a variety of different mechanisms contribute to AlphaFold accuracy. Detailed descriptions of each ablation model, their training details, extended discussion of ablation results and the effect of MSA depth on each ablation are provided in Supplementary Methods 1.13 and Supplementary Fig. 10.(Fig. 5 | Effect of MSA depth and cross-chain contacts. a, Backbone accuracy (lDDT-C) for the redundancy-reduced set of the PDB after our training data cut-off, restricting to proteins in which at most 25% of the long-range contacts are between different heteromer chains. We further consider two groups of proteins based on template coverage at 30% sequence identity: covering more than 60% of the chain (n = 6,743 protein chains) and covering less than 30% of the chain (n = 1,596 protein chains). MSA depth is computed by counting the number of non-gap residues for each position in the MSA (using the Neff weighting scheme; see Methods for details) and taking the median across residues. The curves are obtained through Gaussian kernel average smoothing (window size is 0.2 units in log10(Neff)); the shaded area is the 95% confidence interval estimated using bootstrap of 10,000 samples. b, An intertwined homotrimer (PDB 6SK0) is correctly predicted without input stoichiometry and only a weak template (blue is predicted and green is experimental) MSA depth and cross-chain contacts Although AlphaFold has a high accuracy across the vast majority of deposited PDB structures, we note that there are still factors that affect accuracy or limit the applicability of the model.",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk4_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nThe model uses MSAs and the accuracy decreases substantially when the median alignment depth is less than around 30 sequences (see Fig. 5a for details). We observe a threshold effect where improvements in MSA depth over around 100 sequences lead to small gains. We hypothesize that the MSA information is needed to coarsely find the correct structure within the early stages of the network, but refinement of that prediction into a high-accur",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "43",
    "id_story": "43-01",
    "id_chunk": "43-01_Method_chunk5",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nacy model does not depend crucially on the MSA information. The other substantial limitation that we have observed is that AlphaFold is much weaker for proteins that have few intra-chain or homotypic contacts compared to the number of heterotypic contacts (further details are provided in a companion paper39). This typically occurs for bridging domains within larger complexes in which the shape of the protein is created almost entirely by interactions with other chains in the complex. Conversely, AlphaFold is often able to give high-accuracy predictions for homomers, even when the chains are substantially intertwined (Fig. 5b). We expect that the ideas of AlphaFold are readily applicable to predicting full hetero-complexes in a future system and that this will remove the difficulty with protein chains that have a large number of hetero-contacts.",
    "output": "From genome to proteome\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Abstract\nAbstract: High-Resolution Image Synthesis with Latent Diffusion Models Robin Rombach1 * Andreas Blattmann1  Dominik Lorenz1 Patrick Esser Bj orn Ommer1 Runway ML https://github.com/CompVis/latent-diffusion Abstract By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis and highly competitive performance on various tasks, including text-to-image synthesis, unconditional image generation and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. arXiv:2112.10752v2 [cs.CV] 13 Apr 2022",
    "output": "This has been led by Patrick Esser from Runway and Robin Rombach from the Machine Vision & Learning research group at LMU Munich (formerly CompVis lab at Heidelberg University) building on their prior work on Latent Diffusion Models at CVPR’22, combined with support from communities at Eleuther AI, LAION, and our own generative AI team.\nStable Diffusion is a text-to-image model empowering billions of people to create stunning art within seconds. It is a speed and quality breakthrough, meaning it can run on consumer GPUs. You can see some of the amazing output that this model has created without pre or post-processing on this page.\nStable Diffusion runs on under 10 GB of VRAM on consumer GPUs, generating images at 512x512 pixels in a few seconds. This will allow both researchers and soon the public to run this under a range of conditions, democratizing image generation. We look forward to the open ecosystem that will emerge around this and further models to truly explore the boundaries of latent space."
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Introduction\nIntroduction: 1. Introduction Image synthesis is one of the computer vision fields with the most spectacular recent development, but also among those with the greatest computational demands. Especially high-resolution synthesis of complex, natural scenes is presently dominated by scaling up likelihood-based models, potentially containing billions of parameters in autoregressive (AR) transformers [66,67]. In contrast, the promising results of GANs [3, 27, 40] have been revealed to be mostly confined to data with comparably limited variability as their adversarial learning procedure does not easily scale to modeling complex, multi-modal distributions. Recently, diffusion models [82], which are built from a hierarchy of denoising autoencoders, have shown to achieve impressive Input ours (f = 4) PSNR: 27.4 R-FID: 0.58 DALL-E (f = 8) PSNR: 22.8 R-FID: 32.01 VQGAN (f = 16) PSNR: 19.9 R-FID: 4.98 Figure 1. Boosting the upper bound on achievable quality with less agressive downsampling. Since diffusion models offer excellent inductive biases for spatial data, we do not need the heavy spatial downsampling of related generative models in latent space, but can still greatly reduce the dimensionality of the data via suitable autoencoding models, see Sec. 3. Images are from the DIV2K [1] validation set, evaluated at 5122 px. We denote the spatial downsampling factor by f. Reconstruction FIDs [29] and PSNR are calculated on ImageNet-val. [12]; see also Tab. 8. results in image synthesis [30,85] and beyond [7,45,48,57], and define the state-of-the-art in class-conditional image synthesis [15,31] and super-resolution [72]. Moreover, even unconditional DMs can readily be applied to tasks such as inpainting and colorization [85] or stroke-based synthesis [53], in contrast to other types of generative models [19,46,69]. Being likelihood-based models, they do not exhibit mode-collapse and training instabilities as GANs and, by heavily exploiting parameter sharing, they can model highly complex distributions of natural images without involving billions of parameters as in AR models [67]. Democratizing High-Resolution Image Synthesis DMs belong to the class of likelihood-based models, whose mode-covering behavior makes them prone to spend excessive amounts of capacity (and thus compute resources) on modeling imperceptible details of the data [16, 73]. Although the reweighted variational objective [30] aims to address this by undersampling the initial denoising steps, DMs are still computationally demanding, since training and evaluating such a model requires repeated function evaluations (and gradient computations) in the high-dimensional space of RGB images. As an example, training the most powerful DMs often takes hundreds of GPU days (e.g. 150 - version of the input space render also inference expensive, so that producing 50k samples takes approximately 5 days [15] on a single A100 GPU. This has two consequences for the research community and users in general: Firstly, training such a model requires massive computational resources only available to a small fraction of the field, and leaves a huge carbon footprint [65, 86]. Secondly, evaluating an already trained model is also expensive in time and memory, since the same model architecture must run sequentially for a large number of steps (e.g. 25 - 1000 steps in [15]). To increase the accessibility of this powerful model class and at the same time reduce its significant resource consumption, a method is needed that reduces the computational complexity for both training and sampling. Reducing the computational demands of DMs without impairing their performance is, therefore, key to enhance their accessibility. Departure to Latent Space Our approach starts with the analysis of already trained diffusion models in pixel space: Fig. 2 shows the rate-distortion trade-off of a trained model. As with any likelihood-",
    "output": "Stability AI and our collaborators are proud to announce the first stage of the release of Stable Diffusion to researchers. Our friends at Hugging Face will host the model weights once you get access. The code is available here, and the model card is here. We are working together towards a public release soon.\n“We are delighted to release the first in a series of benchmark open source Stable Diffusion models that will enable billions to be more creative, happy and communicative. This model builds on the work of many excellent researchers and we look forward to the positive effect of this and similar models on society and science in the coming years as they are used by billions worldwide”. - Emad, CEO, Stability AI"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Introduction\nbased model, learning can be roughly divided into two stages: First is a perceptual compression stage which removes high-frequency details but still learns little semantic variation. In the second stage, the actual generative model learns the semantic and conceptual composition of the data (semantic compression). We thus aim to first find a perceptually equivalent, but computationally more suitable space, in which we will train diffusion models for high-resolution image synthesis.",
    "output": "Stability AI and our collaborators are proud to announce the first stage of the release of Stable Diffusion to researchers. Our friends at Hugging Face will host the model weights once you get access. The code is available here, and the model card is here. We are working together towards a public release soon.\n“We are delighted to release the first in a series of benchmark open source Stable Diffusion models that will enable billions to be more creative, happy and communicative. This model builds on the work of many excellent researchers and we look forward to the positive effect of this and similar models on society and science in the coming years as they are used by billions worldwide”. - Emad, CEO, Stability AI"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Introduction\nFollowing common practice [11, 23, 66, 67, 96], we separate training into two distinct phases: First, we train an autoencoder which provides a lower-dimensional (and thereby efficient) representational space which is perceptually equivalent to the data space. Importantly, and in contrast to previous work [23,66], we do not need to rely on excessive spatial compression, as we train DMs in the learned latent space, which exhibits better scaling properties with respect to the spatial dimensionality. The reduced complexity also provides efficient image generation from the latent space with a single network pass. We dub the resulting model class Latent Diffusion Models (LDMs). A notable advantage of this approach is that we need to train the universal autoencoding stage only once and can therefore reuse it for multiple DM trainings or to explore possibly completely different tasks [81]. This enables efficient exploration of a large number of diffusion models for various image-to-image and text-to-image tasks. For the latter, we design an architecture that connects transformers to the DM’s UNet backbone [71] and enables arbitrary types of token-based conditioning mechanisms, see Sec. 3.3. In sum, our work makes the following contributions: (i) In contrast to purely transformer-based approaches [23,66], our method scales more graceful to higher dimensional data and can thus (a) work on a compression level which provides more faithful and detailed reconstructions than previous work (see Fig. 1) and (b) can be efficiently Figure 2. Illustrating perceptual and semantic compression: Most bits of a digital image correspond to imperceptible details. While DMs allow to suppress this semantically meaningless information by minimizing the responsible loss term, gradients (during training) and the neural network backbone (training and inference) still need to be evaluated on all pixels, leading to superfluous computations and unnecessarily expensive optimization and inference. We propose latent diffusion models (LDMs) as an effective generative model and a separate mild compression stage that only eliminates imperceptible details. Data and images from [30]. applied to high-resolution synthesis of megapixel images. (ii) We achieve competitive performance on multiple tasks (unconditional image synthesis, inpainting, stochastic super-resolution) and datasets while significantly lowering computational costs. Compared to pixel-based diffusion approaches, we also significantly decrease inference costs. (iii) We show that, in contrast to previous work [93] which learns both an encoder/decoder architecture and a score-based prior simultaneously, our approach does not require a delicate weighting of reconstruction and generative abilities. This ensures extremely faithful reconstructions and requires very little regularization of the latent space. (iv) We find that for densely conditioned tasks such as super-resolution, inpainting and semantic synthesis, our model can be applied in a convolutional fashion and render large, consistent images of 10242 px. (v) Moreover, we design a general-purpose conditioning mechanism based on cross-attention, enabling multi-modal training. We use it to train class-conditional, text-to-image and layout-to-image models. (vi) Finally, we release pretrained latent diffusion and autoencoding models at https : / / github . com/CompVis/latent-diffusion which might be reusable for a various tasks besides training of DMs [81].",
    "output": "Stability AI and our collaborators are proud to announce the first stage of the release of Stable Diffusion to researchers. Our friends at Hugging Face will host the model weights once you get access. The code is available here, and the model card is here. We are working together towards a public release soon.\n“We are delighted to release the first in a series of benchmark open source Stable Diffusion models that will enable billions to be more creative, happy and communicative. This model builds on the work of many excellent researchers and we look forward to the positive effect of this and similar models on society and science in the coming years as they are used by billions worldwide”. - Emad, CEO, Stability AI"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Method\nMethod: 3. Method To lower the computational demands of training diffusion models towards high-resolution image synthesis, we observe that although diffusion models allow to ignore perceptually irrelevant details by undersampling the corresponding loss terms [30], they still require costly function evaluations in pixel space, which causes huge demands in computation time and energy resources. We propose to circumvent this drawback by introducing an explicit separation of the compressive from the generative learning phase (see Fig. 2). To achieve this, we utilize an autoencoding model which learns a space that is perceptually equivalent to the image space, but offers significantly reduced computational complexity. Such an approach offers several advantages: (i) By leaving the high-dimensional image space, we obtain DMs which are computationally much more efficient because sampling is performed on a low-dimensional space. (ii) We exploit the inductive bias of DMs inherited from their UNet architecture [71], which makes them particularly effective for data with spatial structure and therefore alleviates the need for aggressive, quality-reducing compression levels as required by previous approaches [23, 66]. (iii) Finally, we obtain general-purpose compression models whose latent space can be used to train multiple generative models and which can also be utilized for other downstream applications such as single-image CLIP-guided synthesis [25]. 3.1. Perceptual Image Compression Our perceptual compression model is based on previous work [23] and consists of an autoencoder trained by combination of a perceptual loss [106] and a patch-based [33] adversarial objective [20,23,103]. This ensures that the reconstructions are confined to the image manifold by enforcing local realism and avoids bluriness introduced by relying solely on pixel-space losses such as L2 or L1 objectives. More precisely, given an image x RHW 3 in RGB space, the encoder E encodes x into a latent representa- tion z = E(x), and the decoder D reconstructs the image from the latent, giving x = D(z) = D(E(x)), where z Rhwc. Importantly, the encoder downsamples the image by a factor f = H/h = W/w, and we investigate different downsampling factors f = 2m, with m N. In order to avoid arbitrarily high-variance latent spaces, we experiment with two different kinds of regularizations. The first variant, KL-reg., imposes a slight KL-penalty towards a standard normal on the learned latent, similar to a VAE [46, 69], whereas VQ-reg. uses a vector quantization layer [96] within the decoder. This model can be interpreted as a VQGAN [23] but with the quantization layer absorbed by the decoder. Because our subsequent DM is designed to work with the two-dimensional structure of our learned latent space z = E(x), we can use relatively mild compression rates and achieve very good reconstructions. This is in contrast to previous works [23, 66], which relied on an arbitrary 1D ordering of the learned space z to model its distribution autoregressively and thereby ignored much of the inherent structure of z. Hence, our compression model preserves details of x better (see Tab. 8). The full objective and training details can be found in the supplement. 3.2. Latent Diffusion Models Diffusion Models [82] are probabilistic models designed to learn a data distribution p(x) by gradually denoising a normally distributed variable, which corresponds to learning the reverse process of a fixed Markov Chain of length T. For image synthesis, the most successful models [15,30,72] rely on a reweighted variant of the variational lower bound on p(x), which mirrors denoising score-matching [85]. These models can be interpreted as an equally weighted sequence of deno",
    "output": "Stable Diffusion Launch Announcement 10 agosto 2022 Stability AI and our collaborators are proud to announce the first stage of the release of Stable Diffusion to researchers. Our friends at Hugging Face will host the model weights once you get access. The code is available here, and the model card is here. We are working together towards a public release soon. This has been led by Patrick Esser from Runway and Robin Rombach from the Machine Vision & Learning research group at LMU Munich (formerly CompVis lab at Heidelberg University) building on their prior work on Latent Diffusion Models at CVPR’22, combined with support from communities at Eleuther AI, LAION, and our own generative AI team. Stable Diffusion is a text-to-image model empowering billions of people to create stunning art within seconds. It is a speed and quality breakthrough, meaning it can run on consumer GPUs. You can see some of the amazing output that this model has created without pre or post-processing on this page. The model builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain, and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all. User generated images from Stable Diffusion Beta The core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai. Stable Diffusion runs on under 10 GB of VRAM on consumer GPUs, generating images at 512x512 pixels in a few seconds. This will allow both researchers and soon the public to run this under a range of conditions, democratizing image generation. We look forward to the open ecosystem that will emerge around this and further models to truly explore the boundaries of latent space. The model was trained on our 4,000 A100 Ezra-1 AI ultracluster over the last month as the first of a series of models exploring this and other approaches. We have been testing the model at scale with over 10,000 beta testers that are creating 1.7 million images a day. image one: a generated painting of a woman with red hair looking away while the sun shine above her, image two: a generated painting on a gloomy night in a street alley as lightning strikes User generated image from Stable Diffusion Beta This output has given us numerous insights as we prepare for a public release soon. This will provide the template for the release of many open models we are currently training to unlock human potential. We will also be releasing open synthetic datasets based on this output for further research. We aim to set new standards of collaboration and reproducibility for the models that we create and support and will share our learnings in the coming weeks. We hope to progressively increase the number of collaborators for our benchmark models. If you would like to help, please join one of the communities we support and/or reach out to info@stability.ai Some comments by various folks: “EleutherAI has spent the past two years advancing open source large-scale AI research. We are thrilled to be working with and supporting like-minded researchers to enable scientific access to these emerging technologies” - Stella Biderman, Lead Researcher at EleutherAI \"With this project we continue to pursue our mission to make state of the art machine learning accessible for people from all over the world. 100% open. 100% free.\" - Christoph, Organizational Lead & researcher at LAION e.V. “We are excited to see what will be built with the current models as well as to see what further works will be coming out of open, collaborative research efforts!” - Patrick (Runway) and Robin (LMU) \"We're excited that state of the art text-to-image models are being built openly and we are happy to collaborate with CompVis and Stability.ai towards safely and ethically release the models to the public and help democratize ML capabilities with the whole community\" - Apolinário,"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Method\nising autoencoders (xt, t); t = 1 . . . T, which are trained to predict a denoised variant of their input xt, where xt is a noisy version of the input x. The corresponding objective can be simplified to (Sec. B) LDM = Ex,N(0,1),",
    "output": "Stable Diffusion Launch Announcement 10 agosto 2022 Stability AI and our collaborators are proud to announce the first stage of the release of Stable Diffusion to researchers. Our friends at Hugging Face will host the model weights once you get access. The code is available here, and the model card is here. We are working together towards a public release soon. This has been led by Patrick Esser from Runway and Robin Rombach from the Machine Vision & Learning research group at LMU Munich (formerly CompVis lab at Heidelberg University) building on their prior work on Latent Diffusion Models at CVPR’22, combined with support from communities at Eleuther AI, LAION, and our own generative AI team. Stable Diffusion is a text-to-image model empowering billions of people to create stunning art within seconds. It is a speed and quality breakthrough, meaning it can run on consumer GPUs. You can see some of the amazing output that this model has created without pre or post-processing on this page. The model builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain, and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all. User generated images from Stable Diffusion Beta The core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai. Stable Diffusion runs on under 10 GB of VRAM on consumer GPUs, generating images at 512x512 pixels in a few seconds. This will allow both researchers and soon the public to run this under a range of conditions, democratizing image generation. We look forward to the open ecosystem that will emerge around this and further models to truly explore the boundaries of latent space. The model was trained on our 4,000 A100 Ezra-1 AI ultracluster over the last month as the first of a series of models exploring this and other approaches. We have been testing the model at scale with over 10,000 beta testers that are creating 1.7 million images a day. image one: a generated painting of a woman with red hair looking away while the sun shine above her, image two: a generated painting on a gloomy night in a street alley as lightning strikes User generated image from Stable Diffusion Beta This output has given us numerous insights as we prepare for a public release soon. This will provide the template for the release of many open models we are currently training to unlock human potential. We will also be releasing open synthetic datasets based on this output for further research. We aim to set new standards of collaboration and reproducibility for the models that we create and support and will share our learnings in the coming weeks. We hope to progressively increase the number of collaborators for our benchmark models. If you would like to help, please join one of the communities we support and/or reach out to info@stability.ai Some comments by various folks: “EleutherAI has spent the past two years advancing open source large-scale AI research. We are thrilled to be working with and supporting like-minded researchers to enable scientific access to these emerging technologies” - Stella Biderman, Lead Researcher at EleutherAI \"With this project we continue to pursue our mission to make state of the art machine learning accessible for people from all over the world. 100% open. 100% free.\" - Christoph, Organizational Lead & researcher at LAION e.V. “We are excited to see what will be built with the current models as well as to see what further works will be coming out of open, collaborative research efforts!” - Patrick (Runway) and Robin (LMU) \"We're excited that state of the art text-to-image models are being built openly and we are happy to collaborate with CompVis and Stability.ai towards safely and ethically release the models to the public and help democratize ML capabilities with the whole community\" - Apolinário,"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Method\nt h  (xt, t)2 2 i , (1) with t uniformly sampled from 1, . . . , T. Generative Modeling of Latent Representations With our trained perceptual compression models consisting of E and D, we now have access to an efficient, low-dimensional latent space in which high-frequency, imperceptible details are abstracted away. Compared to the high-dimensional pixel space, this space is more suitable for likelihood-based generative models, as they can now (i) focus on the important, semantic bits of the data and (ii) train in a lower dimensional, computationally much more efficient space. Unlike previous work that relied on autoregressive, attention-based transformer models in a highly compressed, discrete latent space [23,66,103], we can take advantage of image-specific inductive biases that our model offers. This Latent Space Conditioning Semantic Diffusion Process Map Denoising U-Net Text Repres entations Images Pixel Space denoising step switch skip connection crossattention concat Figure 3. We condition LDMs either via concatenation or by a more general cross-attention mechanism. See Sec. 3.3 includes the ability to build the underlying UNet primarily from 2D convolutional layers, and further focusing the objective on the perceptually most relevant bits using the reweighted bound, which now reads LLDM := EE(x),N(0,1),t h  (zt, t)2 2 i . (2) The neural backbone (, t) of our model is realized as a time-conditional UNet [71]. Since the forward process is fixed, zt can be efficiently obtained from E during training, and samples from p(z) can be decoded to image space with a single pass through D. 3.3. Conditioning Mechanisms Similar to other types of generative models [56, 83], diffusion models are in principle capable of modeling conditional distributions of the form p(z|y). This can be implemented with a conditional denoising autoencoder (zt, t, y) and paves the way to controlling the synthesis process through inputs y such as text [68], semantic maps [33,61] or other image-to-image translation tasks [34]. In the context of image synthesis, however, combining the generative power of DMs with other types of conditionings beyond class-labels [15] or blurred variants of the input image [72] is so far an under-explored area of research. We turn DMs into more flexible conditional image generators by augmenting their underlying UNet backbone with the cross-attention mechanism [97], which is effective for learning attention-based models of various input modalities [35,36]. To pre-process y from various modalities (such as language prompts) we introduce a domain specific encoder  that projects y to an intermediate representation (y) RMd , which is then mapped to the intermediate layers of the UNet via a cross-attention layer implementing Attention(Q, K, V ) = softmax QKT  V , with  d Q = W (i) Q  i(zt), K = W (i) K  (y), V = W (i) V  (y). CelebAHQ FFHQ LSUN-Churches LSUN-Beds ImageNet Figure 4. Samples from LDMs trained on CelebAHQ [39], FFHQ [41], LSUN-Churches [102], LSUN-Bedrooms [102] and classconditional ImageNet [12], each with a resolution of 256  256. Best viewed when zoomed in. For more samples cf. the supplement. Rddi , W (i) Q Rdd & W (i) K Rdd are learnable projection matrices [36,97]. See Fig. 3 for a visual depiction. Based on image-conditioning pairs, we then learn the conditional LDM via LLDM := EE(x),y,N(0,1),t h (zt, t,",
    "output": "Stable Diffusion Launch Announcement 10 agosto 2022 Stability AI and our collaborators are proud to announce the first stage of the release of Stable Diffusion to researchers. Our friends at Hugging Face will host the model weights once you get access. The code is available here, and the model card is here. We are working together towards a public release soon. This has been led by Patrick Esser from Runway and Robin Rombach from the Machine Vision & Learning research group at LMU Munich (formerly CompVis lab at Heidelberg University) building on their prior work on Latent Diffusion Models at CVPR’22, combined with support from communities at Eleuther AI, LAION, and our own generative AI team. Stable Diffusion is a text-to-image model empowering billions of people to create stunning art within seconds. It is a speed and quality breakthrough, meaning it can run on consumer GPUs. You can see some of the amazing output that this model has created without pre or post-processing on this page. The model builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain, and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all. User generated images from Stable Diffusion Beta The core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai. Stable Diffusion runs on under 10 GB of VRAM on consumer GPUs, generating images at 512x512 pixels in a few seconds. This will allow both researchers and soon the public to run this under a range of conditions, democratizing image generation. We look forward to the open ecosystem that will emerge around this and further models to truly explore the boundaries of latent space. The model was trained on our 4,000 A100 Ezra-1 AI ultracluster over the last month as the first of a series of models exploring this and other approaches. We have been testing the model at scale with over 10,000 beta testers that are creating 1.7 million images a day. image one: a generated painting of a woman with red hair looking away while the sun shine above her, image two: a generated painting on a gloomy night in a street alley as lightning strikes User generated image from Stable Diffusion Beta This output has given us numerous insights as we prepare for a public release soon. This will provide the template for the release of many open models we are currently training to unlock human potential. We will also be releasing open synthetic datasets based on this output for further research. We aim to set new standards of collaboration and reproducibility for the models that we create and support and will share our learnings in the coming weeks. We hope to progressively increase the number of collaborators for our benchmark models. If you would like to help, please join one of the communities we support and/or reach out to info@stability.ai Some comments by various folks: “EleutherAI has spent the past two years advancing open source large-scale AI research. We are thrilled to be working with and supporting like-minded researchers to enable scientific access to these emerging technologies” - Stella Biderman, Lead Researcher at EleutherAI \"With this project we continue to pursue our mission to make state of the art machine learning accessible for people from all over the world. 100% open. 100% free.\" - Christoph, Organizational Lead & researcher at LAION e.V. “We are excited to see what will be built with the current models as well as to see what further works will be coming out of open, collaborative research efforts!” - Patrick (Runway) and Robin (LMU) \"We're excited that state of the art text-to-image models are being built openly and we are happy to collaborate with CompVis and Stability.ai towards safely and ethically release the models to the public and help democratize ML capabilities with the whole community\" - Apolinário,"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Method\n(y))2 2 i , (3) where both  and  are jointly optimized via Eq. 3. This conditioning mechanism is flexible as  can be parameterized with domain-specific experts, e.g. (unmasked) transformers [97] when y are text prompts (see Sec. 4.3.1)",
    "output": "Stable Diffusion Launch Announcement 10 agosto 2022 Stability AI and our collaborators are proud to announce the first stage of the release of Stable Diffusion to researchers. Our friends at Hugging Face will host the model weights once you get access. The code is available here, and the model card is here. We are working together towards a public release soon. This has been led by Patrick Esser from Runway and Robin Rombach from the Machine Vision & Learning research group at LMU Munich (formerly CompVis lab at Heidelberg University) building on their prior work on Latent Diffusion Models at CVPR’22, combined with support from communities at Eleuther AI, LAION, and our own generative AI team. Stable Diffusion is a text-to-image model empowering billions of people to create stunning art within seconds. It is a speed and quality breakthrough, meaning it can run on consumer GPUs. You can see some of the amazing output that this model has created without pre or post-processing on this page. The model builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain, and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all. User generated images from Stable Diffusion Beta The core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai. Stable Diffusion runs on under 10 GB of VRAM on consumer GPUs, generating images at 512x512 pixels in a few seconds. This will allow both researchers and soon the public to run this under a range of conditions, democratizing image generation. We look forward to the open ecosystem that will emerge around this and further models to truly explore the boundaries of latent space. The model was trained on our 4,000 A100 Ezra-1 AI ultracluster over the last month as the first of a series of models exploring this and other approaches. We have been testing the model at scale with over 10,000 beta testers that are creating 1.7 million images a day. image one: a generated painting of a woman with red hair looking away while the sun shine above her, image two: a generated painting on a gloomy night in a street alley as lightning strikes User generated image from Stable Diffusion Beta This output has given us numerous insights as we prepare for a public release soon. This will provide the template for the release of many open models we are currently training to unlock human potential. We will also be releasing open synthetic datasets based on this output for further research. We aim to set new standards of collaboration and reproducibility for the models that we create and support and will share our learnings in the coming weeks. We hope to progressively increase the number of collaborators for our benchmark models. If you would like to help, please join one of the communities we support and/or reach out to info@stability.ai Some comments by various folks: “EleutherAI has spent the past two years advancing open source large-scale AI research. We are thrilled to be working with and supporting like-minded researchers to enable scientific access to these emerging technologies” - Stella Biderman, Lead Researcher at EleutherAI \"With this project we continue to pursue our mission to make state of the art machine learning accessible for people from all over the world. 100% open. 100% free.\" - Christoph, Organizational Lead & researcher at LAION e.V. “We are excited to see what will be built with the current models as well as to see what further works will be coming out of open, collaborative research efforts!” - Patrick (Runway) and Robin (LMU) \"We're excited that state of the art text-to-image models are being built openly and we are happy to collaborate with CompVis and Stability.ai towards safely and ethically release the models to the public and help democratize ML capabilities with the whole community\" - Apolinário,"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Experiments / Results\nExperiments / Results: 4. Experiments LDMs provide means to flexible and computationally tractable diffusion based image synthesis of various image modalities, which we empirically show in the following. Firstly, however, we analyze the gains of our models compared to pixel-based diffusion models in both training and inference. Interestingly, we find that LDMs trained in VQregularized latent spaces sometimes achieve better sample quality, even though the reconstruction capabilities of VQregularized first stage models slightly fall behind those of their continuous counterparts, cf. Tab. 8. A visual comparison between the effects of first stage regularization schemes on LDM training and their generalization abilities to resolutions > 2562 can be found in Appendix D.1. In E.2 we list details on architecture, implementation, training and evaluation for all results presented in this section. 4.1. On Perceptual Compression Tradeoffs This section analyzes the behavior of our LDMs with different downsampling factors f 1, 2, 4, 8, 16, 32 (abbreviated as LDM-f, where LDM-1 corresponds to pixel-based DMs). To obtain a comparable test-field, we fix the computational resources to a single NVIDIA A100 for all experiments in this section and train all models for the same number of steps and with the same number of parameters. Tab. 8 shows hyperparameters and reconstruction performance of the first stage models used for the LDMs com- pared in this section. Fig. 6 shows sample quality as a function of training progress for 2M steps of class-conditional models on the ImageNet [12] dataset. We see that, i) small downsampling factors for LDM-1,2 result in slow training progress, whereas ii) overly large values of f cause stagnating fidelity after comparably few training steps. Revisiting the analysis above (Fig. 1 and 2) we attribute this to i) leaving most of perceptual compression to the diffusion model and ii) too strong first stage compression resulting in information loss and thus limiting the achievable quality. LDM-4-16 strike a good balance between efficiency and perceptually faithful results, which manifests in a significant FID [29] gap of 38 between pixel-based diffusion (LDM-1) and LDM-8 after 2M training steps. In Fig. 7, we compare models trained on CelebAHQ [39] and ImageNet in terms sampling speed for different numbers of denoising steps with the DDIM sampler [84] and plot it against FID-scores [29]. LDM-4-8 outperform models with unsuitable ratios of perceptual and conceptual compression. Especially compared to pixel-based LDM-1, they achieve much lower FID scores while simultaneously significantly increasing sample throughput. Complex datasets such as ImageNet require reduced compression rates to avoid reducing quality. In summary, LDM-4 and -8 offer the best conditions for achieving high-quality synthesis results. 4.2. Image Generation with Latent Diffusion We train unconditional models of 2562 images on CelebA-HQ [39], FFHQ [41], LSUN-Churches and -Bedrooms [102] and evaluate the i) sample quality and ii) their coverage of the data manifold using ii) FID [29] and ii) Precision-and-Recall [50]. Tab. 1 summarizes our results. On CelebA-HQ, we report a new state-of-the-art FID of 5.11, outperforming previous likelihood-based models as well as GANs. We also outperform LSGM [93] where a latent diffusion model is trained jointly together with the first stage. In contrast, we train diffusion models in a fixed space ’A street sign that reads “Latent Diffusion” ’ ’A zombie in the style of Picasso’ ’An image of an animal half mouse half octopus’ ’An illustration of a slightly conscious neural network’ ’A painting of a squirrel eating a burger’",
    "output": "User generated images from Stable Diffusion Beta\nThe core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai.\nUser generated image from Stable Diffusion Beta"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Experiments / Results\n’A watercolor painting of a chair that looks like an octopus’ ’A shirt with the inscription: “I love generative models!” ’ Figure 5. Samples for user-defined text prompts from our model for text-to-image synthesis, LDM-8 (KL), which was trained on the LAION [78] database. Samples generated with 200 DDIM steps and",
    "output": "User generated images from Stable Diffusion Beta\nThe core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai.\nUser generated image from Stable Diffusion Beta"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Experiments / Results\n= 1.0. We use unconditional guidance [32] with s = 10.0. Figure 6. Analyzing the training of class-conditional LDMs with different downsampling factors f over 2M train steps on the ImageNet dataset. Pixel-based LDM-1 requires substantially larger train times compared to models with larger downsampling factors (LDM-4-16). Too much perceptual compression as in LDM-32 limits the overall sample quality. All models are trained on a single NVIDIA A100 with the same computational budget. Results obtained with 100 DDIM steps [84] and  = 0. Figure 7. Comparing LDMs with varying compression on the CelebA-HQ (left) and ImageNet (right) datasets. Different markers indicate 10, 20, 50, 100, 200 sampling steps using DDIM, from right to left along each line. The dashed line shows the FID scores for 200 steps, indicating the strong performance of LDM- 4-8. FID scores assessed on 5000 samples. All models were trained for 500k (CelebA) / 2M (ImageNet) steps on an A100. and avoid the difficulty of weighing reconstruction quality against learning the prior over the latent space, see Fig. 1-2. We outperform prior diffusion based approaches on all but the LSUN-Bedrooms dataset, where our score is close to ADM [15], despite utilizing half its parameters and requiring 4-times less train resources (see Appendix E.3.5). Text-to-Image Synthesis on LAION. 1.45B Model. CelebA-HQ 256  256 FFHQ 256  256 Method FID  Prec.  Recall  Method FID  Prec.  Recall  DC-VAE [63] 15.8 - - ImageBART [21] 9.57 - - VQGAN+T. [23] (k=400) 10.2 - - U-Net GAN (+aug) [77] 10.9 (7.6) - - PGGAN [39] 8.0 - - UDM [43] 5.54 - - LSGM [93] 7.22 - - StyleGAN [41] 4.16 0.71 0.46 UDM [43] 7.16 - - ProjectedGAN [76] 3.08 0.65 0.46 LDM-4 (ours, 500-s) 5.11 0.72 0.49 LDM-4 (ours, 200-s) 4.98 0.73 0.50 LSUN-Churches 256  256 LSUN-Bedrooms 256  256 Method FID  Prec.  Recall  Method FID  Prec.  Recall  DDPM [30] 7.89 - - ImageBART [21] 5.51 - - ImageBART [21] 7.32 - - DDPM [30] 4.9 - - PGGAN [39] 6.42 - - UDM [43] 4.57 - - StyleGAN [41] 4.21 - - StyleGAN [41] 2.35 0.59 0.48 StyleGAN2 [42] 3.86 - - ADM [15] 1.90 0.66 0.51 ProjectedGAN [76] 1.59 0.61 0.44 ProjectedGAN [76] 1.52 0.61 0.34 LDM-8(ours, 200-s) 4.02 0.64 0.52 LDM-4 (ours, 200-s) 2.95 0.66 0.48 Table 1. Evaluation metrics for unconditional image synthesis. CelebA-HQ results reproduced from [43, 63, 100], FFHQ from [42, 43]. : N-s refers to N sampling steps with the DDIM [84] sampler. : trained in KL-regularized latent space. Additional results can be found in the supplementary. Text-Conditional Image Synthesis Method FID  IS Nparams CogView [17] 27.10 18.20 self-ranking, rejection rate 0.017 LAFITE [109] 26.94 26.02 GLIDE[59] 12.24 - Make-A-Scene[26] 11.84 - c.f.g for AR models [98] s = 5 LDM-KL-8 23.31 20.030.33 1.45B LDM",
    "output": "User generated images from Stable Diffusion Beta\nThe core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai.\nUser generated image from Stable Diffusion Beta"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Experiments / Results\n-KL-8-G 12.63 30.290.42 1.45B Table 2. Evaluation of text-conditional image synthesis on the 256 256-sized MS-COCO [51] dataset: with 250 DDIM [84] steps our model is on par with the most recent diffusion [59] and autoregressive [26] methods despite using significantly less parameters. /:Numbers from [109]/ [26] Moreover, LDMs consistently improve upon GAN-based methods in Precision and Recall, thus confirming the advantages of their mode-covering likelihood-based training objective over adversarial approaches. In Fig. 4 we also show qualitative results on each dataset. Figure 8. Layout-to-image synthesis with an LDM on COCO [4], see Sec. 4.3.1. Quantitative evaluation in the supplement D.3. 4.3. Conditional Latent Diffusion 4.3.1 Transformer Encoders for LDMs By introducing cross-attention based conditioning into LDMs we open them up for various conditioning modalities previously unexplored for diffusion models. For textto-image image modeling, we train a 1.45B parameter KL-regularized LDM conditioned on language prompts on LAION-400M [78]. We employ the BERT-tokenizer [14] and implement as a transformer [97] to infer a latent code which is mapped into the UNet via (multi-head) crossattention (Sec. 3.3). This combination of domain specific experts for learning a language representation and visual synthesis results in a powerful model, which generalizes well to complex, user-defined text prompts, cf. Fig. 8 and 5. For quantitative analysis, we follow prior work and evaluate text-to-image generation on the MS-COCO [51] validation set, where our model improves upon powerful AR [17, 66] and GAN-based [109] methods, cf. Tab. 2. We note that applying classifier-free diffusion guidance [32] greatly boosts sample quality, such that the guided LDM-KL-8-G is on par with the recent state-of-the-art AR [26] and diffusion models [59] for text-to-image synthesis, while substantially reducing parameter count. To further analyze the flexibility of the cross-attention based conditioning mechanism we also train models to synthesize images based on semantic layouts on OpenImages [49], and finetune on COCO [4], see Fig. 8. See Sec. D.3 for the quantitative evaluation and implementation details. Lastly, following prior work [3, 15, 21, 23], we evaluate our best-performing class-conditional ImageNet models with f 4, 8 from Sec. 4.1 in Tab. 3, Fig. 4 and Sec. D.4. Here we outperform the state of the art diffusion model ADM [15] while significantly reducing computational requirements and parameter count, cf. Tab 18. Method FID IS Precision Recall Nparams BigGan-deep [3] 6.95 203.62.6 0.87 0.28 - ADM [15] 10.94 100.98 0.69 0.63 ADM-G [15] 4.59 186.7 0.82 0.52 LDM-4 (ours) 10.56 103.491.24 0.71 0.62 LDM-4-G (ours) 3.60 247.675.59 0.87 0.48 250 steps, c.f.g [32], s = 1.5 Table 3. Comparison of a class-conditional ImageNet LDM with recent state-of-the-art methods for class-conditional image generation on ImageNet [12]. A more detailed comparison with additional baselines can be found in D.4, Tab. 10 and F. c.f.g. denotes classifier-free guidance with a scale s as proposed in [32]. purpose image-to-image translation models. We use this to train models for semantic synthesis, super-resolution (Sec. 4.4) and inpainting (Sec. 4.5). For semantic synthesis, we use images of landscapes paired with semantic maps [23, 61]",
    "output": "User generated images from Stable Diffusion Beta\nThe core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai.\nUser generated image from Stable Diffusion Beta"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Experiments / Results\nand concatenate downsampled versions of the semantic maps with the latent image representation of a f = 4 model (VQ-reg., see Tab. 8). We train on an input resolution of 2562 (crops from 3842) but find that our model generalizes to larger resolutions and can generate images up to the megapixel regime when evaluated",
    "output": "User generated images from Stable Diffusion Beta\nThe core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai.\nUser generated image from Stable Diffusion Beta"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Experiments_/_Results_chunk4",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Experiments / Results\nin a convolutional manner (see Fig. 9). We exploit this behavior to also apply the super-resolution models in Sec. 4.4 and the inpainting models in Sec. 4.5 to generate large images between 5122 and 10242. For this application, the signal-to-noise ratio (induced by the scale of the latent space) significantly affects the results. In Sec. D.1 we illustrate this when learning an LDM on (i) the latent space as provided by a f = 4 model (KL-reg., see Tab. 8), and (ii) a rescaled version, scaled by the component-wise standard deviation. The latter, in combination with classifier-free guidance [32], also enables the direct synthesis of > 2562 images for the text-conditional LDM-KL-8-G as in Fig. 13. Figure 9. A LDM trained on 2562 resolution can generalize to larger resolution (here: 5121024) for spatially conditioned tasks such as semantic synthesis of landscape images. See Sec. 4.3.2. 4.4. Super-Resolution with Latent Diffusion LDMs can be efficiently trained for super-resolution by diretly conditioning on low-resolution images via concatenation (cf. Sec. 3.3). In a first experiment, we follow SR3 Method FID  IS  PSNR  SSIM  Nparams [ samples bicubic LDM-SR SR3 Image Regression [72] 15.2 121.1 27.9 0.801 N/A SR3 [72] 5.2 180.1 26.4 0.762 N/A LDM-4 (ours, 100 steps) 2.8/4.8 166.3 24.43.8 0.690.14 4.62 emphLDM-4 (ours, big, 100 steps) 2.4/4.3 174.9 24.74.1 0.710.15 4.5 LDM-4 (ours, 50 steps, guiding) 4.4/6.4 153.7 25.83.7 0.740.12 0.38 Table 5. 4 upscaling results on ImageNet-Val. (2562); : FID features computed on validation split, : FID features computed on train split; : Assessed on a NVIDIA A100 train throughput sampling throughput train+val FID@2k Model (reg.-type) samples/sec. @256 @512 hours/epoch epoch 6 LDM-1 (no first stage) 0.11 0.26 0.07 20.66 24.74 LDM-4 (KL, w/ attn) 0.32 0.97 0.34 7.66 15.21 LDM-4 (VQ, w/ attn) 0.33 0.97 0.34 7.04 14.99 LDM-4 (VQ, w/o attn) 0.35 0.99 0.36 6.66 15.95 Table 6. Assessing inpainting efficiency. : Deviations from Fig. 7 due to varying GPU settings/batch sizes cf. the supplement. Figure 10. ImageNet 64256 super-resolution on ImageNet-Val. LDM-SR has advantages at rendering realistic textures but SR3 can synthesize more coherent fine structures. See appendix for additional samples and cropouts. SR3 results from [72]. 4.5. Inpainting with Latent Diffusion Inpainting is the task of filling masked regions of an image with new content either because parts of the image are are corrupted or to replace existing but undesired content within the image. We evaluate how our general approach for conditional image generation compares to more specialized, state-of-the-art approaches for this task. Our evaluation follows the protocol of LaMa [88], a recent inpainting model that introduces a specialized architecture relying on Fast Fourier Convolutions [8]. The exact training & evaluation protocol on Places [108] is described in Sec. E.2.2. We first analyze the effect of different design choices for the first stage. In particular, we compare the inpainting efficiency of LDM-1 (i.e. a pixel-based conditional DM) with LDM-4, for both KL and VQ regularizations, as well as VQLDM-4 without any attention in the first stage (see Tab. 8), where the latter reduces GPU memory for decoding at high resolutions. For comparability, we fix the number of parameters for all models",
    "output": "User generated images from Stable Diffusion Beta\nThe core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai.\nUser generated image from Stable Diffusion Beta"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Experiments_/_Results_chunk5",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Experiments / Results\n. Tab. 6 reports the training and sampling throughput at resolution 2562 and 5122, the total training time in hours per epoch and the FID score on the validation split after six epochs. Overall, we observe a speed-up of at least 2.7 between pixel- and latent-based diffusion models while improving FID scores by a factor of at least 1.6. The comparison with other inpainting approaches in Tab. 7 shows that our model with attention improves the overall image quality as measured by FID over that of [88]. LPIPS between the unmasked images and our samples is slightly higher than that of [88]. We attribute this to [88] only producing a single result which tends to recover more of an average image compared to the diverse results produced by our LDM cf. Fig. 21. Additionally in a user study (Tab. 4) human subjects favor our results over those of [88]. Based on these initial results, we also trained a larger diffusion model (big in Tab. 7) in the latent space of the VQregularized first stage without attention. Following [15], the UNet of this diffusion model uses attention layers on three levels of its feature hierarchy, the BigGAN [3] residual block for up- and downsampling and has 387M parameters [72] and fix the image degradation to a bicubic interpolation with 4-downsampling and train on ImageNet following SR3’s data processing pipeline. We use the f = 4 autoencoding model pretrained on OpenImages (VQ-reg., cf. Tab. 8) and concatenate the low-resolution conditioning y and the inputs to the UNet, i.e.  is the identity. Our qualitative and quantitative results (see Fig. 10 and Tab. 5) show competitive performance and LDM-SR outperforms SR3 in FID while SR3 has a better IS. A simple image regression model achieves the highest PSNR and SSIM scores; however these metrics do not align well with human perception [106] and favor blurriness over imperfectly aligned high frequency details [72]. Further, we conduct a user study comparing the pixel-baseline with LDM-SR. We follow SR3 [72] where human subjects were shown a low-res image in between two high-res images and asked for preference. The results in Tab. 4 affirm the good performance of LDM-SR. PSNR and SSIM can be pushed by using a post-hoc guiding mechanism [15] and we implement this image-based guider via a perceptual loss, see Sec. D.6. SR on ImageNet Inpainting on Places User Study Pixel-DM (f1) LDM-4 LAMA [88] LDM-4 Task 1: Preference vs GT  16.0% 30.4% 13.6% 21.0% Task 2: Preference Score  29.4% 70.6% 31.9% 68.1% Table 4. Task 1: Subjects were shown ground truth and generated image and asked for preference. Task 2: Subjects had to decide between two generated images. More details in E.3.6 Since the bicubic degradation process does not generalize well to images which do not follow this pre-processing, we also train a generic model, LDM-BSR, by using more diverse degradation. The results are shown in Sec. D.6.1. s ]() input result Figure 11. Qualitative results on object removal with our big, w/ ft inpainting model. For more results, see Fig. 22. instead of 215M. After training, we noticed a discrepancy in the quality of samples produced at resolutions 2562 and 5122, which we hypothesize to be caused by the additional attention modules. However, fine-tuning the model for half an epoch at resolution 5122 allows the model to adjust to the new feature statistics and sets a new state of the art FID on image inpainting (big, w/o attn, w/ ft in Tab. 7, Fig. 11.).",
    "output": "User generated images from Stable Diffusion Beta\nThe core dataset was trained on LAION-Aesthetics, a soon-to-be-released subset of LAION 5B. LAION-Aesthetics was created with a new CLIP-based model that filtered LAION-5B based on how “beautiful” an image was, building on ratings from the alpha testers of Stable Diffusion. LAION-Aesthetics will be released with other subsets in the coming days on laion.ai.\nUser generated image from Stable Diffusion Beta"
  },
  {
    "id_paper": "12",
    "id_story": "12-02",
    "id_chunk": "12-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Investor, Developer, Researcher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 6. Conclusion We have presented latent diffusion models, a simple and efficient way to significantly improve both the training and sampling efficiency of denoising diffusion models without degrading their quality. Based on this and our crossattention conditioning mechanism, our experiments could demonstrate favorable results compared to state-of-the-art methods across a wide range of conditional image synthesis tasks without task-specific architectures.",
    "output": "Stable Diffusion Launch Announcement\nThe model builds upon the work of the team at CompVis and Runway in their widely used latent diffusion model combined with insights from the conditional diffusion models by our lead generative AI developer Katherine Crowson, Dall-E 2 by Open AI, Imagen by Google Brain, and many others. We are delighted that AI media generation is a cooperative field and hope it can continue this way to bring the gift of creativity to all."
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Abstract\nAbstract: In Ghana, people with communication challenges could beneft from gaze-based Augmented and Assistive Communication devices 5 (AACs), widely used in countries with greater resources. However, there is limited evidence about the potential of such devices by 6 7 people with communication disabilities in the Global South. Our study sought to evaluate the usability, identifying barriers and 8 facilitators of adoption of a freely available Android-based eye-gaze AAC application called Look to Speak. The study included training 9 of 10 local speech and language therapists and 15 people with communication difculties. Our fndings highlight how, despite some 10 initial successes and the positive opinions of clients, caregivers and speech and language therapists the Look to Speak application 11 largely failed to deliver substantial communication benefts to individual users. This was due to a combination of factors including the 12 high cognitive load, design faws of the application - such as the lack of optimization of the selection process depending on the chosen 13 interaction mode, and lack of wheelchairs with adequate postural support, which are necessary for users to be able to successfully 14 15 utilise the application. We contribute insights surrounding the mismatch between expectations and reality of gaze-base AACs, and 16 considerations about the broader ecosystem required to support adoption and impact of such technologies in Ghana.",
    "output": "Technology is not enough: Exploring the Infrastructure needed for Gaze-based Mobile Communication Technology Adoption Gifty Ayoka, Giulia Barbareschi, Richard Cave, Catherine Holloway This study investigates the barriers to the effective adoption of gaze-based Augmentative and Alternative Communication (AAC) technology, specifically Google’s free ‘Look to Speak’ app in Ghana. While such technology holds promise for individuals with complex communication difficulties, especially in low resource settings, the research highlights that technology alone is insufficient for successful implementation. The study identifies significant deficits in social, technical, and service infrastructure that impede adoption, potentially outweighing any functional benefits of the technology itself. The research identified critical challenges in the form of a lack of readily available technical support for device maintenance and software updates, coupled with limited access to trained speech and language therapists (SLTs) and caregivers capable of supporting users. The url emphasises the importance of ‘human infrastructure’, including the availability of informed caregivers and community support, and ‘service infrastructure’, encompassing provision, policy, and personnel within assistive technology services. The research proposes a \"technology deficit model\" where inadequacies in human and service infrastructure significantly diminish the potential impact of technological innovations."
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Introduction\nIntroduction: The frst Global Report on Assistive Technology (AT) [44] found that 2.5 billion people need AT today, a fgure likely 29 to rise to 3.5 billion by 2050, yet in some countries, access is as low as 3% for those who need it. Therefore, many 30 people are still without access to the AT they need to carry out activities of daily life, and this is especially true for 31 32 the 28-49% of people globally with communication disabilities [27]. In the last decade, digital devices, and especially 33 smartphones with accessible applications, have become increasingly important as AT for the billion of disabled people 34 worldwide, as they ofer a variety of tools that enable people to address access barriers experienced in daily life 35 [7, 8, 48, 49, 57, 58]. In contrast to often more costly and less available dedicated assistive devices such as Braille readers 36 37 or digital communication boards, low-cost Android smartphones are available in most of the Global South through a 38 variety of diferent providers [46, 67]. However, to date studies looking at the use and impact of smartphone applications 39 to support the needs of people with communication difculties in the Global South are still relatively scarce [6, 15, 39]. 40 41 Impaired speech makes communication challenging and is compounded by stigma, which can isolate or marginalise 42 people [16, 68]. Often, people who have impaired speech will be understood by and communicate with and through 43 trusted individuals, limiting their speech to others due to the risk of ‘failing’ to communicate within a social exchange, 44 45 Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for proft or commercial advantage and that copies bear this notice and the full citation on the frst page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specifc permission and/or a fee. Request permissions from permissions@acm.org. 46 47 48 49 Manuscript submitted to ACM 50 51 Manuscript submitted to ACM 1 52 2 Anon. leading to a feeling of inferiority and reduced participation in the moment and cumulatively over time [13]. These 53 issues can also be more pronounced in Ghana and other countries in the Global South where disability stigma and 54 55 other systemic failures contribute to lack of access to support and subsequent further marginalization [24, 70, 71]. 56 People with conditions such as Cerebral Palsy or Stroke can have very limited ability to articulate sounds or may not 57 be able to form any or only some full words. When this is the case, gaze-based communication devices can provide 58 a vital bridge to supporting communication [40]. When used, gaze-based technologies convert purposeful looking 59 60 behaviours, such as gaze fxations and gaze transfers, to select text or icons on a screen [29, 52, 64], which can then be 61 read aloud by the device. 62 In Ghana and other West African countries, there is a lack of trained Speech and Language Therapy services with 63 trained therapists (SLTs), and poor access to communication-based assistive technologies [6, 26, 39, 71]. However, 64 65 there is also a fast-changing technology infrastructure; providing new fundamental approaches to support people 66 with communication difculties in their day-to-day lives [34]. Whilst gaze-based technologies are largely available in 67 high-resource settings, they are rarely found in low-resource settings [39]. The vast majority of gaze-based technologies 68 are, for many, prohibitively expensive. For example, the Tobii PCEye camera costs approximately £2000 and requires 69 70 additional software and hardware, while ‘all in one’ hardware devices with eye gaze and software can frequently cost 71 £8000 or more 1. People who require these devices are often those from the most marginalised segments of society and, 72 as a result of systemic injustice, including the",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Introduction\nunafordable cost of the AACs they require, the ones with the least met 73 needs [14, 39]. 74 75 For people living with signifcant physical disability and who can utilise purposeful ‘looking’ behaviours [52], 76 eye-gaze control technology is argued to support increased autonomy by providing access to computers and speech- 77 generating communication software [25]. This technology may also enable increased environmental control and 78 facilitate participation",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Introduction\nin a variety of activities, including using social media and listening to music, in addition to 79 80 supporting communication [30]. Eye-Gaze technology normally utilises an infrared camera, which tracks and responds 81 to eye movements in conjunction with specialist software and applications [69]. The technology enables a person to 82 make selections when deliberately fxing their gaze on a target on the screen for a set period of time. Globally, there 83 appears to be limited literature to support the decision for the most suitable candidates for eye gaze, and how it should 84 85 be implemented [29]. 86 In a thematic qualitative analysis of narrative responses from 126 professionals supporting people to use eye- 87 gaze devices in the UK, key recommendations for incorporating eye-gaze control technology included: ensuring 88 optimal seating and positioning, involving a multidisciplinary team, tailoring implementation and content to individual 89 90 needs, engaging in regular practice sessions, supporting communication partners and consideration of motivation and 91 personal preferences [30]. While the emergence of novel gaze-based applications on low-cost Android smartphones 92 can potentially make such technologies more easier to access for people with severe communication disabilities in the 93 Global South, previous studies have pointed to how infrastructural shortcomings and design which fail to account for 94 95 contextual factors can hinder the ability of individuals to beneft from assistive applications on smartphone devices 96 [6, 7, 9, 28, 46, 50]. 97 Our study sought to evaluate the usability of the free smartphone application Look to Speak, a gaze-based AAC 98 application available for the Android operating system for users with severe communication disabilities in Ghana. 99 100 Through training and evaluation sessions with 10 Ghanaian SLTs and 15 clients, we examined what are the barriers and 101 102 103 104 Manuscript submitted to ACM 3 Exploring the Usability of Gaze-based Mobile Communication in Ghana facilitators that infuence the efectiveness and impact of the application, to extrapolate implications for adoption of 105 gaze-based smartphone-based AAC in the Global South. Our results show that while participants had positive opinions 106 107 of Look to Speak, the application often failed to ofer signifcant advantages for many users. These shortcomings were 108 due to a combination of factors including sub-optimal choices in the design of the application, which are partially 109 dictated by technological constraints but also due to inconsistency in interaction modalities, lack of supporting assistive 110 technologies such as adaptive seating which are needed to ofer appropriate postural support, and poor familiarity with 111 112 gaze-based AAC from stakeholders, which made it difcult to evaluate the suitability of the technology compared to 113 potential alternatives. Based on these results we provide recommendation for more context appropriate smartphone- 114 based AAC applications, as well as broader implications around the need to integrate design-level and ecosystem-level 115 considerations to support the adoption of gaze-based smartphone applications in the Global South.",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Method\nMethod: 3.1 Positionality 237 As a collective of authors, we represent a diversity of identities and values and have worked collaboratively together 238 239 across several years. Part of the team is Ghana-based, some UK-based and one member is currently in Japan. All of us 240 have benefted from higher education and work experience in relevant sectors which we recognise as a privilege and an 241 opportunity to develop deep insight. Some of us experience impairments ourselves, including difculty in articulating 242 speech, and some of us have clinical experience, while others of us have a background in global health or technology 243 244 for development. Across the group, we ofer disciplinary experience in Engineering (including computer science) and 245 Inclusion (social, economic, political and disability justice). This research is part of a trajectory of collaborations in 246 Ghana, as part of the [project name annonymised for review], which seeks to understand how to leapfrog barriers to 247 AT access for people living in lower-income settings to enable inclusion. 248 249 Our normative position aligns with other work on feminist HCI, postcolonialism, and disability justice [6, 23, 31, 33, 43], 250 was not to fnd ways to make the existing technology work for the people that need it, or worse attempt to teach 251 disabled participants how to adapt to the technology. Instead, it was to objectively and kindly establish how and if it 252 was useful as a tool to enable communication for those that need it. In keeping with our overarching research intention, 253 254 we sought to understand ‘what works’ and what does not in the context we examined. 255 Like any study such as this, complexities arise and are handled as the work progresses. We sought to keep the 256 question ‘Are we being too optimistic about the usefulness of the technology?’ intentionally held as we navigated the 257 research. The results presented in this paper tell the story of our evolved understanding and this critical refection. As 258 259 260 Manuscript submitted to ACM 6 Anon. we believe that the work presented in this paper is made possible by expertise and contribution as a collective and with 261 no hierarchical order, we present all authors as equal contributors and simply list ourselves in alphabetical order. 262 263 264 3.2 Look to Speak Application 265 . Look to Speak is an Android application developed by Google Creative Lab. Designed to assist individuals with speech 266 267 and motor impairments, the app enables people to communicate by selecting images or pre-written phrases using 268 eye movements alone. Using the smartphone’s front-facing camera, users can navigate through a customisable list of 269 images or phrases by looking left, right, or up, which the app then vocalises aloud. 270 The device needs to be positioned at eye level, ensuring the front camera can clearly detect eye movements. Ideally, 271 272 the phone should be mounted on a stand. The Look to Speak interface displays images/phrases divided into two columns: 273 left and right (see Figure 1, step 1). To select a phrase, look of-screen in the direction (left or right) corresponding to 274 the column containing the desired phrase. This action narrows down the choices (see Figure 1, step 2) 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 Fig. 1. Workflow for a user in selecting an item using Look to Speak application 301 302 303 Once the application detects the selection, all the items on the relative side of the screen are rearranged across the 304 two sides, and the system waits for the user to use their eye movement to make a new selection (see Figure 1, step 3). 305 The process is repeated until the user makes the fnal selection, and then the associated phrase is spoken aloud by the 306 app (see Figure 1, step 4). A confrmation screen is",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Method\nshown when a fnal selection is made (see Figure 1, step 5). Looking 307 308 upwards pauses the app or cancels the current selection, and to resume, glance left and then right. 309 A settings tab enables customization of images or phrases, adjusts gaze sensitivity, and practice using the app. This 310 cannot be accessed using eye movement. 311 312 Manuscript submitted to ACM 7 Exploring the Usability",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Method\nof Gaze-based Mobile Communication in Ghana Look to Speak Includes a basic set of words that can be displayed in writing, as icons, or using pictures loaded by the 313 user. New words, expressions, and icons can be added according to the individual’s needs, and the system simply reads 314 315 aloud in English the words or sentences typed by the user when confguring a particular icon. 316 Operating Look To Speak can feel counter-intuitive in at least two ways: frstly to select an item you have to look 317 away from the item of the phone screen, rather than looking at it. Additionally, an item is not selected immediately, 318 only the available options on the side of the screen that the item sits on according to the procedure explained above. 319 320 This means that if there are multiple objects to choose from (there can be up to 16) then multiple steps of looking left or 321 right of the screen may be required to choose a single item, up to three selections if one is using all 16 available slots. 322 323 324 3.3 Collaboration with local SLTs 325 To ensure that participants would be supported beyond the duration of the project, we used our connections to recruit 326 ten SLTs operating in diferent parts of Ghana and who had experience working with a variety of clients living with 327 328 severe physical and speech limitations due to a variety of conditions, including Cerebral Palsy, Parkinson’s Disease, and 329 Stroke. The SLTs participated in a training session to go through the operation and customisation of Look-To-Speak. 330 At the start of the session, our team provided an overview of the Look to Speak application, illustrating how one can 331 create and add words to the application, arrange existing items, customise settings depending on individual preferences, 332 333 as well as verifying and adjusting the position of the phone using the provided adjustable holds procured by our team 334 to ensure that the camera would be able to track the eyes of the users, and discuss potential challenges that might arise 335 due to light variation especially in outdoor environments. After familiarising themselves with the interface, the SLTs 336 then worked in pairs to confgure Look To Speak, role-playing how they would introduce the tool to a new client. An 337 338 image of the SLTs practising with the application is shown in Figure 2. 339 Our experience is that the operation of Look To Speak can take some time to learn, and the cognitive skill required 340 to learn is sometimes a barrier to practical use. We discussed these challenges in the training sessions and that they 341 should be considered ahead of introducing Look To Speak to a client. Based on our previous clinical experience we 342 343 recommended the SLTs confgure Look To Speak with only two available selections initially and provide scafolded 344 support to help the client get used to how Look To Speak works. Additional selections can be confgured as the client’s 345 confdence in using the tool grows. We also recommended making the selections something that each client might fnd 346 relevant and might choose in daily life, not an abstract option or concept unrelated to their lived experience. 347 348 Additionally, as no Ghanaian language is currently available in Look To Speak, we recommended that the SLTs use 349 the ‘Text Free Mode’, which enables photos, symbols or images to be selected rather than only text (Text Free Mode 350 also allows both images and text be displayed at the same time if preferred). However, once an image is selected, text 351 describing the item will be ‘spoken’ in the default language of the phone setting (English in this case) – there is no way 352 353 of turning the read aloud feature of in the settings, and simply displaying the chosen image to a conversational partner. 354 The only strategy is reducing the volume of the phone to zero. 355 356 357 3.4 Participants 358 The local SLTs recruited participants identifed as having severe motor and",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Method\nspeech limitations who may beneft from 359 eye- movement-based phrase selection and who had the cognitive skills to operate Look To Speak. To qualify for 360 361 participation, individuals had to be over 18 years old, be able to provide informed consent, and be able to communicate 362 in English or a Ghanaian language spoken by at least one of the SLTs. There was no restriction on aetiology. 363 364",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Method\nManuscript submitted to ACM 8 Anon. 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 Fig. 2. Three SLTs practising the use of the Look to Speak application during the training session 396 397 398 Fifteen people participated in the study. Twelve are living with Cerebral Palsy, two with Stroke and one with 399 Parkinson’s Disease. Seven identify as male, and eight as female. The youngest person was 18 years old and the oldest 400 78 years old. Before this research, none of the SLTs or participants were aware of or had used Look To Speak. All 401 402 participants had access to a mobile phone, but several did not own a smartphone with the required specifcations to 403 download and use the Look To Speak application. If participants’ mobile phones did not meet the minimum specifcation 404 (Android version 8), we provided them with a Samsung A05, which they could retain beyond the end of the project. 405 If participants already 10 possessed a smartphone meeting the required characteristics, we provided the equivalent 406 407 compensation of approximately 120 USD. In our exchanges with the ethics committee at «annonymised for review» 408 we identifed the potential risk of coercing participants to join the research out of interest in obtaining a phone or 409 receiving substantial compensation. To mitigate this, we leveraged the connection and existing relationships of trust that 410 participants had with local organisations and SLTs. Participants were explained how joining the study was voluntary, 411 412 and they could withdraw at any time and retain their phone or the compensationprovided as an alternative. Their 413 participation in the study would also not afect any SLT services they were already receiving or would request in the 414 future. However, prolonged engagement with the research team could mean that we could provide support with the 415 416 Manuscript submitted to ACM 9 Exploring the Usability of Gaze-based Mobile Communication in Ghana application if they needed it and help get their voices heard as we planned to lobby for requested changes with the 417 Google Look To Speak team. 418 419 420 3.5 Procedure and Data Collection 421 422 Onboarding sessions with participants were carried out at the ofce of the SLT in the local hospital or clinic, or at 423 participants’ homes, depending on their preference. Onboarding sessions were conducted by the treating local SLT with 424 support from at least three members of the research team. Participants were invited to bring along a family member or a 425 support person if they wished to do so. After obtaining informed consent from participants and setting up their mobile 426 427 phones and Google accounts, we downloaded the Look To Speak application from the Play Store. The local treating SLT 428 then worked with the client and carers to identify items familiar to the client to set up in Look To Speak so that the 429 client could initially train on 2 items, progressing to 4 and more as and when the client demonstrated ease of use. 430 The local SLT led the session, talking in the client’s preferred language, and demonstrated how Look To Speak could 431 432 be operated, selecting on the app images of items that the client is known to choose to have frequently (for example, a 433 drink or a snack). One research team member is a physiotherapist and supported the carers with positioning the app in 434 the most accessible way for the client when assistance was needed. 435 Finally, we conducted semi-structured interviews with participants and their supporters to understand their existing 436 437 communication difculties in daily life, their aspirations for more equitable and accessible communication in various 438 aspects of life, their expectations for the application, the difculties encountered during the onboarding session, and 439 any other feedback about the session or the application. As most participants had extremely little if any residual speech, 440 we relied on supporters and their SLT",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Method\ns to provide details as needed and invited participants to express themselves 441 442 either verbally or non-verbally to their prefer extent. Some participants and supporters also showed us short videos, 443 pictures, meaningful objects and in one cases even their social media profles to illustrate specifc episodes of relevance 444 in relation to their communication modalities in everyday life, their thoughts on the application and aspirations for 445 appropriate communication tools. When external digital material, such as",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Method\npictures or videos, belonging to the participant 446 447 the researchers took notes and detailed descriptions to record the information while preserving the privacy of the 448 participants as much as possible. Onboarding sessions were video recorded with the consent of participants and their 449 supporters consent. 450 451 452 3.6 Data Analysis 453 Our data corpus consists of notes, pictures, videos and transcribed recordings of semi-structured interviews and 454 455 debriefng sessions with SLTs. We analysed data collaboratively using refexive thematic analysis with a bottom-up 456 approach, with members of the research team refecting on codes individually and discussing them with the other 457 members during debriefng discussions after each session with a client and for a total of 4 group meetings that took 458 place after data collection was completed. After conducting the four meetings, data had been organised in 85 codes 459 460 grouped under three diferent themes: Application promises and shortcomings, Ecosystem successes and failures, 461 Knowledge gaps and awareness of workarounds. For example, the theme \"Application promises and shortcomings\" 462 categorized the ways in which the Look to Speak application would support participants communication needs as well 463 as the way in which particular design features, or lack thereof, caused unnecessary usability challenges or breakdowns 464 465 in communication. Examples of codes included in the theme are: \"Lack of language support\", \"Confusing support of 466 multimodal interaction\", and \"Hear my voice\".",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\nExperiments / Results: As none of the clients, supporters or SLTs had previous practical experiences with gaze-based AAC, they were all 471 understandably excited by the prospect or trailing the Look to Speak application. However, results from our trials 472 indicate that while most of the clients were able to operate the application, at least to a point, Look to Speak was 473 474 unlikely to become a functional tool for communication in daily life. This is due to a variety of factors that afected 475 if and how participants could utilize Look to Speak, how well it would ft their contextual circumstances, and what 476 impact it could have on their lives. In the following sections ,we present the insights that emerged from our research, 477 conceptualised across three diferent teams. 478 479 480 4.1 Application promises and shortcomings 481 482 Of the 15 clients who took part in our study, the majority had little or no functional residual speech, with most being 483 able to consistently produce basic vocalizations to communicate with their supporters (P2, P4, P5, P6, P8, P11, P12, 484 and P15), others being able to occasionally utter some simple words in daily communication (P1, P13, P14), two who 485 had limited issues with speech articulation and were primarily afected by challenges with word fnding or incoherent 486 487 speech due to aphasia (P7, P9), and two who had no functional residual speech or vocalization (P3, P10). 488 The Look-to-Speak application was particularly suited for clients with varying conditions and degrees of residual 489 speech, especially those with little or no speaking abilities. After learning about the application and seeing their Speech 490 and Language Therapists (SLTs) demonstrate its use, all clients showed interest in trying it.. However, during the 491 492 introduction of the project with both P7 and P9 it became clear that their verbal abilities likely exceeded the ones that 493 Look to Speak could provide,. Despite this both clients as well as their accompanying supporters expressed an interest 494 in trying out the application to evaluate if it could ofer any advantages in daily life. For some participants and their 495 supporters, the primary motivation was that Look-to-Speak was the frst tool they encountered that could help clients 496 497 communicate with people unfamiliar with them. This can be seen by P2’s mother and P9: 498 \"When P2 moves or makes a sound I know what she wants to tell me because I am her mother, but other people never 499 understand or listen if I don’t explain. I think this thing can help her say things to others when she wants\" - P2’s mother 500 \"Saying what I want is hard sometimes and maybe this program in the phone could help me\" - P9 501 502 Others were more cautious, wanting to explore whether the application would indeed be helpful. 503 A few participants displayed a fast understanding of how to operate the Look to Speak application using their eye 504 movements, with P4 being able to master selections with more than two items on the screen after a remarkably short 505 time. P4 is a 25-year-old power wheelchair user who lives at home with her family. She is a frequent social media user 506 507 and was able to post and exchange messages with friends, using her tongue to type out messages into a smartphone 508 positioned on a tray table in front of her wheelchair, which she was keen on demonstrating to the researcher to show 509 she could interact with the phone in various ways. After trialling Look To Speak, P4 showed how she could use eye 510 movement to call her carer and ask for a drink using the synthesised voice in English. Both P4 and her cousin, who was 511 512 one of her carers, did not feel that Look to Speak would substitute daily communication, but stated that it could be a 513 useful tool to call when specifc needs arise or when she wanted to use repeated common sentences. 514 \"This can",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\nhelp me and P4 when I am in the kitchen as she can tell me what she wants, and it is easy for me to understand 515 because I have understanding of English\" - P4’s cousin 516 517 As mentioned by P4’s cousin, when pronouncing selected words Look to Speak utilized English. More specifcally 518 Look to Speak currently supports 17 languages through the speech to text interface embedded in the Android operating 519 520",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\nManuscript submitted to ACM 11 Exploring the Usability of Gaze-based Mobile Communication in Ghana system. However, none of these are languages commonly spoken in Ghana, with the exception of English. Most of the 521 clients and supporters stated that they would have preferred for the application to speak in their own local Ghanaian 522 523 language (\"I do not know any English, and nobody in the village does, so they could not communicate with my brother 524 if he uses this phone. Only he could understand.\" - P10 sister). To support clients, SLTs experimented with phonemic 525 spellings of words to try to enable Look to Speak to respond using words that were better suited to the client and 526 their communication partners. Although this was judged to be partially successful - the spoken words were often 527 528 understandable even though they were pronounced incorrectly - it would have clearly been better for the languages 529 spoken by Ghanaians to be available in the frst place. 530 Although the communication opportunities that Look to Speak supported often did not meet functional requirements 531 for many situations, the simple experience of being able to select and speak the desired words represented a meaningful 532 533 opportunity to connect for clients and their caregivers. P1 for example is a 19-year-old girl with Cerebral Palsy who has 534 almost no functional speech. She had been abandoned as a child because of her disability, but thanks to the support of 535 the local church, she lives, together with her two adopted brothers P2 and P3, who had a similar background, in a house 536 in Accra where their three adoptive mothers help to care for them. After experimenting with simple \"Yes\" and \"No\" 537 538 options to gain confdence. When asked which words she wanted to add, she pointed at her mother and mimicked a 539 kiss, indicating that she wanted to add the words \"I love you\". Using Look to Speak, she was able to express her love for 540 her parents by saying the words \"Mum, I love you\" - P1, causing her mother to hug her warmly, both smiling with joy at 541 being able to share such a moment. The mother of P2, who was also present, stated: 542 543 \"We know that our children want to say many things, but it is so diferent when they can say what they want\" - P2’s 544 mother. 545 P2, in a similar fashion to P6, P11, and P13 and P15 preferred utilizing the touch screen to make selections with 546 their fngers rather than using their eye movement, as shown in Figure 3. This is actually a possible option, as Look 547 548 to Speak does not deactivate the touch screen so participants can make selections by touching the corresponding 549 side of the screen (left, right, or up to snooze the application), following the exact same procedure they utilized to 550 make gaze-based selections. However, the application does not provide diferent settings depending on the interaction 551 modality, meaning that the camera would still detect their eye movement, causing unwanted selection or snoozing 552 553 which generated frustration (\"I think he is getting tired because the application makes mistakes when he is trying to 554 chose\" - P15’s mother). Although we attempted to cover the smartphone front camera to minimize mistakes, this caused 555 Look to Speak to snooze itself as the application could not detect the eyes of the user. 556 557 4.2 Ecosystem successes and failures 558 559 The creativity displayed by SLTs and clients’ supporters in experimenting with phonemic spelling to overcome the 560 language limitations of the Look to Speak application showed how, as documented in previous research on mobile 561 technology in the Global South [7, 8, 55], the human infrastructure can often step in to bridge the gaps left by 562 563 technological mismatches. Supporters and SLTs also helped users to customise the options, for example adjusting for 564 the often preferred ‘text-free’ option over the ‘text only",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\n’ option for the user interface. SLTs explained how this was 565 sometimes due to the low literacy of their clients, but also because when utilizing the phonemic spelling for the local 566 Ghanaian, especially for words that included characters not used in English such as , seeing the word written would 567 568 only cause confusion. 569 \"Some of the clients don’t read well so looking at the pictures is easier, and if you write the Ghanai",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\nan words they sound 570 correct but this is not how you normally write them\" - SLT of P8, P9, and P12 571 572 Manuscript submitted to ACM 12 Anon. 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 Fig. 3. P11 atempting to use look to speak using his hands as assisted by the SLT and researcher 603 604 605 606 Supporters and SLTs would also play a huge role in helping, participants practice, provide encouragement as they 607 learned to navigate through the selection process, and ofer suggestions which could facilitate their loved ones in 608 selecting words that were meaningful to them (\"I want to try use the world football and Jollof rice as my brother is 609 610 amazing at football and he likes to eat a lot of rice that our mother cooks\" - P12’s younger sister). Supporters would also 611 step in to provide reassurance when the technology failed to meet the needs of a client. This was evident in the case of 612 P7, a 66-year-old man who lives at home with his wife. P7 has been diagnosed with Parkinson’s disease for more than 613 three years and often requires extended time to start speaking or fnd the right words – frequently more than a minute 614 615 or two. His voice is often low volume - a near whisper, making communication with his wife very challenging for 616 both of them (\"Early in the morning he can communicate more when he is refreshed, but as the day goes and he becomes 617 tired sometimes he cannot say things anymore\" - P7’s wife). Both of them hoped that Look to Speak could be useful to 618 speed communication especially when he became tired. However, after testing the app, it was clear that the progression 619 620 of Parkinson’s had afected P7’s eye movements – shifting gaze from one point to another during the selection of 621 phrases was particularly difcult and fatiguing. The SLT and researcher advised exploring other AAC options, such as a 622 touch-based communication device with pre-programmed phrases, or a communication board which could provide 623 624 Manuscript submitted to ACM 13 Exploring the Usability of Gaze-based Mobile Communication in Ghana more reliable support as the disease progressed. P7 was visibly discouraged by the faillure of Look to Speak, but his 625 wife was quick to ofer support \"Don’t worry, this is not your fault, this application does not work but with [name of the 626 627 SLT] we can try other things that are easier for us\" - P7’s wife. 628 Despite the dedication they demonstrated there was one substantial issue which was experienced by P5, P14 and P15 629 which supporters and SLTs could not address during the trials. Look To Speak requires stable positioning so that the 630 phone camera can always see the face of the user to allow them to select items (See Figure 4). This was a particular 631 632 difculty for these clients who had complex postural support needs, did not have appropriate seating or a wheelchair, 633 and experienced muscle spasms which required frequent and recurrent repositioning by carers. One of our team is an 634 experienced Physiotherapist and was frequently able to support with the positioning in the sessions and ofer advice 635 to carers about how to use common cushions to provide some postural support. However, available strategies could 636 637 not address clients needs satisfactory. As an example, P14 who is 19 years old and lives with her mother in temporary 638 housing within a camp originally designated for refugees. As the mother explained (\"She has Cerebral Palsy, so she 639 cannot talk and moving is difcult. But she can understand me when I talk to her in English. She",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\ncan sit, but she gets 640 uncomfortable and her back hurts so she prefers to stay on the foor in the hour because she moves more freely\" ­ 641 642 P14). P14 does not have access to a wheelchair and was seated in a communal child car safety seat to try Look To 643 Speak. Due to frequent involuntary movement, we were unable to consistently position Look To Speak so that she 644 could easily",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk4_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\nselect items. It was clear that P14 understood how to operate Look To Speak wanted to communicate 645 through it and has the cognitive skills to use it, but the need for consistent repositioning, made the application too 646 647 cumbersome for her and her mother. This was further exacerbated by the lack of feedback from Look to Speak in case 648 of misalignment. While the application provides a warning message on the screen (in English only) when the phone 649 camera cannot see a face, it does not ofer help when the positioning can be improved. This lack of feedback makes it 650 extremely challenging for clients and supporters to what the cause of a problem may be when someone looks away 651 652 from the screen to select an item and nothing happens as it could be due to incorrect positioning as much as by the user 653 having difculty performing the correct eye movement. 654 655 4.3 Knowledge gaps and awareness of workarounds 656 657 Although some participants experienced some degrees of success when operating Look to Speak, for most of the referred 658 clients, the initial assessment and trials showed that alternative approaches for AAC or communication support, such 659 as direct access touch based communication boards, would have likely been preferential. Six clients demonstrated very 660 signifcant cognitive communication difculty and could not operate or access Look to Speak or the smartphone at all, 661 662 by any modality. For example, P8 is a 60 years old woman who is living with a stroke that has severely afected her 663 communication and left her with signifcant muscle weakness on her right side. P8 experienced aphasia as a result of 664 the stroke which makes it difcult for her to fnd and articulate words (\"Sometimes she gets very upset as she wants to 665 say something but cannot\" - Carer in the facility where P8 lives). Although the SLT, P8 and the carers at the facility 666 667 hoped that Look to Speak could help her express her desires more clearly and communicate with others, after several 668 attempts, it was clear that the cognitive challenges to use the app were a barrier for her as she struggled to move her 669 gaze to make selections. We discussed with P8 and her team what might be some alternative AAC options, such as 670 simplifed boards with icons, continued use of non-verbal communication, as well as the use of simple signs. 671 672 P8 was not an isolated case, and for the majority of the clients, even those who were willing to continue attempting 673 to use Look to Speak for daily life, we discussed with clients and supporters potential alternatives which could be more 674 functional in everyday life. The high rate of unsuccessful referral from SLT was partially due to the lack of knowledge 675 676 Manuscript submitted to ACM 14 Anon. 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 Fig. 4. P4 operating the Look to Speak application while siting on her wheelchair, showing good alignment between the position of the eyes and the camera 707 708 709 710 concerning the strengths and limits of this type of AAC, which is difcult to acquire without direct experience (\"At the 711 start of the project, even with the explanation and training it was hard to know which clients might beneft from the 712 application. Now that I have seen many assessments I can think better of which client needs the application can meet ­ 713 714 Participating SLT). 715 An additional reason for referral which was discussed with the SLT of P12 was linked to the lack of success or 716 availability of other AAC approaches and ‘trying out’ eye gaze as a possibility. This has been documented in other 717 contexts as well [20], where eye gaze may be seen as something to try when the SLT and clients have run out of possible 718 719 alternatives. Moreover, P12",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk4_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\n’s SLT explained how sometime excessive perseverance could be due to fear of stigmatization 720 from parents or supporter who pushed for the use of technology in the hope their loved ones could overcome their 721 speech difculties. 722 \"The mother is very worried about her son being bullied if he cannot communicate with words, so when we suggest using 723 724 signs, they would still prefer the app even if it is not the best",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Experiments_/_Results_chunk5",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Experiments / Results\nsolution\" - P12 725 Although it is good practice to try out diferent options based on the strengths and needs of the client, technology 726 should only be included where it fts the needs of a user. This may indicate that training may be benefcial - in client 727 728 Manuscript submitted to ACM 15 Exploring the Usability of Gaze-based Mobile Communication in Ghana needs-based AAC and options for communication strategies in addition to AAC - such as conversation partner training. 729 Moreover, although fear of stigmatization and the desire for verbal communication is understandable, pushing a person 730 731 with communication difculties to persevere with a technology that does not address their needs will simply cause 732 unnecessary frustration to the individual, often with individuals blaming themselves for technological failures, and 733 preventing them from exploring more benefcial strategies.",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "38",
    "id_story": "38-02",
    "id_chunk": "38-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Researcher, Teacher, Policy Maker\n\nSection: Conclusion / Future Work\nConclusion / Future Work: In conclusion, our research into the novel free-to-use Look To Speak gaze-based AAC application with people living in 849 850 Ghana with communication challenges demonstrated that while the application ofered some positive experience to a 851 number of clients, it did not sufciently meet the needs of the majority of participants for a functional communication 852 aid. For many the application was too cognitively demanding, and the lack of local language support made it difcult 853 for people to to be understood by others. Participants and their supporters often developed strategies, such as utilizing 854 855 phonemic spelling to produce audio in local language, to bridge certain design shortcomings in the application, but 856 challenges including lack of postural support necessary for correct positioning, could not be adequately addressed. 857 Finally, due to low familiarity with the technology, local SLTs struggled to identify potentially suitable clients, which 858 highlights the need for better training about gaze-based interfaces and other AAC amongst professionals in the Global 859 860 South.",
    "output": "A Cookbook for Community-driven Data Collection of Impaired Speech in Low-Resource Languages Sumaya Ahmed Salihs, Isaac Wiafe, Jamal Abdulai, Gifty Ayoka, Richard Cave, Akon Ekpezu, Catherine Holloway, Katrin Tomanek This study presents CDLI’s community-driven methodology for the creation of an impaired speech corpus in a low-resource language (LRL), specifically Akan, spoken by around 22 million people, or 80% of the population in Ghana. The project adapted an open-source data collection app, incorporating both image and text prompts appropriate for people living with impaired speech. Data collection involved in-person and virtual methods, with speech and language therapist screening of potential participants based on speech severity and cognitive skills. Thirty hours of audio data were collected from people living with cerebral palsy, stammering, and cleft palate. The url discusses the challenges encountered in data collection and transcription of Akan – with its still evolving writing system. The url explores adaptation of the open-source Whisper model by fine-tuning a base Akan model (trained on approximately 100 hours of unimpaired speech in Akan and using the collected impaired speech data. Initial results demonstrate a median relative WER reduction of 21.7% on the impaired speech test set, highlighting the significant performance gap of standard ASR on disordered speech (baseline median WER of 84.6%). The study identifies data quality and transcription inconsistencies as key areas for future improvement. The resulting dataset, cookbook, and open-source tools will be publicly available. How people living with amyotrophic lateral sclerosis use personalized automatic speech recognition technology to support communication Richard Cave Amyotrophic lateral sclerosis (ALS) - also known as Motor Neurone Disease (MND) - is a progressive, ultimately fatal disease causing progressive muscular weakness. Most people living with ALS (plwALS) experience speech change (often referred to as dysarthria), eventually becoming unable to communicate using natural speech. Many wish to use speech for as long as possible. Personalized automated speech recognition (ASR) model technology, such as Google's Project Relate, is argued to better recognize speech with dysarthria, supporting maintenance of understanding through real-time captioning. The objectives of this study are how plwALS and communication partners use Relate in everyday conversation over a period of up to 12 months and how it may change with any decline in speech over time.The study videoed interactions between three plwALS and communication partners. We assessed ASR caption accuracy and how well they preserved meaning. Conversation analysis was used to identify participants' own organizational practices in the accomplishment of interaction. Thematic analysis was used to understand better the participants' experiences of using ASR captions.All plwALS reported lower-than-expected ASR accuracy when used in conversation and felt ASR captioning was useful in certain contexts. All participants liked the concept of live captioning and were hopeful that future improvements to ASR accuracy may support their communication in everyday life. Training is needed on best practices for customization and practical use of ASR technology and for the limitations of ASR in conversational settings. Support is needed for those less confident with technology and to reduce misplaced allocation of ownership of captioning errors, risking negative effects on psychological well-being. url to Full Content Developing African Language Models for Atypical Speech Richard Cave, Catherine Holloway, Gifty Ayoka, Katrin Tomanek, Giulia Barbareschi, Victoria Austin This url outlines the need for and ongoing development of automated speech recognition (ASR) models for people living with impaired speech in African languages and support innovation of apps and tools for functional use in everyday conversation. While English language ASR models exist for interpreting impaired speech, no known work has addressed language models for African languages. The Centre for Digital Language Inclusion (CDLI) was established to address this gap by creating technologies that support individuals with atypical speech in local languages and cultures, starting with ten African languages.The development of ASR for impaired speech in Low Resource Languages (LRLs) faces significant barriers, primarily due to the lack of recorded speech samples. Existing datasets are almost exclusively in American English, with very limited representation of other languages, and even fewer LRLs. English-focused models often exhibit poor accuracy with how English is spoken in Africa. To overcome these challenges, CDLI adopts a community-led, user-centric research practice, involving partnering with local institutions to collect recordings of impaired speech, developing open-source tools for"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: ABSTRACT In this work, we investigate the problem of lip-syncing a talking face video of an arbitrary identity to match a target speech segment. Current works excel at producing accurate lip movements on a static image or videos of specific people seen during the training Both of the authors have contributed equally to this research. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. MM ’20, October 12–16, 2020, Seattle, WA, USA ACM ISBN 978-1-4503-7988-5/20/10...$15.00 Rudrabha Mukhopadhyay radrabha.m@research.iiit.ac.in IIIT, Hyderabad, India C V Jawahar jawahar@iiit.ac.in IIIT, Hyderabad, India phase. However, they fail to accurately morph the lip movements of arbitrary identities in dynamic, unconstrained talking face videos, resulting in significant parts of the video being out-of-sync with the new audio. We identify key reasons pertaining to this and hence resolve them by learning from a powerful lip-sync discriminator. Next, we propose new, rigorous evaluation benchmarks and metrics to accurately measure lip synchronization in unconstrained videos. Extensive quantitative evaluations on our challenging benchmarks show that the lip-sync accuracy of the videos generated by our Wav2Lip model is almost as good as real synced videos. We provide a demo video clearly showing the substantial impact of our Wav2Lip model and evaluation benchmarks on our website: cvit.iiit.ac.in/research/projects/cvit-projects/a-lip-sync-expertis-all-you-need-for-speech-to-lip-generation-in-the-wild. The code and models are released here: github.com/Rudrabha/Wav2Lip. You can also try out the interactive demo at this link: bhaasha.iiit.ac.in/ lipsync. CCS CONCEPTS • Computing methodologies Computer vision; Learning from critiques; Phonology / morphology. KEYWORDS lip sync;video generation;talking face generation ACM Reference Format: K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C V Jawahar. 2020. A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild. In Proceedings of the 28th ACM International Conference on Multimedia (MM ’20), October 12–16, 2020, Seattle, WA, USA. ACM, New 1",
    "output": "1. Introduction\nFigure 2: Original input video\n‍\n5. Conclusion"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: INTRODUCTION With the exponential rise in the consumption of audio-visual content [21], rapid video content creation has become a quintessential need. At the same time, making these videos accessible in different languages is also a key challenge. For instance, a deep learning lecture series, a famous movie, or a public address to the nation, if translated to desired target languages, can become accessible to millions of new viewers. A crucial aspect of translating such talking face videos or creating new ones is correcting the lip sync to match the desired target speech. Consequently, lip-syncing talking face videos to match a given input audio stream has received considerable attention [6, 13, 17, 18, 23] in the research community. Initial works [19, 22] using deep learning in this space learned a mapping from speech representations to lip landmarks using several hours of a single speaker. More recent works [13, 23] in this line directly generate images from speech representations and show exceptional generation quality for specific speakers which they have been trained upon. Numerous practical applications, however, require models that can readily work for generic identities and speech inputs. This has led to the creation of speaker-independent speech to lip generation models [17, 18] that are trained on thousands of identities and voices. They can generate accurate lip motion on a single, static image of any identity in any voice, including that of a synthetic speech generated by a text-to-speech system [18]. However, to be used for applications like translating a lecture/TV series, for example, these models need to be able to morph the broad diversity of lip shapes present in these dynamic, unconstrained videos as well, and not just on static images. Our work builds upon this latter class of speaker-independent works that aspire to lip-sync talking face videos of any identity and voice. We find that these models that work well for static images are unable to accurately morph the large variety of lip shapes in unconstrained video content, leading to significant portions of the generated video being out-of-sync with the new target audio. A viewer can recognize an out-of-sync video segment as small as just 0.050.1 seconds [9] in duration. Thus, convincingly lip-syncing a real-world video to an entirely new speech is quite challenging, given the tiny degree of allowed error. Further, the fact that we are aiming for a speaker-independent approach without any additional speaker-specific data overhead makes our task even more difficult. Real-world videos contain rapid pose, scale, and illumination changes and the generated face result must also seamlessly blend into the original target video. We start by inspecting the existing speaker-independent approaches for speech to lip generation. We find that these models do not adequately penalize wrong lip shapes, either as a result of using only reconstruction losses or weak lip-sync discriminators. We adapt a powerful lip-sync discriminator that can enforce the generator to consistently produce accurate, realistic lip motion. Next, we re-examine the current evaluation protocols and devise new, rigorous evaluation benchmarks derived from three standard test sets. We also propose reliable evaluation metrics using SyncNet [9] to precisely evaluate lip sync in unconstrained videos. We also collect and release ReSyncED, a challenging set of real-world videos that can benchmark how the models will perform in practice. We conduct extensive quantitative and subjective human evaluations and outperform previous methods by a large margin across all benchmarks. Our key contributions/claims are as follows: • We propose a novel lip-synchronization network, Wav2Lip, that is significantly more accurate than previous works for lip-syncing arbitrary talking face videos in the wild with arbitrary speech. • We propose a new evaluation framework, consisting of new benchmarks and metrics, to enable a fair judgment of lip synchronization in unconstrained videos. • We collect and release ReSyncED, a Real-world lip-Sync Evaluation Dataset to benchmark the performance of the lip-sync models on completely unseen videos in the wild. • Wav2Lip is the first speaker-independent model to generate videos with lip",
    "output": "3. Pytorch to OpenVINOTM Model Conversion\n$ python export_openvino.py\n--inference_device: specify the device to run OpenVINOTMinference.\nThe generated video will be saved as results/result_voice.mp4"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\n-sync accuracy that matches the real synced videos. Human evaluations indicate that the generated videos of Wav2Lip are preferred over existing methods and unsynced versions more than 90% of the time. A demo video can be found on our website1 with several qualitative examples that clearly illustrate the impact of our model. We will also release an interactive demo on the website allowing users to try out the model using audio and video samples of their choice.",
    "output": "3. Pytorch to OpenVINOTM Model Conversion\n$ python export_openvino.py\n--inference_device: specify the device to run OpenVINOTMinference.\nThe generated video will be saved as results/result_voice.mp4"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nThe rest of the paper is organized as follows: Section 2 surveys the recent developments in the area of speech to lip generation, Section 3 discusses the issues with the existing works and describes our proposed approach to mitigate them, Section 4 proposes a new, reliable evaluation framework. We describe the various potential applications and address some of the ethical concerns in Section 5 and conclude in Section 6. 2",
    "output": "3. Pytorch to OpenVINOTM Model Conversion\n$ python export_openvino.py\n--inference_device: specify the device to run OpenVINOTMinference.\nThe generated video will be saved as results/result_voice.mp4"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 3 ACCURATE SPEECH-DRIVEN LIP-SYNCING FOR VIDEOS IN THE WILD Our core architecture can be summed up as “Generating accurate lip-sync by learning from a well-trained lip-sync expert\". To understand this design choice, we first identify two key reasons why existing architectures (section 2.2) produce inaccurate lip-sync for videos in the wild. We argue that the loss functions, namely the L1 reconstruction loss used in both the existing works [17, 18] and the discriminator loss in LipGAN [18] are inadequate to penalize inaccurate lip-sync generation. 3.1 Pixel-level Reconstruction loss is a Weak Judge of Lip-sync The face reconstruction loss is computed for the whole image, to ensure correct pose generation, preservation of identity, and even background around the face. The lip region corresponds to less than 4% of the total reconstruction loss (based on the spatial extent), so a lot of surrounding image reconstruction is first optimized before the network starts to perform fine-grained lip shape correction. This is further supported by the fact that the network begins morphing lips only at around half-way (11th epoch) through its training process (20 epochs [18]). Thus, it is crucial to have an additional discriminator to judge lip-sync, as also done in LipGAN [18]. But, how powerful is the discriminator employed in LipGAN? 3.2 A Weak Lip-sync Discriminator We find that the LipGAN’s lip-sync discriminator is only about 56% accurate while detecting off-sync audio-lip pairs on the LRS2 test set. For comparison, the expert discriminator that we will use in this work is 91% accurate on the same test set. We hypothesize two major reasons for this difference. Firstly, LipGAN’s discriminator uses a single frame to check for lip-sync. In Table 3, we show that a small temporal context is very helpful while detecting lip-sync. Secondly, the generated images during training contain a lot of artifacts due to the large scale and pose variations. We argue that training the discriminator in a GAN setup on these noisy generated images, as done in LipGAN, results in the discriminator focusing on the visual artifacts instead of the audio-lip correspondence. This leads to a large drop in off-sync detection accuracy (Table 3). We argue and show that the “real\", accurate concept of lip-sync captured from the actual video frames can be used to accurately discriminate and enforce lip-sync in the generated images. 3.3 A Lip-sync Expert Is All You Need Based on the above two findings, we propose to use a pre-trained expert lip-sync discriminator that is accurate in detecting sync in real videos. Also, it should not be fine-tuned further on the generated frames like it is done in LipGAN. One such network that has been used to correct lip-sync errors for creating large lip-sync datasets [1, 3] is the SyncNet [9] model. We propose to adapt and train a modified version of SyncNet [9] for our task. 3.3.1 Overview of SyncNet. SyncNet [9] inputs a window V of Tv consecutive face frames (lower half only) and a speech segment S of size Ta D, where Tv and Ta are the video and audio timesteps respectively. It is trained to discriminate sync between audio and video by randomly sampling an audio window Ta D that is either aligned with the video (in-sync) or from a different time-step (out-of-sync). It contains a face encoder and an audio encoder, both comprising of a stack of 2D-convolutions. L2 distance is computed between the embeddings generated from these encoders, and the model is trained with",
    "output": "The exported OpenVINOTM model will be saved in the checkpoints folder.\n--face_detection_path: path of face detection OpenVINOTMIR"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\na max-margin loss to minimize (or maximize) the distance between synced (or unsynced) pairs. Figure 2: Our approach generates accurate lip-sync by learning from an “already well-trained lip-sync expert\". Unlike previous works that employ only a reconstruction loss [17] or train a discriminator in a GAN setup [18], we use",
    "output": "The exported OpenVINOTM model will be saved in the checkpoints folder.\n--face_detection_path: path of face detection OpenVINOTMIR"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\na pre-trained discriminator that is already quite accurate at detecting lip-sync errors. We show that fine-tuning it further on the noisy generated faces hampers the discriminator’s ability to measure lip-sync, thus also affecting the generated lip shapes. Additionally, we also employ a visual quality discriminator to improve the visual quality along with the sync accuracy. 3.3.2 Our expert lip-sync discriminator. We make the following changes to SyncNet [9] to train an expert lip-sync discriminator that suits our lip generation task. Firstly, instead of feeding grayscale images concatenated channel-wise as in the original model, we feed color images. Secondly, our model is significantly deeper, with residual skip connections [15]. Thirdly, inspired by this public implementation2, we use a different loss function: cosine-similarity with binary cross-entropy loss. That is, we compute a dot product between the ReLU-activated video and speech embeddings v,s to yield a single value between [0, 1] for each sample that indicates the probability that the input audio-video pair is in sync: Psync = v s max(v2 s2,) (1) We train our expert lip-sync discriminator on the LRS2 train split (29 hours) with a batch size of 64, with Tv = 5 frames using the Adam optimizer [12] with an initial learning rate of 1e3. Our expert lip-sync discriminator is about 91% accurate on the LRS2 test set, while the discriminator used in LipGAN is only 56% accurate on the same test set. 3.4 Generating Accurate Lip-sync by learning from a Lip-sync Expert Now that we have an accurate lip-sync discriminator, we can now use it to penalize the generator (Figure 2) for inaccurate generation during the training time. We start by describing the generator architecture. 3.4.1 Generator Architecture Details. We use a similar generator architecture as used by LipGAN [18]. Our key contribution lies in training this with the expert discriminator. The generator G contains three blocks: (i) Identity Encoder, (ii) Speech Encoder, and a (iii) Face Decoder. The Identity Encoder is a stack of residual convolutional layers that encode a random reference frame R, concatenated with a pose-prior P (target-face with lower-half masked) along the channel axis. The Speech Encoder is also a stack of 2Dconvolutions to encode the input speech segment S which is then concatenated with the face representation. The decoder is also a stack of convolutional layers, along with transpose convolutions for upsampling. The generator is trained to minimize L1 reconstruction loss between the generated frames Lд and ground-truth frames LG: N Lrecon = 1 i=1 ||Lд LG ||1 (2) N Thus, the generator is similar to the previous works, a 2D-CNN encoder-decoder network that generates each frame independently. How do we then employ our pre-trained expert lip-sync discriminator that needs a temporal window of Tv = 5 frames as input? 3.4.2 Penalizing Inaccurate Lip Generation. During training, as the expert discriminator trained in section 3.3 processes Tv = 5 contiguous frames at a time, we would also need the generator G to generate all the Tv = 5 frames. We sample a random contiguous window for the reference frames, to ensure as much temporal consistency of pose, etc. across the Tv window. As our generator processes each frame independently, we stack the time-steps along the batch dimension while feeding the reference frames to get an input shape of (N Tv,H,W , 3), where N, H, W are the batch-size, height, and width respectively. While feeding the generated frames to the expert discriminator, the time-steps are concatenated",
    "output": "The exported OpenVINOTM model will be saved in the checkpoints folder.\n--face_detection_path: path of face detection OpenVINOTMIR"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nalong the channel-dimension as was also done during the training of the discriminator. The resulting input shape to the expert discriminator is (N,H/2,W , 3Tv), where only the lower half of the generated face is used for discrimination. The generator is also trained to minimize the “expert sync-loss\" Esync from the expert discriminator:",
    "output": "The exported OpenVINOTM model will be saved in the checkpoints folder.\n--face_detection_path: path of face detection OpenVINOTMIR"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nN  Esync = 1 i=1 log(Pi sync) (3) N where Pisync is calculated according to Equation 1. Note that the expert discriminator’s weights remain frozen during the training of the generator. This strong discrimination based purely on the lip-sync concept learned from real videos forces the generator to also achieve realistic lip-sync to minimize the lip-sync loss Esync. 3.5 Generating Photo-realistic Faces In our experiments, we observed that using a strong lip-sync discriminator forces the generator to produce accurate lip shapes. However, it sometimes results in the morphed regions to be slightly blurry or contain slight artifacts. To mitigate this minor loss in quality, we train a simple visual quality discriminator in a GAN setup along with the generator. Thus, we have two discriminators, one for sync accuracy and another for better visual quality. The lip-sync discriminator is not trained in a GAN setup for reasons explained in 3.2. On the other hand, since the visual quality discriminator does not perform any checks on lip-sync and only penalizes unrealistic face generations, it is trained on the generated faces. The discriminator D consists of a stack of convolutional blocks. Each block consists of a convolutional layer followed by a Leaky ReLU activation [20]. The discriminator is trained to maximize the objective function Ldisc (Equation 5): Lдen = ExLд[loд(1 D(x)] (4) Ldisc = ExLG [loд(D(x))] + Lдen (5) where Lд corresponds to the images from the generator G, and LG corresponds to the real images. The generator minimizes Equation 6, which is the weighted sum of the reconstruction loss (Equation 2), the synchronization loss (Equation 3) and the adversarial loss Lдen (Equation 4): Ltotal = (1 sw sд)  Lrecon + sw  Esync + sд  Lдen (6) where sw is the synchronization penalty weight, sд is the adversarial loss which are empirically set to 0.03 and 0.07 in all our experiments. Thus, our complete network is optimized for both superior sync-accuracy and quality using two disjoint discriminators. We train our model only on the LRS2 train set [1], with a batch size of 80. We use the Adam optimizer [12] with an initial learning rate of 1e4 and betas 1 = 0.5, 2 = 0.999 for both the generator and visual quality discriminator D. Note that the lip-sync discriminator is not fine-tuned further, so its weights are frozen. We conclude the description of our proposed architecture by explaining how it works during the inference on real videos. Similar to LipGAN [18], the model generates a talking face video frame-by-frame. The visual input at each time-step is the current face crop (from the source frame), concatenated with the same current face crop with lower-half masked to be used as a pose prior. Thus, during inference, the model does not need to change the pose, significantly reducing artifacts. The corresponding audio segment is also given as input to the speech sub-network, and the network generates the input face crop, but with the mouth region morphed. All our code and models will be released publicly. We will now quantitatively evaluate our novel approach against previous models.",
    "output": "The exported OpenVINOTM model will be saved in the checkpoints folder.\n--face_detection_path: path of face detection OpenVINOTMIR"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: 4 QUANTITATIVE EVALUATION Despite training only on the LRS2 train set, we evaluate our model across 3 different datasets. But before doing so, we re-investigate the current evaluation framework followed in prior works and why it is far from being an ideal way to evaluate works in this space. 4.1 Re-thinking the Evaluation Framework for Speech-driven Lip-Syncing in the Wild The current evaluation framework for speaker-independent lipsyncing judges the models differently from how it is used while lip-syncing a real video. Specifically, instead of feeding the current frame as a reference (as described in the previous section), a random frame in the video is chosen as the reference to not leak the correct lip information during evaluation. We strongly argue that the evaluation framework in the previous paragraph is not ideal for evaluating the lip-sync quality and accuracy. Upon a closer examination of the above-mentioned evaluation system, we observed a few key limitations, which we discuss below. 4.1.1 Does not reflect the real-world usage. As discussed before, during generation at test time, the model must not change the pose, as the generated face needs to be seamlessly pasted into the frame. However, the current evaluation framework feeds random reference frames in the input, thus demanding the network to change the pose. Thus, the above system does not evaluate how the model would be used in the real world. 4.1.2 Inconsistent evaluation. As the reference frames are chosen at random, this means the test data is not consistent across different works. This would lead to an unfair comparison and hinder the reproducibility of results. 4.1.3 Does not support checking for temporal consistency. As the reference frames are randomly chosen at each time-step, temporal consistency is already lost as the frames are generated at random poses and scales. The current framework cannot support a new metric or a future method that aims to study the temporal consistency aspect of this problem. 4.1.4 Current metrics are not specific to lip-sync. The existing metrics, such as SSIM [27] and PSNR, were developed to evaluate overall image quality and not fine-grained lip-sync errors. Although LMD [4] focuses on the lip region, we found that lip landmarks can be quite inaccurate on generated faces. Thus, there is a need for a metric that is designed specifically for measuring lip-sync errors. LRW [8] LRS2 [1] LRS3 [3] Method LSE-D LSE-C FID LSE-D LSE-C FID LSE-D LSE-C FID Speech2Vid [17] 13.14 1.762 11.15 14.23 1.587 12.32 13.97 1.681 11.91 LipGAN [18] 10.05 3.350 2.833 10.33 3.199 4.861 10.65 3.193 4.732 Wav2Lip (ours) 6.512 7.490 3.189 6.386 7.789 4.887 6.652 7.887 4.844 Wav2Lip + GAN (ours) 6.774 7.263 2.475 6.469 7.781 4.446 6.986 7.574 4.350 Real Videos 7.012 6.931 — 6.736 7.838 — 6.956 7.592 — Table 1: We propose two new metrics “Lip-Sync Error-Distance\" (lower is better) and “Lip-Sync Error-Confidence\" (higher is better), that can reliably measure the lip-sync accuracy in unconstrained videos. We see that the lip-sync accuracy of the videos generated using Wav2Lip is almost as good as real synced videos. Note that we only train on the train set on LRS2 [1], but we comfortably generalize across all datasets without any further fine-tuning. We also report the FID score (lower is better), which clearly shows that using a visual quality discriminator improves the quality by a significant margin. 4.2 A Novel Benchmark and Metric for Evalu",
    "output": "2. Setup Environment\n4. Run pipeline inference with OpenVINOTM Runtime\n--audio: input audio with voice information\n--static: set True to use single frame for face detection for fast inference"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nating Lip-Sync in the Wild The reason for sampling random frames for evaluation is because, the current frame is already in sync with the speech, leading to leakage of lip-shape in the input itself. And previous works have not tried sampling different speech segments instead of sampling a different frame, as the ground-truth lip shape for the sampled speech is unavailable. 4.2.1 A",
    "output": "2. Setup Environment\n4. Run pipeline inference with OpenVINOTM Runtime\n--audio: input audio with voice information\n--static: set True to use single frame for face detection for fast inference"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nMetric to Measure the Lip-Sync Error. We propose to use the pre-trained SyncNet [9] available publicly3 to measure the lip-sync error between the generated frames and the randomly chosen speech segment. The accuracy of SyncNet averaged over a video clip is over 99% [9]. Thus, we believe this can be a good automatic evaluation method that explicitly tests for accurate lipsync in unconstrained videos in the wild. Note that this is not the expert lip-sync discriminator that we have trained above, but the one released by Chung and Zisserman [9], which was trained on a different, non-public dataset. Using a SyncNet resolves major issues of the existing evaluation framework. We no longer need to sample random, temporally incoherent frames and SyncNet also takes into account short-range temporal consistency while evaluating lip-sync. Thus, we propose two new metrics automatically determined using the SyncNet model. The first is the average error measure calculated in terms of the distance between the lip and audio representations, which we code-name as “LSE-D\" (“Lip Sync Error - Distance\"). A lower LSE-D denotes a higher audio-visual match, i.e., the speech and lip movements are in sync. The second metric is the average confidence score, which we code-name as “LSE-C\" (Lip Sync Error - Confidence). Higher the confidence, the better the audio-video correlation. A lower confidence score denotes that there are several portions of the video with completely out-of-sync lip movements. Further details can be found in the SyncNet paper [9]. 4.2.2 A Consistent Benchmark to Evaluate Lip-sync in the wild. Now that we have an automatic, reliable metric that can be computed for any video and audio pairs, we can sample random speech samples instead of a random frame at each time-step. Thus, we can create a list of pairs of video and a pseudo-randomly chosen audio as a consistent test set. We create three consistent benchmarks test sets, one each using the test set videos of LRS2 [1], LRW [8], and LRS3 [3] respectively. For each video Vs, we take the audio from another randomly-sampled video Vt with the condition that the length of the speech Vt be less than Vs. We create 14K audio-video pairs using LRS2. Using the LRW test set, we create 28K pairs, and this set measures the performance on frontal/near-frontal videos [2]. We also create 14K pairs using the LRS3 test set, which will be a benchmark for lip-syncing in profile views as well. The complete evaluation toolkit will be publicly released for consistent and reliable benchmarking of lip-syncing videos in the wild. 4.3 Comparing the Models on the New Benchmark We compare the previous two approaches [17, 18] on our newly created test set using the LSE-D and LSE-C metrics. During inference, we now feed the same reference and pose-prior at each time-step, similar to how it has been described before in the architecture section. The mean LSE-D and LSE-C scores are shown in Table 1 for the audio-video pairs in all three test splits. Additionally, to measure the quality of the generated faces, we also report the Frchet Inception Distance (FID). Our method outperforms previous approaches by a large margin indicating the significant effect of strong lip-sync discrimination. We can also see the significant improvement in quality after using a visual quality discriminator along with a lip-sync expert discriminator. However, we observe a minor drop in sync accuracy after using the visual quality discriminator. Thus, we will release both of these models, as they have a slight trade-off between visual quality and sync accuracy. 4.4 Real-World Evaluation Apart from evaluating on just the standard datasets, our new evaluation framework and",
    "output": "2. Setup Environment\n4. Run pipeline inference with OpenVINOTM Runtime\n--audio: input audio with voice information\n--static: set True to use single frame for face detection for fast inference"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nmetrics allow us to evaluate on real-world videos on which these models are most likely to be used. Further, given the sensitivity of humans to audio-lip synchronization [9], it is necessary to also evaluate our results with the help of human evaluators. Thus, contrary to the previous works on speakerindependent lip-syncing, we conduct both quantitative and human evaluation experiments on unconstrained real videos from the web for the first time. Thus, we",
    "output": "2. Setup Environment\n4. Run pipeline inference with OpenVINOTM Runtime\n--audio: input audio with voice information\n--static: set True to use single frame for face detection for fast inference"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\ncollect and publicly release “ReSyncED\" a “Real-world Evaluation Dataset\" to subjectively and objectively benchmark the performance of lip-sync works. 4.4.1 Curating ReSyncED. All our videos are downloaded from YouTube. We specifically choose three types of video examples. The first type “Dubbed\", contains videos where the audio is naturally out-of-sync, such as dubbed movie clips or public addresses that are live translated to a different language (so the addresser’s lips Method Video Type LSE-D LSE-C FID Sync Acc. Visual Qual. Overall Experience Preference Unsynced Orig. Videos 12.63 0.896 — 0.21 4.81 3.07 3.15% Speech2Vid [17] 14.76 1.121 19.31 1.14 0.93 0.84 0.00% LipGAN [18] 10.61 2.857 12.87 2.98 3.91 3.45 2.35% Wav2Lip (ours) 6.843 7.265 15.65 4.13 3.87 4.04 34.3% Wav2Lip + GAN (ours) 7.318 6.851 11.84 4.08 4.12 4.13 60.2% Without Lip-syncing Dubbed 17.12 2.014 — 0.15 4.56 2.98 3.24% Speech2Vid [17] 15.22 1.086 19.98 0.87 0.79 0.73 0.00% LipGAN [18] 11.01 3.341 14.60 3.42 3.77 3.57 3.16% Wav2Lip (ours) 6.691 8.220 14.47 4.24 3.68 4.01 29.1% Wav2Lip + GAN (ours) 7.066 8.011 13.12 4.18 4.05 4.15 64.5% Without Lip-syncing Random 16.89 2.557 — 0.11 4.67 3.32 8.32% Speech2Vid [17] 14.39 1.471 17.96 0.76 0.71 0.69 0.00% LipGAN [18] 10.90 3.279 11.91 2.87 3.69 3.14 1.64% Wav2Lip (ours) 6.659 8.126 12.77 3.98 3.87 3.92 41.2% Wav2Lip + GAN (ours) 7.225 7.651 11.15 3.85 4.13 4.05 51.2% Untranslated Videos 7.767 7.047 — 4.83 4.91 — — Table 2: Real world evaluation using our newly collected ReSyncED benchmark. We evaluate using both quantitative metrics and human evaluation scores across three classes of real videos. We can see that in all cases, the Wav2Lip model produces high-quality, accurate lip-syncing videos. Specifically, the metrics indicate that our lip-synced videos are as good as the real synced videos. We also note that human evaluations indicate that there is a scope for improvement when trying to lip-sync TTS generated speech. Finally, it is worth noting that our lip-synced videos are preferred over existing methods or the actual unsynced videos over 90% of the time. TTS are out-of-sync with the translated speech). The second type is “Random\", where we have a collection of videos and we create random audio-visual pairs similar to 4.2.2. The third and final type of videos, “TTS\", has been specifically chosen for benchmarking the lip-syncing performance on synthetic speech obtained from a text-to-speech system. This is essential for future works that aspire to automatically translate videos (Face-to-Face Translation [18]) or rapidly create new video content. We manually transcribe the text, use Google Translate (about 5 languages totally) and publicly available text-to-speech models to generate synthetic translated speech for the videos in this category. The task is to correct lip movements in the original videos to match this synthetic speech. 4.4.2 Real-world Evaluation on ReSyncED. We first evaluate the generated real video results using our new automatic metrics, “LSED\" and “LSE-C\" obtained from SyncNet [9]. For the human evaluation, we ask 14 evaluators",
    "output": "2. Setup Environment\n4. Run pipeline inference with OpenVINOTM Runtime\n--audio: input audio with voice information\n--static: set True to use single frame for face detection for fast inference"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nto judge the different synced versions of the videos based on the following parameters: (a) Sync Accuracy (b) Visual Quality (to evaluate the extent of visual artifacts), (c) Overall Experience (to evaluate the overall experience of the audio-visual content), and (d) Preference, where the viewer chooses the version of the video that is most appealing to watch. The first three parameters are scored between 1",
    "output": "2. Setup Environment\n4. Run pipeline inference with OpenVINOTM Runtime\n--audio: input audio with voice information\n--static: set True to use single frame for face detection for fast inference"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Experiments_/_Results_chunk4",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\n5, and (d) is a single-choice voting, and we report the percentage of votes obtained by a model. We evaluate each of the three classes of videos separately and report our results in Table 2. An outcome worth noting is that the previous works [17, 18] which produce several out-of-sync segments are less preferred over the unsynced version as the latter still preserves good Visual quality. Thus, ours is the first work that provides a significant improvement over unsynced talking face videos in-thewild. We also show some qualitative comparisons in Figure 3 which contains a few generated samples from the ReSyncED test set. 4.5 Is our expert discriminator best among the alternatives? Model Fine-tuned? Off-sync Acc. LSE-D LSE-C Tv = 1 [18]  55.6% 10.33 3.19 Ours, Tv = 1  79.3% 8.583 4.845 Ours, Tv = 3  72.3% 10.14 3.214 Ours, Tv = 3  87.4% 7.230 6.533 Ours, Tv = 5  73.6% 9.953 3.508 Ours, Tv = 5  91.6% 6.386 7.789 Table 3: A larger temporal window allows for better lip-sync discrimination. On the other hand, training the lip-sync discriminator on the generated faces deteriorates its ability to detect off-sync audio-lip pairs. Consequently, training a lipsync generator using such a discriminator leads to poorly lip-synced videos. Our expert discriminator uses Tv = 5 video frames to measure the lip-sync error. It is also not fine-tuned on the generated faces in a GAN setup. We justify these two design choices in this ablation study. We can test the discriminator’s performance by randomly sampling in-sync and off-sync pairs from the LRS2 test set. We vary the size ofTv = 1, 3, 5 to understand its effect on detecting sync. We also fine-tune/freeze each of the three variants of Tv while training the Wav2Lip model. Thus, we get a total of 6 variations in Table 3 from which we can clearly make two observations. Increasing the temporal window size Tv consistently provides a better lip-sync discrimination performance. More importantly, we see that if we fine-tune the discriminator on the generated faces that contain artifacts, then the discriminator loses its ability to detect out-of-sync audio-visual pairs. We argue that this happens because the finetuned discriminator focuses on the visual artifacts in the generated Figure 3: Examples of faces generated from our proposed models (green and yellow outlines). We compare with the current best approach [18] (red outline). The text is shown for illustration to denote the utterance being spoken in the frame shown. We can see that our model produces accurate, natural lip shapes. The addition of a visual quality discriminator also significantly improves the visual quality. We strongly encourage the reader to check out the demo video on our website. faces for discrimination, rather than the fine-grained audio-lip correspondence. Thus, it classifies the real unsynced pairs as “in-sync\", since these real face images do not contain any artifacts. Down the line, using such a weak discriminator leads to poor lip-sync penalization for our generator, resulting in poorly lip-synced talking face videos.",
    "output": "2. Setup Environment\n4. Run pipeline inference with OpenVINOTM Runtime\n--audio: input audio with voice information\n--static: set True to use single frame for face detection for fast inference"
  },
  {
    "id_paper": "21",
    "id_story": "21-01",
    "id_chunk": "21-01_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: CONCLUSION In this work, we proposed a novel approach to generate accurate lipsynced videos in the wild. We have highlighted two major reasons why current approaches are inaccurate while lip-syncing unconstrained talking face videos. Based on this, we argued that a pretrained, accurate lip-sync “expert\" can enforce accurate, natural lip motion generation. Before evaluating our model, we re-examined the current quantitative evaluation framework and highlight several major issues. To resolve them, we proposed several new evaluation benchmarks and metrics, and also a real-world evaluation set. We believe future works can be reliably judged in this new framework. Our Wav2Lip model outperforms the current approaches by a large margin in both quantitative metrics and human evaluations. We also investigated the reasons behind our design choices in the discriminator in an ablation study. We encourage the readers to view the demo video on our website. We believe our efforts and ideas in this problem can lead to new directions such as synthesizing expressions and head-poses along with the accurate lip movements.",
    "output": "Wav2Lip is a novel approach to generate accurate 2D lip-synced videos in the wild with only one video and an audio clip. Wav2Lip leverages an accurate lip-sync “expert\" model and consecutive face frames for accurate, natural lip motion generation.\nIn this blog, we introduce how to enable and optimize Wav2Lippipeline with OpenVINOTM.\nHere is Wav2Lip pipeline overview:\nFigure 1: Wav2Lip pipeline overview\n$ git clone https://github.com/sammysun0711/openvino_aigc_samples.git $ cd Wav2Lip $ conda create -n wav2lip python=3.8 $ conda activate wav2lip $ pip install -r requirments.txt $ sudo apt-get install ffmpeg\nDownload the Wav2lip pytorch model from url and move it to the checkpoints folder.\n$ python inference_ov.py --face_detection_path checkpoints/face_detection.xml --wav2lip_path checkpoints/wav2lip.xml --inference_device CPU --face data_video_sun_5s.mp4 --audio data_audio_sun_5s.wav\nHere are the parameters with descriptions:\n--wav2lip_path: path of wav2lip openvinoTM IR\nHere is an example to compare original video and generated video after the Wav2Lip pipeline:\nFigure 3: Wav2Lip generated video\nIn this blog, we introduce how to deploy wav2lip pipeline with OpenVINOTM as follows:"
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Abstract\nAbstract: Abstract. We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis3. Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-tospeech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples, including comparisons to state-of-the-art techniques and a user study. https://justusthies.github.io/posts/neural-voice-puppetry/ 2 J. Thies et al.",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Introduction_chunk1_sub1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Introduction\nIntroduction: Introduction In the recent years, speech-based interaction with computers made significant progress. Digital voice assistants are now ubiquitous due to their integration into many commodity devices such as smartphone, tvs, cars, etc.; even companies use more and more machine learning techniques to drive service bots that interact with their customers. These virtual agents aim for a user-friendly man-machine interface while keeping maintenance costs low. However, a significant challenge is to appeal to humans by delivering information through a medium that is most comfortable to them. While speech-based interaction is already very successful, such as shown in virtual assistants like Siri, Alexa, Google, etc., the visual counterpart is largely missing. This comes to no surprise given that a user would also like to associate the visuals of a face with the generated audio, similar to the ideas behind video conferencing. In fact, the level of engagement for audio-visual interactions is higher than for purely audio ones [10,28]. The aim of this work is to provide the missing visual channel by introducing Neural Voice Puppetry, a photo-realistic facial animation method that can be used in the scenario of a visual digital assistant. To this end, we build on the recent advances in text-to-speech synthesis literature [16,24], which is able to provide a synthetic audio stream from a text that can be generated by a digital agent. As visual basis, we leverage a short target video of a real person. The key component of our method is to estimate lip motions that fit the input audio and to render the appearance of the target person in a convincing way. This mapping from audio to visual output is trained using the ground truth information that we can gather from a target video (aligned real audio and image data). We designed Neural Voice Puppetry to be an easy to use audio-to-video translation tool which does not require vast amount of video footage of a single target video or any manual user input. In our experiments, the target videos are comparably short (2-3 min), thus, allowing us to work on a large amount of video footage that can be downloaded from the Internet. To enable this easy applicability to new videos, we generalize specific parts of our pipeline. Specifically, we compute a latent expression space that is generalized among multiple persons (in our experiments 116). This also ensures the capability of being able to handle different audio inputs. Besides the generation of a visual appearance of a digital agent, our method can also be used as audio-based facial reenactment. Facial reenactment is the process of re-animating a target video in a photo-realistic manner with the expressions of a source actor [32,38]. It enables a variety of applications, ranging from consumer-level teleconferencing through photo-realistic virtual avatars [23,30,31] to movie production applications such as video dubbing [12,19]. Recently, several authors started to exploit the audio signal for facial reenactment [5,27,35]. This has the potential of avoiding failures of visualbased approaches, when the visual signal is not reliable, e.g., due to occluded face, noise, distorted views and so on. Many of these approaches, however, lack video-realism [5,35], since they work in a normalized space of facial imagery (cropped, frontal faces), to be agnostic to head movements. An exception is the work of Suwajanakorn et al. [27], where they have shown photo-realistic videos of President Obama that can be synthesized just from the audio signal. This approach, however, requires very large quantities of data for training (17 hours of President Obama weekly speeches) and, thus, limits its application and generalization to other identities. In contrast, our method only needs 2-3 min of a target video to learn the person-specific talking style and appearance. Our underlying latent 3D model space inherently learns 3D consistency and temporal stability that allows us to generate natural, full frame imagery. Especially, it enables the disentanglement of rigid head motions from facial expressions",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Introduction_chunk1_sub2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Introduction\n. To enable photo-realistic renderings of digital assistants as well as audio-driven facial reenactment, we have the following contributions: – A temporal network architecture called Audio2ExpressionNet is proposed to map an audio stream to a 3D blendshape basis that can represent personspecific talking styles. Exploiting features from a pre-trained speech-to-text network, we generalize the Audio2ExpressionNet on a",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Introduction_chunk2",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Introduction\ndataset of news-speaker. – Based on a short target video sequence (2-3 min), we extract a representation of person-specific talking styles, since our goal is to preserve the talking style of the target video during reenactment. – A novel light-weight neural rendering network using neural textures is presented that allows us to generate photo-realistic video content reproducing the person-specific appearance. It surpasses the quality and speed of stateof-the-art neural rendering methods [11,33]. 2",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Method_chunk1_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Method\nMethod: 3 Overview Neural Voice Puppetry consists of two main components (see Fig. 2): a generalized and a specialized part. A generalized network predicts a latent expression vector, thus, spanning an audio-expression space. This audio-expression space is shared among all persons and allows for reenactment, i.e., transferring the predicted motions from one person to another. To ensure generalizability w.r.t. the input audio, we use features extracted by a pretrained speech-to-text network [14] as input to estimate the audio-expressions. The audio-expressions are interpreted as blendshape coefficients of a 3D face model rig. This face model rig is person-specific and is optimized in the second part of our pipeline. This specialized stage captures the idiosyncrasies of a target person including the facial motion and appearance. It is trained on a short video sequence of 2 3 minutes (in comparison to hours that are required by state-of-the-art methods). The 3D facial motions are represented as delta-blendshapes which we constrain to be in the subspace of a generic face template [1,32]. A neural texture in conjunction with a novel neural rendering network is used to store and to rerender the appearance of the face of an individual person. Neural Voice Puppetry 5 6 J. Thies et al. Fig. 3: Samples of the training corpus used to optimize the Audio2ExpressionNet. 4 Data In contrast to previous model-based methods, Neural Voice Puppetry is based on ’in-the-wild’ videos that can be download from the internet. The videos have to be synced with the audio stream, such that we can extract ground truth pairs of audio features and image content. In our experiments the videos have a resolution of 512 512 with 25fps. Training Corpus for the Audio2ExpressionNet: Fig. 3 shows an overview of our video training corpus that is used for the training of the small network that predicts the ’audio expressions’ from the input audio features (see Sec. 5.1). The dataset consists of 116 videos with an average length of 1.7min (in total 302750 frames). We selected the training corpus, such that the persons are in a neutral mood (commentators of the German public TV). Target Sequences: For a target sequence, we extract the person-specific talking style in the sequence. I.e., we compute a mapping from the generalized audioexpression space to the actual facial movements of the target actor (see Sec. 5.3). The sequences are 2 3min long and, thus, easy to obtain from the Internet. 4.1 Preprocessing: In an automatic preprocessing step, we extract face tracking information as well as audio features needed for training. Our method is using a statistical face model and deltablendshapes [1,32] to represent a 3D latent space for modelling facial animation. The 3D face model space reduces the face space to only a few hundred parameters (100 for shape, 100 for albedo and 76 for expressions) and stays fixed in this work. Using the dense face tracking method of Thies et al. [32], we estimate the model parameters for every frame of a sequence. During tracking, we extract the per-frame expression parameters that are used to train the audio to expression network. To train our neural renderer, we also store the rasterized texture coordinates of the reconstructed face mesh. Audio-feature Extraction: The video contains a synced audio stream. We use the recurrent feature extractor of the pre-trained speech-to-text model DeepSpeech [14] (v0.1.0). Similar to Voca [7], we extract a window of character logits per video frame. Each window consists of 16 time intervals a 20ms, resulting in an audio feature of 16 29. The DeepSpeech model is generalized among thousands of different voices, trained on Mozilla’s CommonVoice dataset",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Method_chunk1_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Method\n. 5. Method To enable photo-realistic facial reenactment based on audio signals, we employ a of our pipeline is the audio-based expression estimation. Since every person has his own talking style and, thus, different expressions, we establish person-specific expression spaces that can be computed for every target sequence. To ensure generalization among multiple persons, we created a latent audio-expression space that is shared by",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Method_chunk2_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Method\nall persons. From this audio-expression space, one can map to the person specific expression space, enabling reenactment. Given the estimated expression and the extracted audio features, we apply a novel light-weight neural rendering technique that generates the final output image. 5.1 Audio2ExpressionNet Our method is designed to generate temporally smooth predictions of facial motions. To this end, we employ a deep neural network with two stages. First, we predict per-frame facial expression predictions. These expressions are potentially noisy, thus, we use an expression aware temporal filtering network. Given the noisy per-frame predictions as input the neural network predicts filter weights to compute smooth audio-expressions for a single frame. The per-frame as well as the filtering network can be trained jointly and outputs audio-expression coefficients. This audio-expression space is shared among multiple persons and is interpreted as blendshape coefficients. Per person, we compute a blendshape basis which is in the subspace of our generic face model [32]. The networks are trained with a loss that works on a vertex level of this face model. Per-frame Audio-Expression Estimation Network: Since our goal is a generalized audio-based expression estimation, we rely on generalized audio features. We use the RNN-part of the speech to text approach DeepSpeech [14] to extract these features. These features represent the logits of the DeepSpeech alphabet for 20ms audio signal. For each video frame, we extract a time window of 16 features around the frame that consist of 29 logits (length of the DeepSpeech alphabet is 29). This, 1629 tensor is input to our per-frame estimation network (see Fig. 4). To map from this feature space to the per-frame audio-expression space, we apply 4 convolutional layer and 3 fully connected layer. Specifically, we apply 1D convolutions with kernel dimensions (3) and stride (2), filtering in the time dimension. The convolutional layers have a bias and are followed by a leaky ReLU (slope 0.02). The feature dimensions are reduced successively from (16 29),(8 32),(4 32),(2 64) to (1 64). This reduced feature is input to the fully connected layers that have a bias and are also followed by a leaky ReLU (0.02), except the last layer. The fully connected layers map the 64 features from the convolutional network to 128, then to 64 and, finally, to the audio-expression space of dimension 32, where a TanH activation is applied. Fig. 4: Audio2ExpressionNet: (a) Per-frame audio-expression estimation network that gets DeepSpeech features as input, (b) to get smooth audio-expressions, we employ a content-aware filtering along the time dimension. Temporally Stable Audio-Expression Estimation: To generate temporally stable audio-expression predictions, we jointly learn a filtering network that gets T per-frame estimates as input (see Fig. 4 (b)). Specifically, we estimate the audio-expressions for frame t using a linear combination of the per-frame predictions of the timesteps tT/2 to t+T/2. The weights for the linear combination are computed using a neural network that gets the audio-expressions as input (which results in an expression-aware filtering). The filter weight prediction network consists of five 1D convolutions followed by a linear layer with softmax activation (see supplemental material for detailed description). This content aware temporal filtering is also inspired by the self-attention mechanism [37]. Person-specific Expressions: To retrieve the 3D model from this audioexpression space, we learn a person-specific audio-expression blendshape basis which we constrain by the generic blendshape basis of our statistical face model. I.e., the audio-expression blendshapes of a person are a linear combination of the generic blendshapes. This linear relation, results in a linear mapping from the audio-expression space which is output of the generalized network to the generic blends",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Method_chunk2_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Method\nhape basis. This linear mapping is person specific, resulting in N matrices with dimension 7632 during training (N being the number of training sequences and 76 being the number of generic blendshapes). Loss: The network and the mapping matrices are learned end-to-end using the visually tracked training corpus and a vertex-based loss function, with",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Method\na higher weight (10x) on the mouth region of the face model. Specifically, we compute a vertex-to-vertex distance from the audio-based predicted and the visually tracked face model in terms of a root mean squared (RMS) distance: Lexpr = RMS(vt v t ) +   Ltemp with vt, the vertices based on the filtered expression estimation of frame t and v t being the visual tracked face vertices. In addition to the absolute loss between predictions and the visual tracked face geometry, we use a temporal loss that considers the vertex displacements of consecutive frames: Ltemp = RMS((vt vt1) (v t v t1)) + RMS((vt+1 vt) (v t+1 v t )) + RMS((vt+1 vt1) (v t+1 v t1)) These forward, backward and central differences are weighted with  (in our experiments  = 20). The losses are measured in millimeters. 5.2 Neural Face Rendering Based on the recent advances in neural rendering, we employ a novel lightweight neural rendering technique that is based on neural textures to store the appearance of a face. Our rendering pipeline synthesizes the lower face in the target video based on the audio-driven expression estimations. Specifically, we use two networks (see supplemental material for an overview figure). One network that focuses on the face interior, and another network that embeds this rendering into the original image. The estimated 3D face model is rendered using the rigid pose observed from the original target image using a neural texture [33]. The neural texture has a resolution of 256  256  16. The network for the face interior translates these rendered feature descriptors to RGB colors. The network is using a similar structure as a classical U-Net with 5 layers. But instead of using strided convolutions that result in a downsampling in each layer, we are using dilated convolutions with increasing dilation factor and a stride of one. Instead of transposed convolutions we are using standard convolutions. All convolutions have kernel size 3  3. Note, dilated instead of strided convolutions do not increase the number of learnable parameters, but it increases the memory load during training and testing. Dilated convolutions reduce visual artifacts and result in smoother results (also temporally, see video). The second network that blends the face interior with the ’background image’ has the same structure. To remove potential movements of the chin in the background image, we erode the background image around the rendered face. The second network inpaints these missing regions. Loss: We use a per-frame loss function that is based on an l1 loss to measure absolute errors and a VGG style loss [17]. Lrendering = l1(I, I) + l1(I, I) + V GG(I, I) with I being the final synthetic image, Ithe ground truth image and I the intermediate result of the first network that focuses on the face interior (loss is masked to this region). 5.3 Training Our training procedure has two stages – the generalization and the specialization phase. In the first phase, we train the Audio2ExpressionNet among all sequences Neural Voice Puppetry 9 10 J. Thies et al. from our dataset (see Sec. 4) in a supervised fashion. Given the visual face tracking information, we know the 3D face model of a specific person for every frame. In the training process, we reproduce these 3D reconstructions based on the audio input by optimizing the network parameters and the person-specific mappings from the audio-expression space to the 3D space. In the second phase, the rendering network for a specific target sequence is trained. Given the ground truth images and the visual tracking information, we train the neural renderer end-to-end including the neural texture. New Target Video: Since the audio-based expression estimation network is generalized among multiple persons, we can apply it to unseen actors. The person specific mapping between the predicted audio-expression space coefficients and the expression space of the new person can be obtained by solving a linear system of equations. Specifically, we extract the audio-",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Method\nexpression for all training images and compute the linear mapping to the expressions that are visually estimated. In addition to this step, the person-specific rendering network for the new target video is trained from scratch (see supplement for further information). 5.4 Inference At test time, we only require a source audio sequence. Based on the target actor selection, we use the corresponding person-specific mapping. The mapping from the audio features to the person specific expression space takes less than 2ms on an Nvidia 1080Ti. Generation of the 3D model and the rasterization using these predictions takes another 2ms. The deferred neural rendering takes 5ms which results in a real-time capable pipeline. Text-to-Video: Our pipeline is trained on real video sequences, where the audio is in sync with the visual content. Thus, we learned a mapping directly from audio to video that ensures synchronicity. Instead of going directly from text to video, where such a natural training corpus is not available, we synthesize voice from the text and feed this into our pipeline. For our experiments we used samples from the DNN-based text-to-speech demo of IBM Watson4. Which gives us state-of-the-art synthetic audio streams that are comparable to the synthetic voices of virtual assistants.",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Experiments / Results\nExperiments / Results: Results Neural Voice Puppetry has several important use cases, i.e., audio-driven video avatars, video dubbing and text-driven video synthesis of a talking head, see supplemental video. In the following sections, we discuss these results including comparisons to state-of-the-art approaches. 4 https://text-to-speech-demo.ng.bluemix.net/ Fig. 5: Self-reenactment: Evaluation of our rendering network and the audioprediction network. Error plot shows the euclidean photo-metric error. 6.1 Ablation studies Self-reenactment: We use self-reenactment to evaluate our pipeline (Fig. 5), since it gives us access to a ground truth video sequence where we can also retrieve visual face tracking. As a distance measurement, we use an l2 distance in color space (colors in [0,1]) and the corresponding PSNR values. Using this measure, we evaluate the rendering network and the entire reenactment pipeline. Specifically, we compare the results using visual tracked mouth movements to the results using audio-based predictions (see video). The mean color difference of the re-rendering on the test sequence of 645 frame is 0.003 for the visual and 0.005 for the audio-based expressions, which corresponds to a PSNR of 41.48 and 36.65 respectively. In addition to the photo-metric measurements, we computed the in 0.022 for visual tracking and 0.055 for the audio-based predictions. In the supplemental video, we also show a side-by-side comparison of our rendering network using dilated convolutions and our network with strided convolutions (and a kernel size of 4 to reduce block artifacts in the upsampling). Both networks are trained with the same number of epochs (50). As can be seen, dilated convolutions lead to visually more pleasing results (smoother in spatial and temporal domain). As a comparison to the results using dilated convolutions reported above, strided convolutions result in a lower PSNR of 40.12 with visual tracking and 36.32 with audio-based predictions. Temporal smoothness: We also evaluated the benefits of using a temporalbased expression prediction network. Besides temporally smooth predictions shown in the supplemental video, it also improves the prediction accuracy of the mouth shape. The relative 2D mouth landmark error improves from 0.058 (per frame prediction) to 0.055 (temporal prediction). Generalization / Transferability: Our results are covering different target persons which demonstrates the wide applicability of our method, including the reproduction of different person-specific talking styles and appearances. As can be seen in the supplemental video, the expression estimation network that is trained on multiple target sequences (302750 frames) results in more coherent predictions than the network solely trained on a sequence of Obama (3145 frames). The usage of more target videos increases the training corpus size and the variety of input voices and, thus, leads to more robustness. In the video, we also show a comparison of the transfer from different source languages to different target videos that are originally also in different languages. In Tab. 1, we show the corresponding quantitative measurements of the achieved lip sync using SyncNet [4]. SyncNet is trained on the BBC news program (English), nevertheless, the authors state that it works also good for other languages. As a reference for the measurements of the different videos, we list the values for the original target videos. Higher confidence values are better, while a value below 1 refers to uncorrelated audio video streams. The original video of Macron has the lowest measured confidence which propagates to the reenactment results.Table 1: Analysis of generated videos with different source/target languages. Based on SyncNet [4], we measure the audio-visual sync (offset/confidence). As a reference, we list the sync measurements for the original target video (right). 6.2 Comparisons to state-of-the-art methods In the following, as well as in the",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Experiments / Results\nsupplemental document, we compare to modelbased and pure image-based approaches for audio-driven facial reenactment. Preliminary User Study: In a preliminary user study, we evaluated the visual quality and audio-visual sync of the state-of-the-art methods. The user study is based on videos taken from the supplemental materials of the respective publications (assuming the authors showing the best case scenario). Note that the videos of the different methods show (pot",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Experiments_/_Results_chunk2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Experiments / Results\nentially) different persons (see supplemental material). In total, 56 attendees with a computer science background judged upon synchronicity and visual quality (’very bad’, ’bad’, ’neither bad nor good’, ’good’, ’very good’) of 24 videos in randomized order (in total). In Fig. 6, we show the percentage of attendees that rated the specific approach with good or very good. As can be seen, the 2D image-based approaches achieve a high audio-visual sync (especially, Vougioukas [36]), but they lack visual quality and are not able to synthesize natural videos (outside of the normalized space). Our approach gives the best visual quality and also a high audio-visual sync, similar to state-of-the-art video-based reenactment approaches like Thies et al. [33]. Image-based Methods: Our method aims for high quality output that is embedded in a real video, including the person-specific talking style, exploiting an explicit 3D model representation of the face to ensure 3D consistent movements. This is fundamentally different from image-based approaches that are operating in a normalized space of facial imagery (cropped, frontal faces) and do not capture person-specific talking styles, but, therefore, can be applied to single input Fig. 6: User study: percentage of attendees (in total 56) that rated the visual and audio-visual quality good or very good. [Chung et al. 2017] Ours Reference (2/6.219) (2/4.538) (1/5.811) (a) Fig. 7: Visual quality comparison to the image-based methods (a) ’You said that?’ [5] and (b) ’Realistic Speech-Driven Facial Animation with GANs’ [36] (driven by the same input audio stream, respectively), including the synchronicity measurements using SyncNet [4] (offset/confidence). images. In Fig. 7 as well as in the video, we show generated images of stateof-the-art image-based methods [5,36]. It illustrates the inherent visual quality differences that has also been quantified in our user study (see Fig. 6). The figure also includes the quantitative synchronicity measurements using SyncNet. Especially, Vougioukas et al. [36] achieves a high confidence score, while our method is in the range of the target video it has been trained on (compare to Fig. 6). Model-based Audio-Driven Methods: In Fig. 8 we show a representative image from a comparison to Taylor et al. [29], Karras et al. [18] and Suwajanakorn et al. [27]. Only the method of Suwajanakorn et al. is able to produce photo-realistic output. The method is fitted to the scenario where a large video dataset of the target person is available and, thus, limited in its applicability. They demonstrate it on sequences of Obama, using 14 hours of training data and 3 hours for validation. In contrast, our method works on short 2 3 min target video clips. Measuring the audio visual sync with SyncNet [4], the generated Obama videos in Fig. 8 (top row) result in 2 / 5.9 (offset/confidence) for the person-specific approach of Suwajanakorn et al., and 0 / 5.2 for our generalized expression prediction network. Neural Voice Puppetry 13 [Vougioukas et al. 2019] Ours Reference (2/7.607) (3/5.883) (6/4.859) (b) 14 J. Thies et al. Fig. 8: Comparison to state-of-the-art audio-driven model-based video avatars using the same input audio stream. Our approach is applicable to multiple targets, especially, where only 2 3min of training data are available.",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "18",
    "id_story": "18-02",
    "id_chunk": "18-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Student, Developer\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion We presented a novel audio-driven facial reenactment approach that is generalized among different audio sources. This allows us not only to synthesize videos of a talking head from an audio sequence from another person, but also to generate a photo-realistic video based on a synthesized voice. I.e., text-driven video synthesis can be achieved that is in sync with artificial voice. We hope that our work is a stepping stone in the direction to photo-realistic audio-visual assistants.",
    "output": "Called Neural Voice Puppetry, artificial intelligence can now make use of audio-driven facial video synthesis, or in other words, create audio deepfakes. When fed an audio sequence of a source person or digital assistant, this system generates a photo-realistic output video of a target person that is in sync with the audio of the source input. This is made possible with a deep neural network that employs a latent 3D face model space.\n\n\n\n\n\nThis approach can be generalized across different people, allowing one to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. In the real world, it can be used for video avatars, video dubbing, and text-driven video synthesis of a talking head."
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Abstract\nAbstract: Synthesizing Obama: Learning Lip Sync from Audio SUPASORN SUWAJANAKORN, STEVEN M. SEITZ, and IRA KEMELMACHER-SHLIZERMAN, University of Washington Fig. 1. Given input Obama audio and a reference video, we synthesize photorealistic, lip-synced video of Obama speaking those words. Given audio of President Barack Obama, we synthesize a high quality video of him speaking with accurate lip sync, composited into a target video clip. Trained on many hours of his weekly address footage, a recurrent neural network learns the mapping from raw audio features to mouth shapes. Given the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results. CCS Concepts: •Computing methodologies Image-based rendering; Image manipulation; Animation; Shape modeling; Additional Key Words and Phrases: Audio, Face Synthesis, LSTM, RNN, Big data, Videos, Audiovisual Speech, Uncanny Valley, Lip Sync ACM Reference format: Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman. 2017. Synthesizing Obama: Learning Lip Sync from Audio. ACM Trans. Graph. 36, 4, Article 95 (July 2017), 13 pages. 1",
    "output": "In the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video."
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Introduction\nIntroduction: INTRODUCTION How much can you infer about someone’s persona from their video footage? Imagine learning how to replicate the sound and cadence of a person’s voice, how they speak, what they say, how they converse and interact, and how they appear and express themselves. With tools like Skype, FaceTime, and other video conferencing solutions, we are increasingly capturing video footage of ourselves. In the case of public figures, there is significant video footage available online, in the form of interviews, speeches, newscasts, etc. Analyzing this video is quite challenging, however, as the faces are ofen shown in a near-profile view, the face region is small, and the lighting, dress, hair, and make-up varies significantly from one interview to the next (also, most of this video is proprietary). Tis is the author’s version of the work. It is posted here for your personal use. Not for redistribution. Te definitive Version of Record was published in ACM Transactions on Output Obama Video In this paper, we do a case study on President Barack Obama, and focus on the specific task of learning to generate video of Obama from his voice and stock footage. Barack Obama is ideally suited as an initial test subject for a number of reasons. First, there exists an abundance of video footage from his weekly presidential addresses— 17 hours, and nearly two million frames, spanning a period of eight years. Importantly, the video is online and public domain, and hence well suited for academic research and publication. Furthermore, the quality is high (HD), with the face region occupying a relatively large part of the frame. And, while lighting and composition varies a bit from week to week, and his head pose changes significantly, the shots are relatively controlled with the subject in the center and facing the camera. Finally, Obama’s persona in this footage is consistent—it is the President addressing the nation directly, and adopting a serious and direct tone. Despite the availability of such promising data, the problem of generating mouth video from audio is extremely difficult, due in part to the technical challenge of mapping from a one-dimensional signal to a (3D) time-varying image, but also due to the fact that humans are extremely atuned to subtle details in the mouth region; many previous atempts at simulating talking heads have produced results that look uncanny. In addition to generating realistic results, this paper represents the first atempt to solve the audio speech to video speech problem by analyzing a large corpus of existing video data of a single person. As such, it opens to the door to modeling other public figures, or ourselves (through analyzing Skype footage, e.g.,). Audio to video, aside from being interesting purely from a scientific standpoint, has a range of important practical applications. Te ability to generate high quality video from audio could significantly reduce the amount of bandwidth needed in video coding/transmission (which makes up a large percentage of current internet bandwidth). For hearing-impaired people, video synthesis could enable lip-reading from over-the-phone audio. And digital humans are central to entertainment applications like film special effects and games. Input audio",
    "output": "The findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Method\nMethod: 1. Recurrent Neural Network Sparse shape ... ... 2. Mouth Synthesis Mouth texture ... ... 3. Re-timing Target video ... ... 4. Final Composite Final output ... ... Fig. 2. Our system first converts audio input to a time-varying sparse mouth shape. Based on this mouth shape, we generate photo-realistic mouth texture, that is composited into the mouth region of a target video. Before the final composite, the mouth texture sequence and the target video are matched and re-timed so that the head motion appears natural and fits the input speech. Our approach is based on synthesizing video from audio in the region around the mouth, and using compositing techniques to borrow the rest of the head and torso from other stock footage (Fig. 2). Our compositing approach builds on similar talking head techniques like Face2Face [Ties et al. 2016], although Face2Face transfers the mouth from another video sequence whereas we synthesize the mouth shape directly from audio. A main contribution is our recurrent neural network technique for synthesizing mouth shape from audio, trained on millions of video frames, that is significantly simpler than prior methods, yet produces very convincing results. We evaluated many different network architectures to arrive at our solution, but found that a surprisingly simple approach based on standard LSTM techniques produces excellent results. In addition, our approach for generating photorealistic mouth texture preserves fine detail in the lips and teeth, and reproduces time-varying wrinkles and dimples around the mouth and chin. 2",
    "output": "Researchers at the University of Washington are putting former President Barack Obama’s words into his own mouth to demonstrate breakthrough technology in the field of computer vision. By turning audio clips into realistic-looking lip-synced video, the implication is that a moving face could be applied to historic audio recordings or be used to improve video conferencing.\n\nThe results are detailed in a url being presented Aug. 2 at SIGGRAPH 2017, a leading conference in computer graphics and interactive techniques. Obama was chosen as a subject because the machine-learning technique needs a hefty supply of available video to train itself on.\n\nIn the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\n\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\n\n“These type of results have never been shown before,” Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen School of Computer Science & Engineering, said in a UW news release. “Realistic audio-to-video conversion has practical applications like improving video conferencing for meetings, as well as futuristic ones such as being able to hold a conversation with a historical figure in virtual reality by creating visuals just from audio. This is the kind of breakthrough that will help enable those next steps.”\n\nThe findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\n\n“There are millions of hours of video that already exist from interviews, video chats, movies, television programs and other sources. And these deep learning algorithms are very data hungry, so it’s a good match to do it this way,” Suwajanakorn said.\n\nThe technique also combines previous research which generated a good deal of attention near the end of 2015 and even won Innovation of the Year at the 2016 GeekWire Awards.\n\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\n\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video.\n\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Experiments_/_Results_chunk1_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Experiments / Results\nExperiments / Results: EXPERIMENTS In this section, we describe implementation details, evaluations, comparisons to prior work, limitations, and applications. 4.1 Running times and hardware: We report the following runtime based on NVIDIA TitanX for RNN inference, and Intel Core i7-5820K for other computations. For a source audio of 66 seconds in length, on a single CPU core, it took 45 minutes in total to produce a 30fps output video. Te breakdown is as follows: 5 seconds to run RNN inference and generate 1980 mouth shapes (30fps for 66 seconds); mouth texture synthesis took 0.1s per frame (3.3 minutes total); and the final composite including chin correction, masking, and rendering took 0.35s per frame (11.5 minutes total). Te retiming dynamic programming solution took 0.2s for the entire sequence, with an additional 4s per repeated frame for optical flow interpolation [Liu et al. 2008]. In practice, we parallelized most computations on a 24-core CPU and reduced the runtime from 45 to 3 minutes total (0.1s per frame). For network training, the time for [Suwajanakorn et al. 2014] to preprocess 17-hour Obama video (pose estimation, frontalization) took around 2 weeks on 10 cluster nodes of Intel Xeon E5530. Te network training for 300 epochs on NVIDIA TitanX and Intel Core i7-5820K took around 2 hours. 4.2 LSTM Architecture and Data For training we downloaded 300 weekly addresses available online2 spanning 2009 to 2016. Each address lasts about 3 minutes on average, resulting in total of 17 hours of video. We extracted frames at 30fps and obtained around 1.9 million video frames. We randomly split out 20% of the addresses (3 hours) for validation and used 80% (14 hours) for training. Our network consists of 60 LSTM nodes (dimension of c) and uses a 20 step time-delay d, corresponding to 200ms. We train the network with a batch size of 100 using truncated backpropagation through time with 100 time steps. We use the ADAM optimizer [Kingma and Ba 2014] with learning rate 0.001, implemented in TensorFlow [Abadi et al. 2016]. Each dimension in the input vector is normalized by its mean and variance, but the output is unnormalized to keep the relative importances of the PCA coefficients. Training took 3 hours in total for 300 epochs on a NVIDIA TitanX. We found that augmenting the LSTM with a time delay was critical for improving validation loss and visual quality. Tis modification effectively increases the receptive field beyond that of the MFCC window (25ms) to at least 200ms of future audio context. Figure 9 shows validation losses by varying the time delay steps for our single-layer 60-node LSTM network. Without the time delay, both the training and validation losses are high and the visual lip-sync quality is poor. Table 1 shows validations losses for varying time delays as well as the number of LSTM nodes. We found that for our architecture, the time delay of 200ms gives consistently lower validation losses across different numbers of LSTM nodes. Performance decreases beyond 200ms, likely due to the need to propagate information further across time. Time Delay: 50ms 100ms 200ms 400ms L1-30 5.824 4.946 4.572 5.147 L1-60 5.782 4.877 4.400 5.089 L1-90 5.726 4.841 4.391 5.009 L1-120 5.732 4.946 4.572 5.147 Table 1. Validation losses for single-layer (L1) networks with varying (30, 60, 90, and 120) LSTM nodes and time delays. Other network architectures are evaluated in Table 2 by keeping the time delay at 200ms and varying the number of stacked layers, LSTM nodes, and the dropout probability of",
    "output": "Researchers at the University of Washington are putting former President Barack Obama’s words into his own mouth to demonstrate breakthrough technology in the field of computer vision. By turning audio clips into realistic-looking lip-synced video, the implication is that a moving face could be applied to historic audio recordings or be used to improve video conferencing.\n\nThe results are detailed in a url being presented Aug. 2 at SIGGRAPH 2017, a leading conference in computer graphics and interactive techniques. Obama was chosen as a subject because the machine-learning technique needs a hefty supply of available video to train itself on.\n\nIn the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\n\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\n\n“These type of results have never been shown before,” Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen School of Computer Science & Engineering, said in a UW news release. “Realistic audio-to-video conversion has practical applications like improving video conferencing for meetings, as well as futuristic ones such as being able to hold a conversation with a historical figure in virtual reality by creating visuals just from audio. This is the kind of breakthrough that will help enable those next steps.”\n\nThe findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\n\n“There are millions of hours of video that already exist from interviews, video chats, movies, television programs and other sources. And these deep learning algorithms are very data hungry, so it’s a good match to do it this way,” Suwajanakorn said.\n\nThe technique also combines previous research which generated a good deal of attention near the end of 2015 and even won Innovation of the Year at the 2016 GeekWire Awards.\n\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\n\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video.\n\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Experiments_/_Results_chunk1_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Experiments / Results\nthe standard RNN regularization [Zaremba et al. 2014]. Additionally, we explored other regularization techniques, e.g. variational RNN dropout [Gal 2015] on a few configurations but did not find a major improvement. Tese validation losses have high Loss Loss 8 8 Training Validation Training Validation 7 7 6 6 5 5 4 4 3 3 2 2 1 1 0 0 0 10 20 30 40",
    "output": "Researchers at the University of Washington are putting former President Barack Obama’s words into his own mouth to demonstrate breakthrough technology in the field of computer vision. By turning audio clips into realistic-looking lip-synced video, the implication is that a moving face could be applied to historic audio recordings or be used to improve video conferencing.\n\nThe results are detailed in a url being presented Aug. 2 at SIGGRAPH 2017, a leading conference in computer graphics and interactive techniques. Obama was chosen as a subject because the machine-learning technique needs a hefty supply of available video to train itself on.\n\nIn the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\n\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\n\n“These type of results have never been shown before,” Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen School of Computer Science & Engineering, said in a UW news release. “Realistic audio-to-video conversion has practical applications like improving video conferencing for meetings, as well as futuristic ones such as being able to hold a conversation with a historical figure in virtual reality by creating visuals just from audio. This is the kind of breakthrough that will help enable those next steps.”\n\nThe findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\n\n“There are millions of hours of video that already exist from interviews, video chats, movies, television programs and other sources. And these deep learning algorithms are very data hungry, so it’s a good match to do it this way,” Suwajanakorn said.\n\nThe technique also combines previous research which generated a good deal of attention near the end of 2015 and even won Innovation of the Year at the 2016 GeekWire Awards.\n\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\n\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video.\n\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Experiments_/_Results_chunk2_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Experiments / Results\n50 60 70 80 0 100 200 300 Epoch Time Delay Step a) b) Fig. 9. a) shows the losses at the end of the training of networks with varying time delay steps from 0 to 80 (800ms) trained with 300 unfold time steps. b) plots loss during training of our single-layer LSTM network with 20 time delay steps. Dropout probability: 0 0.1 0.3 0.5 L2-30 4.449 4.587 4.881 5.252 L2-60 4.389 4.420 4.621 4.923 L2-90 4.403 4.347 4.498 4.754 L3-30 4.409 4.548 4.850 5.237 L3-60 4.402 4.386 4.585 4.881 L3-90 4.439 4.310 4.487 4.718 Table 2. Validation losses for two (L2) and three (L3) layers networks with various LSTM nodes and dropout probability. variance and do not necessarily translate to beter visual lip-sync quality afer reaching a certain value. One reason is that we do not have ground-truth mouth landmarks and instead rely on landmark estimates from imperfect algorithms. While we filtered out clear failure cases such as when the mouth shape is irregular or not detected, other error cases remain in both the training and validation sets. As such, zero loss does not equate to perfect visual results. We believe recently developed techniques such as recurrent batch normalization can further improve the loss, but larger accuracy gains may require improving the landmark data. For our purpose, we opt for the simplest network (L1-60, with 200ms delay) that achieves a similar loss and empirically similar visual quality to the more complex models. 4.2.1 Word/phoneme overlap between target and input audio: Here we investigate the amount of phoneme and word overlap between the audio from a target video (used for mouth synthesis) and the input audio. For 4-Obama result (Video E), we compute the percentage of words in input audio that exist in each of the four target videos: Top-lef 41%, -right 45%, botom-lef 52%, -right 48%. In other words, half of the words spoken in the input audio do not exist in the target videos. In terms of phoneme, diphone, triphone, tetraphone, pentaphone overlaps (average across 4 targets): 99.9%, 82.9%, 35.7%, 12.1%, 4.9%, respectively. Tere is less than 5% chance to find similar 5 consecutive phonemes in the target videos. (An average word in the input consists of 3.9 phonemes.) 4.2.2 Training set size: Finally, we evaluated the effect of varying the amount of training data on the quality of the output video. We trained our network with: 0.35% of the data (3 minutes total), 10% (1 hour total), 50% (7 hours), and the full test dataset (14 hours). Te supplementary video shows how quality improves with more training data. In particular, with more training data, lip-sync quality improves significantly and mouth jiter is reduced. Tere is significant improvement at each step, even from 7 hours to 14, indicating that having a large amount of training data is critical. 4.3 Lower Face Synthesis Evaluation Figure 10 shows comparison of our synthesis algorithm to the classic AAM approach [Cootes et al. 2001] that is prevalent in visual speech synthesis, and to a recent detail-enhancing Laplacian pyramid technique [Suwajanakorn et al. 2015]. We observe that both AAM and [Suwajanakorn et al. 2015] show significant blurriness. Te blurriness appears due to use of data captured in uncontrolled and uncalibrated conditions, i.e., faces can be non-frontal, under different lighting, etc. Te results in the figure are computed using the same set of frontalized face images as used in our algorithm, and even if lighting-invariant optical flow (i.e., collection flow [KemelmacherShlizerman and Seitz 2012",
    "output": "Researchers at the University of Washington are putting former President Barack Obama’s words into his own mouth to demonstrate breakthrough technology in the field of computer vision. By turning audio clips into realistic-looking lip-synced video, the implication is that a moving face could be applied to historic audio recordings or be used to improve video conferencing.\n\nThe results are detailed in a url being presented Aug. 2 at SIGGRAPH 2017, a leading conference in computer graphics and interactive techniques. Obama was chosen as a subject because the machine-learning technique needs a hefty supply of available video to train itself on.\n\nIn the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\n\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\n\n“These type of results have never been shown before,” Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen School of Computer Science & Engineering, said in a UW news release. “Realistic audio-to-video conversion has practical applications like improving video conferencing for meetings, as well as futuristic ones such as being able to hold a conversation with a historical figure in virtual reality by creating visuals just from audio. This is the kind of breakthrough that will help enable those next steps.”\n\nThe findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\n\n“There are millions of hours of video that already exist from interviews, video chats, movies, television programs and other sources. And these deep learning algorithms are very data hungry, so it’s a good match to do it this way,” Suwajanakorn said.\n\nThe technique also combines previous research which generated a good deal of attention near the end of 2015 and even won Innovation of the Year at the 2016 GeekWire Awards.\n\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\n\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video.\n\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Experiments_/_Results_chunk2_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Experiments / Results\n] is performed as in [Suwajanakorn et al. 2015]), the resulted synthesis is blurry in the teeth area due to fine misalignment across photos. Figure 10 also compares weighted mean, median and mode, for generating facial texture from the same set of image candidates. Mean produces the blurriest result among all three, and mode appears noisy even when the sparse sampling (i.e.,",
    "output": "Researchers at the University of Washington are putting former President Barack Obama’s words into his own mouth to demonstrate breakthrough technology in the field of computer vision. By turning audio clips into realistic-looking lip-synced video, the implication is that a moving face could be applied to historic audio recordings or be used to improve video conferencing.\n\nThe results are detailed in a url being presented Aug. 2 at SIGGRAPH 2017, a leading conference in computer graphics and interactive techniques. Obama was chosen as a subject because the machine-learning technique needs a hefty supply of available video to train itself on.\n\nIn the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\n\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\n\n“These type of results have never been shown before,” Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen School of Computer Science & Engineering, said in a UW news release. “Realistic audio-to-video conversion has practical applications like improving video conferencing for meetings, as well as futuristic ones such as being able to hold a conversation with a historical figure in virtual reality by creating visuals just from audio. This is the kind of breakthrough that will help enable those next steps.”\n\nThe findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\n\n“There are millions of hours of video that already exist from interviews, video chats, movies, television programs and other sources. And these deep learning algorithms are very data hungry, so it’s a good match to do it this way,” Suwajanakorn said.\n\nThe technique also combines previous research which generated a good deal of attention near the end of 2015 and even won Innovation of the Year at the 2016 GeekWire Awards.\n\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\n\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video.\n\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Experiments_/_Results_chunk3_sub1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Experiments / Results\nlow number of candidates) is handled by using larger frequency bins or counting by merging nearby points in color space. Tis behavior can also be understood in an optimization framework where mean, median, and mode correspond to a minimization with L2, L1, L0 norms, respectively. In the case of mode, the summation of the quasi-convex L0 has multiple minima and is sensitive to image noise and misalignment, whereas L2 produces an over-smooth average. Median strikes a good balance between these two, which we’ve seen in practice translates to beter edge-preserving properties than mean and is less noisy than mode. In Figure 11 we compare to the recent Face2face algorithm [Ties et al. 2016]. We provide the same source video (a weekly presidential address) to both methods. Note that we use only the source audio as input, whereas their technique requires the source video–i.e., they effectively have access to the ground truth mouth appearance. In addition, we provide the same target video (a different weekly presidential address) to both methods. Te Face2face were produced by the authors running their original system on our videos. Te differences between the two methods are best viewed in the supplementary video. Some observations: our method tends to produce more realistic and Obama-like lip and head motion. Additionally, our method captures time-varying wrinkles and creases around the mouth whereas Face2face’s texture for the surrounding skin appears more static. Te teeth produced by Face2face sometimes appear non-rigid with occasional ghosting artifacts, whereas teeth produced by our method appear rigid and temporally smoother. Te differences perhaps expected, since our system is trained on many videos of Obama and tuned to produce high quality realistic results a) Mean b) Median c) Mode d) AAM e) [Suwajanakorn f) Ours et al. 2015] Fig. 10. Mouth synthesis comparison to weighted-mean (a), weightedmedian (b), weighted-mode (c), AAM-based techniques (d), and [Suwajanakorn et al. 2015] (e). For all results shown here, we first frontalize all training images using the same frontalization technique as our result, and for (a,b,c,e), we use identical weights to ours computed from Equation 11. Notice how other techniques produce blurry results on our training dataset that contains mouth images from a real speech with natural head motion. a) Face2face [Thies et al. 2016] b) Our results Fig. 11. Comparison to Face2face [Thies et al. 2016] for four different utterances in the same speech using the same source video. Note that [Thies et al. 2016] requires the video of the input speech to drive the animation and focuses on the real-time puppetry application whereas ours aims to synthesize a visual speech given only a recorded audio of Obama. Notice how our method can synthesize more realistic mouths with natural creases around the mouth. The differences between the two approaches are best viewed in the supplementary video. while Face2face is aimed at a real-time puppetry scenario that uses only two videos (source and target). In addition to the above comparisons, we compiled a 4 minute video showing related face animation results of the last 20 years. Te end of our supplementary video includes this compilation, as a helpful visual reference for our work. algorithm. We evaluate this on Video A: the average standard deviation of the frame indices of 100 image candidates used to produce one output texture is 931.8 frames (spanning across 31 seconds) with standard deviation 88.5 (3 seconds). Te span (or the min and max) of the frame indices are always at least 2,448 frames apart (3,648 total). 4.4 Target Video Re-timing Evaluation For all results shown, we use the following fixed parameters: Te blinking detector was implemented by first normalizing eye landmarks by making the distance between lef and right eyes a unit distance. Ten blinking is",
    "output": "Researchers at the University of Washington are putting former President Barack Obama’s words into his own mouth to demonstrate breakthrough technology in the field of computer vision. By turning audio clips into realistic-looking lip-synced video, the implication is that a moving face could be applied to historic audio recordings or be used to improve video conferencing.\n\nThe results are detailed in a url being presented Aug. 2 at SIGGRAPH 2017, a leading conference in computer graphics and interactive techniques. Obama was chosen as a subject because the machine-learning technique needs a hefty supply of available video to train itself on.\n\nIn the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\n\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\n\n“These type of results have never been shown before,” Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen School of Computer Science & Engineering, said in a UW news release. “Realistic audio-to-video conversion has practical applications like improving video conferencing for meetings, as well as futuristic ones such as being able to hold a conversation with a historical figure in virtual reality by creating visuals just from audio. This is the kind of breakthrough that will help enable those next steps.”\n\nThe findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\n\n“There are millions of hours of video that already exist from interviews, video chats, movies, television programs and other sources. And these deep learning algorithms are very data hungry, so it’s a good match to do it this way,” Suwajanakorn said.\n\nThe technique also combines previous research which generated a good deal of attention near the end of 2015 and even won Innovation of the Year at the 2016 GeekWire Awards.\n\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\n\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video.\n\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Experiments_/_Results_chunk3_sub2",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Experiments / Results\ndetected if the average eye area is less than 0.025. For indicating silence, we use a threshold of 0.001 on the average audio volume within a window. Te resulting binary flags with 0-gap less than 0.5 seconds are connected, and then dilated with kernel of size 0.1 seconds. Te dilation and erosion kernels for removing salt and pepper noise for blinking are 3, 3 frames respectively. In the supplementary video",
    "output": "Researchers at the University of Washington are putting former President Barack Obama’s words into his own mouth to demonstrate breakthrough technology in the field of computer vision. By turning audio clips into realistic-looking lip-synced video, the implication is that a moving face could be applied to historic audio recordings or be used to improve video conferencing.\n\nThe results are detailed in a url being presented Aug. 2 at SIGGRAPH 2017, a leading conference in computer graphics and interactive techniques. Obama was chosen as a subject because the machine-learning technique needs a hefty supply of available video to train itself on.\n\nIn the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\n\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\n\n“These type of results have never been shown before,” Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen School of Computer Science & Engineering, said in a UW news release. “Realistic audio-to-video conversion has practical applications like improving video conferencing for meetings, as well as futuristic ones such as being able to hold a conversation with a historical figure in virtual reality by creating visuals just from audio. This is the kind of breakthrough that will help enable those next steps.”\n\nThe findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\n\n“There are millions of hours of video that already exist from interviews, video chats, movies, television programs and other sources. And these deep learning algorithms are very data hungry, so it’s a good match to do it this way,” Suwajanakorn said.\n\nThe technique also combines previous research which generated a good deal of attention near the end of 2015 and even won Innovation of the Year at the 2016 GeekWire Awards.\n\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\n\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video.\n\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "02",
    "id_story": "02-01",
    "id_chunk": "02-01_Experiments_/_Results_chunk4",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Student, Journalist, Developer\n\nSection: Experiments / Results\n(Video D) we show results generated with and without re-timing (Section 3.3). Since our dynamic programming approach also solves for the best starting frame of the target video, for comparison we start both target videos at this same frame. Without retiming, there are occasional head motion and expression changes during vocal pauses which appears unnatural, as indicated by the red arrow. We also evaluate consistency of the re-timing approach across different target videos (Video E in sup. video), by generating output videos with four different targets. Most of the time, all four results start moving when Obama starts a new sentence and stop during long pauses. Occasionally, Obama moves slightly during small pauses, but fast motion during pauses is avoided. 4.5 Results & Applications All of the result figures and supplementary videos are generated from input speeches that are not in the training or validation sets. In Figure 12, we run on the audio of Obama weekly address “Standing with Orlando” from YouTube with ID3 nIxM8rL5GVE and use four different target videos also from weekly addresses (E3gfMumXCjI, 3vPdtajOJfw, 25GOnaY8ZCY, k4OZOTaf3lk). Fully generated videos for each of the targets are presented in supplementary Video E. Our results are realistic with convincing, natural head motion. When compared to prior techniques (Video I), our mouth texture looks sharper than those that rely on AAM and more temporally smooth than those that use frames from original footage. Te upper teeth look sharp and realistic. We do note that the lower teeth and tongue appear blurrier and less plausible when compared to the groundtruth footage, e.g., in Video A. In Figure 13 and Video F, we compare our synthesized textures to the original video of the input speech. In Video J, we show the pixel difference map (mean of absolute RGB differences) between the groundtruth video of the input audio and our result using the same input video as the target video (no retiming). Our results show good agreement in terms of the mouth Fig. 12. Results for the same input speech using four different target videos. Results along each column are generated from the same uterance. shapes, timing, and overall mouth animation, but the mouth region appears less detailed and smoother compared to the groundtruth. To evaluate generalization to other types of input speech not from the weekly addresses on which we trained, we ran our system on the audio of Obama’s interview with Steve Harvey (qMlLjFPCO4M), 60 Minutes (F8MxP9adPO8), and the View (Hdn1iX1a528). Our method can handle casual speech and generates plausible mouth animation even during a mumble or hesitation. We also test on the voice of a quarter century younger Obama from 1990 (7XGi3FGVmA0) and on a voice impressionist (vSAA5GH6OFg). Te lipsync quality still looks reasonable although it degrades somewhat as the voice deviates from our training audio. One useful application of our method is speech summarization, inspired by [Berthouzoz et al. 2012]. I.e., given a long address speech, create a short summary version by manually selecting the desired sections from transcribed text. Our method can then be used to generate a seamless video for the summarized speech shown in Video G. While our result looks similar to [Berthouzoz et al. 2012] for this particular example, an advantage of our system is that we can produce visually seamless cuts even when the head position, lighting, or background has changed between concatenated source footage clips. Original Video for Input Audio a) Our result Original Video for Input Audio b) Our result Fig. 13. Comparison of our mouth shapes to the ground-truth footage of the input audio (note good agreement–more results in supplemental video). a) is a weekly address on climate change (cNVzN62l0Yg), and b) is on health care (deF-f0OqvQ4).",
    "output": "Researchers at the University of Washington are putting former President Barack Obama’s words into his own mouth to demonstrate breakthrough technology in the field of computer vision. By turning audio clips into realistic-looking lip-synced video, the implication is that a moving face could be applied to historic audio recordings or be used to improve video conferencing.\n\nThe results are detailed in a url being presented Aug. 2 at SIGGRAPH 2017, a leading conference in computer graphics and interactive techniques. Obama was chosen as a subject because the machine-learning technique needs a hefty supply of available video to train itself on.\n\nIn the demonstration, video of Obama from a range of appearances is used to deliver audio spoken in separate instances. A recurrent neural network, trained on many hours of the president’s weekly address footage, learns the mapping from raw audio features to mouth shapes. Researchers Supasorn Suwajanakorn, Steven M. Seitz, and Ira Kemelmacher-Shlizerman summarize it like so:\n\nGiven the mouth shape at each time instant, we synthesize high quality mouth texture, and composite it with proper 3D pose matching to change what he appears to be saying in a target video to match the input audio track. Our approach produces photorealistic results.\n\n“These type of results have never been shown before,” Kemelmacher-Shlizerman, an assistant professor at the UW’s Paul G. Allen School of Computer Science & Engineering, said in a UW news release. “Realistic audio-to-video conversion has practical applications like improving video conferencing for meetings, as well as futuristic ones such as being able to hold a conversation with a historical figure in virtual reality by creating visuals just from audio. This is the kind of breakthrough that will help enable those next steps.”\n\nThe findings could also lead to improved video chat performance, since streaming audio over the internet takes up far less bandwidth than video.\n\n“When you watch Skype or Google Hangouts, often the connection is stuttery and low-resolution and really unpleasant, but often the audio is pretty good,” said Seitz, co-author of the url and an Allen School professor. “So if you could use the audio to produce much higher-quality video, that would be terrific.”\n\nThe new method improves upon previous audio-to-video conversion processes which involved filming multiple people in a studio saying the same sentences over and over to try to capture how a particular sound correlates to different mouth shapes, the UW says. Suwajanakorn developed algorithms that can learn from videos that exist “in the wild” on the internet or elsewhere.\n\n“There are millions of hours of video that already exist from interviews, video chats, movies, television programs and other sources. And these deep learning algorithms are very data hungry, so it’s a good match to do it this way,” Suwajanakorn said.\n\nThe technique also combines previous research which generated a good deal of attention near the end of 2015 and even won Innovation of the Year at the 2016 GeekWire Awards.\n\nIt’s pretty fascinating to watch the audio coming from Obama in an original clip and to then shift your focus to the unrelated video and hear the same words but not be distracted by what UW’s algorithm has been able to create with mouth movements. It’s especially true with audio from a much-younger Obama synced to his more current face.\n\nUW says that right now the neural network is designed to learn on one individual at a time, meaning that Obama’s voice, speaking his actual words, is the only information used to “drive” the synthesized video.\n\n“You can’t just take anyone’s voice and turn it into an Obama video,” Seitz said. “We very consciously decided against going down the path of putting other people’s words into someone’s mouth. We’re simply taking real words that someone spoke and turning them into realistic video of that individual.”"
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Abstract\nAbstract: ABSTRACT Lipreading is the task of decoding text from the movement of a speaker’s mouth. Traditional approaches separated the problem into two stages: designing or learning visual features, and prediction. More recent deep lipreading approaches are end-to-end trainable (Wand et al., 2016; Chung & Zisserman, 2016a). However, existing work on models trained end-to-end perform only word classification, rather than sentence-level sequence prediction. Studies have shown that human lipreading performance increases for longer words (Easton & Basala, 1982), indicating the importance of features capturing temporal context in an ambiguous communication channel. Motivated by this observation, we present LipNet, a model that maps a variable-length sequence of video frames to text, making use of spatiotemporal convolutions, a recurrent network, and the connectionist temporal classification loss, trained entirely end-to-end. To the best of our knowledge, LipNet is the first end-to-end sentence-level lipreading model that simultaneously learns spatiotemporal visual features and a sequence model. On the GRID corpus, LipNet achieves 95.2% accuracy in sentence-level, overlapped speaker split task, outperforming experienced human lipreaders and the previous 86.4% word-level state-of-the-art accuracy (Gergen et al., 2016).",
    "output": "However, before we get lost in bad dreams of AI-powered surveillance states and HAL reading lips in 2001: A Space Odyssey, the research from Oxford has some serious limitations. For a start, the system was trained and tested on a research dataset known as GRID. This is a collection of tens of thousands of short videos of 34 volunteers reading nonsense sentences, as well as captions. Each clip is just three seconds long, and each sentence follows the pattern: command, color, preposition, letter, digit, adverb. Sentences include, for example, “set blue by A four please,” and “place red at C zero again.” Even the words within these patterns are limited — there are just four different commands and colors used. This has led some researchers in the field to suggest that the url’s findings have been overblown, especially after one viral tweet urling to the researchers’ video (below) made the sensationalist claim that the work meant there would be “no more secrets.”"
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Introduction\nIntroduction: INTRODUCTION Lipreading plays a crucial role in human communication and speech understanding, as highlighted by the McGurk effect (McGurk & MacDonald, 1976), where one phoneme’s audio dubbed on top of a video of someone speaking a different phoneme results in a third phoneme being perceived. Lipreading is a notoriously difficult task for humans, specially in the absence of context1. Most lipreading actuations, besides the lips and sometimes tongue and teeth, are latent and difficult to disambiguate without context (Fisher, 1968; Woodward & Barber, 1960). For example, Fisher (1968) gives 5 categories of visual phonemes (called visemes), out of a list of 23 initial consonant phonemes, that are commonly confused by people when viewing a speaker’s mouth. Many of these were asymmetrically confused, and observations were similar for final consonant phonemes. Consequently, human lipreading performance is poor. Hearing-impaired people achieve an accuracy of only 1712% even for a limited subset of 30 monosyllabic words and 2111% for 30 compound words (Easton & Basala, 1982). An important goal, therefore, is to automate lipreading. Machine lipreaders have enormous practical potential, with applications in improved hearing aids, silent dictation in public spaces, security, speech recognition in noisy environments, biometric identification, and silent-movie processing. Machine lipreading is difficult because it requires extracting spatiotemporal features from the video (since both position and motion are important). Recent deep learning approaches attempt to extract those features end-to-end. Most existing work, however, performs only word classification, not sentence-level sequence prediction. In this paper, we present LipNet, which is to the best of our knowledge, the first end-to-end sentence-level lipreading model. As with modern deep learning based automatic speech recognition (ASR), LipNet is trained end-to-end to make sentence-level predictions. Our model operates at the character-level, using spatiotemporal convolutional neural networks (STCNNs), recurrent neural networks (RNNs), and the connectionist temporal classification loss (CTC) Graves et al. (2006). Our empirical results on the GRID corpus (Cooke et al., 2006), one of the few public sentence-level datasets, show that LipNet attains a 95.2% sentence-level word accuracy, in a overlapped speakers split that is popular for benchmarking lipreading methods. The previous best accuracy reported on an aligned word classification version of this task was 86.4% (Gergen et al., 2016). Furthermore, LipNet can generalise across unseen speakers in the GRID corpus with an accuracy of 88.6%. We also compare the performance of LipNet with that of hearing-impaired people who can lipread on the GRID corpus task. On average, they achieve an accuracy of 52.3%, in contrast to LipNet’s 1.69 higher accuracy in the same sentences. Finally, by applying saliency visualisation techniques (Zeiler & Fergus, 2014; Simonyan et al., 2013), we interpret LipNet’s learned behaviour, showing that the model attends to phonologically important regions in the video. Furthermore, by computing intra-viseme and inter-viseme confusion matrices at the phoneme level, we show that almost all of LipNet’s few erroneous predictions occur within visemes, since context is sometimes insufficient for disambiguation. 2",
    "output": "Instead, the two researchers think that lip-reading AI could help people with hearing disabilities, especially in noisy environments where it’s difficult for computers to isolate speech. For example, someone wearing a camera built into a pair of glasses could get clear, frontal footage of someone they’re talking to at a party, and an ancestor of LipNet could then transcribe the conversation in real time, feeding it into their ear. “Anywhere you have speech recognition and a camera, we can improve that,” says Assael. He also mentions silent dictation to Siri or Google Assistant as a possible use-case. In the future, then, perhaps those of us who don’t like speaking to our computers, can’t just have them read our lips instead."
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nMethod: 3 LIPNET LipNet is a neural network architecture for lipreading that maps variable-length sequences of video frames to text sequences, and is trained end-to-end. In this section, we describe LipNet’s building blocks and architecture. 3.1 SPATIOTEMPORAL CONVOLUTIONS Convolutional neural networks (CNNs), containing stacked convolutions operating spatially over an image, have been instrumental in advancing performance in computer visions tasks such as object recognition that receive an image as input (Krizhevsky et al., 2012). A basic 2D convolution layer from C channels to C′ channels (without a bias and with unit stride) computes kw X C X [conv(x, w)]c′ij = i′=1 c=1 for input x and weights w RC′Ckwkh where we define xcij = 0 for i, j out of bounds. Spatiotemporal convolutional neural networks (STCNNs) can process video data by convolving across time, as well as the spatial dimensions (Karpathy et al., 2014; Ji et al., 2013). Hence similarly, kt X kw X C X [stconv(x, w)]c′tij = t′=1 i′=1 c=1 3.2 GATED RECURRENT UNIT Gated Recurrent Unit (GRU) (Chung et al., 2014) is a type of recurrent neural network (RNN) that improves upon earlier RNNs by adding cells and gates for propagating information over more timesteps and learning to control this information flow. It is similar to the Long Short-Term Memory (LSTM) RNN (Hochreiter & Schmidhuber, 1997). We use the standard formulation: [ut, rt]T = sigm(Wzzt + Whht1 + bg) ht = tanh(Uzzt + Uh(rt ht1) + bh) ht = (1 ut) ht1 + ut  ht where z := z1, . . . , zT  is the input sequence to the RNN, denotes element-wise multiplication, and sigm(r) = 1/(1 + exp(r)). We use a bidirectional GRU (Bi-GRU) as introduced by Graves & Schmidhuber (2005) in the context of LSTMs: one RNN maps z1, . . . , zT  7 h1, . . . ,  hT , and another zT , . . . , z1 7 h1, . . . ,  hT , then ht := [ ht,  ht]. The Bi-GRU ensures that ht depends on zt′ for all t′. To parameterise a distribution over sequences, at time-step t let p(ut|z) = softmax(mlp(ht; Wmlp)), where mlp is a feed-forward network with weights Wmlp. Then we can define the distribution over length-T sequences as p(u1, . . . , uT |z) = Q 1tT p(ut|z), where T is determined by z, the input to the GRU. In LipNet, z is the output of the STCNN. 3.3 CONNECTIONIST TEMPORAL CLASSIFICATION The connectionist temporal classification (CTC) loss (Graves et al., 2006) is widely used in modern speech recognition as it eliminates the need for training data that aligns inputs to target outputs (Amodei et al., 2015; Graves & Jaitly, 2014; Maas et al., 2015). Given a model that outputs a sequence of discrete distributions over the token classes (vocabulary) augmented with a",
    "output": "Lip reading is a tricky business. Test results vary, but on average, most people recognize just one in 10 words when watching someone’s lips, and the accuracy of self-proclaimed experts tends to vary — there are certainly no lip-reading savants. Now, though, some researchers claim that AI techniques like deep learning could help solve this problem. After all, AI methods that focus on crunching large amounts of data to find common patterns have helped improve audio speech recognition to near-human levels of accuracy, so why can’t the same be done for lip reading?\n\nMuch more accurate than humans, but working with very limited data\n\nThe researchers from the University of Oxford’s AI lab have made a promising — if crucially limited — contribution to the field, creating a new lip-reading program using deep learning. Their software, dubbed LipNet, was able to outperform experienced lip readers to a significant degree, achieving 93.4 percent accuracy in certain tests, compared to 52.3 percent accuracy from human lip readers. And even in its current, early stages, the software is extremely fast — processing silent video into text transcripts in nearly real time.\n\nHowever, before we get lost in bad dreams of AI-powered surveillance states and HAL reading lips in 2001: A Space Odyssey, the research from Oxford has some serious limitations. For a start, the system was trained and tested on a research dataset known as GRID. This is a collection of tens of thousands of short videos of 34 volunteers reading nonsense sentences, as well as captions. Each clip is just three seconds long, and each sentence follows the pattern: command, color, preposition, letter, digit, adverb. Sentences include, for example, “set blue by A four please,” and “place red at C zero again.” Even the words within these patterns are limited — there are just four different commands and colors used. This has led some researchers in the field to suggest that the url’s findings have been overblown, especially after one viral tweet urling to the researchers’ video (below) made the sensationalist claim that the work meant there would be “no more secrets.”\n\nThis is certainly not the case. Speaking to The Verge, two of the researchers behind the url, Yannis Assael and Brendan Shillingford, readily admitted they were working with “restricted vocabulary and grammar,” but said this was due to limitations in available data. “The dataset is small but it’s a good indication we could perform just as well with a much bigger dataset,” says Assael.\n\nReally, this won't help with surveillance at all\n\nBoth Assael and Shillingford are also keen to stress that their work has no application in the world of surveillance, simply because lip reading requires you to see the subject’s tongue — meaning that the video has to be straight on and well-lit to get a good result. “It’s technically impossible or at least very, very difficult,” to use any lip-reading software for surveillance says Assael, adding that frame rate is also a factor; and something that is usually neglected with CCTV. He says: “And if you do have frontal video of someone taken with a very good camera, then you probably have a directional microphone [pointed at them] as well!” (On the subject of surveillance, Assael notes that although one of the url’s supervisors also works with Google’s AI division DeepMind, Google itself had no involvement with LipNet’s development.)\n\nInstead, the two researchers think that lip-reading AI could help people with hearing disabilities, especially in noisy environments where it’s difficult for computers to isolate speech. For example, someone wearing a camera built into a pair of glasses could get clear, frontal footage of someone they’re talking to at a party, and an ancestor of LipNet could then transcribe the conversation in real time, feeding it into their ear. “Anywhere you have speech recognition and a camera, we can improve that,” says Assael. He also mentions silent dictation to Siri or Google Assistant as a possible use-case. In the future, then, perhaps those of us who don’t like speaking to our computers, can’t just have them read our lips instead."
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nspecial “blank” token, CTC computes the probability of a sequence by marginalising over all sequences that are defined as equivalent to this sequence. This simultaneously removes the need for alignments and addresses variable-length sequences. Let V denote the set of tokens that the model classifies at a single time-step of its output (vocabulary), and the blank-augmented vocabulary V = V   kh X j′=1 wc′ci′j′xc,i+i′,j+j′, kh X j′=1 wc′ct′i′j′xc,t+t′,i+i′,j+j′. time (x3) t frames STCNN + Spatial Pooling Figure 1: LipNet architecture. A sequence of T frames is used as input, and is processed by 3 layers of STCNN, each followed by a spatial max-pooling layer. The features extracted are processed by end-to-end model is trained with CTC. where denotes the CTC blank symbol. Define the function B : V V that, given a string over V , deletes adjacent duplicate characters and removes blank tokens. For a label sequence y V , CTC defines p(y|x) = P uB1(y) s.t. |u|=T p(u1, . . . , uT |x), where T is the number of time-steps in the sequence model. For example, if T = 3, CTC defines the probability of a string “am” as p(aam) + p(amm) + p( am) + p(a m) + p(am ). This sum is computed efficiently by dynamic programming, allowing us to perform maximum likelihood. 3.4 LIPNET ARCHITECTURE Figure 1 illustrates the LipNet architecture, which starts with 3(spatiotemporal convolutions, channel-wise dropout, spatial max-pooling). Subsequently, the features extracted are followed by two Bi-GRUs. The Bi-GRUs are crucial for efficient further aggregation of the STCNN output. Finally, a linear transformation is applied at each time-step, followed by a softmax over the vocabulary augmented with the CTC blank, and then the CTC loss. All layers use rectified linear unit (ReLU) activation functions. More details including hyperparameters can be found in Table 3 of Appendix A. 4 LIPREADING EVALUATION In this section, we evaluate LipNet on the GRID corpus. The augmentation methods employed don’t make use of external data and rely purely on the GRID corpus. 4.1 DATA AUGMENTATION Preprocessing: The GRID corpus consists of 34 subjects, each narrating 1000 sentences. The videos for speaker 21 are missing, and a few others are empty or corrupt, leaving 32746 usable videos. We employ a split (unseen speakers; not previously used in the literature) holding out the data of two male speakers (1 and 2) and two female speakers (20 and 22) for evaluation (3971 videos). The remainder is used for training (28775 videos). We also use a sentence-level variant of the split (overlapped speakers) similar to Wand et al. (2016), where 255 random sentences from each speaker are used for evaluation. All remaining data from all speakers is pooled together for training. All videos are 3 seconds long with a frame rate of 25fps. The videos were processed with the DLib face detector, and the iBug face landmark predictor (Sagonas et al., 2013) with 68 landmarks coupled with an online Kalman Filter. Using these landmarks, we apply an affine transformation to extract a mouth-centred crop of size 100  50 pixels per frame. We standardise the RGB channels over the whole training set to have zero mean and unit variance. Bi-GRU Linear CTC loss (x2) decay rate of 0.925. Third, to encourage resilience to varying motion speeds by deletion and duplication of frames, this is performed with a per-frame probability of 0.05. The same augmentation methods were followed in all proposed baselines and models. 4.2 BASELINES To evaluate LipNet, we compare its performance to that of three hearing-impaired",
    "output": "Lip reading is a tricky business. Test results vary, but on average, most people recognize just one in 10 words when watching someone’s lips, and the accuracy of self-proclaimed experts tends to vary — there are certainly no lip-reading savants. Now, though, some researchers claim that AI techniques like deep learning could help solve this problem. After all, AI methods that focus on crunching large amounts of data to find common patterns have helped improve audio speech recognition to near-human levels of accuracy, so why can’t the same be done for lip reading?\n\nMuch more accurate than humans, but working with very limited data\n\nThe researchers from the University of Oxford’s AI lab have made a promising — if crucially limited — contribution to the field, creating a new lip-reading program using deep learning. Their software, dubbed LipNet, was able to outperform experienced lip readers to a significant degree, achieving 93.4 percent accuracy in certain tests, compared to 52.3 percent accuracy from human lip readers. And even in its current, early stages, the software is extremely fast — processing silent video into text transcripts in nearly real time.\n\nHowever, before we get lost in bad dreams of AI-powered surveillance states and HAL reading lips in 2001: A Space Odyssey, the research from Oxford has some serious limitations. For a start, the system was trained and tested on a research dataset known as GRID. This is a collection of tens of thousands of short videos of 34 volunteers reading nonsense sentences, as well as captions. Each clip is just three seconds long, and each sentence follows the pattern: command, color, preposition, letter, digit, adverb. Sentences include, for example, “set blue by A four please,” and “place red at C zero again.” Even the words within these patterns are limited — there are just four different commands and colors used. This has led some researchers in the field to suggest that the url’s findings have been overblown, especially after one viral tweet urling to the researchers’ video (below) made the sensationalist claim that the work meant there would be “no more secrets.”\n\nThis is certainly not the case. Speaking to The Verge, two of the researchers behind the url, Yannis Assael and Brendan Shillingford, readily admitted they were working with “restricted vocabulary and grammar,” but said this was due to limitations in available data. “The dataset is small but it’s a good indication we could perform just as well with a much bigger dataset,” says Assael.\n\nReally, this won't help with surveillance at all\n\nBoth Assael and Shillingford are also keen to stress that their work has no application in the world of surveillance, simply because lip reading requires you to see the subject’s tongue — meaning that the video has to be straight on and well-lit to get a good result. “It’s technically impossible or at least very, very difficult,” to use any lip-reading software for surveillance says Assael, adding that frame rate is also a factor; and something that is usually neglected with CCTV. He says: “And if you do have frontal video of someone taken with a very good camera, then you probably have a directional microphone [pointed at them] as well!” (On the subject of surveillance, Assael notes that although one of the url’s supervisors also works with Google’s AI division DeepMind, Google itself had no involvement with LipNet’s development.)\n\nInstead, the two researchers think that lip-reading AI could help people with hearing disabilities, especially in noisy environments where it’s difficult for computers to isolate speech. For example, someone wearing a camera built into a pair of glasses could get clear, frontal footage of someone they’re talking to at a party, and an ancestor of LipNet could then transcribe the conversation in real time, feeding it into their ear. “Anywhere you have speech recognition and a camera, we can improve that,” says Assael. He also mentions silent dictation to Siri or Google Assistant as a possible use-case. In the future, then, perhaps those of us who don’t like speaking to our computers, can’t just have them read our lips instead."
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Method_chunk3_sub1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\npeople who can lipread, as well as three ablation models inspired by recent state-of-the-art work (Chung & Zisserman, 2016a; Wand et al., 2016). Hearing-Impaired People: This baseline was performed by three members of the Oxford Students’ Disability Community. After being introduced to the grammar of the GRID corpus, they observed 10 minutes of annotated videos from the training dataset, then annotated 300 random videos from the evaluation dataset. When uncertain, they were asked to pick the most probable answer. Baseline-LSTM: Using the sentence-level training setup of LipNet, we replicate the model architecture of the previous deep learning GRID corpus state-of-the-art (Wand et al., 2016). See Appendix A for more implementation details. Baseline-2D: Based on the LipNet architecture, we replace the STCNN with spatial-only convolutions similar to those of Chung & Zisserman (2016a). Notably, contrary to the results we observe with LipNet, Chung & Zisserman (2016a) report 14% and 31% poorer performance of their STCNNs compared to the 2D architectures in their two datasets. Baseline-NoLM: Identical to LipNet, but with the language model used in beam search disabled. 4.3 PERFORMANCE EVALUATION To measure the performance of LipNet and the baselines, we compute the word error rate (WER) and the character error rate (CER), standard metrics for the performance of ASR models. We produce approximate maximum-probability predictions from LipNet by performing CTC beam search. WER (or CER) is defined as the minimum number of word (or character) insertions, substitutions, and deletions required to transform the prediction into the ground truth, divided by the number of words (or characters) in the ground truth. Note that WER is usually equal to classification error when the predicted sentence has the same number of words as the ground truth, particularly in our case since almost all errors are substitution errors. Table 2 summarises the performance of LipNet compared to the baselines. According to the literature, the accuracy of human lipreaders is around 20% (Easton & Basala, 1982; Hilder et al., 2009). As expected, the fixed sentence structure and the limited subset of words for each position in the GRID corpus facilitate the use of context, increasing performance. On the unseen speakers split, the three hearing-impaired people achieve 57.3%, 50.4%, and 35.5% WER respectively, yielding an average of 47.7% WER. Table 2: Performance of LipNet on the GRID dataset compared to the baselines, measured on two splits: (a) evaluating on only unseen speakers, and (b) evaluating on a 255 video subset of each speakers’ sentences. Unseen Speakers Overlapped Speakers Method CER WER CER WER Hearing-Impaired Person (avg) 47.7% Baseline-LSTM 38.4% 52.8% 15.2% 26.3% Baseline-2D 16.2% 26.7% 4.3% 11.6% Baseline-NoLM 6.7% 13.6% 2.0% 5.6% LipNet 6.4% 11.4% 1.9% 4.8% The WER for unseen speakers Baseline-2D is 26.7%, whereas for LipNet it is 2.3 lower, at 11.4%. Similarly, the error rate for overlapped speakers was 2.4 lower for LipNet compared to Baseline2D. Both results demonstrate the importance of combining STCNNs with RNNs. This performance difference confirms the intuition that extracting spatiotemporal features using a STCNN is better than aggregating spatial-only features. This observation contrasts with the empirical observations of Chung & Zisserman (2016a). Furthermore, LipNet’s use of STCNN, RNNs, and CTC cleanly allow processing both variable-length input and variable-length output sequences, whereas the architectures of Chung & Zisserman (2016a) and Chung & Zisserman (2016b)",
    "output": "Lip reading is a tricky business. Test results vary, but on average, most people recognize just one in 10 words when watching someone’s lips, and the accuracy of self-proclaimed experts tends to vary — there are certainly no lip-reading savants. Now, though, some researchers claim that AI techniques like deep learning could help solve this problem. After all, AI methods that focus on crunching large amounts of data to find common patterns have helped improve audio speech recognition to near-human levels of accuracy, so why can’t the same be done for lip reading?\n\nMuch more accurate than humans, but working with very limited data\n\nThe researchers from the University of Oxford’s AI lab have made a promising — if crucially limited — contribution to the field, creating a new lip-reading program using deep learning. Their software, dubbed LipNet, was able to outperform experienced lip readers to a significant degree, achieving 93.4 percent accuracy in certain tests, compared to 52.3 percent accuracy from human lip readers. And even in its current, early stages, the software is extremely fast — processing silent video into text transcripts in nearly real time.\n\nHowever, before we get lost in bad dreams of AI-powered surveillance states and HAL reading lips in 2001: A Space Odyssey, the research from Oxford has some serious limitations. For a start, the system was trained and tested on a research dataset known as GRID. This is a collection of tens of thousands of short videos of 34 volunteers reading nonsense sentences, as well as captions. Each clip is just three seconds long, and each sentence follows the pattern: command, color, preposition, letter, digit, adverb. Sentences include, for example, “set blue by A four please,” and “place red at C zero again.” Even the words within these patterns are limited — there are just four different commands and colors used. This has led some researchers in the field to suggest that the url’s findings have been overblown, especially after one viral tweet urling to the researchers’ video (below) made the sensationalist claim that the work meant there would be “no more secrets.”\n\nThis is certainly not the case. Speaking to The Verge, two of the researchers behind the url, Yannis Assael and Brendan Shillingford, readily admitted they were working with “restricted vocabulary and grammar,” but said this was due to limitations in available data. “The dataset is small but it’s a good indication we could perform just as well with a much bigger dataset,” says Assael.\n\nReally, this won't help with surveillance at all\n\nBoth Assael and Shillingford are also keen to stress that their work has no application in the world of surveillance, simply because lip reading requires you to see the subject’s tongue — meaning that the video has to be straight on and well-lit to get a good result. “It’s technically impossible or at least very, very difficult,” to use any lip-reading software for surveillance says Assael, adding that frame rate is also a factor; and something that is usually neglected with CCTV. He says: “And if you do have frontal video of someone taken with a very good camera, then you probably have a directional microphone [pointed at them] as well!” (On the subject of surveillance, Assael notes that although one of the url’s supervisors also works with Google’s AI division DeepMind, Google itself had no involvement with LipNet’s development.)\n\nInstead, the two researchers think that lip-reading AI could help people with hearing disabilities, especially in noisy environments where it’s difficult for computers to isolate speech. For example, someone wearing a camera built into a pair of glasses could get clear, frontal footage of someone they’re talking to at a party, and an ancestor of LipNet could then transcribe the conversation in real time, feeding it into their ear. “Anywhere you have speech recognition and a camera, we can improve that,” says Assael. He also mentions silent dictation to Siri or Google Assistant as a possible use-case. In the future, then, perhaps those of us who don’t like speaking to our computers, can’t just have them read our lips instead."
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Method_chunk3_sub2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nonly handle the former. Baseline-LSTM exhibits the lowest performance, in both unseen and overlapped speakers, with 52.8% and 26.3% WER, respectively. Interestingly, although Baseline-LSTM replicates the architecture of Wand et al. (2016), and despite the numerous data augmentation methods, the model performs 1.3 lower than the reported 79.6% word-level accuracy illustrating",
    "output": "Lip reading is a tricky business. Test results vary, but on average, most people recognize just one in 10 words when watching someone’s lips, and the accuracy of self-proclaimed experts tends to vary — there are certainly no lip-reading savants. Now, though, some researchers claim that AI techniques like deep learning could help solve this problem. After all, AI methods that focus on crunching large amounts of data to find common patterns have helped improve audio speech recognition to near-human levels of accuracy, so why can’t the same be done for lip reading?\n\nMuch more accurate than humans, but working with very limited data\n\nThe researchers from the University of Oxford’s AI lab have made a promising — if crucially limited — contribution to the field, creating a new lip-reading program using deep learning. Their software, dubbed LipNet, was able to outperform experienced lip readers to a significant degree, achieving 93.4 percent accuracy in certain tests, compared to 52.3 percent accuracy from human lip readers. And even in its current, early stages, the software is extremely fast — processing silent video into text transcripts in nearly real time.\n\nHowever, before we get lost in bad dreams of AI-powered surveillance states and HAL reading lips in 2001: A Space Odyssey, the research from Oxford has some serious limitations. For a start, the system was trained and tested on a research dataset known as GRID. This is a collection of tens of thousands of short videos of 34 volunteers reading nonsense sentences, as well as captions. Each clip is just three seconds long, and each sentence follows the pattern: command, color, preposition, letter, digit, adverb. Sentences include, for example, “set blue by A four please,” and “place red at C zero again.” Even the words within these patterns are limited — there are just four different commands and colors used. This has led some researchers in the field to suggest that the url’s findings have been overblown, especially after one viral tweet urling to the researchers’ video (below) made the sensationalist claim that the work meant there would be “no more secrets.”\n\nThis is certainly not the case. Speaking to The Verge, two of the researchers behind the url, Yannis Assael and Brendan Shillingford, readily admitted they were working with “restricted vocabulary and grammar,” but said this was due to limitations in available data. “The dataset is small but it’s a good indication we could perform just as well with a much bigger dataset,” says Assael.\n\nReally, this won't help with surveillance at all\n\nBoth Assael and Shillingford are also keen to stress that their work has no application in the world of surveillance, simply because lip reading requires you to see the subject’s tongue — meaning that the video has to be straight on and well-lit to get a good result. “It’s technically impossible or at least very, very difficult,” to use any lip-reading software for surveillance says Assael, adding that frame rate is also a factor; and something that is usually neglected with CCTV. He says: “And if you do have frontal video of someone taken with a very good camera, then you probably have a directional microphone [pointed at them] as well!” (On the subject of surveillance, Assael notes that although one of the url’s supervisors also works with Google’s AI division DeepMind, Google itself had no involvement with LipNet’s development.)\n\nInstead, the two researchers think that lip-reading AI could help people with hearing disabilities, especially in noisy environments where it’s difficult for computers to isolate speech. For example, someone wearing a camera built into a pair of glasses could get clear, frontal footage of someone they’re talking to at a party, and an ancestor of LipNet could then transcribe the conversation in real time, feeding it into their ear. “Anywhere you have speech recognition and a camera, we can improve that,” says Assael. He also mentions silent dictation to Siri or Google Assistant as a possible use-case. In the future, then, perhaps those of us who don’t like speaking to our computers, can’t just have them read our lips instead."
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nthe difficulty of a sentence-level task even in a restricted grammar. Finally, by disabling the language model, the Baseline-NoLM exhibits approximately 1.2 higher WER than our proposed model. 4.4 LEARNED REPRESENTATIONS In this section, we analyse the learned representations of LipNet from a phonological perspective. First, we create saliency visualisations (Simonyan et al., 2013; Zeiler & Fergus, 2014) to illustrate where LipNet has learned to attend. In particular, we feed an input into the model and greedily decode an output sequence, yielding a CTC alignment u  V (following the notation of Sections 3.2 and 3.3). Then, we compute the gradient of P t p(ut|x) with respect to the input video frame sequence, but unlike Simonyan et al. (2013), we use guided backpropagation (Springenberg et al., 2014). Second, we train LipNet to predict ARPAbet phonemes, instead of characters, to analyse visual phoneme similarities using intra-viseme and inter-viseme confusion matrices. 4.4.1 SALIENCY MAPS We apply saliency visualisation techniques to interpret LipNet’s learned behaviour, showing that the model attends to phonologically important regions in the video. In particular, in Figure 2 we analyse two saliency visualisations for the words please and lay for speaker 25, based on Ashby (2013). p l e e a a s s   e (a) l    a a   y (b) 1 2 3 4 5 6 7 8 9 10 11 Figure 2: Saliency maps for the words (a) please and (b) lay, produced by backpropagation to the input, showing the places where LipNet has learned to attend. The pictured transcription is given by greedy CTC decoding. CTC blanks are denoted by ‘ ’. The production of the word please requires a great deal of articulatory movement at the beginning: the lips are pressed firmly together for the bilabial plosive /p/ (frame 1). At the same time, the blade of the tongue comes in contact with the alveolar ridge in anticipation of the following lateral /l/. The lips then part, allowing the compressed air to escape between the lips (frame 2). The jaw and lips then open further, seen in the distance between the midpoints of the upper and lower lips, and the lips spread (increasing the distance between the corners of the mouth), for the close vowel /iy/ (frame 3–4). Since this is a relatively steady-state vowel, lip position remains unchanged for the rest of its duration (frames 4–8), where the attention level drops considerably. The jaw and the lips then close slightly, as the blade of the tongue needs to be brought close to the alveolar ridge, for /z/ (frames 9–10), where attention resumes. Lay is interesting since the bulk of frontally visible articulatory movement involves the blade of the tongue coming into contact with the alveolar ridge for /l/ (frames 2–6), and then going down for the vowel /ey/ (frames 7–9). That is exactly where most of LipNet’s attention is focused, as there is little change in lip position. 4.4.2 VISEMES According to DeLand (1931) and Fisher (1968), Alexander Graham Bell first hypothesised that multiple phonemes may be visually identical on a given speaker. This was later verified, giving rise to the concept of a viseme, a visual equivalent of a phoneme (Woodward & Barber, 1960; Fisher, 1968). For our analysis, we use the phoneme-to-viseme mapping of Neti et al. (2000), clustering the phonemes into the following categories: Lip-rounding based vowels (V), Alveolar-semivowels (A), Alveolar-fricatives (B), Alveolar (C), Palato-alveolar (D), Bilabial (E), Dental (F), Labio-dental (G), and Velar (H). The full mapping can be found in Table 4 in Appendix A. The GRID corpus contain 31 out of the 39 phonemes in ARPA",
    "output": "Lip reading is a tricky business. Test results vary, but on average, most people recognize just one in 10 words when watching someone’s lips, and the accuracy of self-proclaimed experts tends to vary — there are certainly no lip-reading savants. Now, though, some researchers claim that AI techniques like deep learning could help solve this problem. After all, AI methods that focus on crunching large amounts of data to find common patterns have helped improve audio speech recognition to near-human levels of accuracy, so why can’t the same be done for lip reading?\n\nMuch more accurate than humans, but working with very limited data\n\nThe researchers from the University of Oxford’s AI lab have made a promising — if crucially limited — contribution to the field, creating a new lip-reading program using deep learning. Their software, dubbed LipNet, was able to outperform experienced lip readers to a significant degree, achieving 93.4 percent accuracy in certain tests, compared to 52.3 percent accuracy from human lip readers. And even in its current, early stages, the software is extremely fast — processing silent video into text transcripts in nearly real time.\n\nHowever, before we get lost in bad dreams of AI-powered surveillance states and HAL reading lips in 2001: A Space Odyssey, the research from Oxford has some serious limitations. For a start, the system was trained and tested on a research dataset known as GRID. This is a collection of tens of thousands of short videos of 34 volunteers reading nonsense sentences, as well as captions. Each clip is just three seconds long, and each sentence follows the pattern: command, color, preposition, letter, digit, adverb. Sentences include, for example, “set blue by A four please,” and “place red at C zero again.” Even the words within these patterns are limited — there are just four different commands and colors used. This has led some researchers in the field to suggest that the url’s findings have been overblown, especially after one viral tweet urling to the researchers’ video (below) made the sensationalist claim that the work meant there would be “no more secrets.”\n\nThis is certainly not the case. Speaking to The Verge, two of the researchers behind the url, Yannis Assael and Brendan Shillingford, readily admitted they were working with “restricted vocabulary and grammar,” but said this was due to limitations in available data. “The dataset is small but it’s a good indication we could perform just as well with a much bigger dataset,” says Assael.\n\nReally, this won't help with surveillance at all\n\nBoth Assael and Shillingford are also keen to stress that their work has no application in the world of surveillance, simply because lip reading requires you to see the subject’s tongue — meaning that the video has to be straight on and well-lit to get a good result. “It’s technically impossible or at least very, very difficult,” to use any lip-reading software for surveillance says Assael, adding that frame rate is also a factor; and something that is usually neglected with CCTV. He says: “And if you do have frontal video of someone taken with a very good camera, then you probably have a directional microphone [pointed at them] as well!” (On the subject of surveillance, Assael notes that although one of the url’s supervisors also works with Google’s AI division DeepMind, Google itself had no involvement with LipNet’s development.)\n\nInstead, the two researchers think that lip-reading AI could help people with hearing disabilities, especially in noisy environments where it’s difficult for computers to isolate speech. For example, someone wearing a camera built into a pair of glasses could get clear, frontal footage of someone they’re talking to at a party, and an ancestor of LipNet could then transcribe the conversation in real time, feeding it into their ear. “Anywhere you have speech recognition and a camera, we can improve that,” says Assael. He also mentions silent dictation to Siri or Google Assistant as a possible use-case. In the future, then, perhaps those of us who don’t like speaking to our computers, can’t just have them read our lips instead."
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Method_chunk5",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Method\nbet. We compute confusion matrices between phonemes and then ao ah ae aa d iy ih ey eh ay aw n uw ow t uw aw ow ah aa eh ao ay ae ey ih iy n d t (a) Lip-rounding vowels (b) Alveolar Figure 3: Intra-viseme and inter-viseme confusion matrices, depicting the three categories with the most confusions, as well as the confusions between viseme clusters. Colours are row-normalised to emphasise the errors. group phonemes into viseme clusters, following Neti et al. (2000). Figure 3 shows the confusion matrices of the 3 most confused viseme categories, as well as the confusions between the viseme categories. The full phoneme confusion matrix is in Figure 4 in Appendix B. Given that the speakers are British, the confusion between /aa/ and /ay/ (Figure 3a) is most probably due to the fact that the first element, and the greater part, of the diphthong /ay/ is articulatorily identical with /aa/: an open back unrounded vowel (Ferragne & Pellegrino, 2010). The confusion of /ih/ (a rather close vowel) and /ae/ (a very open vowel) is at first glance surprising, but in fact in the sample /ae/ occurs only in the word at, which is a function word normally pronounced with a reduced, weak vowel /ah/. /ah/ and /ih/ are the most frequent unstressed vowels and there is a good deal of variation within and between them, e.g. private and watches (Cruttenden, 2014). The confusion within the categories of bilabial stops /p b m/ and alveolar stops /t d n/ (Figures 3b-c) is unsurprising: complete closure at the same place of articulation makes them look practically identical. The differences of velum action and vocal fold vibration are unobservable from the front. Finally, the quality of the viseme categorisation of Neti et al. (2000) is confirmed by the fact that the matrix in Figure 3d is diagonal, with only minor confusion between alveolar (C) and palatoalveolar (D) visemes. Articulatorily, alveolar /s z/ and palato-alveolar /sh zh/ fricatives are distinguished by only a small difference in tongue position: against the palate just behind the alveolar ridge, which is not easily observed from the front. The same can be said about dental /th/ and alveolar /t/.",
    "output": "Lip reading is a tricky business. Test results vary, but on average, most people recognize just one in 10 words when watching someone’s lips, and the accuracy of self-proclaimed experts tends to vary — there are certainly no lip-reading savants. Now, though, some researchers claim that AI techniques like deep learning could help solve this problem. After all, AI methods that focus on crunching large amounts of data to find common patterns have helped improve audio speech recognition to near-human levels of accuracy, so why can’t the same be done for lip reading?\n\nMuch more accurate than humans, but working with very limited data\n\nThe researchers from the University of Oxford’s AI lab have made a promising — if crucially limited — contribution to the field, creating a new lip-reading program using deep learning. Their software, dubbed LipNet, was able to outperform experienced lip readers to a significant degree, achieving 93.4 percent accuracy in certain tests, compared to 52.3 percent accuracy from human lip readers. And even in its current, early stages, the software is extremely fast — processing silent video into text transcripts in nearly real time.\n\nHowever, before we get lost in bad dreams of AI-powered surveillance states and HAL reading lips in 2001: A Space Odyssey, the research from Oxford has some serious limitations. For a start, the system was trained and tested on a research dataset known as GRID. This is a collection of tens of thousands of short videos of 34 volunteers reading nonsense sentences, as well as captions. Each clip is just three seconds long, and each sentence follows the pattern: command, color, preposition, letter, digit, adverb. Sentences include, for example, “set blue by A four please,” and “place red at C zero again.” Even the words within these patterns are limited — there are just four different commands and colors used. This has led some researchers in the field to suggest that the url’s findings have been overblown, especially after one viral tweet urling to the researchers’ video (below) made the sensationalist claim that the work meant there would be “no more secrets.”\n\nThis is certainly not the case. Speaking to The Verge, two of the researchers behind the url, Yannis Assael and Brendan Shillingford, readily admitted they were working with “restricted vocabulary and grammar,” but said this was due to limitations in available data. “The dataset is small but it’s a good indication we could perform just as well with a much bigger dataset,” says Assael.\n\nReally, this won't help with surveillance at all\n\nBoth Assael and Shillingford are also keen to stress that their work has no application in the world of surveillance, simply because lip reading requires you to see the subject’s tongue — meaning that the video has to be straight on and well-lit to get a good result. “It’s technically impossible or at least very, very difficult,” to use any lip-reading software for surveillance says Assael, adding that frame rate is also a factor; and something that is usually neglected with CCTV. He says: “And if you do have frontal video of someone taken with a very good camera, then you probably have a directional microphone [pointed at them] as well!” (On the subject of surveillance, Assael notes that although one of the url’s supervisors also works with Google’s AI division DeepMind, Google itself had no involvement with LipNet’s development.)\n\nInstead, the two researchers think that lip-reading AI could help people with hearing disabilities, especially in noisy environments where it’s difficult for computers to isolate speech. For example, someone wearing a camera built into a pair of glasses could get clear, frontal footage of someone they’re talking to at a party, and an ancestor of LipNet could then transcribe the conversation in real time, feeding it into their ear. “Anywhere you have speech recognition and a camera, we can improve that,” says Assael. He also mentions silent dictation to Siri or Google Assistant as a possible use-case. In the future, then, perhaps those of us who don’t like speaking to our computers, can’t just have them read our lips instead."
  },
  {
    "id_paper": "28",
    "id_story": "28-02",
    "id_chunk": "28-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Student, Journalist\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 5 CONCLUSIONS We proposed LipNet, the first model to apply deep learning to end-to-end learning of a model thatmaps sequences of image frames of a speaker’s mouth to entire sentences. The end-to-end model eliminates the need to segment videos into words before predicting a sentence. LipNet requires neither hand-engineered spatiotemporal visual features nor a separately-trained sequence model. Our empirical evaluation illustrates the importance of spatiotemporal feature extraction and efficient temporal aggregation, confirming the intuition of Easton & Basala (1982). Furthermore, LipNet greatly outperforms a human lipreading baseline, exhibiting 4.1 better performance, and 4.8% WER which is 2.8 lower than the word-level state-of-the-art (Gergen et al., 2016) in the GRID corpus. While LipNet is already an empirical success, the deep speech recognition literature (Amodei et al., 2015) suggests that performance will only improve with more data. In future work, we hope to demonstrate this by applying LipNet to larger datasets, such as a sentence-level variant of that collected by Chung & Zisserman (2016a). Some applications, such as silent dictation, demand the use of video only. However, to extend the range of potential applications of LipNet, we aim to apply this approach to a jointly trained audiovisual speech recognition model, where visual input assists with robustness in noisy environments.",
    "output": "This is certainly not the case. Speaking to The Verge, two of the researchers behind the url, Yannis Assael and Brendan Shillingford, readily admitted they were working with “restricted vocabulary and grammar,” but said this was due to limitations in available data. “The dataset is small but it’s a good indication we could perform just as well with a much bigger dataset,” says Assael."
  },
  {
    "id_paper": "13",
    "id_story": "13-02",
    "id_chunk": "13-02_Abstract_chunk1",
    "section": "Abstract",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Abstract\nAbstract: Face2Face: Real-time Face Capture and Reenactment of RGB Videos Justus Thies1 Michael Zollh ofer2 Marc Stamminger1 Christian Theobalt2 Matthias Nießner3 Proposed online reenactment setup: a monocular target video sequence (e.g., from Youtube) is reenacted based on the expressions of a source actor who is recorded live with a commodity webcam. Abstract We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion. To this end, we first address the under-constrained problem of facial identity recovery from monocular video by non-rigid model-based bundling. At run time, we track facial expressions of both source and target video using a dense photometric consistency measure. Reenactment is then achieved by fast and efficient deformation transfer between source and target. The mouth interior that best matches the re-targeted expression is retrieved from the target sequence and warped to produce an accurate fit. Finally, we convincingly re-render the synthesized target face on top of the corresponding video stream such that it seamlessly blends with the real-world illumination. We demonstrate our method in a live setup, where Youtube videos are reenacted in real time.",
    "output": "The short version: take a YouTube video of someone speaking like, say, George W. Bush. Use a standard RGB webcam to capture a video of someone else emoting and saying something entirely different. Throw both videos into the Face2Face system and, bam, you’ve now got a relatively believable video of George W. Bush’s face — now almost entirely synthesized — doing whatever the actor in the second video wanted the target’s face to do. It even tries to work out what the interior of their mouth should look like as they’re speaking."
  },
  {
    "id_paper": "13",
    "id_story": "13-02",
    "id_chunk": "13-02_Introduction_chunk1",
    "section": "Introduction",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Introduction\nIntroduction: 1. Introduction on RGB [8, 6] as well as RGB-D data [31, 10, 21, 4, 16]. These techniques have become increasingly popular for the animation of virtual CG avatars in video games and movies. It is now feasible to run these face capture and tracking algorithms from home, which is the foundation for many VR and AR applications, such as teleconferencing. In this paper, we employ a new dense markerless facial performance capture method based on monocular RGB data, similar to state-of-the-art methods. However, instead of transferring facial expressions to virtual CG characters, our main contribution is monocular facial reenactment in real-time. In contrast to previous reenactment approaches that run offline [5, 11, 13], our goal is the online transfer of facial expressions of a source actor captured by an RGB sensor to a target actor. The target sequence can be any monocular video; e.g., legacy video footage downloaded from Youtube with a facial performance. We aim to modify the target video in a photo-realistic fashion, such that it is virtually impossible to notice the manipulations. Faithful photo-realistic facial reenactment is the foundation for a variety of applications; for instance, in video conferencing, the video feed can be adapted to match the face motion of a translator, or face videos can be convincingly dubbed to a foreign language. In our method, we first reconstruct the shape identity of the target actor using a new global non-rigid modelbased bundling approach based on a prerecorded training sequence. As this preprocess is performed globally on a set of training frames, we can resolve geometric ambiguities common to monocular reconstruction. At runtime, we track both the expressions of the source and target actor’s video by a dense analysis-by-synthesis approach based on a statistical facial prior. We demonstrate that our RGB tracking accuracy is on par with the state of the art, even with online tracking methods relying on depth data. In order to transfer expressions from the source to the target actor in realtime, we propose a novel transfer functions that efficiently applies deformation transfer [27] directly in the used lowdimensional expression space. For final image synthesis, we re-render the target’s face with transferred expression coefficients and composite it with the target video’s background under consideration of the estimated environment lighting. Finally, we introduce a new image-based mouth synthesis approach that generates a realistic mouth interior by retrieving and warping best matching mouth shapes from the offline sample sequence. It is important to note that we maintain the appearance of the target mouth shape; in contrast, existing methods either copy the source mouth region onto the target [30, 11] or a generic teeth proxy is rendered [14, 29], both of which leads to inconsistent results. Fig. 1 shows an overview of our method. We demonstrate highly-convincing transfer of facial expressions from a source to a target video in real time. We show results with a live setup where a source video stream, which is captured by a webcam, is used to manipulate a target Youtube video. In addition, we compare against stateof-the-art reenactment methods, which we outperform both in terms of resulting video quality and runtime (we are the first real-time RGB reenactment method). In summary, our key contributions are: • dense, global non-rigid model-based bundling, • accurate tracking, appearance, and lighting estimation in unconstrained live RGB video, • person-dependent expression transfer using subspace deformations, • and a novel mouth synthesis approach.",
    "output": "Once upon a time, a photo of something was enough to believe it was real. Sure, you’d have to deal with the occasional Big Foot hoax, but for the most part, those who had the time or talent to create believable fakes were in the minority.\nThen came the age of Photoshop. Edits and fakes are prolific enough that “FAKE!” has become the default; a dubious photo is presumed fake unless proven otherwise.\nIt’s not pixel-perfect yet — even in the relatively low-res clips we’re shown, there’s an uncanny valley effect of something being not quite right. But hot damn is it impressive (and, well, more than a little spooky) even in this early stage."
  },
  {
    "id_paper": "13",
    "id_story": "13-02",
    "id_chunk": "13-02_Method_chunk1",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Method\nMethod: 3. Synthesis of Facial Imagery We use a multi-linear PCA model based on [3, 1, 9]. The first two dimensions represent facial identity – i.e., geometric shape and skin reflectance – and the third dimension controls the facial expression. Hence, we parametrize a face as: This prior assumes a multivariate normal probability distribution of shape and reflectance around the average shape aid R3n and reflectance aalb R3n. The shape Eid R3n80, reflectance Ealb R3n80, and expression Eexp R3n76 basis and the corresponding standard deviations id R80, alb R80, and exp R76 are given. The model has 53K vertices and 106K faces. A synthesized image CS is generated through rasterization of the model under a rigid model transformation (v) and the full perspective transformation (v). Illumination is approximated by the first three bands of Spherical Harmonics (SH) [23] basis functions, assuming Labertian surfaces and smooth distant illumination, neglecting self-shadowing. Synthesis is dependent on the face model parameters , , , the illumination parameters , the rigid transformation R, t, and the camera parameters  defining . The vector of unknowns P is the union of these parameters. 4. Energy Formulation Given a monocular input sequence, we reconstruct all unknown parameters P jointly with a robust variational optimization. The proposed objective is highly non-linear in the unknowns and has the following components: + wregEreg(P) | z  prior E(P)=wcolEcol(P) + wlanElan(P) | z  data . (3) The data term measures the similarity between the synthesized imagery and the input data in terms of photoconsistency Ecol and facial feature alignment Elan. The likelihood of a given parameter vector P is taken into account by the statistical regularizer Ereg. The weights wcol, wlan, and wreg balance the three different sub-objectives. In all of our experiments, we set wcol = 1, wlan = 10, and wreg = 2.5  105. In the following, we introduce the different sub-objectives. Photo-Consistency In order to quantify how well the input data is explained by a synthesized image, we measure the photo-metric alignment error on pixel level: Ecol(P) = 1 X pV CS(p) CI(p)2 , (4) |V| where CS is the synthesized image, CI is the input RGB image, and p V denote all visible pixel positions in CS. We use the l2,1-norm [12] instead of a least-squares formulation to be robust against outliers. In our scenario, distance in color space is based on l2, while in the summation over all pixels an l1-norm is used to enforce sparsity. tected in the RGB stream: Elan(P) = 1 |F| X f jF wconf,j f j ((vj)2 2 . (5) To this end, we employ a state-of-the-art facial landmark tracking algorithm by [24]. Each feature point f j F  R2 comes with a detection confidence wconf,j and corresponds to a unique vertex vj = Mgeo(, ) R3 of our face prior. This helps avoiding local minima in the highlycomplex energy landscape of Ecol(P). Statistical Regularization We enforce plausibility of the synthesized faces based on the assumption of a normal distributed population. To this end, we enforce the parameters to stay statistically close to the mean: 2 + i alb,i 2 + i exp,i 2 . (6) i 76 X 80 X Ereg(P) = id,i i=1 i=1 This commonly-used regularization strategy prevents degenerations of the facial geometry and reflectance, and guides the optimization strategy out of local minima [3]. 5. Data",
    "output": "Once upon a time, a photo of something was enough to believe it was real. Sure, you’d have to deal with the occasional Big Foot hoax, but for the most part, those who had the time or talent to create believable fakes were in the minority.\n\nThen came the age of Photoshop. Edits and fakes are prolific enough that “FAKE!” has become the default; a dubious photo is presumed fake unless proven otherwise.\n\nWe’re not quite to that point with video. Fake videos exist in droves, obviously, but editing a video to be something it’s not introduces a bevy of challenges not found in the editing of a single still frame, and generally requires considerably more time and talent to do right. People will still yell “FAKE!” but it’ll be a quieter yell. As your Facebook feed probably proves, moderately well-faked videos have a much easier time finding believers.\n\nThat might not be the case for long.\n\nThe video up top shows a work-in-progress system called Face2Face (research url here) being built by researchers at Stanford, the Max Planck Institute and the University of Erlangen-Nuremberg.\n\nThe short version: take a YouTube video of someone speaking like, say, George W. Bush. Use a standard RGB webcam to capture a video of someone else emoting and saying something entirely different. Throw both videos into the Face2Face system and, bam, you’ve now got a relatively believable video of George W. Bush’s face — now almost entirely synthesized — doing whatever the actor in the second video wanted the target’s face to do. It even tries to work out what the interior of their mouth should look like as they’re speaking.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nIt’s not pixel-perfect yet — even in the relatively low-res clips we’re shown, there’s an uncanny valley effect of something being not quite right. But hot damn is it impressive (and, well, more than a little spooky) even in this early stage.\n\nWhy spooky? Technology like this will serve to make video less inherently believable. The video’s use of politicians as the editing target is pretty self-aware. In that regard, political hoaxes will hit a lot harder when it’s a video instead of a ‘shopped picture being forwarded around.\n\nDon’t freak out too hard, though. Hoaxes have existed in every medium throughout history. This tech isn’t widely available beyond its researchers just yet; the uncanny valley challenges in stepping from “somethings-kinda-off” to pixel-perfect infallibility aren’t small ones. Just remember that, just like photos before it, being on video doesn’t mean it’s real — and that gets a lil’ bit truer each day."
  },
  {
    "id_paper": "13",
    "id_story": "13-02",
    "id_chunk": "13-02_Method_chunk2",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Method\n-parallel Optimization Strategy The proposed robust tracking objective is a general unconstrained non-linear optimization problem. We minimize this objective in real-time using a novel data-parallel GPUbased Iteratively Reweighted Least Squares (IRLS) solver. The key idea of IRLS is to transform the problem, in each iteration, to a non-linear least-squares problem by splitting the norm in two components: ||r(P)||2 = (||r(Pold)||2)1 | z  constant  ||r(P)||2 2 . Here, r() is a general residual and Pold is the solution computed in the last iteration. Thus, the first part is kept constant during one iteration and updated afterwards. Close in spirit to [29], each single iteration step is implemented using the Gauss-Newton approach. We take a single GN step in every IRLS iteration and solve the corresponding system of normal equations JT J= JT F based on PCG to obtain an optimal linear parameter update . The Jacobian J and the systems’ right hand side JT F are precomputed and stored in device memory for later processing as proposed by Thies et al. [29]. As suggested by [33, 29], we split up the multiplication of the old descent direction d with the system matrix JT J in the PCG solver into two successive matrixvector products. Additional details regarding the optimization framework are provided in the supplemental material. 6. Non-Rigid Model-Based Bundling on the proposed objective, we jointly estimate all parameters over k key-frames of the input video sequence. The estimated unknowns are the global identity ,  and intrinsics  as well as the unknown per-frame pose k, Rk, tkk and illumination parameters kk. We use a similar data-parallel optimization strategy as proposed for model-to-frame tracking, but jointly solve the normal equations for the entire keyframe set. For our non-rigid modelbased bundling problem, the non-zero structure of the corresponding Jacobian is block dense. Our PCG solver exploits the non-zero structure for increased performance (see additional document). Since all keyframes observe the same face identity under potentially varying illumination, expression, and viewing angle, we can robustly separate identity from all other problem dimensions. Note that we also solve for the intrinsic camera parameters of , thus being able to process uncalibrated video footage. 7. Expression Transfer To transfer the expression changes from the source to the target actor while preserving person-specificness in each actor’s expressions, we propose a sub-space deformation transfer technique. We are inspired by the deformation transfer energy of Sumner et al. [27], but operate directly in the space spanned by the expression blendshapes. This not only allows for the precomputation of the pseudo-inverse of the system matrix, but also drastically reduces the dimensionality of the optimization problem allowing for fast real-time transfer rates. Assuming source identity S and target identity T fixed, transfer takes as input the neutral S N, deformed source S, and the neutral target T N expression. Output is the transferred facial expression T directly in the reduced sub-space of the parametric prior. As proposed by [27], we first compute the source deformation gradients Ai R33 that transform the source triangles from neutral to deformed. The deformed target vi = M i(T , T ) is then found based on the undeformed state vi = M i(T , T N) by solving a linear least-squares problem. Let (i0, i1, i2) be the vertex indices of the i-th triangle, V = [vi1 vi0, vi2 vi0] and V = [vi1 vi0, vi2 vi0], then the optimal unknown target deformation T is the minimizer of: |F | X AiV V 2 E(T ) = F . (7) i=1 This problem can be rewritten in the canonical least-squares form by substitution: E(T ) = AT b 2 2",
    "output": "Once upon a time, a photo of something was enough to believe it was real. Sure, you’d have to deal with the occasional Big Foot hoax, but for the most part, those who had the time or talent to create believable fakes were in the minority.\n\nThen came the age of Photoshop. Edits and fakes are prolific enough that “FAKE!” has become the default; a dubious photo is presumed fake unless proven otherwise.\n\nWe’re not quite to that point with video. Fake videos exist in droves, obviously, but editing a video to be something it’s not introduces a bevy of challenges not found in the editing of a single still frame, and generally requires considerably more time and talent to do right. People will still yell “FAKE!” but it’ll be a quieter yell. As your Facebook feed probably proves, moderately well-faked videos have a much easier time finding believers.\n\nThat might not be the case for long.\n\nThe video up top shows a work-in-progress system called Face2Face (research url here) being built by researchers at Stanford, the Max Planck Institute and the University of Erlangen-Nuremberg.\n\nThe short version: take a YouTube video of someone speaking like, say, George W. Bush. Use a standard RGB webcam to capture a video of someone else emoting and saying something entirely different. Throw both videos into the Face2Face system and, bam, you’ve now got a relatively believable video of George W. Bush’s face — now almost entirely synthesized — doing whatever the actor in the second video wanted the target’s face to do. It even tries to work out what the interior of their mouth should look like as they’re speaking.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nIt’s not pixel-perfect yet — even in the relatively low-res clips we’re shown, there’s an uncanny valley effect of something being not quite right. But hot damn is it impressive (and, well, more than a little spooky) even in this early stage.\n\nWhy spooky? Technology like this will serve to make video less inherently believable. The video’s use of politicians as the editing target is pretty self-aware. In that regard, political hoaxes will hit a lot harder when it’s a video instead of a ‘shopped picture being forwarded around.\n\nDon’t freak out too hard, though. Hoaxes have existed in every medium throughout history. This tech isn’t widely available beyond its researchers just yet; the uncanny valley challenges in stepping from “somethings-kinda-off” to pixel-perfect infallibility aren’t small ones. Just remember that, just like photos before it, being on video doesn’t mean it’s real — and that gets a lil’ bit truer each day."
  },
  {
    "id_paper": "13",
    "id_story": "13-02",
    "id_chunk": "13-02_Method_chunk3",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Method\n. (8) Figure 2: Mouth Retrieval: we use an appearance graph to retrieve new mouth frames. In order to select a frame, we enforce similarity to the previously-retrieved frame while minimizing the distance to the target expression. expression is included in the right-hand side b R6|F |. b varies with S and is computed on the GPU for each new input frame. The minimizer of the quadratic energy can be computed by solving the corresponding normal equations. Since the system matrix is constant, we can precompute its Pseudo Inverse using a Singular Value Decomposition (SVD). Later, the small 76  76 linear system is solved in real-time. No additional smoothness term as in [27, 4] is needed, since the blendshape model implicitly restricts the result to plausible shapes and guarantees smoothness. 8. Mouth Retrieval For a given transferred facial expression, we need to synthesize a realistic target mouth region. To this end, we retrieve and warp the best matching mouth image from the target actor sequence. We assume that sufficient mouth variation is available in the target video. It is also important to note that we maintain the appearance of the target mouth. This leads to much more realistic results than either copying the source mouth region [30, 11] or using a generic 3D teeth proxy [14, 29]. Our approach first finds the best fitting target mouth frame based on a frame-to-cluster matching strategy with a novel feature similarity metric. To enforce temporal coherence, we use a dense appearance graph to find a compromise between the last retrieved mouth frame and the target mouth frame (cf. Fig. 2). We detail all steps in the following. Similarity Metric Our similarity metric is based on geometric and photometric features. The used descriptor K = R, , F, L of a frame is composed of the rotation R, expression parameters , landmarks F, and a Local Binary Pattern (LBP) L. We compute these descriptors KS for every frame in the training sequence. The target descriptor KT consists of the result of the expression transfer and the LBP of the frame of the driving actor. We measure the distance between a source and a target descriptor as follows: The first term Dp measures the distance in parameter space: Dp(KT , KS t ) = T S t 2 2 + RT RS t 2 F . The second term Dm measures the differential compatibility of the sparse facial landmarks:  FT i FT j 2 FS t,i FS t,j2 2 . Dm(KT , KS t ) = X (i,j) Here, is a set of predefined landmark pairs, defining distances such as between the upper and lower lip or between the left and right corner of the mouth. The last term Da is an appearance measurement term composed of two parts: Da(KT , KS t , t) = Dl(KT , KS t ) + wc(KT , KS t )Dc(, t) .  is the last retrieved frame index used for the reenactment in the previous frame. Dl(KT , KS t ) measures the similarity based on LBPs that are compared via a Chi Squared Distance (for details see [13]). Dc(, t) measures the similarity between the last retrieved frame  and the video frame t based on RGB cross-correlation of the normalized mouth frames. Note that the mouth frames are normalized based on the models texture parameterization (cf. Fig. 2). To facilitate fast frame jumps for expression changes, we incorporate the weight wc(KT , KS t ) = e(Dm(KT ,KS t ))2. We apply this frame-to-frame distance measure in a frame-tocluster matching strategy, which enables real-time rates and mitigates high-frequency jumps between mouth frames. Frame-to-Cluster Matching Utilizing the proposed similarity metric, we cluster the target actor sequence into k = 10 clusters using a modified k-means algorithm that is based on the pairwise distance function D",
    "output": "Once upon a time, a photo of something was enough to believe it was real. Sure, you’d have to deal with the occasional Big Foot hoax, but for the most part, those who had the time or talent to create believable fakes were in the minority.\n\nThen came the age of Photoshop. Edits and fakes are prolific enough that “FAKE!” has become the default; a dubious photo is presumed fake unless proven otherwise.\n\nWe’re not quite to that point with video. Fake videos exist in droves, obviously, but editing a video to be something it’s not introduces a bevy of challenges not found in the editing of a single still frame, and generally requires considerably more time and talent to do right. People will still yell “FAKE!” but it’ll be a quieter yell. As your Facebook feed probably proves, moderately well-faked videos have a much easier time finding believers.\n\nThat might not be the case for long.\n\nThe video up top shows a work-in-progress system called Face2Face (research url here) being built by researchers at Stanford, the Max Planck Institute and the University of Erlangen-Nuremberg.\n\nThe short version: take a YouTube video of someone speaking like, say, George W. Bush. Use a standard RGB webcam to capture a video of someone else emoting and saying something entirely different. Throw both videos into the Face2Face system and, bam, you’ve now got a relatively believable video of George W. Bush’s face — now almost entirely synthesized — doing whatever the actor in the second video wanted the target’s face to do. It even tries to work out what the interior of their mouth should look like as they’re speaking.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nIt’s not pixel-perfect yet — even in the relatively low-res clips we’re shown, there’s an uncanny valley effect of something being not quite right. But hot damn is it impressive (and, well, more than a little spooky) even in this early stage.\n\nWhy spooky? Technology like this will serve to make video less inherently believable. The video’s use of politicians as the editing target is pretty self-aware. In that regard, political hoaxes will hit a lot harder when it’s a video instead of a ‘shopped picture being forwarded around.\n\nDon’t freak out too hard, though. Hoaxes have existed in every medium throughout history. This tech isn’t widely available beyond its researchers just yet; the uncanny valley challenges in stepping from “somethings-kinda-off” to pixel-perfect infallibility aren’t small ones. Just remember that, just like photos before it, being on video doesn’t mean it’s real — and that gets a lil’ bit truer each day."
  },
  {
    "id_paper": "13",
    "id_story": "13-02",
    "id_chunk": "13-02_Method_chunk4",
    "section": "Method",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Method\n. For every cluster, we select the frame with the minimal distance to all other frames within that cluster as a representative. During runtime, we measure the distances between the target descriptor KT and the descriptors of cluster representatives, and choose the cluster whose representative frame has the minimal distance as the new target frame. Appearance Graph We improve temporal coherence by building a fully-connected appearance graph of all video frames. The edge weights are based on the RGB crosscorrelation between the normalized mouth frames, the distance in parameter space Dp, and the distance of the landmarks Dm. The graph enables us to find an inbetween frame that is both similar to the last retrieved frame and the retrieved target frame (see Fig. 2). We compute this perfect match by finding the frame of the training sequence that minimizes the sum of the edge weights to the last retrieved and current target frame. We blend between the previouslyretrieved frame and the newly-retrieved frame in texture CPU GPU FPS SparseFT MouthRT DenseFT DeformTF Synth 5.97ms 1.90ms 22.06ms 3.98ms 10.19ms 27.6Hz 4.85ms 1.50ms 21.27ms 4.01ms 10.31ms 28.1Hz 5.57ms 1.78ms 20.97ms 3.95ms 10.32ms 28.4Hz Table 1: Avg. run times for the three sequences of Fig. 8, from top to bottom. Standard deviations w.r.t. the final frame rate are 0.51, 0.56, and 0.59 fps, respectively. Note that CPU and GPU stages run in parallel. space on a pixel level after optic flow alignment. Before blending, we apply an illumination correction that considers the estimated Spherical Harmonic illumination parameters of the retrieved frames and the current video frame. Finally, we composite the new output frame by alpha blending between the original video frame, the illumination-corrected, projected mouth frame, and the rendered face model.",
    "output": "Once upon a time, a photo of something was enough to believe it was real. Sure, you’d have to deal with the occasional Big Foot hoax, but for the most part, those who had the time or talent to create believable fakes were in the minority.\n\nThen came the age of Photoshop. Edits and fakes are prolific enough that “FAKE!” has become the default; a dubious photo is presumed fake unless proven otherwise.\n\nWe’re not quite to that point with video. Fake videos exist in droves, obviously, but editing a video to be something it’s not introduces a bevy of challenges not found in the editing of a single still frame, and generally requires considerably more time and talent to do right. People will still yell “FAKE!” but it’ll be a quieter yell. As your Facebook feed probably proves, moderately well-faked videos have a much easier time finding believers.\n\nThat might not be the case for long.\n\nThe video up top shows a work-in-progress system called Face2Face (research url here) being built by researchers at Stanford, the Max Planck Institute and the University of Erlangen-Nuremberg.\n\nThe short version: take a YouTube video of someone speaking like, say, George W. Bush. Use a standard RGB webcam to capture a video of someone else emoting and saying something entirely different. Throw both videos into the Face2Face system and, bam, you’ve now got a relatively believable video of George W. Bush’s face — now almost entirely synthesized — doing whatever the actor in the second video wanted the target’s face to do. It even tries to work out what the interior of their mouth should look like as they’re speaking.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nIt’s not pixel-perfect yet — even in the relatively low-res clips we’re shown, there’s an uncanny valley effect of something being not quite right. But hot damn is it impressive (and, well, more than a little spooky) even in this early stage.\n\nWhy spooky? Technology like this will serve to make video less inherently believable. The video’s use of politicians as the editing target is pretty self-aware. In that regard, political hoaxes will hit a lot harder when it’s a video instead of a ‘shopped picture being forwarded around.\n\nDon’t freak out too hard, though. Hoaxes have existed in every medium throughout history. This tech isn’t widely available beyond its researchers just yet; the uncanny valley challenges in stepping from “somethings-kinda-off” to pixel-perfect infallibility aren’t small ones. Just remember that, just like photos before it, being on video doesn’t mean it’s real — and that gets a lil’ bit truer each day."
  },
  {
    "id_paper": "13",
    "id_story": "13-02",
    "id_chunk": "13-02_Experiments_/_Results_chunk1",
    "section": "Experiments / Results",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Experiments / Results\nExperiments / Results: 9. Results Live Reenactment Setup Our live reenactment setup consists of standard consumer-level hardware. We capture a live video with a commodity webcam (source), and download monocular video clips from Youtube (target). In our experiments, we use a Logitech HD Pro C920 camera running at 30Hz in a resolution of 640  480; although our approach is applicable to any consumer RGB camera. Overall, we show highly-realistic reenactment examples of our algorithm on a variety of target Youtube videos at a resolution of 1280720. The videos show different subjects in different scenes filmed from varying camera angles; each video is reenacted by several volunteers as source actors. Reenactment results are generated at a resolution of 1280  720. We show real-time reenactment results in Fig. 8 and in the accompanying video. Runtime For all experiments, we use three hierarchy levels for tracking (source and target). In pose optimization, we only consider the second and third level, where we run one and seven Gauss-Newton steps, respectively. Within a Gauss-Newton step, we always run four PCG steps. In addition to tracking, our reenactment pipeline has additional stages whose timings are listed in Table 1. Our method runs in real-time on a commodity desktop computer with an NVIDIA Titan X and an Intel Core i7-4770. Tracking Comparison to Previous Work Face tracking alone is not the main focus of our work, but the following comparisons show that our tracking is on par with or exceeds the state of the art. Shi et al. 2014 [26]: They capture face performances offline from monocular unconstrained RGB video. The closeups in Fig. 4 show that our online approach yields a closer Figure 3: Comparison of our RGB tracking to Cao et al. [7], and to RGB-D tracking by Thies et al. [29]. Figure 4: Comparison of our tracking to Shi et al. [26]. From left to right: RGB input, reconstructed model, overlay with input, close-ups on eye and cheek. Note that Shi et al. perform shape-from-shading in a post process. face fit, particularly visible at the silhouette of the input face. We believe that our new dense non-rigid bundle adjustment leads to a better shape identity estimate than their sparse approach. Cao et al. 2014 [7]: They capture face performance from monocular RGB in real-time. In most cases, our and their method produce similar high-quality results (see Fig. 3); our identity and expression estimates are slightly more accurate though. Thies et al. 2015 [29]: Their approach captures face performance in real-time from RGB-D, Fig. 3. Results of both approaches are similarly accurate; but our approach does not require depth data. Figure 5: Comparison against FaceShift RGB-D tracking. Figure 6: Dubbing: Comparison to Garrido et al. [14]. FaceShift 2014: We compare our tracker to the commercial real-time RGB-D tracker from FaceShift, which is based on the work of Weise et al. [31]. Fig. 5 shows that we obtain similar results from RGB only. Reenactment Evaluation In Fig. 6, we compare our approach against state-of-the art reenactment by Garrido et al. [14]. Both methods provide highly-realistic reenactment results; however, their method is fundamentally offline, as they require all frames of a sequence to be present at any time. In addition, they rely on a generic geometric teeth proxy which in some frames makes reenactment less convincing. In Fig. 7, we compare against the work by Thies et al. [29]. Runtime and visual quality are similar for both approaches; however, their geometric teeth proxy leads to undesired appearance changes in the reenacted mouth. Moreover, Thies et al. use an RGB-D camera, which limits the application range; they cannot reenact Youtube videos. We show additional comparisons in the supplemental material against Dale et al. [11] and Garrido et al. [13].",
    "output": "Techcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW"
  },
  {
    "id_paper": "13",
    "id_story": "13-02",
    "id_chunk": "13-02_Conclusion_/_Future_Work_chunk1",
    "section": "Conclusion / Future Work",
    "input": "Generate a story for these personas: General Public, Investor, Journalist, Developer\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 11. Conclusion The presented approach is the first real-time facial reenactment system that requires just monocular RGB input. Our live setup enables the animation of legacy video footage – e.g., from Youtube – in real time. Overall, we believe our system will pave the way for many new and exciting applications in the fields of VR/AR, teleconferencing, or on-thefly dubbing of videos with translated audio.",
    "output": "We’re not quite to that point with video. Fake videos exist in droves, obviously, but editing a video to be something it’s not introduces a bevy of challenges not found in the editing of a single still frame, and generally requires considerably more time and talent to do right. People will still yell “FAKE!” but it’ll be a quieter yell. As your Facebook feed probably proves, moderately well-faked videos have a much easier time finding believers.\nWhy spooky? Technology like this will serve to make video less inherently believable. The video’s use of politicians as the editing target is pretty self-aware. In that regard, political hoaxes will hit a lot harder when it’s a video instead of a ‘shopped picture being forwarded around.\nDon’t freak out too hard, though. Hoaxes have existed in every medium throughout history. This tech isn’t widely available beyond its researchers just yet; the uncanny valley challenges in stepping from “somethings-kinda-off” to pixel-perfect infallibility aren’t small ones. Just remember that, just like photos before it, being on video doesn’t mean it’s real — and that gets a lil’ bit truer each day."
  }
]