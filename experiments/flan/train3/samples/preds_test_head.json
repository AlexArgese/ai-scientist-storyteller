{"idx": 0, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech", "prediction": "A speech synthesis architecture for text-to-speech is proposed.", "reference": "Generation of linguistic features (phonemes, duration) that are extracted from the text. Generation of features that represent corresponding extracted speech signals (cepstrum, frequency, linear spectrogram, Mel spectrogram).\nDeep neural networks are powerful tools that can approximate an arbitrarily complex function – bring some space of input data X into output space Y. In the context of our task, X and Y will be text and audio recordings with speech, respectively.\nWe chose the LJSpeech dataset as a training data set, which contains 13’100 audio tracks, 2-10 seconds each, and a text file corresponding to each record (aka lyrics). Based on the above-mentioned transformations, the sound is encoded into the Mel-spectrogram. The text is then tokenized and turned into a sequence of integers. Additionally we normalized all texts, transcribed all numbers into words (“5” —> “five”), and decoded abbreviations (“Mrs. Robinson” —> “Misses Robinson”). So, after preprocessing, we received sets of arrays of numerical sequences and Mel-spectrograms recorded as .npy files.\nIn order to match all the dimensions in the tensor batches at the learning stage, we’ve added paddings to short sequences. For sequences in text format, 0 was reserved for padding, and for spectrograms, frames, the values of which were slightly lower than the minimum value of the spectrograms defined by us. This is recommended for separating these paddings from noise and silence.\nThe decoder output from each step will be the predicted spectrogram. However, this is not the end. To improve the spectrogram quality, it is passed through the PostNet module, which is a stack of 5 one-dimensional convolutional layers with 512 filters in each and a filter size of 5. Each layer (except the last) is followed by batch-normalization and tangent activation. To return to the spectrogram dimension, we skip the output of the PostNet through a fully-connected layer with 80 neurons and add the obtained data to the initial result of the decoder. We then receive the Mel-spectrogram generated from the text.\nWhile working with seq2seq-models you will face the problem of different lengths of sequences in the batch. It is simply solved by adding paddings – reserved characters (in case of encoder input data), or frames with specific values (in case of a decoder). However, you should properly apply the error function to the predicted and real spectrograms. We used a mask in the error function for reading the errors excluding paddings."}
{"idx": 1, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.", "prediction": "a neural vocoder that combines the best of the previous approaches, and a linguistic model that predicts mel spectrograms from a sequence of characters.", "reference": "Conventional Text-to-Speech Approaches\nThe task of computer speech synthesis has long been the focus of scientists and engineers. However, classic approaches do not synthesize speech indistinguishable from that of a human. That is why, here, as in many other areas, deep learning has come to the rescue.\nThis method is based on the pre-recording of short audio sequences, which are then combined to create coherent speech. It turns out very clean and clear but is absolutely devoid of emotion and tone components, so it sounds unnatural. This happens because it is impossible to get an audio recording of all possible words uttered in all possible combinations of emotions and prosody.\nThe use of the concatenated TTS method is limited due to a large amount of data and enormous development time required. Therefore, a statistical parametric speech synthesis method, which explores the very nature of the data, was developed. It generates speech by combining certain parameters such as frequency, amplitude spectrum, etc.\nParametric synthesis consists of two stages:\nThese manually-configured parameters, along with linguistic features, are transmitted to the vocoder model, which performs many complex transformations to generate a sound wave. In this case, the vocoder evaluates speech parameters such as phase, prosody, tone, and others.\nIf we can approximate the parameters that define speech on each of its samples, then we can create a parametric model. Parametric synthesis requires less data and effort than concatenative systems.\nTheoretically, parametric synthesis is simple, but in practice, there are many artifacts that lead to the production of muffled speech with a buzzing sidetone, which sounds unnatural. This is happening because at each stage of the synthesis, we encode some features to achieve a realistic speech production. However, the selected data is based on our understanding of the speech, and in fact, human knowledge is not entirely comprehensive so the selected features will not necessarily be the best fit for any situation. Here is where deep learning takes the stage.\nF is a convolution operation,"}
{"idx": 2, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM", "prediction": "Feature Prediction Network with Attention consists of a sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation Mel-frequency spectrograms are related to the linear-frequency spectrogram, i.e., the short-time Fourier transform magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequencies details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation", "reference": "The input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results.\nAnother crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data:\nAttention defines the part of the encoder data that should be used at the current decoder step.\nThen the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet.\nAt each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step. Probably this correspondence will be direct. For example, if we have an input text sequence of 200 characters and the corresponding spectrogram of 800 frames, then there will be 4 frames per character. However, speech generated on the basis of such a spectrogram would sound unnatural. We say some words faster, others slower, sometimes we make pauses, and sometimes we don’t. It is impossible to foresee all possible contexts. That is why attention is a key element of the entire system: it sets the correspondence between the decoder pitch and the information from the encoder. In this way we obtain the information necessary to generate a specific frame. The greater value of the attention weight, the more “attention” should be “paid” to a respective part of the encoder data when generating a spectrogram frame.\nAt the training stage, not only the visual assessment of the quality of spectrograms and attention is important, but also the generated audio. However, using WaveNet as a vocoder during the training phase is an impermissible luxury in terms of time. Therefore, we used the Griffin-Lim algorithm, which enables a partial restore of the signal after fast Fourier transforms. Why partial? The fact is, while converting a signal into spectrograms, we lose information about the phase. However, the quality of audio obtained in this way is enough to understand if we are moving in the right direction.\nIf the batches are formed on the basis of their length (not randomly), it would speed up the learning process and improve the quality of generated spectrograms.\nAfter a specific learning step, use a dynamic learning rate. This helps reduce the value of errors and increase the quality of the generated spectrograms.\nAfter creating a model and fail to train it in batches from the entire data set, try retraining it in one batch. If you succeed, you will get an alignment and recreated audio based on the generated spectrograms that contain speech (or at least its similarity). This will confirm that the overall architecture is correct, and only small details are missing."}
{"idx": 3, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages", "prediction": "a new attention architecture for decoding.", "reference": "The input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results.\nAnother crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data:\nAttention defines the part of the encoder data that should be used at the current decoder step.\nThen the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet.\nAt each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step. Probably this correspondence will be direct. For example, if we have an input text sequence of 200 characters and the corresponding spectrogram of 800 frames, then there will be 4 frames per character. However, speech generated on the basis of such a spectrogram would sound unnatural. We say some words faster, others slower, sometimes we make pauses, and sometimes we don’t. It is impossible to foresee all possible contexts. That is why attention is a key element of the entire system: it sets the correspondence between the decoder pitch and the information from the encoder. In this way we obtain the information necessary to generate a specific frame. The greater value of the attention weight, the more “attention” should be “paid” to a respective part of the encoder data when generating a spectrogram frame.\nAt the training stage, not only the visual assessment of the quality of spectrograms and attention is important, but also the generated audio. However, using WaveNet as a vocoder during the training phase is an impermissible luxury in terms of time. Therefore, we used the Griffin-Lim algorithm, which enables a partial restore of the signal after fast Fourier transforms. Why partial? The fact is, while converting a signal into spectrograms, we lose information about the phase. However, the quality of audio obtained in this way is enough to understand if we are moving in the right direction.\nIf the batches are formed on the basis of their length (not randomly), it would speed up the learning process and improve the quality of generated spectrograms.\nAfter a specific learning step, use a dynamic learning rate. This helps reduce the value of errors and increase the quality of the generated spectrograms.\nAfter creating a model and fail to train it in batches from the entire data set, try retraining it in one batch. If you succeed, you will get an alignment and recreated audio based on the generated spectrograms that contain speech (or at least its similarity). This will confirm that the overall architecture is correct, and only small details are missing."}
{"idx": 4, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic", "prediction": "Context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction.", "reference": "The input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results.\nAnother crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data:\nAttention defines the part of the encoder data that should be used at the current decoder step.\nThen the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet.\nAt each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step. Probably this correspondence will be direct. For example, if we have an input text sequence of 200 characters and the corresponding spectrogram of 800 frames, then there will be 4 frames per character. However, speech generated on the basis of such a spectrogram would sound unnatural. We say some words faster, others slower, sometimes we make pauses, and sometimes we don’t. It is impossible to foresee all possible contexts. That is why attention is a key element of the entire system: it sets the correspondence between the decoder pitch and the information from the encoder. In this way we obtain the information necessary to generate a specific frame. The greater value of the attention weight, the more “attention” should be “paid” to a respective part of the encoder data when generating a spectrogram frame.\nAt the training stage, not only the visual assessment of the quality of spectrograms and attention is important, but also the generated audio. However, using WaveNet as a vocoder during the training phase is an impermissible luxury in terms of time. Therefore, we used the Griffin-Lim algorithm, which enables a partial restore of the signal after fast Fourier transforms. Why partial? The fact is, while converting a signal into spectrograms, we lose information about the phase. However, the quality of audio obtained in this way is enough to understand if we are moving in the right direction.\nIf the batches are formed on the basis of their length (not randomly), it would speed up the learning process and improve the quality of generated spectrograms.\nAfter a specific learning step, use a dynamic learning rate. This helps reduce the value of errors and increase the quality of the generated spectrograms.\nAfter creating a model and fail to train it in batches from the entire data set, try retraining it in one batch. If you succeed, you will get an alignment and recreated audio based on the generated spectrograms that contain speech (or at least its similarity). This will confirm that the overall architecture is correct, and only small details are missing."}
{"idx": 5, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.", "prediction": "The WaveNet stack is a multi-core, multi-processor, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core,", "reference": "The input data will be the text. The output will be a Mel-spectrogram — a low-level representation obtained by applying a fast Fourier transform to a discrete audio signal. We should also note that the spectrograms obtained in this way still need to be normalized by compressing the dynamic range. This enables reducing the natural ratio between the loudest and quietest sound on the recording. In our experiments, the use of spectrograms reduced to the [-4; 4] range has shown the best results.\nAnother crucial element of the system is the mechanism of soft attention — a relatively new and popular technique. At each decoding step, “attention” forms the context vector and updates the attention weight, using following data:\nAttention defines the part of the encoder data that should be used at the current decoder step.\nThen the concatenation of the output data of LSTM-layers with the same (or possibly other) context vector is directed into a fully connected layer with 80 neurons, which corresponds to the number of spectrogram channels. This final decoder layer forms the predicted spectrogram frame by frame. Its output serves as an input to the next time step of the decoder in PreNet.\nAt each step of the decoder, we tried to decode one frame of the spectrogram. However, it is not entirely clear what information the encoder needs to use at each step. Probably this correspondence will be direct. For example, if we have an input text sequence of 200 characters and the corresponding spectrogram of 800 frames, then there will be 4 frames per character. However, speech generated on the basis of such a spectrogram would sound unnatural. We say some words faster, others slower, sometimes we make pauses, and sometimes we don’t. It is impossible to foresee all possible contexts. That is why attention is a key element of the entire system: it sets the correspondence between the decoder pitch and the information from the encoder. In this way we obtain the information necessary to generate a specific frame. The greater value of the attention weight, the more “attention” should be “paid” to a respective part of the encoder data when generating a spectrogram frame.\nAt the training stage, not only the visual assessment of the quality of spectrograms and attention is important, but also the generated audio. However, using WaveNet as a vocoder during the training phase is an impermissible luxury in terms of time. Therefore, we used the Griffin-Lim algorithm, which enables a partial restore of the signal after fast Fourier transforms. Why partial? The fact is, while converting a signal into spectrograms, we lose information about the phase. However, the quality of audio obtained in this way is enough to understand if we are moving in the right direction.\nIf the batches are formed on the basis of their length (not randomly), it would speed up the learning process and improve the quality of generated spectrograms.\nAfter a specific learning step, use a dynamic learning rate. This helps reduce the value of errors and increase the quality of the generated spectrograms.\nAfter creating a model and fail to train it in batches from the entire data set, try retraining it in one batch. If you succeed, you will get an alignment and recreated audio based on the generated spectrograms that contain speech (or at least its similarity). This will confirm that the overall architecture is correct, and only small details are missing."}
{"idx": 6, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side", "prediction": "a given time frame, we use a speech generator to generate the speech. We use a speech generator to generate the speech. We use a speech generator to generate the speech. We use a speech generator to generate the speech.", "reference": "Applying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, i-1 — previous attention weights, hj — j-th hidden encoder state, i-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5. A distinctive feature of this module is that the dropout is used in it not only on the stage of learning the model but also at the output stage. Why did we mention that the context vector might already be different? One possible approach is the recalculation of the context vector once the latent state of the LSTM network is obtained. However, in our experiments, this approach was not justified. In addition to the projection onto the 80-neural fully-connected layer, the concatenation of the output data of LSTM-layers with the context vector is directed into a fully-connected layer with one neuron, followed by sigmoid activation – this is known as the stop token prediction layer. It predicts the probability that the frame created at the step of the decoder is the final one. This layer is designed to generate a spectrogram not of a fixed length but of an arbitrary one at the stage of model output. At the output stage this element determines the number of decoder steps. It can be considered as a binary classifier. All convolutional modules are regularized with dropout layers with a rate of 0.50, and recurrent layers with a newer zoneout method with a rate of 0.10. It is quite simple: instead of submitting the hidden state and cell state obtained at the current step to the next step of an LSTM-network, we replaced part of the data with the values from the previous step. This is done both at the training and output stages. At the same time, only the hidden state is exposed to the zoneout method at each step, which is transmitted to the next LSTM step, while the output of the LSTM cell at the current step remains unchanged. We chose PyTorch as a deep learning framework. Although at the time of network implementation, it was in a state of pre-release. PyTorch is a very powerful tool for building and training artificial neural networks. We had experience using TensorFlow and Keras frameworks, however due to the need to implement non-standard custom structures, we’ve chosen PyTorch. However, it doesn’t mean that one framework is better than the other; the choice of framework depends on various factors. The network was learning using a backpropagation algorithm. ADAM was used as an optimizer. The model has been learning on a single GPU GeForce 1080 Ti with 11 GB of RAM. While working with such a huge model, it is important to monitor how the learning process goes. Here, TensorBoard became a convenient tool. We tracked the value of the errors in both training and validation iterations. In addition, we displayed target spectrograms, predicted spectrograms on the training and validation stages, and performed an alignment, which is additive accumulated attention weights from all the training stages. Use a version control system that clearly describes all changes. While searching for optimal architecture, changes occur constantly. Make sure you are"}
{"idx": 7, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel", "prediction": "a wide range of speech data.", "reference": "Applying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, i-1 — previous attention weights, hj — j-th hidden encoder state, i-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5. A distinctive feature of this module is that the dropout is used in it not only on the stage of learning the model but also at the output stage. Why did we mention that the context vector might already be different? One possible approach is the recalculation of the context vector once the latent state of the LSTM network is obtained. However, in our experiments, this approach was not justified. In addition to the projection onto the 80-neural fully-connected layer, the concatenation of the output data of LSTM-layers with the context vector is directed into a fully-connected layer with one neuron, followed by sigmoid activation – this is known as the stop token prediction layer. It predicts the probability that the frame created at the step of the decoder is the final one. This layer is designed to generate a spectrogram not of a fixed length but of an arbitrary one at the stage of model output. At the output stage this element determines the number of decoder steps. It can be considered as a binary classifier. All convolutional modules are regularized with dropout layers with a rate of 0.50, and recurrent layers with a newer zoneout method with a rate of 0.10. It is quite simple: instead of submitting the hidden state and cell state obtained at the current step to the next step of an LSTM-network, we replaced part of the data with the values from the previous step. This is done both at the training and output stages. At the same time, only the hidden state is exposed to the zoneout method at each step, which is transmitted to the next LSTM step, while the output of the LSTM cell at the current step remains unchanged. We chose PyTorch as a deep learning framework. Although at the time of network implementation, it was in a state of pre-release. PyTorch is a very powerful tool for building and training artificial neural networks. We had experience using TensorFlow and Keras frameworks, however due to the need to implement non-standard custom structures, we’ve chosen PyTorch. However, it doesn’t mean that one framework is better than the other; the choice of framework depends on various factors. The network was learning using a backpropagation algorithm. ADAM was used as an optimizer. The model has been learning on a single GPU GeForce 1080 Ti with 11 GB of RAM. While working with such a huge model, it is important to monitor how the learning process goes. Here, TensorBoard became a convenient tool. We tracked the value of the errors in both training and validation iterations. In addition, we displayed target spectrograms, predicted spectrograms on the training and validation stages, and performed an alignment, which is additive accumulated attention weights from all the training stages. Use a version control system that clearly describes all changes. While searching for optimal architecture, changes occur constantly. Make sure you are"}
{"idx": 8, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better", "prediction": "The main purpose of this paper is to present a method for obtaining spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning", "reference": "Applying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, i-1 — previous attention weights, hj — j-th hidden encoder state, i-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5. A distinctive feature of this module is that the dropout is used in it not only on the stage of learning the model but also at the output stage. Why did we mention that the context vector might already be different? One possible approach is the recalculation of the context vector once the latent state of the LSTM network is obtained. However, in our experiments, this approach was not justified. In addition to the projection onto the 80-neural fully-connected layer, the concatenation of the output data of LSTM-layers with the context vector is directed into a fully-connected layer with one neuron, followed by sigmoid activation – this is known as the stop token prediction layer. It predicts the probability that the frame created at the step of the decoder is the final one. This layer is designed to generate a spectrogram not of a fixed length but of an arbitrary one at the stage of model output. At the output stage this element determines the number of decoder steps. It can be considered as a binary classifier. All convolutional modules are regularized with dropout layers with a rate of 0.50, and recurrent layers with a newer zoneout method with a rate of 0.10. It is quite simple: instead of submitting the hidden state and cell state obtained at the current step to the next step of an LSTM-network, we replaced part of the data with the values from the previous step. This is done both at the training and output stages. At the same time, only the hidden state is exposed to the zoneout method at each step, which is transmitted to the next LSTM step, while the output of the LSTM cell at the current step remains unchanged. We chose PyTorch as a deep learning framework. Although at the time of network implementation, it was in a state of pre-release. PyTorch is a very powerful tool for building and training artificial neural networks. We had experience using TensorFlow and Keras frameworks, however due to the need to implement non-standard custom structures, we’ve chosen PyTorch. However, it doesn’t mean that one framework is better than the other; the choice of framework depends on various factors. The network was learning using a backpropagation algorithm. ADAM was used as an optimizer. The model has been learning on a single GPU GeForce 1080 Ti with 11 GB of RAM. While working with such a huge model, it is important to monitor how the learning process goes. Here, TensorBoard became a convenient tool. We tracked the value of the errors in both training and validation iterations. In addition, we displayed target spectrograms, predicted spectrograms on the training and validation stages, and performed an alignment, which is additive accumulated attention weights from all the training stages. Use a version control system that clearly describes all changes. While searching for optimal architecture, changes occur constantly. Make sure you are"}
{"idx": 9, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.", "prediction": "WaveNet is a deep convolutional neural network that is used as a vocoder. WaveNet uses a post-net to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary post-net to incorporate past and future frames after decoding to incorporate past and future frames after decoding to improve the feature predictions. We evaluate our model with and without the post-net to answer this question, and found that without it, our model only obtains a MOS score of 4.429 0.071, compared to 4.526 0.066 with it, meaning that empirically the post-net is still an important part of the network design. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much", "reference": "Applying Deep Learning for Data Pre-Processing Let’s examine the network blocks and their modules in detail. The first layer of the encoder is the embedding layer, which creates 512-dimensional vectors based on a sequence of natural numbers that represent symbols. Further along, embedding vectors are directed into a block of 3 one-dimensional convolutional layers. Each layer includes 512 filters with a length of 5. This value is a good filter size because it captures a certain character, as well as two previous and two next neighbors. Each convolutional layer is followed by a mini-batch normalization and ReLU activation. The tensors obtained after the convolutional block are directed to bidirectional LSTM layers, each per 256 neurons. The forward and backward results are concatenated. The decoder has a recurrent architecture. So the output from the previous step (one frame of the spectrogram) is used at each next step. the projection of the previous hidden state of the decoder’s RNN network onto the fully connected layer, the projection of the output of the encoder data on a fully connected layer, si-1 — previous hidden state of the decoder’s LSTM network, i-1 — previous attention weights, hj — j-th hidden encoder state, i-1 – previous attention weights. First, the decoder output from the previous step follows into a small PreNet module, which is a stack of two fully connected layers of 256 neurons each, alternating with dropout layers with a rate of 0.5. A distinctive feature of this module is that the dropout is used in it not only on the stage of learning the model but also at the output stage. Why did we mention that the context vector might already be different? One possible approach is the recalculation of the context vector once the latent state of the LSTM network is obtained. However, in our experiments, this approach was not justified. In addition to the projection onto the 80-neural fully-connected layer, the concatenation of the output data of LSTM-layers with the context vector is directed into a fully-connected layer with one neuron, followed by sigmoid activation – this is known as the stop token prediction layer. It predicts the probability that the frame created at the step of the decoder is the final one. This layer is designed to generate a spectrogram not of a fixed length but of an arbitrary one at the stage of model output. At the output stage this element determines the number of decoder steps. It can be considered as a binary classifier. All convolutional modules are regularized with dropout layers with a rate of 0.50, and recurrent layers with a newer zoneout method with a rate of 0.10. It is quite simple: instead of submitting the hidden state and cell state obtained at the current step to the next step of an LSTM-network, we replaced part of the data with the values from the previous step. This is done both at the training and output stages. At the same time, only the hidden state is exposed to the zoneout method at each step, which is transmitted to the next LSTM step, while the output of the LSTM cell at the current step remains unchanged. We chose PyTorch as a deep learning framework. Although at the time of network implementation, it was in a state of pre-release. PyTorch is a very powerful tool for building and training artificial neural networks. We had experience using TensorFlow and Keras frameworks, however due to the need to implement non-standard custom structures, we’ve chosen PyTorch. However, it doesn’t mean that one framework is better than the other; the choice of framework depends on various factors. The network was learning using a backpropagation algorithm. ADAM was used as an optimizer. The model has been learning on a single GPU GeForce 1080 Ti with 11 GB of RAM. While working with such a huge model, it is important to monitor how the learning process goes. Here, TensorBoard became a convenient tool. We tracked the value of the errors in both training and validation iterations. In addition, we displayed target spectrograms, predicted spectrograms on the training and validation stages, and performed an alignment, which is additive accumulated attention weights from all the training stages. Use a version control system that clearly describes all changes. While searching for optimal architecture, changes occur constantly. Make sure you are"}
{"idx": 10, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "prediction": "A fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predict mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "reference": "Now we have data representing text and audio that are suitable for processing by a neural network. Let’s consider the architecture of the feature prediction net, which we will call Tacotron 2, named after the central element of the entire synthesis system.\nTacotron 2 Architecture Explained\nTacotron 2 is not one network, but two: Feature prediction net and NN-vocoder WaveNet. Feature prediction net is considered as the main network, while the WaveNet vocoder plays the role of a supplemental system.\nTacotron2 has sequence to sequence architecture. It consists of an encoder, which creates internal representation of the input signal (symbolic tokens), and a decoder, which turns this representation into a Mel-spectrogram. A very important element of the network is the PostNet, designed to improve the spectrogram generated by the decoder.\nSome of the modules use information from the previous step of the decoder. But on the first step, the information will be zero-value tensors, which is a common approach in creating recurrent structures.\nThe Workflow with Tacontron 2\nThe output of PreNet in concatenation with the context vector, obtained as a result of the attention mechanism, is directed to the entrance of the unidirectional two-layer LSTM network with 1024 neurons in each layer.\nVisualization of Tacotron 2 Processing\nHowever, after all the modules started working properly, something like this is displayed:\nWe have accumulated some thoughts on the development process with text to speech synthesis so far. Some of them are common, others are more specific and unique. Here are our tips for those who consider Tacotron 2 as a text-to-speech solution for their projects.\nGeneral Tips on the Workflow with Tacontron 2:\nIn Tacontron 2 and similar architectures, you should follow the principles of encapsulation: 1 class equals 1 Python-module. This approach is rarely found in ML tasks but helps to structure the code, speed up debugging, and the development process.\nBuilding and Training of Text-to-Speech Models Based on Tacontron 2 Architecture:\nThank you for reading until the very end. As a bonus, here are all examples of speech generation from text using Tacotron 2 architecture that is not contained in the training set:"}
{"idx": 11, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech", "prediction": "A speech synthesis architecture for text-to-speech is proposed.", "reference": "WaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles."}
{"idx": 12, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.", "prediction": "A: The paper is written by a team of researchers from the University of California, Berkeley. The authors are a team of researchers who have developed a sequence-to-sequence architecture for producing magnitude spectrograms from a sequence of characters. The architecture is based on the Tacotron architecture, which is a sequence-to-sequence architecture for producing magnitude spectrograms from a sequence of characters. The architecture is based on the Griffin-Lim algorithm, which is a phase estimation algorithm for vocoding the resulting magnitude spectrograms. The authors note that Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. The authors also note that Griffin-Lim is a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet.", "reference": "The new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron."}
{"idx": 13, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM", "prediction": "dimensional representation of the audio signal. It is a low-dimensional representation that is easier to train and is more invariant to phase than waveform samples. It is a low-dimensional representation that is easier to train and is more invariant to phase than waveform samples. It is a low-dimensional representation that is easier to train and is more invariant to phase than waveform samples. It is a low-dimensional representation that is easier to train and is more invariant to phase than waveform samples.", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."}
{"idx": 14, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages", "prediction": "a new attention architecture that is able to learn to learn from the context vectors of the encoded sequence.", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."}
{"idx": 15, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic", "prediction": "LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction.", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."}
{"idx": 16, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.", "prediction": "The WaveNet stack is a multi-core, multi-processor, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core,", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."}
{"idx": 17, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side", "prediction": "We use a mixture of logistics and feature prediction networks to generate the waveforms. We use a mixture of waveforms and feature prediction networks to generate the waveforms. We use a mixture of waveforms and feature prediction networks to generate the waveforms.", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."}
{"idx": 18, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel", "prediction": "a large number of words and sentences.", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."}
{"idx": 19, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better", "prediction": "The first wavenet was developed in the late 1980s. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary software package that was developed by the WaveNet Corporation. WaveNet was a proprietary", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."}
{"idx": 20, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.", "prediction": "WaveNet is a deep learning vocoder that uses a convolutional network to generate a mel spectrogram. The vocoder uses a post-net to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary post-net to incorporate past and future frames after decoding to incorporate past and future frames after decoding to improve the feature predictions. We evaluate our model with and without the post-net to answer this question, and found that without it, our model only obtains a MOS score of 4.429 0.071, compared to 4.526 0.066 with it, meaning that empirically the post-net is still an important part of the network design. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since ", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise.\n\nThe new technique takes the best pieces of two of Google’s previous speech generation projects: WaveNet and the original Tacotron.\n\nWaveNet produced what I called “eerily convincing” speech one audio sample at a time, which probably sounds like overkill to anyone who knows anything about sound design. But while it is effective, WaveNet requires a great deal of metadata about language to start out: pronunciation, known linguistic features, etc. Tacotron synthesized more high-level features, such as intonation and prosody, but wasn’t really suited for producing a final speech product.\n\nTacotron 2 uses pieces of both, though I will frankly admit that at this point we have reached the limits of my technical expertise, such as it is. But from what I can tell, it uses text and narration of that text to calculate all the linguistic rules that systems normally have to be explicitly told. The text itself is converted into a Tacotron-style “mel-scale spectrogram” for purposes of rhythm and emphasis, while the words themselves are generated using a WaveNet-style system.\n\nThe resulting audio, several examples of which you can listen to here, is pretty much as good as or better than anything out there. The rhythm of speech is convincing, though perhaps a bit too chipper. It mainly stumbles on words with pronunciations that aren’t particularly intuitive, perhaps due to their origin outside American English, such as “decorum,” where it emphasizes the first syllable, and “Merlot,” which it hilariously pronounces just as it looks. “And in extreme cases it can even randomly generate strange noises,” the researchers write.\n\nLastly, there is no way to control the tone of speech — for example upbeat or concerned — although accents and other subtleties can be baked in as they could be with WaveNet.\n\nLowering the barrier for training a system means more and better ones can be trained, and new approaches integrated without having to re-evaluate a complex manually defined ruleset or source new such rulesets for new languages or speech styles.\n\nTechcrunch event Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Save $450 on your TechCrunch All Stage pass Build smarter. Scale faster. Connect deeper. Join visionaries from Precursor Ventures, NEA, Index Ventures, Underscore VC, and beyond for a day packed with strategies, workshops, and meaningful connections. Boston, MA | REGISTER NOW\n\nThe researchers have submitted it for consideration at the IEEE International Conference on Acoustics, Speech and Signal Processing; you can read the url itself at arXiv."}
{"idx": 21, "input": "Generate a story for these personas: General Public, Investor, Developer, Journalist\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "prediction": "A fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predict mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "reference": "Creating convincing artificial speech is a hot pursuit right now, with Google arguably in the lead. The company may have leapt ahead again with the announcement today of Tacotron 2, a new method for training a neural network to produce realistic speech from text that requires almost no grammatical expertise."}
{"idx": 22, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech", "prediction": "A speech synthesis architecture for text-to-speech is proposed.", "reference": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."}
{"idx": 23, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.", "prediction": "a neural vocoder that combines the best of the previous approaches, and a linguistic model that predicts mel spectrograms from a sequence of characters.", "reference": "Text to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."}
{"idx": 24, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM", "prediction": "Using a low-level acoustic representation: mel-frequency spectrograms, we choose a low-level acoustic representation: mel-frequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. Mel-frequency spectrograms are related to the linear-frequency spectrogram, i.e., the short-time Fourier transform magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequencies details, which are dominated by fricatives and other noise bursts and generally do not need", "reference": "A Brief Summary\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter."}
{"idx": 25, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages", "prediction": "a new attention architecture for decoding.", "reference": "A Brief Summary\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter."}
{"idx": 26, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic", "prediction": "Context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction.", "reference": "A Brief Summary\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter."}
{"idx": 27, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.", "prediction": "Section: WaveNet is a multi-core, multi-processor, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core,", "reference": "A Brief Summary\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter."}
{"idx": 28, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side", "prediction": "a given time frame, we use a speech generator to generate the speech. We use a speech generator to generate the speech. We use a speech generator to generate the speech. We use a speech generator to generate the speech.", "reference": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."}
{"idx": 29, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel", "prediction": "a wide range of speech data.", "reference": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."}
{"idx": 30, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better", "prediction": "Section: WaveNet is a software tool for audio processing. It is a free and open source audio processing tool. WaveNet is a free and open source audio processing tool. It is a free and open source audio processing tool.", "reference": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."}
{"idx": 31, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.", "prediction": "WaveNet is a deep convolutional neural network that is used as a vocoder. WaveNet uses a post-net to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary post-net to incorporate past and future frames after decoding to incorporate past and future frames after decoding to improve the feature predictions. We evaluate our model with and without the post-net to answer this question, and found that without it, our model only obtains a MOS score of 4.429 0.071, compared to 4.526 0.066 with it, meaning that empirically the post-net is still an important part of the network design. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much", "reference": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\n\nA Brief Summary\n\n“The original study can be found at arxiv.org/pdf/1712.05884.pdf”\n\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\n\nText to Speech (TTS) which is also sometimes called Speech synthesis, is technology that enables text to be converted into speech sounds imitative of the human voice.\n\nTacotron 2 is a neural network architecture for text to speech that uses a\n\nrecurrent sequence-to-sequence feature prediction that maps the text character embeddings to the mel-scale spectrograms. To sum that down Tacotron takes text as an input and uses Natural Language Processing tools to convert it to character embedded sequences and then thrown into a recurrent sequence-to-sequence feature that predicts a sequence of mel spectrum frames. The mel spectrum is a visual way of looking at audio data in a time-frequency domain with the bins representing pitch classes. The model also generates a time-domain waveform that is taken from the mel spectrum using a modified version of an architecture known as WaveNet. This is achieved by doing a inverse Fourier Transform which is a function that transforms data from the time-frequence domain to the time-power domain. Having the 2 different acoustic representations allows us to train the two separately.\n\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder.\n\nThe encoder converts a character sequence into a hidden feature representation and the decoder takes that to predict a spectrogram while the decoder in an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time.\n\nA modified version of the WaveNet architecture is used to invert the mel spectrogram feature representation into time-domain samples. The WaveNet output is used to predict the parameters for the training process. The training process starts with training the feature prediction network on its own followed by training a modified WaveNet that is separate from the first WaveNet made and is training on the predicted feature prediction network. This aligns the waveforms in a coherent and linear matter.\n\nThe network cycles thru the waveforms and saves the ones that line up with the ground truth frame until all the input text character embeddings have been used. This output synthesizes a vocoder that mimics the human voice."}
{"idx": 32, "input": "Generate a story for these personas: Developer, Student, Researcher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "prediction": "A fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predict mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "reference": "Text To Speech With Tacotron 2: Stephen William 3 min read · Nov 9, 2021 -- Listen Share\nAs text to speech becomes more and more popular and more ingrained to our everyday life I thought it would be a good idea to go over a Text to Speech architecture called Tacotron 2.\nTacotron 2 is a neural network architecture for text to speech that uses a\nIn Tacotron the mel spectrograms are computed through a short-time Fourier transform using a 50ms frame size and a Hann window function. The Hann window function is a smoothing technique that averages the frequencies to produce and visualize a smoother and more bearable waveform. The audio data is scaled to a range between 125 Hz to 7.6 kHz which is the range of the human voice and a Log compression is also put in place to limit the dynamic range or how loud and quiet the audio data can be. This filtering will make it easier for the network to analyze and predict the audio as well as save processing power. The 2 acoustic representations are then fed into a neural network that is composed of an encoder and a decoder."}
{"idx": 33, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech", "prediction": "A speech synthesis architecture for text-to-speech is proposed.", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 34, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.", "prediction": "a neural vocoder that combines the best of the previous approaches, and a linguistic model that predicts mel spectrograms from a sequence of characters.", "reference": "While our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 35, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM", "prediction": "Feature Prediction Network with Attention consists of a sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation Mel-frequency spectrograms are related to the linear-frequency spectrogram, i.e., the short-time Fourier transform magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequencies details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 36, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages", "prediction": "a new attention architecture for decoding.", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 37, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic", "prediction": "Context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction.", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 38, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.", "prediction": "The WaveNet stack is a multi-core, multi-processor, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core,", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 39, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side", "prediction": "a given time frame, we use a speech generator to generate the speech. We use a speech generator to generate the speech. We use a speech generator to generate the speech. We use a speech generator to generate the speech.", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 40, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel", "prediction": "a wide range of speech data.", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 41, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better", "prediction": "The main purpose of this paper is to present a method for obtaining spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning inputs to WaveNet. The method is based on the use of mel-scale spectrograms as conditioning", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 42, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.", "prediction": "WaveNet is a deep convolutional neural network that is used as a vocoder. WaveNet uses a post-net to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary post-net to incorporate past and future frames after decoding to incorporate past and future frames after decoding to improve the feature predictions. We evaluate our model with and without the post-net to answer this question, and found that without it, our model only obtains a MOS score of 4.429 0.071, compared to 4.526 0.066 with it, meaning that empirically the post-net is still an important part of the network design. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\n\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings.\n\nWhile our samples sound great, there are still some difficult problems to be tackled. For example, our system has difficulties pronouncing complex words (such as “decorum” and “merlot”), and in extreme cases it can even randomly generate strange noises. Also, our system cannot yet generate audio in realtime. Furthermore, we cannot yet control the generated speech, such as directing it to sound happy or sad. Each of these is an interesting research problem on its own."}
{"idx": 43, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "prediction": "A fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predict mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "reference": "Generating very natural sounding speech from text (text-to-speech, TTS) has been a research goal for decades. There has been great progress in TTS research over the last few years and many individual pieces of a complete TTS system have greatly improved. Incorporating ideas from past work such as Tacotron and WaveNet, we added more improvements to end up with our new system, Tacotron 2. Our approach does not use complex linguistic and acoustic features as input. Instead, we generate human-like speech from text using neural networks trained using only speech examples and corresponding text transcripts.\nA full description of our new system can be found in our paper “Natural TTS Synthesis by Conditioning WaveNet on Mel Spectrogram Predictions.” In a nutshell it works like this: We use a sequence-to-sequence model optimized for TTS to map a sequence of letters to a sequence of features that encode the audio. These features, an 80-dimensional audio spectrogram with frames computed every 12.5 milliseconds, capture not only pronunciation of words, but also various subtleties of human speech, including volume, speed and intonation. Finally these features are converted to a 24 kHz waveform using a WaveNet-like architecture.\nYou can listen to some of the Tacotron 2 audio samples that demonstrate the results of our state-of-the-art TTS system. In an evaluation where we asked human listeners to rate the naturalness of the generated speech, we obtained a score that was comparable to that of professional recordings."}
{"idx": 44, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Abstract\nAbstract: ABSTRACT This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture. Index Terms— Tacotron 2, WaveNet, text-to-speech", "prediction": "A speech synthesis architecture for text-to-speech is proposed.", "reference": "First step transforms the text into time-aligned features, such as mel spectrogram, or F0 frequencies and other linguistic features;\n“William Shakespeare was an English poet, playwright and actor, widely regarded as the greatest writer in the English language and the world’s greatest dramatist. He is often called England’s national poet and the ‘Bard of Avon’.”\nAfter following the steps in the Jupyter notebook, you will be able to provide English text to the model and it will generate an audio output file. All of the scripts to reproduce the results have been published on GitHub in our NVIDIA Deep Learning Examples repository, which contains several high-performance training recipes that use Tensor Cores. Additionally, we developed a Jupyter notebook for users to create their own container image, then download the dataset and reproduce the training and inference results step-by-step.\nTacotron 22 is a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms, as shown in Figure 1.\nThe network is composed of an encoder (blue) and a decoder (orange) with attention. The encoder converts a character sequence into a hidden feature representation, which serves as input to the decoder to predict a spectrogram. Input text (yellow) is presented using a learnt 512-dimensional character embedding, which are passed through a stack of three convolutional layers (each containing 512 filters with shape 5 × 1), followed by batch normalization and ReLU activations. The encoder output is passed to an attention network (gray) which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step.\nWaveGlow 1 is a flow-based network capable of generating high-quality speech from mel spectrograms. WaveGlow combines insights from Glow5 and WaveNet6 in order to provide fast, efficient and high-quality audio synthesis, without the need for auto-regression. WaveGlow is implemented using only a single network, trained with only a single cost function: making the training procedure simple and stable. Our current model synthesizes samples at 55 * 22,050 = 1,212,750, which is 55 times faster than “real-time” at 22,050 samples per second sampling rate. The Mean Opinion Score (MOS) show that it delivers audio quality as good as the best publicly available WaveNet implementation trained on the same dataset.\nRun Jupyter Notebook Step-by-Step\nYou can customize the content of the text file, depending on its length, you may need to increase the –max-decoder-steps option to 2,000. The Tacotron 2 model was trained on the LJ Speech dataset with audio samples no longer than 10 seconds, which corresponds to about 860 mel spectrograms. Therefore the inference is expected to work well with generating audio samples of similar length. We set the mel spectrogram length limit to 2,000 (about 23 seconds), since in practice it still produces the correct voice. If needed, users can split longer phrases into multiple sentences and synthesize them separately."}
{"idx": 45, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Introduction\nIntroduction: 1. INTRODUCTION Generating natural speech from text (text-to-speech synthesis, TTS) remains a challenging task despite decades of investigation [1]. Over time, different techniques have dominated the field. Concatenative synthesis with unit selection, the process of stitching small units of pre-recorded waveforms together [2, 3] was the state-of-the-art for many years. Statistical parametric speech synthesis [4, 5, 6, 7], which directly generates smooth trajectories of speech features to be synthesized by a vocoder, followed, solving many of the issues that concatenative synthesis had with boundary artifacts. However, the audio produced by these systems often sounds muffled and unnatural compared to human speech. WaveNet [8], a generative model of time domain waveforms, produces audio quality that begins to rival that of real human speech and is already used in some complete TTS systems [9, 10, 11]. The inputs to WaveNet (linguistic features, predicted log fundamental frequency (F0), and phoneme durations), however, require significant domain expertise to produce, involving elaborate text-analysis systems as well as a robust lexicon (pronunciation guide). Tacotron [12], a sequence-to-sequence architecture [13] for producing magnitude spectrograms from a sequence of characters, simplifies the traditional speech synthesis pipeline by replacing the production of these linguistic and acoustic features with a single neural network trained from data alone. To vocode the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [14] for phase estimation, followed by an inverse short-time Fourier transform. As the authors note, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and lower audio quality than approaches like WaveNet. In this paper, we describe a unified, entirely neural approach to speech synthesis that combines the best of the previous approaches: a sequence-to-sequence Tacotron-style model [12] that generates mel spectrograms, followed by a modified WaveNet vocoder [10, 15]. Trained directly on normalized character sequences and corresponding speech waveforms, our model learns to synthesize natural sounding speech that is difficult to distinguish from real human speech. Deep Voice 3 [11] describes a similar approach. However, unlike our system, its naturalness has not been shown to rival that of human speech. Char2Wav [16] describes yet another similar approach to end-to-end TTS using a neural vocoder. However, they use different intermediate representations (traditional vocoder features) and their model architecture differs significantly.", "prediction": "a vocoder that translates the mel spectrograms into speech.", "reference": "Converting text into high quality, natural-sounding speech in real time has been a challenging conversational AI task for decades. State-of-the-art speech synthesis models are based on parametric neural networks1. Text-to-speech (TTS) synthesis is typically done in two steps.\nInput:\nOutput:\nWaveGlow model2 instead of WaveNet to synthesize waveforms.\nAfter reading this blog, try the Jupyter notebook to get hands-on experience generating audio from text in real-time."}
{"idx": 46, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Method\nMethod: 2. MODEL ARCHITECTURE Our proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation In this work we choose a low-level acoustic representation: melfrequency spectrograms, to bridge the two components. Using a representation that is easily computed from time-domain waveforms allows us to train the two components separately. This representation is also smoother than waveform samples and is easier to train using a squared error loss because it is invariant to phase within each frame. A mel-frequency spectrogram is related to the linear-frequency spectrogram, i.e., the short-time Fourier transform (STFT) magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such an auditory frequency scale has the effect of emphasizing details in lower frequencies, which are critical to speech intelligibility, while de-emphasizing high frequency details, which are dominated by fricatives and other noise bursts and generally do not need to be modeled with high fidelity. Because of these properties, features derived from the mel scale have been used as an underlying representation for speech recognition for many decades [17]. While linear spectrograms discard phase information (and are therefore lossy), algorithms such as Griffin-Lim [14] are capable of estimating this discarded information, which enables time-domain conversion via the inverse short-time Fourier transform. Mel spectrograms discard even more information, presenting a challenging inverse problem. However, in comparison to the linguistic and acoustic features used in WaveNet, the mel spectrogram is a simpler, lowerlevel acoustic representation of audio signals. It should therefore be straightforward for a similar WaveNet model conditioned on mel spectrograms to generate audio, essentially as a neural vocoder. Indeed, we will show that it is possible to generate high quality audio from mel spectrograms using a modified WaveNet architecture. 2.2. Spectrogram Prediction Network As in Tacotron, mel spectrograms are computed through a shorttime Fourier transform (STFT) using a 50 ms frame size, 12.5 ms frame hop, and a Hann window function. We experimented with a 5 ms frame hop to match the frequency of the conditioning inputs in the original WaveNet, but the corresponding increase in temporal resolution resulted in significantly more pronunciation issues. We transform the STFT magnitude to the mel scale using an 80 channel mel filterbank spanning 125 Hz to 7.6 kHz, followed by log dynamic range compression. Prior to log compression, the filterbank output magnitudes are clipped to a minimum value of 0.01 in order to limit dynamic range in the logarithmic domain. The network is composed of an encoder and a decoder with attention. The encoder converts a character sequence into a hidden feature representation which the decoder consumes to predict a spectrogram. Input characters are represented using a learned 512-dimensional character embedding, which are passed through a stack of 3 convolutional layers each containing 512 filters with shape 5 1, i.e., where each filter spans 5 characters, followed by batch normalization [18] and ReLU activations. As in Tacotron, these convolutional layers model longer-term context (e.g., N-grams) in the input character sequence. The output of the final convolutional layer is passed into a single bi-directional [19] LSTM", "prediction": "Feature Prediction Networks (FPNs) are a type of feature learning architecture that can be used to train a feature model. They are used to predict a sequence of mel spectrogram frames from an input character sequence. A modified version of WaveNet is used to generate time-domain waveform samples conditioned on the predicted mel spectrogram frames. The proposed system consists of two components, shown in Figure 1: (1) a recurrent sequence-to-sequence feature prediction network with attention which predicts a sequence of mel spectrogram frames from an input character sequence, and (2) a modified version of WaveNet which generates time-domain waveform samples conditioned on the predicted mel spectrogram frames. 2.1. Intermediate Feature Representation Mel-frequency spectrograms are related to the linear-frequency spectrogram, i.e., the short-time Fourier transform magnitude. It is obtained by applying a nonlinear transform to the frequency axis of the STFT, inspired by measured responses from the human auditory system, and summarizes the frequency content with fewer dimensions. Using such", "reference": "Here is an example of what you can achieve using this model:\nThe decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer.\nWaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\nThe Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow."}
{"idx": 47, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Method\n[20] layer containing 512 units (256 in each direction) to generate the encoded features. The encoder output is consumed by an attention network which summarizes the full encoded sequence as a fixed-length context vector for each decoder output step. We use the location-sensitive attention from [21], which extends the additive attention mechanism [22] to use cumulative attention weights from previous decoder time steps as an additional feature. This encourages", "prediction": "a new attention architecture for decoding.", "reference": "Here is an example of what you can achieve using this model:\nThe decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer.\nWaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\nThe Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow."}
{"idx": 48, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Method\nthe model to move forward consistently through the input, mitigating potential failure modes where some subsequences are repeated or ignored by the decoder. Attention probabilities are computed after projecting inputs and location features to 128-dimensional hidden representations. Location features are computed using 32 1-D convolution filters of length 31. The decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous time step is first passed through a small pre-net containing 2 fully connected layers of 256 hidden ReLU units. We found that the pre-net acting as an information bottleneck was essential for learning attention. The prenet output and attention context vector are concatenated and passed through a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each Waveform Mel Spectrogram Samples WaveNet Post-Net MoL Linear Projection Pre-Net Layers Linear Projection Stop Token Location Sensitive Attention Character Embedding Layers Bidirectional LSTM Input Text Fig. 1. Block diagram of the Tacotron 2 system architecture. post-net layer is comprised of 512 filters with shape 5 1 with batch normalization, followed by tanh activations on all but the final layer. We minimize the summed mean squared error (MSE) from before and after the post-net to aid convergence. We also experimented with a log-likelihood loss by modeling the output distribution with a Mixture Density Network [23, 24] to avoid assuming a constant variance over time, but found that these were more difficult to train and they did not lead to better sounding samples. In parallel to spectrogram frame prediction, the concatenation of decoder LSTM output and the attention context is projected down to a scalar and passed through a sigmoid activation to predict the probability that the output sequence has completed. This “stop token” prediction is used during inference to allow the model to dynamically determine when to terminate generation instead of always generating for a fixed duration. Specifically, generation completes at the first frame for which this probability exceeds a threshold of 0.5. The convolutional layers in the network are regularized using dropout [25] with probability 0.5, and LSTM layers are regularized using zoneout [26] with probability 0.1. In order to introduce output variation at inference time, dropout with probability 0.5 is applied only to layers in the pre-net of the autoregressive decoder. In contrast to the original Tacotron, our model uses simpler building blocks, using vanilla LSTM and convolutional layers in the encoder and decoder instead of “CBHG” stacks and GRU recurrent layers. We do not use a “reduction factor”, i.e., each decoder step corresponds to a single spectrogram frame. 2.3. WaveNet Vocoder We use a modified version of the WaveNet architecture from [8] to invert the mel spectrogram feature representation into time-domain waveform samples. As in the original architecture, there are 30 dilated convolution layers, grouped into 3 dilation cycles, i.e., the dilation rate of layer k (k = 0 . . . 29) is 2k (mod 10). To work with the 12.5 ms frame hop of the spectrogram frames, only 2 upsampling layers are used in the conditioning stack instead of 3 layers. Instead of predicting discretized buckets with a softmax layer, we follow PixelCNN++ [27] and Parallel WaveNet [28] and use a 10component mixture of logistic", "prediction": "LSTM layers are used to predict the target spectrogram frame. The LSTM layer is a stack of 2 uni-directional LSTM layers with 1024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction.", "reference": "Here is an example of what you can achieve using this model:\nThe decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer.\nWaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\nThe Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow."}
{"idx": 49, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Method\ndistributions (MoL) to generate 16-bit samples at 24 kHz. To compute the logistic mixture distribution, the WaveNet stack output is passed through a ReLU activation followed by a linear projection to predict parameters (mean, log scale, mixture weight) for each mixture component. The loss is computed as the negative log-likelihood of the ground truth sample.", "prediction": "Section: WaveNet is a multi-core, multi-processor, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core, multi-core,", "reference": "Here is an example of what you can achieve using this model:\nThe decoder is an autoregressive recurrent neural network which predicts a mel spectrogram from the encoded input sequence one frame at a time. The prediction from the previous timestep is first passed through a small pre-net containing two fully connected layers of 256 hidden ReLU units. The prenet output and attention context vector are concatenated and passed to a stack of two LSTM layers with 1,024 units. The concatenation of the LSTM output and the attention context vector is projected through a linear transform to predict the target spectrogram frame. Finally, the predicted mel spectrogram is passed through a 5-layer convolutional post-net which predicts a residual to add to the prediction to improve the overall reconstruction. Each post-net layer is comprised of 512 filters with shape 5 × 1 with batch normalization, followed by tanh activations on all but the final layer.\nWaveGlow is a generative model that generates audio by sampling from a distribution. To use a neural network as a generative model, we take samples from a simple distribution, in our case, a zero mean spherical Gaussian with the same number of dimensions as our desired output, and put those samples through a series of layers that transforms the simple distribution to one which has the desired distribution. In this case, we model the distribution of audio samples conditioned on a mel spectrogram.\nThe Tacotron2 and WaveGlow models are trained separately and independently — both models obtain mel spectrograms from short time Fourier transform (STFT) during training. These mel spectrograms are used for loss computation in case of Tacotron 2 and as conditioning input to the network in case of WaveGlow."}
{"idx": 50, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Experiments / Results\nExperiments / Results: 3. EXPERIMENTS & RESULTS 3.1. Training Setup Our training process involves first training the feature prediction network on its own, followed by training a modified WaveNet independently on the outputs generated by the first network. To train the feature prediction network, we apply the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999,  = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. This ensures that each predicted frame exactly aligns with the target waveform samples. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999,  = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates. Therefore we maintain an exponentially-weighted moving average of the network parameters over update steps with a decay of 0.9999 – this version is used for inference (see also [29]). To speed up convergence, we scale the waveform targets by a factor of 127.5 which brings the initial outputs of the mixture of logistics layer closer to the eventual distributions. We train all models on an internal US English dataset[12], which contains 24.6 hours of speech from a single professional female speaker. All text in our datasets is spelled out. e.g., “16” is written as “sixteen”, i.e., our models are all trained on normalized text. 3.2. Evaluation When generating speech in inference mode, the ground truth targets are not known. Therefore, the predicted outputs from the previous step are fed in during decoding, in contrast to the teacher-forcing configuration used for training. We randomly selected 100 fixed examples from the test set of our internal dataset as the evaluation set. Audio generated on this set are sent to a human rating service similar to Amazon’s Mechanical Turk where each sample is rated by at least 8 raters on a scale from 1 to 5 with 0.5 point increments, from which a subjective mean opinion score (MOS) is calculated. Each evaluation is conducted independently from each other, so the outputs of two different models are not directly compared when raters assign a score to them. Note that while instances in the evaluation set never appear in the training set, there are some recurring patterns and common words between the two sets. While this could potentially result in an inflated MOS compared to an evaluation set consisting of sentences generated from random words, using this set allows us to compare to the ground truth. Since all the systems we compare are trained on the same data, relative comparisons are still meaningful. Table 1 shows a comparison of our method against various prior systems. In order to better isolate the effect of using mel spectrograms as features, we compare to a WaveNet conditioned on linguistic features[8] with similar modifications to the WaveNet architecture as introduced above. We also compare to the original Tacotron that predicts linear spectrograms and uses Griffin-Lim to synthesize audio, as well as concatenative [30] and parametric [31] baseline systems, both of which have been used in production at Google. We find that the proposed system significantly outpeforms all other TTS systems, and results in an MOS comparable to that of the ground truth audio.  System MOS Parametric 3.492  0.096 Tacotron (Griffin-Lim) 4.001  0.087 Concatenative 4.166  0.091 WaveNet (Linguistic) 4.341  0.051 Ground truth 4.582  0.053 Tacotron 2 (this paper) 4.526  0.066 Table 1. Mean Opinion Score (MOS) evaluations with 95% confidence intervals computed from the t-distribution for various systems. We also conduct a side", "prediction": "We train the feature prediction network on the outputs generated by the first network. We use the standard maximum-likelihood training procedure (feeding in the correct output instead of the predicted output on the decoder side, also referred to as teacher-forcing) with a batch size of 64 on a single GPU. We use the Adam optimizer [29] with 1 = 0.9, 2 = 0.999, = 106 and a learning rate of 103 exponentially decaying to 105 starting after 50,000 iterations. We also apply L2 regularization with weight 106. We then train our modified WaveNet on the ground truth-aligned predictions of the feature prediction network. That is, the prediction network is run in teacher-forcing mode, where each predicted frame is conditioned on the encoded input sequence and the corresponding previous frame in the ground truth spectrogram. We train with a batch size of 128 distributed across 32 GPUs with synchronous updates, using the Adam optimizer with 1 = 0.9, 2 = 0.999, = 108 and a fixed learning rate of 104. It helps quality to average model weights over recent updates.", "reference": "Dropout instead of Zoneout to regularize the LSTM layers;\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures.\nUsing mixed precision training previously required two steps:\nPorting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values.\nMixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed.\nMixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook.\nTraining Performance\nTable 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch.\nNumber of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86\nNumber of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22\nTable 2: Training performance results for WaveGlow model\nAs shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size. Speech quality depends on model size and training set size; using Tensor Cores with automatic mixed precision makes it possible to train higher quality models in the same amount of time.\nConsidering model size and amount of training needed for high quality, GPUs offer a most suitable hardware architecture with an optimal combination of throughput, bandwidth, scalability, and ease of use.\nInference Performance\nTraining\nThe training loss is averaged over an entire training epoch, whereas the validation loss is averaged over the validation dataset. Performance is reported in total input tokens per second for the Tacotron 2 model, and in total output samples per second for the WaveGlow model. Both measures are recorded as train_iter_items/sec (after each iteration) and train_epoch_items/sec (averaged over epoch) in the output log. The result is averaged over an entire training epoch and summed over all GPUs that were included in the training.\nBy default, our training scripts will launch mixed precision training with Tensor cCores. You can change this behavior by removing the –fp16-run flag."}
{"idx": 51, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Experiments / Results\n-by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). The overall mean score of 0.270 0.155 shows that raters have a small but statistically significant preference towards ground truth over our results. See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. Fig. 2. Synthesized vs. ground truth: 800 ratings on 100 items. We ran a separate rating experiment on the custom 100-sentence test set from Appendix E of [11], obtaining a MOS of 4.354. In a manual analysis of the error modes of our system, counting errors in each category independently, 0 sentences contained repeated words, 6 contained mispronunciations, 1 contained skipped words, and 23 were subjectively decided to contain unnatural prosody, such as emphasis on the wrong syllables or words, or unnatural pitch. End-point prediction failed in a single case, on the input sentence containing the most characters. These results show that while our system is able to reliably attend to the entire input, there is still room for improvement in prosody modeling. Finally, we evaluate samples generated from 37 news headlines to test the generalization ability of our system to out-of-domain text. On this task, our model receives a MOS of 4.1480.124 while WaveNet conditioned on linguistic features receives a MOS of 4.137 0.128. A side-by-side evaluation comparing the output of these systems also shows a virtual tie – a statistically insignificant preference towards our results by 0.142 0.338. Examination of rater comments shows that our neural system tends to generate speech that feels more natural and human-like, but it sometimes runs into pronunciation difficulties, e.g., when handling names. This result points to a challenge for end-to-end approaches – they require training on data that cover intended usage. 3.3. Ablation Studies 3.3.1. Predicted Features versus Ground Truth While the two components of our model were trained separately, the WaveNet component depends on the predicted features for training. An alternative is to train WaveNet independently on mel spectrograms extracted from ground truth audio. We explore this in Table 2. Synthesis Training Predicted Ground truth Predicted 4.526 0.066 4.449 0.060 Ground truth 4.362 0.066 4.522 0.055 Table 2. Comparison of evaluated MOS for our system when WaveNet trained on predicted/ground truth mel spectrograms are made to synthesize from predicted/ground truth mel spectrograms. As expected, the best performance is obtained when the features used for training match those used for inference. However, when trained on ground truth features and made to synthesize from predicted features, the result is worse than the opposite. This is due to the tendency of the predicted spectrograms to be oversmoothed and less detailed than the ground truth – a consequence of the squared error loss optimized by the feature prediction network. When trained on ground truth spectrograms, the network does not learn to generate high quality speech waveforms from oversmoothed features. 3.3.2. Linear Spectrograms Instead of predicting mel spectrograms, we experiment with training to predict linear-frequency spectrograms instead, making it possible to invert the spectrogram using Griffin-Lim. System MOS Tacotron 2 (Linear + G-L) 3.944 0.091 Tacotron 2 (Linear + WaveNet) 4.510 0.054 Tacotron 2 (Mel + WaveNet) 4.526 0.066 Table 3. Comparison of evaluated MOS for Griffin-Lim vs. WaveNet as a vocoder, and using 1,025-dimensional linear spectrograms vs. 80-dimensional mel", "prediction": "Section: Experiments / Results -by-side evaluation between audio synthesized by our system and the ground truth. For each pair of utterances, raters are asked to give a score ranging from -3 (synthesized much worse than ground truth) to 3 (synthesized much better than ground truth). See Figure 2 for a detailed breakdown. The comments from raters indicate that occasional mispronunciation by our system is the primary reason for this preference. See Figure 2 for a detailed breakdown.", "reference": "Dropout instead of Zoneout to regularize the LSTM layers;\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures.\nUsing mixed precision training previously required two steps:\nPorting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values.\nMixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed.\nMixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook.\nTraining Performance\nTable 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch.\nNumber of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86\nNumber of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22\nTable 2: Training performance results for WaveGlow model\nAs shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size. Speech quality depends on model size and training set size; using Tensor Cores with automatic mixed precision makes it possible to train higher quality models in the same amount of time.\nConsidering model size and amount of training needed for high quality, GPUs offer a most suitable hardware architecture with an optimal combination of throughput, bandwidth, scalability, and ease of use.\nInference Performance\nTraining\nThe training loss is averaged over an entire training epoch, whereas the validation loss is averaged over the validation dataset. Performance is reported in total input tokens per second for the Tacotron 2 model, and in total output samples per second for the WaveGlow model. Both measures are recorded as train_iter_items/sec (after each iteration) and train_epoch_items/sec (averaged over epoch) in the output log. The result is averaged over an entire training epoch and summed over all GPUs that were included in the training.\nBy default, our training scripts will launch mixed precision training with Tensor cCores. You can change this behavior by removing the –fp16-run flag."}
{"idx": 52, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Experiments / Results\nspectrograms as conditioning inputs to WaveNet. As noted in [10], WaveNet produces much higher quality audio compared to Griffin-Lim. However, there is not much difference between the use of linear-scale or mel-scale spectrograms. As such, the use of mel spectrograms seems to be a strictly better", "prediction": "The main purpose of this article is to describe the experiments that have been conducted to evaluate the use of mel-scale spectrograms as conditioning inputs to WaveNet. The experiments were conducted in the following order: WaveNet was developed in the late 1990s. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from a source that is a source of noise. WaveNet was developed to analyze the audio signal from ", "reference": "Dropout instead of Zoneout to regularize the LSTM layers;\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures.\nUsing mixed precision training previously required two steps:\nPorting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values.\nMixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed.\nMixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook.\nTraining Performance\nTable 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch.\nNumber of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86\nNumber of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22\nTable 2: Training performance results for WaveGlow model\nAs shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size. Speech quality depends on model size and training set size; using Tensor Cores with automatic mixed precision makes it possible to train higher quality models in the same amount of time.\nConsidering model size and amount of training needed for high quality, GPUs offer a most suitable hardware architecture with an optimal combination of throughput, bandwidth, scalability, and ease of use.\nInference Performance\nTraining\nThe training loss is averaged over an entire training epoch, whereas the validation loss is averaged over the validation dataset. Performance is reported in total input tokens per second for the Tacotron 2 model, and in total output samples per second for the WaveGlow model. Both measures are recorded as train_iter_items/sec (after each iteration) and train_epoch_items/sec (averaged over epoch) in the output log. The result is averaged over an entire training epoch and summed over all GPUs that were included in the training.\nBy default, our training scripts will launch mixed precision training with Tensor cCores. You can change this behavior by removing the –fp16-run flag."}
{"idx": 53, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Experiments / Results\nchoice since it is a more compact representation. It would be interesting to explore the trade-off between the number of mel frequency bins versus audio quality in future work. processing network to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary when WaveNet is used as the vocoder. To answer this question, we compared our model with and without the post-net, and found that without it, our model only obtains a MOS score of 4.429  0.071, compared to 4.526  0.066 with it, meaning that empirically the post-net is still an important part of the network design. 3.3.4. Simplifying WaveNet A defining feature of WaveNet is its use of dilated convolution to increase the receptive field exponentially with the number of layers. We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. As shown in Table 4, we find that our model can generate highquality audio using as few as 12 layers with a receptive field of 10.5 ms, compared to 30 layers and 256 ms in the baseline model. These results confirm the observations in [9] that a large receptive field size is not an essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller than the baseline and the quality degrades significantly even though the stack is as deep as the baseline model. This indicates that the model requires sufficient context at the time scale of waveform samples in order to generate high quality sound. Total layers Num cycles Dilation cycle size Receptive field (samples / ms) MOS 30 3 10 6,139 / 255.8 4.526  0.066 24 4 6 505 / 21.0 4.547  0.056 12 2 6 253 / 10.5 4.481  0.059 30 30 1 61 / 2.5 3.930  0.076 Table 4. WaveNet with various layer and receptive field sizes.", "prediction": "WaveNet is a vocoder that uses a convolutional network to generate audio. WaveNet uses a post-net to incorporate past and future frames after decoding to improve the feature predictions. However, because WaveNet already contains convolutional layers, one may wonder if the post-net is still necessary post-net to incorporate past and future frames after decoding to incorporate past and future frames after decoding to improve the feature predictions. Post-net We evaluate models with varying receptive field sizes and number of layers to test our hypothesis that a shallow network with a small receptive field may solve the problem satisfactorily since mel spectrograms are a much closer representation of the waveform than linguistic features and already capture long-term dependencies across frames. These results confirm the observations in [9] that a large receptive field size is not essential factor for audio quality. However, we hypothesize that it is the choice to condition on mel spectrograms that allows this reduction in complexity. On the other hand, if we eliminate dilated convolutions altogether, the receptive field becomes two orders of magnitude smaller", "reference": "Dropout instead of Zoneout to regularize the LSTM layers;\nMixed precision training offers significant computational speedup by performing operations in half-precision format, while storing minimal information in single-precision (FP32) to retain as much information as possible in critical parts of the network. Enabling mixed precision takes advantage of Tensor Cores on Volta and Turing GPUs delivering significant speedups in training time — up to 3x overall speedup on the most arithmetically intense model architectures.\nUsing mixed precision training previously required two steps:\nPorting the model to use the FP16 data type where appropriate; Manually adding loss scaling to preserve small gradient values.\nMixed precision is enabled in PyTorch by using the Automatic Mixed Precision (AMP), library from APEX that casts variables to half-precision upon retrieval, while storing variables in single-precision format. To preserve small gradient magnitudes in backpropagation, a loss scaling step must be included when applying gradients. In PyTorch, loss scaling can be easily applied by using the scale_loss() method provided by AMP. The scaling value to be used can be dynamic or fixed.\nMixed precision training with tensor cores can be enabled by adding the –amp-run flag in the training script, you can see the example in our Jupyter notebook.\nTraining Performance\nTable 1 and Table 2 compare the training performance of the modified Tacotron 2 and WaveGlow models with mixed precision and FP32, using the PyTorch-19.06-py3 NGC container on an NVIDIA DGX-1 with 8-V100 16GB GPUs. Performance numbers (in output mel spectrograms per second for Tacotron 2 and output samples per second for WaveGlow) were averaged over an entire training epoch.\nNumber of GPUs Mixed Precision mels/sec FP32 mels/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 20,992 12,933 1.62 1.00 1.00 4 74,989 46,115 1.63 3.57 3.57 8 140,060 88,719 1.58 6.67 6.86\nNumber of GPUs Mixed Precision samples/sec FP32 samples/sec Speed-up with Mixed Precision Multi-GPU Weak Scaling with Mixed Precision Multi-GPU Weak Scaling with FP32 1 81,503 36,671 2.22 1.00 1.00 4 275,803 124,504 2.22 3.38 3.40 8 583,887 264,903 2.20 7.16 7.22\nTable 2: Training performance results for WaveGlow model\nAs shown in Table 1 and 2, using Tensor Cores for mixed precision training achieves a substantial speedup and scales efficiently to 4/8 GPUs. Mixed precision training also maintains the same accuracy as single-precision training and allows bigger batch size. Speech quality depends on model size and training set size; using Tensor Cores with automatic mixed precision makes it possible to train higher quality models in the same amount of time.\nConsidering model size and amount of training needed for high quality, GPUs offer a most suitable hardware architecture with an optimal combination of throughput, bandwidth, scalability, and ease of use.\nInference Performance\nTraining\nThe training loss is averaged over an entire training epoch, whereas the validation loss is averaged over the validation dataset. Performance is reported in total input tokens per second for the Tacotron 2 model, and in total output samples per second for the WaveGlow model. Both measures are recorded as train_iter_items/sec (after each iteration) and train_epoch_items/sec (averaged over epoch) in the output log. The result is averaged over an entire training epoch and summed over all GPUs that were included in the training.\nBy default, our training scripts will launch mixed precision training with Tensor cCores. You can change this behavior by removing the –fp16-run flag."}
{"idx": 54, "input": "Generate a story for these personas: Developer, Researcher\n\nSection: Conclusion / Future Work\nConclusion / Future Work: 4. CONCLUSION This paper describes Tacotron 2, a fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predicts mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "prediction": "A fully neural TTS system that combines a sequence-to-sequence recurrent network with attention to predict mel spectrograms with a modified WaveNet vocoder. The resulting system synthesizes speech with Tacotron-level prosody and WaveNet-level audio quality. This system can be trained directly from data without relying on complex feature engineering, and achieves state-of-the-art sound quality close to that of natural human speech.", "reference": "The optimized Tacotron2 model2 and the new WaveGlow model1 take advantage of Tensor Cores on NVIDIA Volta and Turing GPUs to convert text into high quality natural sounding speech in real-time. The generated audio has a clear human-like voice without background noise.\nOur TTS system is a combination of two neural network models:\nThe Tacotron 2 and WaveGlow model form a TTS system that enables users to synthesize natural sounding speech from raw transcripts without any additional prosody information.\nTacotron 2 Model\nOur implementation of the Tacotron 2 model differs from the model described in1, we use:\nTable 1: Training performance results for modified Tacotron 2 model\nTable 3 and Table 4 show inference statistics for the Tacotron2 and WaveGlow text-to-speech system, gathered from 1,000 inference runs, on 1-V100 and 1-T4 GPU, respectively. Latency is measured from the start of Tacotron2 inference to the end of WaveGlow inference. The tables include average latency, standard deviation, and latency confidence intervals (percent values). Throughput is measured as the number of generated audio samples per second. RTF is the real-time factor which tells how many seconds of speech are generated in 1 second of wall time.\nTable 3: Inference statistics for Tacotron2 and WaveGlow system on 1-V100 GPU\nTable 4: Inference statistics for Tacotron2 and WaveGlow system on 1-T4 GPU\nFollow the scripts on GitHub or run the Jupyter notebook step-by-step, to train Tacotron 2 and WaveGlow v1.5 models. In the Jupyter notebook, we provided scripts that are fully automated to download and pre-process the LJ Speech dataset; After the data preparation step, use the provided Dockerfile to build the modified Tacotron 2 and WaveGlow container, and start a detached session in the container; To train our model using AMP with Tensor Cores or using FP32, perform the training step using the default parameters of the Tacrotron 2 and WaveGlow models using a single GPU or multiple GPUs.\nAfter training Tacotron 2 and WaveGlow models, or downloaded the pre-trained checkpoints for the respective models, you can perform inference which takes text as input, and produces an audio file."}
{"idx": 55, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: Abstract We propose a cross-lingual neural codec language model, VALL-E X, for crosslingual speech synthesis. Specifically, we extend VALL-E [Wang et al., 2023] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-tospeech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at https://aka.ms/vallex. VALL-E X (Cross-Lingual VALL-E) Cross-Lingual Neural Codec Language Modeling Multilingual G2P Multilingual G2P Audio Codec Encoder Source Text Target Text Source Speech Figure 1: The overall framework of VALL-E X, which can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training, and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation. Personalized Target Speech Audio Codec Decoder Language ID Source phoneme sequences Target phoneme sequences Source acoustic tokens Target acoustic tokens 1", "prediction": "VALL-E X is a cross-lingual neural codec language model that can synthesize personalized speech in another language for a monolingual speaker. It is based on VALL-E, which is a multi-lingual conditional codec language model that can predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information."}
{"idx": 56, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: Introduction Recent years have witnessed significant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity [Li et al., 2019, Ren et al., 2019, Tan et al., 2022]. However, these models can only generate high-quality speech for a specific speaker in a specific language. Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. Previous methods to tackle these challenges typically augment end-to-end TTS models with specific subnets for speaker and language control [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2020, Ellinas et al., 2022, Cai et al., 2023]. For example, based on the multi-speaker TTS model, Nachmani and Wolf [2019] introduce multiple encoders for each language and additional loss to keep the speaker’s identity. Zhang et al. [2020] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities. Yang and He [2020] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively. Yang and He [2022] further propose a multi-task learning method with additional tasks of speaker similarity and language identification. Moreover, Cai et al. [2023] investigates cross-lingual multi-speaker text-to-speech synthesis with sufficient or limited bilingual speech training data. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [Zhang et al., 2019, Lee et al., 2022]. Table 1: A comparison between VALL-E X and previous cross-lingual TTS systems. Previous Systems VALL-E X Intermediate representation Mel spectrogram Audio codec codes Training data 13K hours Speech accent Foreign Native Speaker similarity Relative low High In-context learning Zero-shot cross-lingual TTS In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, VALL-E X, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis. Based on the knowledge learned from large-scale multi-lingual speech data, VALL-E X is able to transfer the speech characteristics, including the speaker’s voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems. More specifically, we first obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data. Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an offline neural codec encoder. Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model. As illustrated in Figure 1, after training, VALL-E X can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language. The generated acoustic token sequence is decompressed to the target speech waveform by an offline audio codec", "prediction": "The purpose of this paper is to introduce a new approach to cross-lingual speech synthesis. The proposed approach is based on the multi-speaker TTS model. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi", "reference": "Model versions\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 57, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\ndecoder. VALL-E X is trained on two large-scale multi-speaker datasets1, LibriLight [Kahn et al., 2020] and WenetSpeech [Zhang et al., 2022a], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively. The combination of LibriLight and Wenet", "prediction": "VALL-E X is a multi-speaker decoder that can be trained on a large number of speakers.", "reference": "Model versions\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 58, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which significantly improves the coverage of different speakers and enhances VALL-E X’s generalization capacity. The comparison between VALL-E X and the previous cross-lingual TTS systems are listed in Table 1. We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot crosslingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST). For cross-lingual text-to-speech synthesis, the proposed VALL-E X is evaluated with LibriSpeech [Panayotov et al., 2015] and EMIME [Wester, 2010] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers. For zero-shot speech-to-speech translation, EMIME [Wester, 2010] dataset is used for the evaluation of VALL-E X on bidirectional ChineseEnglish translation tasks, and it contains bilingual audio recordings by the same speakers. We evaluate the proposed VALL-E X framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS). Specifically, due to the strong in-context learning capability, VALL-E X achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker. By training on large-scale speech-transcription data, the proposed VALL-E X significantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness. Furthermore, the human evaluation shows that our VALL-E X outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST). Our contributions can be summarized as follows: • We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data. VALL-E X is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts. • The multi-lingual in-context learning framework enables VALL-E X to generate crosslingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language. • Based on the learned cross-lingual speech modeling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks. • We apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation. We encourage readers to listen to the audio samples on our demo page: https://aka.ms/vallex. 2", "prediction": "VALL-E X is a cross-lingual text-to-speech generation framework that is trained on a large multi-lingual multi-speaker multi-domain unclean speech dataset. The proposed framework achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker.", "reference": "Model versions\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 59, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 3.2 Model Framework Inspired by VALL-E, the cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM MAR is only used to predict the acoustic tokens from the first quantizer of EnCodec model. Formally, based on paired speech-transcription data in any language, let S denote the transcribed phoneme sequence, and A:,1 ai,1|i = 1, . . . , N denotes the first-layer acoustic tokens extracted from the speech X. The decoder MAR, modeling the concatenated sequence S, A:,1, is trained to predict A:,1 autoregressively. It is optimized by maximizing the log-likelihood, Quantization & Phonemization: Quantized tokens Layer L 12 504 31 8 743 ... A:,2:L ...... Layer 2 325 71 435 90 7 ... A:,1 731 284 78 32 669 ... Layer 1 audio codec encoder (Quantization) Multi-lingual speech-transcription pairs (phonemization) G2P tool Phoneme tokens HH AH L OW ... D S N Y Multi-lingual Non-Autoregressive Codec LM Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM MNAR is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer. It is prompted by the phoneme sequence of the current sentence (S) and the acoustic token sequence of another sentence with the same speaker (  A). Here  A is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer l [2, 8], the embeddings of l 1 layers’ acoustic tokens (A:,1:l1) are summed up layerwise as input. The learning objective for the l-layer acoustic tokens A:,l can be calculated as 8 X l=2 log pNAR A:,l | D S,  A:,1:8, A:,1:l1 E ; MNAR (2) LMNAR = where means the sequence concatenation. pNAR(.) computes the pointwise probabilities of A:,l. 3.3 Multi-lingual Training In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-tospeech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (Ss, As) and (St, At) to train our multi-lingual codec LMs MAR and MNAR, where s and t represent two different (source and target) languages. Language ID Module Following multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in VALL-E X. On the one hand, without language ID", "prediction": ") The cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM MAR The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM ", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 60, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n, VALL-E X may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM MAR is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens. 3.4 Cross-Lingual Inference After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we first concatenate source phonemes Ss and target phonemes St as prompts, and take the first-layer source acoustic tokens As :,1 as the decoding prefix, condition on which the multi-lingual autoregressive codec LM MAR generates the first-layer target acoustic tokens At :,1, at i,1 pAR  at i,1 | Ss, St, As :,1, At i,1 ; MAR , i = 1, . . . , (3) where means probability-based sampling. The sampling is stopped until the end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the final generated speech. After obtaining the first-layer target acoustic tokens At :,1 from MAR, multi-lingual non-autoregressive codec LM MNAR is used to predict the rest layers of acoustic tokens n At :,l | l = 2, . . . , 8 o by greedy search, i.e., choosing the tokens with maximum probabilities, At :,l = argmax At :,l pNAR  At :,l | St, As :,1:8, At :,1:l1 ; MNAR , l = 2, . . . , 8. (4) Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens At :,1:8. tl= 2, ... , 8 VALL-E X Inference: A:,l Multi-lingual Non-Autoregressive Codec LM t s A:,1:l1 St A:,1:8 t A:,1 Multi-lingual Autoregressive Codec LM B t aN,1 t a2,1 t ... s a1,1 bos> Ss St A:,1 Figure 3: Inference illustration of the cross-lingual neural codec language model VALL-E X, with two-stage decoding strategies. VALL-E X can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks. 4 VALL-E X Application VALL-E X can be applied to various cross-lingual speech generation tasks. In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in Figure 3. 4.1 Zero-Shot Cross-Lingual TTS The proposed VALL-E X is naturally suitable for zero-shot cross-lingual TTS tasks. Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker’s voice. Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability. Thanks to the in-context learning capability of large language models, VALL-E X surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis. More specifically, given the source speech, source transcript, and target text, we first convert source speech into source acoustic token As using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes Ss and target phonemes St using G2P tool. More specifically, as introduced in", "prediction": "VALL-E X is a multi-lingual speech synthesis system that can be used to generate cross-lingual speech. The main idea of this paper is to introduce a cross-lingual approach to speech synthesis.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 61, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nSection 3.4, we let St be the phonemes extracted from the target text, Ss and As be the phonemes and acoustic tokens extracted from the source speech. Then VALL-E X generates the full-layer target acoustic tokens, which are finally decompressed into the target speech by EnCodec decoder. 4.2 Zero-Shot Speech-to-Speech Translation We can also apply our VALL-E X to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences. Speech Recognition & Translation Model We leverage the improved SpeechUT [Zhang et al., 2022c] as our speech recognition & translation model, which is a unified-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss) and MT corpus (source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more As Application: Zero-shot cross-lingual TTS Ss St Codec Encoder G2P Tool G2P Tool Source text Target text Source speech Application: Zero-shot S2ST Ss St As E Codec Encoder Semantic Semantic Encoder Decoder Speech Encoder Source Speech pre-training details about this model. After pre-training, the model is fine-tuned with (X s, Ss, St) triplet data derived from the ST corpus. Specifically, we perform multi-task learning with the CTC [Graves et al., 2006] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes. Inference Figure 3 shows the inference process of speech-to-speech translation. Given a source speech X s, the speech recognition & translation model first generates the source phonemes Ss from the semantic encoder and the target phonemes St from the semantic decoder. Besides, we use the EnCodec encoder to compress X s into source acoustic tokens As. Then, we concatenate Ss, St, and As, as the input of VALL-E X, to produce the acoustic token sequence for the target speech, as introduced in Section 3.4. The generated acoustic tokens are converted to the final target speech with the decoder of EnCodec. 4.3 Evaluation The proposed model is verified using various evaluation criteria, including speaker similarity (ASVScore), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation. Specifically, we measure speaker similarity between synthesized target speech and groudtruth target speech or source speech as an automatic speaker verification (ASV) task, where a WavLM [Chen et al., 2022] based ASV model is used to calculate the score. To verify the quality of generated speech, we first utilize the ASR system from the released HuBERT-Large model [Hsu et al., 2021] to recognize it into text. For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text. For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text. Finally,", "prediction": "VALL-E X is a speech-to-text translation model that can be applied to zero-shot speech-to-speech translation tasks. SpeechUT is a speech-to-text pre-training framework that uses hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and ST. Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 62, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nto better verify our proposed VALL-E X systems, we adopt the open-source NISQA3 [Mittag and Möller, 2021] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS). 5", "prediction": "The VALL-E X systems are based on the NISQA3 model to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 63, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: Experiments We evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5, OpenSubtitles20186 and WMT20207, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama8, and news domains, respectively. The EnglishChinese ST data is from GigaST [Ye et al., 2022], which is created by translating the transcripts in GigaSpeech [Chen et al., 2021] using a strong machine translation system. Similarly, we create the ChineseEnglish ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above. 3https://github.com/gabrielmittag/NISQA 4https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech 5https://challenger.ai/competition/translation 6https://opus.nlpl.eu/OpenSubtitles2018.php 7https://www.statmt.org/wmt20/translation-task.html 8http://www.opensubtitles.org/ We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [Wester, 2010], which contains bilingual Chinese/English speech recorded by the same speakers. There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350. Zero-shot cross-lingual TTS is evaluated using Librispeech [Panayotov et al., 2015] dev-clean set and EMIME dataset providing English and Chinese data, respectively. We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts. 5.2 Experimental Setup Phonemization & Quantization The right picture of Figure 2 illustrates the phonemization & quantization processes for different languages. All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets. We use a unified phoneme set called BigCiDian9 for two languages which are based on International Phonetic Alphabet (IPA). The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools10 for additional alignment information used for the pre-training of speech recognition & translation model. The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model EnCodec11, which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks.", "prediction": "We train a speech recognition & translation model for cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from Librispeech [Panayotov et al., 2015] containing about 60,000 hours of unlabeled speech. We train a Kaldi4 ASR model on the labeled Librispeech dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5,", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 64, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nModel Architecture For the cross-lingual codec language models, MAR and MNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096. The autoregression is implemented by attention masking in the MAR model. Sinuous position embedding is separately computed for each prompt sequence in MAR and MNAR models. Besides, the M", "prediction": "NAR model has a recurrent architecture with a recurrent model.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 65, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nNAR model uses individual layer normalization for generating each layer of acoustic tokens. We also introduce the model architecture of speech recognition & translation for S2ST in Appendix A.1.2. We call our cross-lingual TTS model and S2ST model as VALL-E X and VALL-E X Trans in the subsequent experiments, respectively. Training Details We optimize each module of VALL-E X individually, including MAR and MNAR. For both modules, The maximum sentence length is set to 20 seconds, so we re-segment the LibriLight data to an average utterance duration of 12 seconds by detecting the consecutive silence phonemes. Fortunately, the WenetSpeech data has already been segmented into short utterances. The maximum learning rate is 5e-4 with warm-up steps of 8,000. The models are trained on 32 V100 GPUs for 800k steps. MAR is trained with the batch size of 120 seconds per GPU, which is 66 seconds for MNAR due to the memory constraint. When optimizing MNAR, instead of accumulating all layer’s loss in Eqn. (2), we randomly select one layer at each optimization step for efficiency. For speech recognition & translation model, the training details can be found in Appendix A.1.3. Baselines We adopt YourTTS12 [Casanova et al., 2022] as our baseline for zero-shot cross-lingual TTS. YourTTS is a zero-shot multi-speaker TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech. Since previous work shows that current end-to-end S2ST systems underperform cascaded S2ST systems [Jia et al., 2022a, Lee et al., 2021b], we also build an S2ST baseline which is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS. The source speech serves as the reference speech when synthesizing the target speech using YourTTS. The ASR model is the released HuBERT model introduced in Section 4.3, and the MT model is a vanilla Transformer trained by ourselves on the MT data introduced in Section 5.1. Since YourTTS is built only for English, we don’t get its performance for EnglishChinese translation direction. Table 2: Zero-shot cross-lingual TTS evaluation for English TTS with Chinese speech as prompts and Chinese TTS with English speech as prompts, using automatic evaluation matrices, including ASV-Score (hypothesis vs. prompt), ASR-WER, and Naturalness. ASV-Score ASR-WER Naturalness English TTS with Chinese as prompts Baseline (YourTTS) 0.300.10 8.53 3.36 VALL-E X 0.360.11 4.07 3.54 Chinese TTS with English as prompts VALL-E X 0.290.10 8.52 3.36 5.3 Zero-Shot Cross-Lingual TTS Evaluation We first select samples with a length between 4 and 10 seconds from LibriSpeech dev-clean set, resulting in 40 speakers and 1373 samples. For English TTS, we randomly select one audio from EMIME set as the Chinese prompt for each target sentence in LibriSpeech dev-clean set. For Chinese TTS, we use extra 149 Chinese text sentences provided by the EMIME set and repeat them to the total number of 1373 so that they can be prompted by the LibriSpeech audios one-by-one. When synthesizing the target language speech, the whole sequence of the source language speech is used as the prompt. Automatic Evaluation Table 2 summarizes the results of cross-lingual zero-shot TTS tasks, including English TTS prompted by Chinese speech and Chinese TTS prompted by English speech. We measure the speaker similarity using the automatic speaker verification (ASV) model, ranging from -1 to +1 given two speech utterances. The larger the value, the more similar the speakers of", "prediction": "VALL-E X is a cross-lingual TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech. The S2ST baseline is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS12 [Casanova et al., 2022].", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 66, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nthe two utterances are. The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of VALL-E X is superior to that of the baseline (0.36 vs 0.30). (2) VALL-E X reduces the WER significantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method", "prediction": "The purpose of this paper is to develop a method to reduce the word embedding error (WER) of TTS with Chinese as prompts. The method is based on the VALL-E X method. The method is based on the following: The speaker similarity between the hypothesis and prompts of the VALL-E X method is superior to that of the baseline (0.36 vs 0.30). The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of the VALL-E X method is superior to that of the baseline (0.36 vs 0.30). (2) VALL-E X reduces the WER significantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 67, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\n. (3) VALL-E X has better speech naturalness than the baseline thanks to the large-scale training data and the large language model capacity. The results of Chinese TTS with English prompts are also listed. Table 3: Human evaluation for zero-shot cross-lingual TTS. SMOS means similarity MOS between generated speech and prompt, and CMOS means comparative MOS based on Baseline. SMOS CMOS (v.s. Baseline) Baseline (YourTTS) 3.420.19 0.00 VALL-E X 4.000.20 +0.24 Human Evaluation We further conduct the human evaluation on 50 randomly selected speech records for zero-shot cross-lingual English TTS with Chinese speech as prompts, including SMOS and CMOS. Note that SMOS ranges from 1 to 5 where the larger the value, the higher the voice similarity, and CMOS ranges from -3 to 3 where the positive number means the new system is better than the baseline. The results are listed in Table 3. Baseline gets 3.42 SMOS scores between generated speech and prompts, while our VALL-E X achieves 4.00, which further demonstrates the model’s superiority in keeping the speech characteristic in the cross-lingual setting. Moreover, to directly compare the speech synthesis quality between the proposed VALL-E X and baseline, we calculate the CMOS score between them evaluated by native speakers on the 50 sentences. The last column of Table 3 shows that VALL-E X obtains the gain of +0.24 CMOS scores than the baseline. Table 4: S2ST performance on EMIME dataset for ChineseEnglish directions. Baseline is a cascaded S2ST system based on speaker embedding. Automatic evaluation matrices include ASVScore, ASR-BLEU, and Naturalness. ASV-Score ASR-BLEU Naturalness tgt vs. src hyp vs. src hyp vs. tgt ChineseEnglish S2ST 0.280.10 0.270.12 27.49 3.44 - w/ oracle target text 0.280.10 0.290.11 80.30 3.43 VALL-E X Trans 0.370.10 0.370.11 30.66 3.54 - w/ oracle target text 0.390.10 0.380.10 86.78 3.54 Baseline (S2ST) 0.580.09 EnglishChinese S2ST VALL-E X Trans 0.580.09 0.480.11 0.530.11 34.45 3.41 - w/ oracle target text 0.470.12 0.550.11 84.00 3.42 Speaker Similarity We first evaluate whether the speaker’s voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in Table 4. Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs. src, hyp vs. src, and hyp vs. tgt). From Table 4 we can find that: (1) For ChineseEnglish, the ASV score of VALL-E X Trans significantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker’s voice. (2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs. src) for the EnglishChinese direction, which suggests that the cross-lingual voice transferability still has the improvement space. (3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not", "prediction": "The VALL-E X is a cross-lingual speech synthesis model based on speaker embeddings. It is a cascaded S2ST system based on speaker embeddings. The baseline is a cascaded S2ST system based on speaker embeddings. The baseline gets 3.42 SMOS scores between generated speech and prompts, while our VALL-E X achieves 4.00.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 68, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nincrease notably, indicating that voice transferability is less affected by the quality of translation. Translation Quality Table 4 also shows the translation performance of VALL-E X Trans. Note that ASR-BLEU with oracle target text as the input of VALL-E X can be seen as the upper bound when translations are exactly correct. With oracle target text as input, V", "prediction": "VALL-E X Trans is a voice-transferable system that can be used to translate oracle target text into a language of choice.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 69, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nALL-E X Trans can achieve the performance of about 8487 BLEU scores, which also reflects the high performance of our neural codec language model. For ChineseEnglish, VALL-E X Trans achieves higher BLEU over the baseline (30.66 vs. 27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task. Speech Naturalness We also evaluate the Naturalness with the open-source NISQA [Mittag and Möller, 2021] for S2ST outputs. As shown in the last column of Table 4, compared to the baseline, VALL-E X Trans achieves a better naturalness score (3.54 vs. 3.44), which shows that VALL-E X can generate more natural target language speech than the baseline. Human Evaluation We randomly sample 56 translation pairs13 to perform a human evaluation using SMOS and MOS matrics for both ChineseEnglish and EnglishChinese directions. Table 5 lists the results of VALL-E X Trans as well as the ChineseEnglish baseline. We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation. For speaker similarity evaluation, VALL-E X Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs. 3.06), demonstrating its superior ability to model speaker property of the proposed VALL-E X. Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91). For speech quality, our VALL-E X slightly outperforms the baseline in ChineseEnglish S2ST in terms of MOS score (3.87 vs. 3.81). Table 5: Subjection evaluation with SMOS and MOS scores on bidirectional ChineseEnglish S2ST tasks. SMOS is measured by comparing with the ground-truth target speech. EnglishChinese S2ST baseline is not reported since it is not supported by the released YourTTS. ChineseEnglish EnglishChinese SMOS MOS SMOS MOS Baseline (S2ST) 3.060.14 3.810.19 - - VALL-E X Trans 4.120.13 3.870.21 3.940.15 3.480.13 Source speech prompt 4.910.05 - 4.640.06 - Oracle target speech - 3.920.17 - 3.880.13 5.5 Analysis In this section, we first analyze the effect of language ID, then explore the foreign accent problems, and qualitatively investigate the ability to maintain voice emotion and synthesize code-switch speech of our proposed model. Effect of Language ID Our VALL-E X is trained with multi-lingual ASR data, which might increase the modeling difficulty for each specific language. We address it by adding language IDs to guide speech synthesis in the autoregressive language codec model. Here, we verify the effectiveness by removing the language ID (LID) or adding the wrong LID (i.e. the source LID). The ASV-Score and ASR-BLEU are reported in Table 6. Without LID or with the wrong language ID, the translation quality decreases, while the speaker similarity between the hypothesis and source speech increases. These results demonstrate the importance of language ID for the accuracy of the content. It also indicates that target LID reduces the transfer of information, which means the model without LID or with source LID will better maintain the sound of the original speaker. Table 6: Evaluation for the effect of language ID on ChineseEnglish EMIME dataset. ASV-Score is computed between synthesized speech and source prompt speech. The last column lists the subjection evaluation score of the foreign accent (from 1 to 5 scores). ASV-Score (vs. src) ASR-BLEU Accent Score ChineseEnglish S2ST VALL-E X Trans 0.370.10 30.66 4.10 w/o Language ID 0.410.10 29.04 2.98 w/ wrong Language ID 0.410.10 29.07 2.55 EnglishChinese S2ST VALL-E X Trans 0.480.11 34.45 4.03 w/o Language ID 0.490.11 30.86 2.35 w/ wrong Language ID 0.500.11 29.70 2.25 Foreign Accent Control L2 (second-language, or foreign) accent", "prediction": "VALL-E X Trans is a speech-to-phoneme translation model that can be used for S2ST tasks. VALL-E X Trans is a speech-to-phoneme translation model that can be used for S2ST tasks. VALL-E X Trans is a speech-to-phoneme translation model that can be used for S2ST tasks. VALL-E X Trans is a speech-to-phoneme translation model that can be used for S2ST tasks.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 70, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nproblem, the synthesized speech sounds like the accents of a foreigner, has arisen in cross-lingual TTS systems [Zhang et al., 2019, Lee et al., 2022]. Automatic Evaluation has shown that adding LID can boost speech quality. Besides, we conduct a subjection evaluation to label foreign accents from 1 to 5 on randomly selected 20 synthesized speech for both English and Chinese, where each sample is measured with a score from 1 to 5 denoting high-status foreign speakers, low-status foreign speakers, middle-status speakers, low-status native speakers, and high-status native speakers, respectively. As summarized in the last column of Table 6, we observed that our VALL-E X can control the accent for the target speech by LID modules. For example, in EnglishChinese, VALL-E X Trans with right LID and without LID get the score of 4.03 and 2.35, respectively. This indicates that by using correct LID embedding, VALL-E X Trans is able to alleviate the foreign accent problem. Please also see the demo for audio examples of VALL-E X Trans with or without language ID. [Um et al., 2020]. Moreover, it is more tempting to reserve the source speaker’s emotion in generated target speech for the S2ST task, which is not explored in previous S2ST work. In these experiments, we adopt the source prompts from the emotional voices dataset EmoV-DB [Um et al., 2020] as inputs of VALL-E X Trans to generate the translated target speech, whose samples are listed on our demo page. We found that the proposed VALL-E X can maintain emotional consistency to a certain extent between the source prompt and the synthesized speech. The underlying reasons are (1) our VALL-E X is trained with large-scale multi-lingual multi-speaker speech-transcription data, which contains various emotional speech records, and (2) the strong in-context learning ability of VALL-E X, like GPT-3 [Brown et al., 2020], promotes the generated speech to reserve the characteristic of the source prompt. Code-Switch Speech Synthesis It is a common phenomenon to use code-switch utterances in bilingual or multi-lingual communities [Cao et al., 2020, Zhao et al., 2020, Manghat et al., 2022]. Code-switch speech synthesis aims to produce a fluent and consistent voice for code-switch text. Although our proposed VALL-E X is trained on multiple monolingual speech data, without special optimization for code-switch setting, VALL-E X provides a promising solution to code-switch speech synthesis. We put the code-switch samples on our demo page, demonstrating that due to its strong in-context learning ability, VALL-E X can synthesize fluent code-switch speech with a consistent voice. 6", "prediction": "VALL-E X Trans to generate the translated target speech.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 71, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion In this work, we propose VALL-E X, a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. VALL-E X is free of the requirement for cross-lingual paired data from the same speakers. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.", "prediction": "VALL-E X is a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.", "reference": "We introduce a language modeling approach for text-to-speech synthesis (TTS). Specifically, we train a neural codec language model (called VALL-E) using discrete codes derived from an off-the-shelf neural audio codec model, and regard TTS as a conditional language modeling task rather than continuous signal regression as in previous work. VALL-E emerges in-context learning capabilities and can be used to synthesize high-quality personalized speech with only a 3-second enrolled recording of an unseen speaker as a prompt. VALL-E significantly outperforms the state-of-the-art zero-shot TTS system in terms of speech naturalness and speaker similarity. In addition, VALL-E could preserve the speaker’s emotion and acoustic environment of the acoustic prompt in synthesis. Extending its capabilities, VALL-E X adapts to multi-lingual scenarios, facilitating cross-lingual zero-shot TTS. Meanwhile, VALL-E R introduces a phoneme monotonic alignment strategy, bolstering the robustness of speech generation. With the integration of repetition-aware sampling and grouped code modeling techniques, VALL-E 2 achieves a groundbreaking milestone: human parity in zero-shot TTS performance on LibriSpeech and VCTK datasets. This marks the first instance of such an achievement, setting a new standard for the field. MELLE is a novel continuous-valued tokens based language modeling approach for text to speech synthesis (TTS). MELLE autoregressively generates continuous mel-spectrogram frames directly from text condition, bypassing the need for vector quantization, which are originally designed for audio compression and sacrifice fidelity compared to mel-spectrograms. FELLE effectively predicts continuous-valued tokens (mel-spectrograms), by leveraging the autoregressive nature of language models and the generative efficacy of flow matching. PALLE combines explicit temporal modeling from AR with parallel generation from NAR, and generates dynamic-length spans at fixed time steps, followed with a second stage, in which low-confidence tokens are iteratively refined in parallel leveraging the global contextual information.\n\nModel versions\n\nEthics statement\n\nVALL-E could synthesize speech that maintains speaker identity and could be used for educational learning, entertainment, journalistic, self-authored content, accessibility features, interactive voice response systems, translation, chatbot, and so on. While VALL-E can speak in a voice like the voice talent, the similarity, and naturalness depend on the length and quality of the speech prompt, the background noise, as well as other factors. It may carry potential risks in the misuse of the model, such as spoofing voice identification or impersonating a specific speaker. We conducted the experiments under the assumption that the user agrees to be the target speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model. If you suspect that VALL-E is being used in a manner that is abusive or illegal or infringes on your rights or the rights of other people, you can report it at the Report Abuse Portal."}
{"idx": 72, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Abstract\nAbstract: Abstract We propose a cross-lingual neural codec language model, VALL-E X, for crosslingual speech synthesis. Specifically, we extend VALL-E [Wang et al., 2023] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-tospeech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at https://aka.ms/vallex. VALL-E X (Cross-Lingual VALL-E) Cross-Lingual Neural Codec Language Modeling Multilingual G2P Multilingual G2P Audio Codec Encoder Source Text Target Text Source Speech Figure 1: The overall framework of VALL-E X, which can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training, and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation. Personalized Target Speech Audio Codec Decoder Language ID Source phoneme sequences Target phoneme sequences Source acoustic tokens Target acoustic tokens 1", "prediction": "VALL-E X is a cross-lingual neural codec language model that can synthesize personalized speech in another language for a monolingual speaker. It is based on VALL-E, which is a multi-lingual conditional codec language model that can predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment.", "reference": "VALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis."}
{"idx": 73, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: Introduction Recent years have witnessed significant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity [Li et al., 2019, Ren et al., 2019, Tan et al., 2022]. However, these models can only generate high-quality speech for a specific speaker in a specific language. Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. Previous methods to tackle these challenges typically augment end-to-end TTS models with specific subnets for speaker and language control [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2020, Ellinas et al., 2022, Cai et al., 2023]. For example, based on the multi-speaker TTS model, Nachmani and Wolf [2019] introduce multiple encoders for each language and additional loss to keep the speaker’s identity. Zhang et al. [2020] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities. Yang and He [2020] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively. Yang and He [2022] further propose a multi-task learning method with additional tasks of speaker similarity and language identification. Moreover, Cai et al. [2023] investigates cross-lingual multi-speaker text-to-speech synthesis with sufficient or limited bilingual speech training data. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [Zhang et al., 2019, Lee et al., 2022]. Table 1: A comparison between VALL-E X and previous cross-lingual TTS systems. Previous Systems VALL-E X Intermediate representation Mel spectrogram Audio codec codes Training data 13K hours Speech accent Foreign Native Speaker similarity Relative low High In-context learning Zero-shot cross-lingual TTS In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, VALL-E X, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis. Based on the knowledge learned from large-scale multi-lingual speech data, VALL-E X is able to transfer the speech characteristics, including the speaker’s voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems. More specifically, we first obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data. Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an offline neural codec encoder. Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model. As illustrated in Figure 1, after training, VALL-E X can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language. The generated acoustic token sequence is decompressed to the target speech waveform by an offline audio codec", "prediction": "Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. The above methods typically augment end-to-end TTS models with specific subnets for speaker and language control. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing cross-lingual multi-speaker text-to-speech synthesis.", "reference": "The traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance."}
{"idx": 74, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Introduction\ndecoder. VALL-E X is trained on two large-scale multi-speaker datasets1, LibriLight [Kahn et al., 2020] and WenetSpeech [Zhang et al., 2022a], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively. The combination of LibriLight and Wenet", "prediction": "VALL-E X is a multi-speaker decoder that can be used to train multi-speaker models.", "reference": "The traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance."}
{"idx": 75, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Introduction\nSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which significantly improves the coverage of different speakers and enhances VALL-E X’s generalization capacity. The comparison between VALL-E X and the previous cross-lingual TTS systems are listed in Table 1. We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot crosslingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST). For cross-lingual text-to-speech synthesis, the proposed VALL-E X is evaluated with LibriSpeech [Panayotov et al., 2015] and EMIME [Wester, 2010] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers. For zero-shot speech-to-speech translation, EMIME [Wester, 2010] dataset is used for the evaluation of VALL-E X on bidirectional ChineseEnglish translation tasks, and it contains bilingual audio recordings by the same speakers. We evaluate the proposed VALL-E X framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS). Specifically, due to the strong in-context learning capability, VALL-E X achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker. By training on large-scale speech-transcription data, the proposed VALL-E X significantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness. Furthermore, the human evaluation shows that our VALL-E X outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST). Our contributions can be summarized as follows: • We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data. VALL-E X is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts. • The multi-lingual in-context learning framework enables VALL-E X to generate crosslingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language. • Based on the learned cross-lingual speech modeling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks. • We apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation. We encourage readers to listen to the audio samples on our demo page: https://aka.ms/vallex. 2", "prediction": "VALL-E X framework is a cross-lingual text-to-speech generation framework that is trained on a large multi-lingual multi-speaker multi-domain unclean speech dataset. The proposed framework achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker.", "reference": "The traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance."}
{"idx": 76, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Method\nMethod: 3.2 Model Framework Inspired by VALL-E, the cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM MAR is only used to predict the acoustic tokens from the first quantizer of EnCodec model. Formally, based on paired speech-transcription data in any language, let S denote the transcribed phoneme sequence, and A:,1 ai,1|i = 1, . . . , N denotes the first-layer acoustic tokens extracted from the speech X. The decoder MAR, modeling the concatenated sequence S, A:,1, is trained to predict A:,1 autoregressively. It is optimized by maximizing the log-likelihood, Quantization & Phonemization: Quantized tokens Layer L 12 504 31 8 743 ... A:,2:L ...... Layer 2 325 71 435 90 7 ... A:,1 731 284 78 32 669 ... Layer 1 audio codec encoder (Quantization) Multi-lingual speech-transcription pairs (phonemization) G2P tool Phoneme tokens HH AH L OW ... D S N Y Multi-lingual Non-Autoregressive Codec LM Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM MNAR is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer. It is prompted by the phoneme sequence of the current sentence (S) and the acoustic token sequence of another sentence with the same speaker (  A). Here  A is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer l [2, 8], the embeddings of l 1 layers’ acoustic tokens (A:,1:l1) are summed up layerwise as input. The learning objective for the l-layer acoustic tokens A:,l can be calculated as 8 X l=2 log pNAR A:,l | D S,  A:,1:8, A:,1:l1 E ; MNAR (2) LMNAR = where means the sequence concatenation. pNAR(.) computes the pointwise probabilities of A:,l. 3.3 Multi-lingual Training In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-tospeech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (Ss, As) and (St, At) to train our multi-lingual codec LMs MAR and MNAR, where s and t represent two different (source and target) languages. Language ID Module Following multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in VALL-E X. On the one hand, without language ID", "prediction": "The cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM MAR is only used to predict the acoustic tokens from the", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 77, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Method\n, VALL-E X may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM MAR is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens. 3.4 Cross-Lingual Inference After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we first concatenate source phonemes Ss and target phonemes St as prompts, and take the first-layer source acoustic tokens As :,1 as the decoding prefix, condition on which the multi-lingual autoregressive codec LM MAR generates the first-layer target acoustic tokens At :,1, at i,1 pAR  at i,1 | Ss, St, As :,1, At i,1 ; MAR , i = 1, . . . , (3) where means probability-based sampling. The sampling is stopped until the end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the final generated speech. After obtaining the first-layer target acoustic tokens At :,1 from MAR, multi-lingual non-autoregressive codec LM MNAR is used to predict the rest layers of acoustic tokens n At :,l | l = 2, . . . , 8 o by greedy search, i.e., choosing the tokens with maximum probabilities, At :,l = argmax At :,l pNAR  At :,l | St, As :,1:8, At :,1:l1 ; MNAR , l = 2, . . . , 8. (4) Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens At :,1:8. tl= 2, ... , 8 VALL-E X Inference: A:,l Multi-lingual Non-Autoregressive Codec LM t s A:,1:l1 St A:,1:8 t A:,1 Multi-lingual Autoregressive Codec LM B t aN,1 t a2,1 t ... s a1,1 bos> Ss St A:,1 Figure 3: Inference illustration of the cross-lingual neural codec language model VALL-E X, with two-stage decoding strategies. VALL-E X can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks. 4 VALL-E X Application VALL-E X can be applied to various cross-lingual speech generation tasks. In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in Figure 3. 4.1 Zero-Shot Cross-Lingual TTS The proposed VALL-E X is naturally suitable for zero-shot cross-lingual TTS tasks. Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker’s voice. Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability. Thanks to the in-context learning capability of large language models, VALL-E X surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis. More specifically, given the source speech, source transcript, and target text, we first convert source speech into source acoustic token As using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes Ss and target phonemes St using G2P tool. More specifically, as introduced in", "prediction": "VALL-E X is a multi-lingual speech synthesis system that can be used to generate cross-lingual speech. It is trained with multi-lingual data and can perform cross-lingual speech synthesis. However, the training process is not suitable for multi-lingual data. Therefore, we propose a method to train VALL-E X to generate cross-lingual speech.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 78, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Method\nSection 3.4, we let St be the phonemes extracted from the target text, Ss and As be the phonemes and acoustic tokens extracted from the source speech. Then VALL-E X generates the full-layer target acoustic tokens, which are finally decompressed into the target speech by EnCodec decoder. 4.2 Zero-Shot Speech-to-Speech Translation We can also apply our VALL-E X to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences. Speech Recognition & Translation Model We leverage the improved SpeechUT [Zhang et al., 2022c] as our speech recognition & translation model, which is a unified-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss) and MT corpus (source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more As Application: Zero-shot cross-lingual TTS Ss St Codec Encoder G2P Tool G2P Tool Source text Target text Source speech Application: Zero-shot S2ST Ss St As E Codec Encoder Semantic Semantic Encoder Decoder Speech Encoder Source Speech pre-training details about this model. After pre-training, the model is fine-tuned with (X s, Ss, St) triplet data derived from the ST corpus. Specifically, we perform multi-task learning with the CTC [Graves et al., 2006] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes. Inference Figure 3 shows the inference process of speech-to-speech translation. Given a source speech X s, the speech recognition & translation model first generates the source phonemes Ss from the semantic encoder and the target phonemes St from the semantic decoder. Besides, we use the EnCodec encoder to compress X s into source acoustic tokens As. Then, we concatenate Ss, St, and As, as the input of VALL-E X, to produce the acoustic token sequence for the target speech, as introduced in Section 3.4. The generated acoustic tokens are converted to the final target speech with the decoder of EnCodec. 4.3 Evaluation The proposed model is verified using various evaluation criteria, including speaker similarity (ASVScore), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation. Specifically, we measure speaker similarity between synthesized target speech and groudtruth target speech or source speech as an automatic speaker verification (ASV) task, where a WavLM [Chen et al., 2022] based ASV model is used to calculate the score. To verify the quality of generated speech, we first utilize the ASR system from the released HuBERT-Large model [Hsu et al., 2021] to recognize it into text. For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text. For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text. Finally,", "prediction": "VALL-E X is a speech-to-text translation model that can be applied to zero-shot speech-to-speech translation tasks. SpeechUT is a speech-to-text pre-training framework that uses hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). SpeechUT is a speech-to-text pre-training framework that uses hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). SpeechUT is a speech-to-text pre-training framework that uses hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). SpeechUT is a speech-to-text pre-training framework that uses hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). SpeechUT is a speech-to-text pre-training framework", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 79, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Method\nto better verify our proposed VALL-E X systems, we adopt the open-source NISQA3 [Mittag and Möller, 2021] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS). 5", "prediction": "The VALL-E X systems are based on the NISQA3 model to evaluate the naturalness of the generated speech and further perform the human evaluation with manual scoring on the generated speech. The NISQA3 model is a human-based model that evaluates the naturalness of the generated speech. The NISQA3 model is a human-based model that evaluates the naturalness of the generated speech.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 80, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: Experiments We evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5, OpenSubtitles20186 and WMT20207, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama8, and news domains, respectively. The EnglishChinese ST data is from GigaST [Ye et al., 2022], which is created by translating the transcripts in GigaSpeech [Chen et al., 2021] using a strong machine translation system. Similarly, we create the ChineseEnglish ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above. 3https://github.com/gabrielmittag/NISQA 4https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech 5https://challenger.ai/competition/translation 6https://opus.nlpl.eu/OpenSubtitles2018.php 7https://www.statmt.org/wmt20/translation-task.html 8http://www.opensubtitles.org/ We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [Wester, 2010], which contains bilingual Chinese/English speech recorded by the same speakers. There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350. Zero-shot cross-lingual TTS is evaluated using Librispeech [Panayotov et al., 2015] dev-clean set and EMIME dataset providing English and Chinese data, respectively. We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts. 5.2 Experimental Setup Phonemization & Quantization The right picture of Figure 2 illustrates the phonemization & quantization processes for different languages. All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets. We use a unified phoneme set called BigCiDian9 for two languages which are based on International Phonetic Alphabet (IPA). The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools10 for additional alignment information used for the pre-training of speech recognition & translation model. The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model EnCodec11, which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks.", "prediction": "VALL-E X is a speech-to-text translation model for cross-lingual TTS and S2ST. It is trained on bilingual speech-to-text datasets. The model is trained on the Librispeech dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the GigaSpeech dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitles dataset. The model is trained on the OpenSubtitle", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 81, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nModel Architecture For the cross-lingual codec language models, MAR and MNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096. The autoregression is implemented by attention masking in the MAR model. Sinuous position embedding is separately computed for each prompt sequence in MAR and MNAR models. Besides, the M", "prediction": "NAR model has a recurrent architecture with a recurrent model.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 82, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nNAR model uses individual layer normalization for generating each layer of acoustic tokens. We also introduce the model architecture of speech recognition & translation for S2ST in Appendix A.1.2. We call our cross-lingual TTS model and S2ST model as VALL-E X and VALL-E X Trans in the subsequent experiments, respectively. Training Details We optimize each module of VALL-E X individually, including MAR and MNAR. For both modules, The maximum sentence length is set to 20 seconds, so we re-segment the LibriLight data to an average utterance duration of 12 seconds by detecting the consecutive silence phonemes. Fortunately, the WenetSpeech data has already been segmented into short utterances. The maximum learning rate is 5e-4 with warm-up steps of 8,000. The models are trained on 32 V100 GPUs for 800k steps. MAR is trained with the batch size of 120 seconds per GPU, which is 66 seconds for MNAR due to the memory constraint. When optimizing MNAR, instead of accumulating all layer’s loss in Eqn. (2), we randomly select one layer at each optimization step for efficiency. For speech recognition & translation model, the training details can be found in Appendix A.1.3. Baselines We adopt YourTTS12 [Casanova et al., 2022] as our baseline for zero-shot cross-lingual TTS. YourTTS is a zero-shot multi-speaker TTS model for everyone, whose speaker information is based on speaker embedding extracted from a reference speech. Since previous work shows that current end-to-end S2ST systems underperform cascaded S2ST systems [Jia et al., 2022a, Lee et al., 2021b], we also build an S2ST baseline which is cascaded by an ASR model, an MT model, and a multi-speaker YourTTS. The source speech serves as the reference speech when synthesizing the target speech using YourTTS. The ASR model is the released HuBERT model introduced in Section 4.3, and the MT model is a vanilla Transformer trained by ourselves on the MT data introduced in Section 5.1. Since YourTTS is built only for English, we don’t get its performance for EnglishChinese translation direction. Table 2: Zero-shot cross-lingual TTS evaluation for English TTS with Chinese speech as prompts and Chinese TTS with English speech as prompts, using automatic evaluation matrices, including ASV-Score (hypothesis vs. prompt), ASR-WER, and Naturalness. ASV-Score ASR-WER Naturalness English TTS with Chinese as prompts Baseline (YourTTS) 0.300.10 8.53 3.36 VALL-E X 0.360.11 4.07 3.54 Chinese TTS with English as prompts VALL-E X 0.290.10 8.52 3.36 5.3 Zero-Shot Cross-Lingual TTS Evaluation We first select samples with a length between 4 and 10 seconds from LibriSpeech dev-clean set, resulting in 40 speakers and 1373 samples. For English TTS, we randomly select one audio from EMIME set as the Chinese prompt for each target sentence in LibriSpeech dev-clean set. For Chinese TTS, we use extra 149 Chinese text sentences provided by the EMIME set and repeat them to the total number of 1373 so that they can be prompted by the LibriSpeech audios one-by-one. When synthesizing the target language speech, the whole sequence of the source language speech is used as the prompt. Automatic Evaluation Table 2 summarizes the results of cross-lingual zero-shot TTS tasks, including English TTS prompted by Chinese speech and Chinese TTS prompted by English speech. We measure the speaker similarity using the automatic speaker verification (ASV) model, ranging from -1 to +1 given two speech utterances. The larger the value, the more similar the speakers of", "prediction": "The baseline for cross-lingual TTS is VALL-E X Trans. The baseline for S2ST is VALL-E X Trans.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 83, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nthe two utterances are. The results show that: (1) for English TTS with Chinese as prompts, the speaker similarity between the hypothesis and prompts of VALL-E X is superior to that of the baseline (0.36 vs 0.30). (2) VALL-E X reduces the WER significantly from the baseline (from 8.53 to 4.07), demonstrating the effectiveness of our method", "prediction": "The purpose of this paper is to introduce a new method for detecting speaker similarity between the hypotheses and prompts of TTS with Chinese as prompts. The method is called VALL-E X. The method is based on the method of detecting speaker similarity between the hypotheses and prompts of TTS with Chinese as prompts. The method is based on the method of detecting speaker similarity between the hypotheses and prompts of TTS with Chinese as prompts. The method is based on the method of detecting speaker similarity between the hypotheses and prompts of TTS with Chinese as prompts. The method is based on the method of detecting speaker similarity between the hypotheses and prompts of TTS with Chinese as prompts. The method is based on the method of detecting speaker similarity between the hypotheses and prompts of TTS with Chinese as prompts. The method is based on the method of detecting speaker similarity between the hypotheses and prompts of TTS with Chinese as prompts. The method is based on the method of detecting speaker similarity between the hypotheses and prompts of T", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 84, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\n. (3) VALL-E X has better speech naturalness than the baseline thanks to the large-scale training data and the large language model capacity. The results of Chinese TTS with English prompts are also listed. Table 3: Human evaluation for zero-shot cross-lingual TTS. SMOS means similarity MOS between generated speech and prompt, and CMOS means comparative MOS based on Baseline. SMOS CMOS (v.s. Baseline) Baseline (YourTTS) 3.420.19 0.00 VALL-E X 4.000.20 +0.24 Human Evaluation We further conduct the human evaluation on 50 randomly selected speech records for zero-shot cross-lingual English TTS with Chinese speech as prompts, including SMOS and CMOS. Note that SMOS ranges from 1 to 5 where the larger the value, the higher the voice similarity, and CMOS ranges from -3 to 3 where the positive number means the new system is better than the baseline. The results are listed in Table 3. Baseline gets 3.42 SMOS scores between generated speech and prompts, while our VALL-E X achieves 4.00, which further demonstrates the model’s superiority in keeping the speech characteristic in the cross-lingual setting. Moreover, to directly compare the speech synthesis quality between the proposed VALL-E X and baseline, we calculate the CMOS score between them evaluated by native speakers on the 50 sentences. The last column of Table 3 shows that VALL-E X obtains the gain of +0.24 CMOS scores than the baseline. Table 4: S2ST performance on EMIME dataset for ChineseEnglish directions. Baseline is a cascaded S2ST system based on speaker embedding. Automatic evaluation matrices include ASVScore, ASR-BLEU, and Naturalness. ASV-Score ASR-BLEU Naturalness tgt vs. src hyp vs. src hyp vs. tgt ChineseEnglish S2ST 0.280.10 0.270.12 27.49 3.44 - w/ oracle target text 0.280.10 0.290.11 80.30 3.43 VALL-E X Trans 0.370.10 0.370.11 30.66 3.54 - w/ oracle target text 0.390.10 0.380.10 86.78 3.54 Baseline (S2ST) 0.580.09 EnglishChinese S2ST VALL-E X Trans 0.580.09 0.480.11 0.530.11 34.45 3.41 - w/ oracle target text 0.470.12 0.550.11 84.00 3.42 Speaker Similarity We first evaluate whether the speaker’s voice is preserved in the generated target speech using speaker similarity (ASV-Score), whose results are listed in Table 4. Because the EMIME test set has paired speech utterances with Chinese and English, we are able to calculate the ASV score among the generated speech (hyp), the source speech (src), as well as the target speech (tgt), resulting in 3 settings (tgt vs. src, hyp vs. src, and hyp vs. tgt). From Table 4 we can find that: (1) For ChineseEnglish, the ASV score of VALL-E X Trans significantly outperforms that of the conventional speaker embedding based S2ST system (Baseline), demonstrating the superiority of our model in terms of maintaining the source speaker’s voice. (2) The ASV score has similar values when the generated speech (hyp) is compared with the source speech (src) and the target speech (tgt), and it is far away from the upper bound (tgt vs. src) for the EnglishChinese direction, which suggests that the cross-lingual voice transferability still has the improvement space. (3) When directly generating speech from the ground-truth (oracle) text which degrades into cross-lingual TTS, the ASV score does not", "prediction": "The VALL-E X is a cross-lingual speech synthesis model based on speaker embeddings. It is a cascaded S2ST system based on speaker embeddings. The baseline is a cascaded S2ST system based on speaker embeddings. The baseline has a SMOS score of 3.42, while the new model has a CMOS score of 4.04, which further demonstrates the model’s superiority in keeping the speech characteristic in the cross-lingual setting.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 85, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nincrease notably, indicating that voice transferability is less affected by the quality of translation. Translation Quality Table 4 also shows the translation performance of VALL-E X Trans. Note that ASR-BLEU with oracle target text as the input of VALL-E X can be seen as the upper bound when translations are exactly correct. With oracle target text as input, V", "prediction": "VALL-E X Trans is a voice-to-voice translation tool that can be used to translate oracle target text into a language of your choice.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 86, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nALL-E X Trans can achieve the performance of about 8487 BLEU scores, which also reflects the high performance of our neural codec language model. For ChineseEnglish, VALL-E X Trans achieves higher BLEU over the baseline (30.66 vs. 27.49), demonstrating the end-to-end speech-to-phoneme translation is more effective against the conventional cascaded speech-to-text translation when applying to S2ST task. Speech Naturalness We also evaluate the Naturalness with the open-source NISQA [Mittag and Möller, 2021] for S2ST outputs. As shown in the last column of Table 4, compared to the baseline, VALL-E X Trans achieves a better naturalness score (3.54 vs. 3.44), which shows that VALL-E X can generate more natural target language speech than the baseline. Human Evaluation We randomly sample 56 translation pairs13 to perform a human evaluation using SMOS and MOS matrics for both ChineseEnglish and EnglishChinese directions. Table 5 lists the results of VALL-E X Trans as well as the ChineseEnglish baseline. We use MOS (from 1 to 5 scores) instead of CMOS because the translated content may be different among models, which is not suitable for CMOS evaluation. For speaker similarity evaluation, VALL-E X Trans outperforms the baseline with 1.06 SMOS scores (4.12 vs. 3.06), demonstrating its superior ability to model speaker property of the proposed VALL-E X. Note that this value still can be improved since it is still far from the SMOS between the source speech prompt and ground truth (4.91). For speech quality, our VALL-E X slightly outperforms the baseline in ChineseEnglish S2ST in terms of MOS score (3.87 vs. 3.81). Table 5: Subjection evaluation with SMOS and MOS scores on bidirectional ChineseEnglish S2ST tasks. SMOS is measured by comparing with the ground-truth target speech. EnglishChinese S2ST baseline is not reported since it is not supported by the released YourTTS. ChineseEnglish EnglishChinese SMOS MOS SMOS MOS Baseline (S2ST) 3.060.14 3.810.19 - - VALL-E X Trans 4.120.13 3.870.21 3.940.15 3.480.13 Source speech prompt 4.910.05 - 4.640.06 - Oracle target speech - 3.920.17 - 3.880.13 5.5 Analysis In this section, we first analyze the effect of language ID, then explore the foreign accent problems, and qualitatively investigate the ability to maintain voice emotion and synthesize code-switch speech of our proposed model. Effect of Language ID Our VALL-E X is trained with multi-lingual ASR data, which might increase the modeling difficulty for each specific language. We address it by adding language IDs to guide speech synthesis in the autoregressive language codec model. Here, we verify the effectiveness by removing the language ID (LID) or adding the wrong LID (i.e. the source LID). The ASV-Score and ASR-BLEU are reported in Table 6. Without LID or with the wrong language ID, the translation quality decreases, while the speaker similarity between the hypothesis and source speech increases. These results demonstrate the importance of language ID for the accuracy of the content. It also indicates that target LID reduces the transfer of information, which means the model without LID or with source LID will better maintain the sound of the original speaker. Table 6: Evaluation for the effect of language ID on ChineseEnglish EMIME dataset. ASV-Score is computed between synthesized speech and source prompt speech. The last column lists the subjection evaluation score of the foreign accent (from 1 to 5 scores). ASV-Score (vs. src) ASR-BLEU Accent Score ChineseEnglish S2ST VALL-E X Trans 0.370.10 30.66 4.10 w/o Language ID 0.410.10 29.04 2.98 w/ wrong Language ID 0.4", "prediction": "VALL-E X Trans is a speech-to-phoneme translation model that can be used for S2ST tasks. VALL-E X Trans achieves higher BLEU scores than the baseline for ChineseEnglish and EnglishChinese. VALL-E X Trans achieves higher naturalness scores than the baseline for EnglishChinese. VALL-E X Trans achieves higher speaker similarity scores than the baseline for ChineseEnglish. VALL-E X Trans achieves higher naturalness scores than the baseline for EnglishChinese. VALL-E X Trans achieves higher speaker similarity scores than the baseline for EnglishChinese. VALL-E X Trans achieves higher naturalness scores than the baseline for EnglishChinese. VALL-E X Trans achieves higher speaker similarity scores than the baseline for EnglishChinese. VALL-E X Trans achieves higher speaker similarity scores than the baseline for EnglishChinese. VALL-E X Trans achieves higher speaker similarity scores than the baseline for EnglishChinese. VALL-E X Trans achieves higher speaker similarity scores than the baseline for EnglishChinese.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 87, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\n10.10 29.07 2.55 EnglishChinese S2ST VALL-E X Trans 0.480.11 34.45 4.03 w/o Language ID 0.490.11 30.86 2.35 w/ wrong Language ID 0.500.11 29.70 2.25 Foreign Accent Control L2 (second-language, or foreign) accent", "prediction": "The following is a list of the languages that are supported by the S2ST VALL-E X Trans. The following is a list of the languages that are supported by the S2ST VALL-E X Trans. The following is a list of the languages that are supported by the S2ST VALL-E X Trans.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 88, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Experiments / Results\nproblem, the synthesized speech sounds like the accents of a foreigner, has arisen in cross-lingual TTS systems [Zhang et al., 2019, Lee et al., 2022]. Automatic Evaluation has shown that adding LID can boost speech quality. Besides, we conduct a subjection evaluation to label foreign accents from 1 to 5 on randomly selected 20 synthesized speech for both English and Chinese, where each sample is measured with a score from 1 to 5 denoting high-status foreign speakers, low-status foreign speakers, middle-status speakers, low-status native speakers, and high-status native speakers, respectively. As summarized in the last column of Table 6, we observed that our VALL-E X can control the accent for the target speech by LID modules. For example, in EnglishChinese, VALL-E X Trans with right LID and without LID get the score of 4.03 and 2.35, respectively. This indicates that by using correct LID embedding, VALL-E X Trans is able to alleviate the foreign accent problem. Please also see the demo for audio examples of VALL-E X Trans with or without language ID. [Um et al., 2020]. Moreover, it is more tempting to reserve the source speaker’s emotion in generated target speech for the S2ST task, which is not explored in previous S2ST work. In these experiments, we adopt the source prompts from the emotional voices dataset EmoV-DB [Um et al., 2020] as inputs of VALL-E X Trans to generate the translated target speech, whose samples are listed on our demo page. We found that the proposed VALL-E X can maintain emotional consistency to a certain extent between the source prompt and the synthesized speech. The underlying reasons are (1) our VALL-E X is trained with large-scale multi-lingual multi-speaker speech-transcription data, which contains various emotional speech records, and (2) the strong in-context learning ability of VALL-E X, like GPT-3 [Brown et al., 2020], promotes the generated speech to reserve the characteristic of the source prompt. Code-Switch Speech Synthesis It is a common phenomenon to use code-switch utterances in bilingual or multi-lingual communities [Cao et al., 2020, Zhao et al., 2020, Manghat et al., 2022]. Code-switch speech synthesis aims to produce a fluent and consistent voice for code-switch text. Although our proposed VALL-E X is trained on multiple monolingual speech data, without special optimization for code-switch setting, VALL-E X provides a promising solution to code-switch speech synthesis. We put the code-switch samples on our demo page, demonstrating that due to its strong in-context learning ability, VALL-E X can synthesize fluent code-switch speech with a consistent voice. 6", "prediction": "VALL-E X Trans to maintain emotional consistency between the source prompt and the synthesized target speech.", "reference": "In the past few years, there have been some great advancements in the field of speech synthesis. With the rapid progress being made by natural language systems, the text is mostly chosen as the initial form to generate speech. A Text-To-Speech (TTS) system rapidly converts natural language into speech. Given a textual input, natural-sounding speech is produced. Currently, there are a number of texts to speech-language models that generate high-quality speech. \n\nThe traditional models are limited to producing the same robotic outputs, which are only according to a particular speaker in a particular language. With the introduction of deep neural networks in the approach, Text-to-speech models have already become more efficient with the added features of maintaining the stress and intonation in the generated speech. These audios seem more human-like and natural. But the feature of Cross-linguality of speech, which wasn’t touched upon yet, has now been added. A Microsoft team of researchers has presented a language model that exhibits cross-lingual speech synthesis performance. \n\nCross-lingual speech synthesis is basically an approach for transmitting a speaker’s voice from one language to another. The cross-lingual neural codec language model that the researchers have introduced is called VALL-E X. It is an extended version of the VALL-E Text to speech model, which has been developed by acquiring strong in-context learning capabilities from the VALL-E TTS model. \n\nThe team has summarized their work as follows – \n\nVALL-E X is a cross-lingual neural codec language mode that consists of massive multilingual, multi-speaker, multi-domain unclean speech data.\nVALL-E X has been designed by training a multilingual conditional codec language model in order to predict the acoustic token sequences of the target language speech. This is done by using both the source language speech and the target language text as the fed prompts.\nThe multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X overcomes the primary challenge of cross-lingual speech synthesis tasks: the foreign accent problem. It can generate speech in a native tongue for any speaker.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation.\nVALL-E X has been evaluated with LibriSpeech and EMIME for both English and Chinese languages, including English Text to speech prompted by Chinese speakers and Chinese TTS prompted by English speakers. It demonstrates high-quality zero-shot cross-lingual speech synthesis performance. This new model undoubtedly seems promising as it overcomes the foreign accent model and adds to the potential for cross-lingual speech synthesis. \n\nCheck out the Paper and Project. All Credit For This Research Goes To the Researchers on This Project. Also, don’t forget to join our 16k+ ML SubReddit, Discord Channel, and Email Newsletter, where we share the latest AI research news, cool AI projects, and more."}
{"idx": 89, "input": "Generate a story for these personas: General Public, Developer, Researcher, Student\n\nSection: Conclusion / Future Work\nConclusion / Future Work: Conclusion In this work, we propose VALL-E X, a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. VALL-E X is free of the requirement for cross-lingual paired data from the same speakers. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.", "prediction": "VALL-E X is a cross-lingual neural codec language model, which can retrain the source language speaker’s voice in the generated target language speech. By training on large-scale multi-lingual multi-speaker speech-transcription data, the proposed VALL-E X demonstrates strong in-context learning capabilities and can support zero-shot cross-lingual text-to-speech and zero-shot voice-retentive speech-to-speech translation tasks. For future work, we plan to expand this method with more data and more languages.", "reference": "The multilingual in-context learning framework allows the production of cross-lingual speech by VALL-E X. It maintains the unseen speaker’s voice, emotion, and speech background.\nVALL-E X has been applied to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Upon experimentation, VALL-E X can beat the strong baseline regarding speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation."}
{"idx": 90, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Abstract\nAbstract: Abstract We propose a cross-lingual neural codec language model, VALL-E X, for crosslingual speech synthesis. Specifically, we extend VALL-E [Wang et al., 2023] and train a multi-lingual conditional codec language model to predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-tospeech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment. Moreover, VALL-E X effectively alleviates the foreign accent problems, which can be controlled by a language ID. Audio samples are available at https://aka.ms/vallex. VALL-E X (Cross-Lingual VALL-E) Cross-Lingual Neural Codec Language Modeling Multilingual G2P Multilingual G2P Audio Codec Encoder Source Text Target Text Source Speech Figure 1: The overall framework of VALL-E X, which can synthesize personalized speech in another language for a monolingual speaker. Taking the phoneme sequences derived from the source and target text, and the source acoustic tokens derived from an audio codec model as prompts, VALL-E X is able to produce the acoustic tokens in the target language, which can be then decompressed to the target speech waveform. Thanks to its powerful in-context learning capabilities, VALL-E X does not require cross-lingual speech data of the same speakers for training, and can perform various zero-shot cross-lingual speech generation tasks, such as cross-lingual text-to-speech synthesis and speech-to-speech translation. Personalized Target Speech Audio Codec Decoder Language ID Source phoneme sequences Target phoneme sequences Source acoustic tokens Target acoustic tokens 1", "prediction": "VALL-E X is a cross-lingual neural codec language model that can synthesize personalized speech in another language for a monolingual speaker. It is based on VALL-E, which is a multi-lingual conditional codec language model that can predict the acoustic token sequences of the target language speech by using both the source language speech and the target language text as prompts. VALL-E X inherits strong incontext learning capabilities and can be applied for zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experimental results show that it can generate high-quality speech in the target language via just one speech utterance in the source language as a prompt while preserving the unseen speaker’s voice, emotion, and acoustic environment.", "reference": "VALL-E-X, unlike RVC (Retrieval-based Voice Conversion), directly reflects the characteristics of short audio samples. Therefore, it might be beneficial to prepare several audio prompts with different emotions or intonations and switch between them as needed. This approach allows for greater flexibility and variety in the synthesized speech, adapting to different emotional tones or speaking styles."}
{"idx": 91, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nIntroduction: Introduction Recent years have witnessed significant advancements in end-to-end text-to-speech (TTS) synthesis, and the quality of synthesized speech is even close to human parity [Li et al., 2019, Ren et al., 2019, Tan et al., 2022]. However, these models can only generate high-quality speech for a specific speaker in a specific language. Cross-lingual speech synthesis is a new emerging task that aims to transfer the speaker’s voice from one language to another. The speech quality for cross-lingual speech synthesis, especially the speaker similarity, is far behind the monolingual TTS models due to two reasons, 1) data scarcity, as it is difficult to collect multi-lingual speech data for the same speaker, and 2) model capacity, as conventional cross-lingual TTS models are not powerful enough to transfer the speaker voice, speech background, and speaker emotion from the source language speech to the target language speech. Previous methods to tackle these challenges typically augment end-to-end TTS models with specific subnets for speaker and language control [Nachmani and Wolf, 2019, Zhang et al., 2020, Yang and He, 2020, Ellinas et al., 2022, Cai et al., 2023]. For example, based on the multi-speaker TTS model, Nachmani and Wolf [2019] introduce multiple encoders for each language and additional loss to keep the speaker’s identity. Zhang et al. [2020] employs a phonemic representation to capture cross-language information and an adversarial network to disentangle speaker identities. Yang and He [2020] incorporate speaker and language networks with speaker and language IDs as input to deal with the multi-speaker and cross-lingual problems respectively. Yang and He [2022] further propose a multi-task learning method with additional tasks of speaker similarity and language identification. Moreover, Cai et al. [2023] investigates cross-lingual multi-speaker text-to-speech synthesis with sufficient or limited bilingual speech training data. However, the above methods fail to effectively extend to zero-shot scenarios for synthesizing target speech from the unseen source speaker, and often suffer from low speaker similarity and L2 (second-language, or foreign) accent problems [Zhang et al., 2019, Lee et al., 2022]. Table 1: A comparison between VALL-E X and previous cross-lingual TTS systems. Previous Systems VALL-E X Intermediate representation Mel spectrogram Audio codec codes Training data 13K hours Speech accent Foreign Native Speaker similarity Relative low High In-context learning Zero-shot cross-lingual TTS In this work, we present a novel approach to address these issues by proposing a simple yet effective cross-lingual neural codec language model, VALL-E X, which leverages strong in-context learning capacities to achieve high-quality zero-shot cross-lingual speech synthesis. Based on the knowledge learned from large-scale multi-lingual speech data, VALL-E X is able to transfer the speech characteristics, including the speaker’s voice, emotions, and the speech background, from the source language to the target language, and also alleviate the foreign accent problems. More specifically, we first obtain the multi-lingual speech-transcription data from existing ASR data or pseudo-labeled speech data. Then we convert the transcriptions to phoneme sequences with a rule-based converter (G2P tool) and the speech data to acoustic tokens with an offline neural codec encoder. Finally, we concatenate the paired phoneme and acoustic token sequences of each language and train a multi-lingual conditional language model. As illustrated in Figure 1, after training, VALL-E X can predict the acoustic tokens of the target language prompted by the phoneme sequences of both languages and the acoustic tokens of the source language. The generated acoustic token sequence is decompressed to the target speech waveform by an offline audio codec", "prediction": "The purpose of this paper is to introduce a new approach to cross-lingual speech synthesis. The proposed approach is based on the multi-speaker TTS model. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi-task learning method. The proposed approach is based on the multi", "reference": "The page linked below contains actual examples of voices synthesized with each system. The results of English voice synthesis with Japanese vocal qualities, found at the end of the page in section `Japanese zero-shot cross-lingual text-to-speech` , are particularly illustrative. You can notice that it’s not just the vocal qualities of the original voice that are captured, but also the emotions.\nThe AR Decoder then generates an intermediate representation A^t_1. Subsequently, S^t, A^s, and A^t_1 are used as inputs to the NAR Decoder to calculate the final intermediate representations A^t_2 to A^t_8. This process enables VALL-E-X to create a nuanced synthesis that mirrors the characteristics of the source voice while delivering the desired speech content.\nModel Size\nvocos: 40.4MB\nwhisper medium: 1.42GB\nFirst, clone the repository.\nThe following commands can be used for speech synthesis. Currently, BLAS runs faster than GPU on macOS, so if necessary, give -e 1 to run using BLAS."}
{"idx": 92, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\ndecoder. VALL-E X is trained on two large-scale multi-speaker datasets1, LibriLight [Kahn et al., 2020] and WenetSpeech [Zhang et al., 2022a], containing about 60,000 hours of English audiobook speech data and 10,000+ hours of multi-domain Chinese ASR data, respectively. The combination of LibriLight and Wenet", "prediction": "VALL-E X is a multi-speaker decoder that can be trained on a large number of speakers.", "reference": "The page linked below contains actual examples of voices synthesized with each system. The results of English voice synthesis with Japanese vocal qualities, found at the end of the page in section `Japanese zero-shot cross-lingual text-to-speech` , are particularly illustrative. You can notice that it’s not just the vocal qualities of the original voice that are captured, but also the emotions.\nThe AR Decoder then generates an intermediate representation A^t_1. Subsequently, S^t, A^s, and A^t_1 are used as inputs to the NAR Decoder to calculate the final intermediate representations A^t_2 to A^t_8. This process enables VALL-E-X to create a nuanced synthesis that mirrors the characteristics of the source voice while delivering the desired speech content.\nModel Size\nvocos: 40.4MB\nwhisper medium: 1.42GB\nFirst, clone the repository.\nThe following commands can be used for speech synthesis. Currently, BLAS runs faster than GPU on macOS, so if necessary, give -e 1 to run using BLAS."}
{"idx": 93, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Introduction\nSpeech makes a large multi-lingual multi-speaker multi-domain unclean speech dataset, which significantly improves the coverage of different speakers and enhances VALL-E X’s generalization capacity. The comparison between VALL-E X and the previous cross-lingual TTS systems are listed in Table 1. We conduct experiments on two kinds of cross-lingual speech generation tasks, zero-shot crosslingual text-to-speech synthesis (XTTS), and zero-shot speech-to-speech translation (S2ST). For cross-lingual text-to-speech synthesis, the proposed VALL-E X is evaluated with LibriSpeech [Panayotov et al., 2015] and EMIME [Wester, 2010] for English and Chinese respectively, including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers. For zero-shot speech-to-speech translation, EMIME [Wester, 2010] dataset is used for the evaluation of VALL-E X on bidirectional ChineseEnglish translation tasks, and it contains bilingual audio recordings by the same speakers. We evaluate the proposed VALL-E X framework from several aspects, including speaker similarity, speech quality (ASR- WER or BLEU), speech naturalness, and human evaluation (e.g., SMOS, MOS, and CMOS). Specifically, due to the strong in-context learning capability, VALL-E X achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker. By training on large-scale speech-transcription data, the proposed VALL-E X significantly reduces the word error rate from 8.53 to 4.07 in the cross-lingual English TTS task, obtains the substantial gain of 3.17 BLEU scores than the strong baseline in S2ST tasks, and achieves better speech naturalness. Furthermore, the human evaluation shows that our VALL-E X outperforms strong baselines in terms of SMOS (4.00 vs. 3.42 in XTTS, 4.12 vs. 3.06 in S2ST), CMOS (+0.24 for ours vs. baseline in XTTS), and MOS (3.87 vs. 3.81 in S2ST). Our contributions can be summarized as follows: • We develop a cross-lingual neural codec language model VALL-E X with large multilingual multi-speaker multi-domain unclean speech data. VALL-E X is a conditional cross-lingual language model predicting the target language acoustic tokens with the source language speech and target language text as prompts. • The multi-lingual in-context learning framework enables VALL-E X to generate crosslingual speech maintaining the unseen speaker’s voice, emotion, and speech background, prompted by only one sentence in the source language. • Based on the learned cross-lingual speech modeling ability with the introduced language ID, VALL-E X can generate speech in a native tongue for any speaker and can significantly reduce the foreign accent problem, which is a well-known problem in cross-lingual speech synthesis tasks. • We apply VALL-E X to zero-shot cross-lingual text-to-speech synthesis and zero-shot speech-to-speech translation tasks. Experiments show that the proposed VALL-E X can beat the strong baseline in terms of speaker similarity, speech quality, translation quality, speech naturalness, and human evaluation. We encourage readers to listen to the audio samples on our demo page: https://aka.ms/vallex. 2", "prediction": "VALL-E X is a cross-lingual text-to-speech generation framework that is trained on a large multi-lingual multi-speaker multi-domain unclean speech dataset. The proposed framework achieves a higher speaker similarity score than the previous SOTA model for the unseen speaker.", "reference": "The page linked below contains actual examples of voices synthesized with each system. The results of English voice synthesis with Japanese vocal qualities, found at the end of the page in section `Japanese zero-shot cross-lingual text-to-speech` , are particularly illustrative. You can notice that it’s not just the vocal qualities of the original voice that are captured, but also the emotions.\nThe AR Decoder then generates an intermediate representation A^t_1. Subsequently, S^t, A^s, and A^t_1 are used as inputs to the NAR Decoder to calculate the final intermediate representations A^t_2 to A^t_8. This process enables VALL-E-X to create a nuanced synthesis that mirrors the characteristics of the source voice while delivering the desired speech content.\nModel Size\nvocos: 40.4MB\nwhisper medium: 1.42GB\nFirst, clone the repository.\nThe following commands can be used for speech synthesis. Currently, BLAS runs faster than GPU on macOS, so if necessary, give -e 1 to run using BLAS."}
{"idx": 94, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nMethod: 3.2 Model Framework Inspired by VALL-E, the cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM MAR is only used to predict the acoustic tokens from the first quantizer of EnCodec model. Formally, based on paired speech-transcription data in any language, let S denote the transcribed phoneme sequence, and A:,1 ai,1|i = 1, . . . , N denotes the first-layer acoustic tokens extracted from the speech X. The decoder MAR, modeling the concatenated sequence S, A:,1, is trained to predict A:,1 autoregressively. It is optimized by maximizing the log-likelihood, Quantization & Phonemization: Quantized tokens Layer L 12 504 31 8 743 ... A:,2:L ...... Layer 2 325 71 435 90 7 ... A:,1 731 284 78 32 669 ... Layer 1 audio codec encoder (Quantization) Multi-lingual speech-transcription pairs (phonemization) G2P tool Phoneme tokens HH AH L OW ... D S N Y Multi-lingual Non-Autoregressive Codec LM Instead of the autoregressive generation pattern, multi-lingual non-autoregressive codec LM MNAR is a non-autoregressive Transformer language model aiming at iteratively generating the rest layers of acoustic tokens from the first layer. It is prompted by the phoneme sequence of the current sentence (S) and the acoustic token sequence of another sentence with the same speaker (  A). Here  A is taken from the previous sentence in the dataset where the adjusted sentences are usually segmented from the same paragraph. It is expected to have the same characteristics of voice (speaker, speed, background, etc) as the current sentence and is used as an additional reference for cloning the target voice. Like VALL-E, for generating acoustic tokens of each layer l [2, 8], the embeddings of l 1 layers’ acoustic tokens (A:,1:l1) are summed up layerwise as input. The learning objective for the l-layer acoustic tokens A:,l can be calculated as 8 X l=2 log pNAR A:,l | D S,  A:,1:8, A:,1:l1 E ; MNAR (2) LMNAR = where means the sequence concatenation. pNAR(.) computes the pointwise probabilities of A:,l. 3.3 Multi-lingual Training In order to learn cross-lingual acoustic conversion information for cross-lingual TTS and speech-tospeech translation tasks, we take advantage of bilingual speech-transcription (ASR) corpus2, pairs of (Ss, As) and (St, At) to train our multi-lingual codec LMs MAR and MNAR, where s and t represent two different (source and target) languages. Language ID Module Following multi-lingual TTS, we leverage a language ID to guide the speech generation for specific languages in VALL-E X. On the one hand, without language ID", "prediction": ") The cross-lingual codec language model VALL-E X (denoted as ) leverages a multi-lingual autoregressive codec LM and a multi-lingual non-autoregressive codec LM to generate acoustic tokens at different granularities, as shown in the left part of Figure 2. We also adopt the neural codec model EnCodec [Défossez et al., 2022] as the acoustic quantizer, which is an encoder-decoder model with L quantization layers. We choose L = 8 in our experiments, for each layer it produces quantized codes of 1024 entries at 75Hz. Multi-lingual Autoregressive Codec LM MAR The multi-lingual autoregressive codec LM MAR is a unidirectional Transformer decoder that autoregressively generates acoustic tokens based on the semantic tokens (phoneme sequence). To make the sentence-level training efficient and accelerate the decoding during inference, similar to VALL-E, the cross-lingual autoregressive codec LM ", "reference": "When these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024).\nPositional Embeddings\nFor positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha.\nIn the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution.\nFor example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2.\nOptimisation using kv_cache\nVALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer.\nA Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time.\nTo optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step.\nThis kv_cache mechanism significantly speeds up the operation of the Transformer.\nencodec_24khz: 93.2MB"}
{"idx": 95, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\n, VALL-E X may be confused to select suitable acoustic tokens for the specific language since it is trained with multi-lingual data. On the other hand, some languages have very different characteristics, for example, Chinese is a tone language while English is a non-tone language, which increases the difficulty of adjusting the speaking style across languages. Our experiments found that adding language information to the input of our multi-lingual autoregressive codec LM MAR is surprisingly effective in guiding the right speaking style and relieving the L2 accent problem, which will be introduced in Section 5.5. Concretely, we embed language IDs into dense vectors and add them to the embeddings of acoustic tokens. 3.4 Cross-Lingual Inference After training, VALL-E X can perform cross-lingual speech synthesis, as shown in Figure 3. In detail, we first concatenate source phonemes Ss and target phonemes St as prompts, and take the first-layer source acoustic tokens As :,1 as the decoding prefix, condition on which the multi-lingual autoregressive codec LM MAR generates the first-layer target acoustic tokens At :,1, at i,1 pAR  at i,1 | Ss, St, As :,1, At i,1 ; MAR , i = 1, . . . , (3) where means probability-based sampling. The sampling is stopped until the end-of-sentence> token is sampled. As mentioned in Section 3.3, language ID is used to control the speaking style of the final generated speech. After obtaining the first-layer target acoustic tokens At :,1 from MAR, multi-lingual non-autoregressive codec LM MNAR is used to predict the rest layers of acoustic tokens n At :,l | l = 2, . . . , 8 o by greedy search, i.e., choosing the tokens with maximum probabilities, At :,l = argmax At :,l pNAR  At :,l | St, As :,1:8, At :,1:l1 ; MNAR , l = 2, . . . , 8. (4) Finally, we use the decoder of EnCodec to synthesize the target speech from the complete target acoustic tokens At :,1:8. tl= 2, ... , 8 VALL-E X Inference: A:,l Multi-lingual Non-Autoregressive Codec LM t s A:,1:l1 St A:,1:8 t A:,1 Multi-lingual Autoregressive Codec LM B t aN,1 t a2,1 t ... s a1,1 bos> Ss St A:,1 Figure 3: Inference illustration of the cross-lingual neural codec language model VALL-E X, with two-stage decoding strategies. VALL-E X can support zero-shot cross-lingual TTS and zero-shot speech-to-speech translation tasks. 4 VALL-E X Application VALL-E X can be applied to various cross-lingual speech generation tasks. In this paper, we take zero-shot cross-lingual TTS and zero-shot speech-to-speech translation as two examples, as illustrated in Figure 3. 4.1 Zero-Shot Cross-Lingual TTS The proposed VALL-E X is naturally suitable for zero-shot cross-lingual TTS tasks. Cross-lingual TTS tries to synthesize the target speech from text with a foreign speaker’s voice. Conventional methods mainly employ additional speaker and language networks to model the speaker and language information respectively, without zero-shot synthesis capability. Thanks to the in-context learning capability of large language models, VALL-E X surprisingly shows the ability to perform zero-shot cross-lingual speech synthesis. More specifically, given the source speech, source transcript, and target text, we first convert source speech into source acoustic token As using the encoder of neural codec model EnCodec, and convert source transcript and target text into source phonemes Ss and target phonemes St using G2P tool. More specifically, as introduced in", "prediction": "VALL-E X is a multi-lingual speech synthesis system that can be used to generate cross-lingual speech. The main idea of this paper is to introduce a cross-lingual approach to speech synthesis.", "reference": "When these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024).\nPositional Embeddings\nFor positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha.\nIn the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution.\nFor example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2.\nOptimisation using kv_cache\nVALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer.\nA Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time.\nTo optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step.\nThis kv_cache mechanism significantly speeds up the operation of the Transformer.\nencodec_24khz: 93.2MB"}
{"idx": 96, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nSection 3.4, we let St be the phonemes extracted from the target text, Ss and As be the phonemes and acoustic tokens extracted from the source speech. Then VALL-E X generates the full-layer target acoustic tokens, which are finally decompressed into the target speech by EnCodec decoder. 4.2 Zero-Shot Speech-to-Speech Translation We can also apply our VALL-E X to zero-shot speech-to-speech translation tasks with additional speech recognition & translation model, which is responsible for synchronously recognizing and translating the source speech to the source and target phoneme sequences. Speech Recognition & Translation Model We leverage the improved SpeechUT [Zhang et al., 2022c] as our speech recognition & translation model, which is a unified-modal speech-unit-text pre-training framework using hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and speech-to-text translation (ST). Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss) and MT corpus (source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more As Application: Zero-shot cross-lingual TTS Ss St Codec Encoder G2P Tool G2P Tool Source text Target text Source speech Application: Zero-shot S2ST Ss St As E Codec Encoder Semantic Semantic Encoder Decoder Speech Encoder Source Speech pre-training details about this model. After pre-training, the model is fine-tuned with (X s, Ss, St) triplet data derived from the ST corpus. Specifically, we perform multi-task learning with the CTC [Graves et al., 2006] loss added on the phoneme encoder predicting the source phonemes and the cross-entropy loss on the phoneme decoder predicting the target phonemes. Inference Figure 3 shows the inference process of speech-to-speech translation. Given a source speech X s, the speech recognition & translation model first generates the source phonemes Ss from the semantic encoder and the target phonemes St from the semantic decoder. Besides, we use the EnCodec encoder to compress X s into source acoustic tokens As. Then, we concatenate Ss, St, and As, as the input of VALL-E X, to produce the acoustic token sequence for the target speech, as introduced in Section 3.4. The generated acoustic tokens are converted to the final target speech with the decoder of EnCodec. 4.3 Evaluation The proposed model is verified using various evaluation criteria, including speaker similarity (ASVScore), speech quality (ASR-WER), translation quality (ASR-BLEU), naturalness, and human evaluation. Specifically, we measure speaker similarity between synthesized target speech and groudtruth target speech or source speech as an automatic speaker verification (ASV) task, where a WavLM [Chen et al., 2022] based ASV model is used to calculate the score. To verify the quality of generated speech, we first utilize the ASR system from the released HuBERT-Large model [Hsu et al., 2021] to recognize it into text. For TTS, speech quality is measured by ASR-WER between the recognized text and the original target text. For S2ST, speech quality is measured by ASR-BLEU between the recognized text and the provided translation text. Finally,", "prediction": "VALL-E X is a speech-to-text translation model that can be applied to zero-shot speech-to-speech translation tasks. SpeechUT is a speech-to-text pre-training framework that uses hidden units as the modality bridge between speech and text. It supports various speech-to-text tasks, including both ASR and ST. Inspired by SpeechLM [Zhang et al., 2022b] which explores different choices of units, we improve SpeechUT by replacing the clustering-based hidden units with phonemes. Specifically, it consists of a speech encoder, a phoneme encoder, and a phoneme decoder. All these components are pre-trained on ASR corpus (source speech X s, source phoneme Ss, target phoneme St), where the phoneme sequences are converted from the text. Please see Appendix A.1.3 for more.", "reference": "When these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024).\nPositional Embeddings\nFor positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha.\nIn the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution.\nFor example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2.\nOptimisation using kv_cache\nVALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer.\nA Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time.\nTo optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step.\nThis kv_cache mechanism significantly speeds up the operation of the Transformer.\nencodec_24khz: 93.2MB"}
{"idx": 97, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Method\nto better verify our proposed VALL-E X systems, we adopt the open-source NISQA3 [Mittag and Möller, 2021] (the NISQA-TTS model) to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech, e.g., mean opinion score (MOS), comparative mean opinion score (CMOS) and similar mean opinion score (SMOS). 5", "prediction": "The VALL-E X systems are based on the NISQA3 model to evaluate the naturalness of the synthetic speech and further perform the human evaluation with manual scoring on the generated speech.", "reference": "When these tokens are subjected to text embedding, they are transformed into vectors of size (1, num_sequence, 1024).\nPositional Embeddings\nFor positional embeddings, sine waves are used. The sine wave has one learning parameter, alpha.\nIn the ARDecoder, sampling from the token logits is done using torch.multinomial instead of ArgMax. torch.multinomial is a method that uses random numbers to determine tokens based on their probability distribution.\nFor example, if token 1 has a probability of 0.6 and token 2 has 0.4, ArgMax would always select token 1. However, torch.multinomial makes a probabilistic selection, meaning there's a 60% chance for token 1 and a 40% chance for token 2.\nOptimisation using kv_cache\nVALL-E-X employs kv_cache for speeding up the ARDecoder’s Transformer.\nA Transformer without kv_cache inputs the past N tokens to output the next token, requiring the embedding of these past N tokens every time.\nTo optimize this, kv_cache stores the embeddings of the past N tokens (kv) instead of computing them each time. Thus, only the most recent token is input for each new step.\nThis kv_cache mechanism significantly speeds up the operation of the Transformer.\nencodec_24khz: 93.2MB"}
{"idx": 98, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nExperiments / Results: Experiments We evaluate the proposed model on zero-shot cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from LibriLight [Kahn et al., 2020] containing about 60,000 hours of unlabeled speech, whose speech data are collected from audiobooks. We train a Kaldi4 ASR model on the labeled Librispeech [Panayotov et al., 2015] dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5, OpenSubtitles20186 and WMT20207, which contain about 13M, 10M, and 50M sentence pairs in conversion, drama8, and news domains, respectively. The EnglishChinese ST data is from GigaST [Ye et al., 2022], which is created by translating the transcripts in GigaSpeech [Chen et al., 2021] using a strong machine translation system. Similarly, we create the ChineseEnglish ST data by translating the transcripts of WenetSpeech using an MT model trained by ourselves on the MT data mentioned above. 3https://github.com/gabrielmittag/NISQA 4https://github.com/kaldi-asr/kaldi/tree/master/egs/librispeech 5https://challenger.ai/competition/translation 6https://opus.nlpl.eu/OpenSubtitles2018.php 7https://www.statmt.org/wmt20/translation-task.html 8http://www.opensubtitles.org/ We evaluate zero-shot S2ST using the Effective Multilingual Interaction in Mobile Environments (EMIME) dataset [Wester, 2010], which contains bilingual Chinese/English speech recorded by the same speakers. There are 25 pairs of bilingual sentences recorded by 7 female and 7 male native Chinese speakers, thus the total number of test examples is 350. Zero-shot cross-lingual TTS is evaluated using Librispeech [Panayotov et al., 2015] dev-clean set and EMIME dataset providing English and Chinese data, respectively. We have two settings in the experiments: (1) Librispeech English TTS with EMIME Chinese speech as prompts; (2) EMIME Chinese TTS with Librispeech Engish speech as prompts. 5.2 Experimental Setup Phonemization & Quantization The right picture of Figure 2 illustrates the phonemization & quantization processes for different languages. All text data, including ASR transcripts and MT/ST translations, are converted by the lexicon provided in ASR datasets. We use a unified phoneme set called BigCiDian9 for two languages which are based on International Phonetic Alphabet (IPA). The ASR transcripts (or pseudo transcripts) are also converted by Kaldi force-alignment tools10 for additional alignment information used for the pre-training of speech recognition & translation model. The speech is quantized into discrete codec codes as the acoustic tokens using the neural audio codec model EnCodec11, which employs residual vector quantization to iteratively quantize speech to a codebook according to the residual after quantization, resulting in multi-layer codebooks.", "prediction": "We train a speech recognition & translation model for cross-lingual TTS including English TTS prompted by Chinese speakers and Chinese TTS prompted by English speakers, and zero-shot S2ST including ChineseEnglish and EnglishChinese directions. We provide the synthesized audio samples on our demo page to better show the performance of VALL-E X. 5.1 Dataset Our VALL-E X is trained using bilingual speech-transcription (ASR) data. The Chinese ASR data are from WenetSpeech [Zhang et al., 2022a] containing 10,000+ hours of multi-domain labeled speech. The English ASR data are from Librispeech [Panayotov et al., 2015] containing about 60,000 hours of unlabeled speech. We train a Kaldi4 ASR model on the labeled Librispeech dataset to generate the pseudo transcripts for the unlabeled LibriLight speech. To train the speech recognition & translation model for S2ST, we also use additional MT and ST data. The MT data are from AI Challenger5,", "reference": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."}
{"idx": 99, "input": "Generate a story for these personas: Developer, Researcher, Student\n\nSection: Experiments / Results\nModel Architecture For the cross-lingual codec language models, MAR and MNAR are both 12-layer Transformer decoders with an attention dimension of 1024 and the FFN dimension of 4096. The autoregression is implemented by attention masking in the MAR model. Sinuous position embedding is separately computed for each prompt sequence in MAR and MNAR models. Besides, the M", "prediction": "NAR model has a recurrent architecture with a recurrent model.", "reference": "Main Features\nTraditionally, when using a voice changer, RVC (Retrieval-based Voice Conversion) was utilized, which required about 10 minutes of pre-recorded voice for training. In contrast, VALL-E-X does not require training and can change vocal characteristics with just a few seconds of audio.\nThe weights that are downloaded for use in VALL-E-X are as follows. Whisper is used to obtain phonemes for calculating audio prompts:\nvallex: 1.48GB\nBelow is a sample script which calculates the audio prompt of the audio file BASIC5000_0001.wav (taken from the JSUT speech corpus) and output Japanese speech for the specified text prompt.\nVALL-E-X can be run in ONNX format using ailia SDK 1.2.15 or later.\nax Inc. has developed ailia SDK, which enables cross-platform, GPU-based rapid inference."}
