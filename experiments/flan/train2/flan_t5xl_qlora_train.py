# -*- coding: utf-8 -*-
"""T5_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B5X8EPJ1md94smgO-6Jqhbt4iowWaIg9

# Installazione librerie
"""

!pip uninstall -y transformers bitsandbytes peft
!pip install transformers==4.39.3 bitsandbytes==0.45.2 peft==0.10.0 accelerate==0.27.2 datasets sentencepiece rouge-score evaluate

"""# Import librerie"""

from datasets import load_dataset
from transformers import (
    AutoTokenizer, AutoModelForSeq2SeqLM,
    Seq2SeqTrainer, Seq2SeqTrainingArguments,
    BitsAndBytesConfig
)
from peft import LoraConfig, get_peft_model, TaskType
import torch
from google.colab import drive

"""# Monta Google Drive"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

"""# Caricamento + split 70/10/20"""

# Carica train (80%) e test (20%)
dataset = load_dataset(
    "json",
    data_files={
        "train": "/content/drive/MyDrive/AIScientist/FlanT5/train_ready_trunc.json",
        "test": "/content/drive/MyDrive/AIScientist/FlanT5/test_ready_trunc.json"
    }
)

# Split train in 87.5% train + 12.5% validation → 70/10
split_dataset = dataset["train"].train_test_split(test_size=0.125, seed=42)
train_data = split_dataset["train"]
val_data = split_dataset["test"]
test_data = dataset["test"]

"""# Parametri base"""

model_name = "google/flan-t5-xl"
MAX_INPUT_LEN = 1024
MAX_TARGET_LEN = 1024
output_dir = "/content/drive/MyDrive/AIScientist/FlanT5/flan_t5xl_lora_1024_1024_v2"

"""# Tokenizer"""

tokenizer = AutoTokenizer.from_pretrained(model_name)

def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["input"],
        max_length=MAX_INPUT_LEN,
        truncation=True,
        padding="max_length"
    )
    labels = tokenizer(
        examples["output"],
        max_length=MAX_TARGET_LEN,
        truncation=True,
        padding="max_length"
    )
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

train_tokenized = train_data.map(preprocess_function, batched=True)
val_tokenized = val_data.map(preprocess_function, batched=True)
test_tokenized = test_data.map(preprocess_function, batched=True)

"""# Config quantizzazione 4-bit"""

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,  # bfloat16 su A100
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

"""# Carica modello quantizzato"""

import transformers

# Salva il metodo originale
original_to = transformers.modeling_utils.PreTrainedModel.to

# Patch per ignorare il controllo bitsandbytes
def safe_to(self, *args, **kwargs):
    if getattr(self, "quantization_method", None) is not None:
        self.quantization_method = None
    return original_to(self, *args, **kwargs)

transformers.modeling_utils.PreTrainedModel.to = safe_to

model = AutoModelForSeq2SeqLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto"
)

"""# Applica LoRA"""

peft_config = LoraConfig(
    task_type=TaskType.SEQ_2_SEQ_LM,
    r=16,               # <--- aumentato da 8 a 16
    lora_alpha=32,      # <--- aumentato proporzionalmente
    lora_dropout=0.1,
    bias="none"
)

model = get_peft_model(model, peft_config)

# Disattiva cache e attiva gradient checkpointing
model.config.use_cache = False
model.gradient_checkpointing_enable()

model.is_loaded_in_4bit = True
model.is_loaded_in_8bit = False
# Fix gradienti input
model.enable_input_require_grads()

# Congela tutto tranne LoRA
for param in model.parameters():
    param.requires_grad = False
for name, param in model.named_parameters():
    if "lora" in name:
        param.requires_grad = True

"""# Metriche ROUGE + BLEU"""

from evaluate import load

rouge = load("rouge")
bleu = load("bleu")

def compute_metrics(eval_pred):
    preds, labels = eval_pred

    # Decodifica predizioni e label
    preds = tokenizer.batch_decode(preds, skip_special_tokens=True)
    labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

    # Calcola ROUGE
    rouge_result = rouge.compute(predictions=preds, references=labels)

    # Calcola BLEU
    bleu_result = bleu.compute(predictions=preds, references=labels)

    # Combina risultati
    result = {
        "rouge1": round(rouge_result["rouge1"] * 100, 2),
        "rouge2": round(rouge_result["rouge2"] * 100, 2),
        "rougeL": round(rouge_result["rougeL"] * 100, 2),
        "bleu": round(bleu_result["bleu"] * 100, 2)
    }
    return result

"""# TrainingArguments"""

training_args = Seq2SeqTrainingArguments(
    output_dir=output_dir,
    evaluation_strategy="steps",
    eval_steps=500,
    save_steps=500,
    logging_steps=100,
    num_train_epochs=3,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=8,   # manteniamo per stabilità
    learning_rate=1e-4,              # leggermente più alto
    fp16=True,
    save_total_limit=2,
    predict_with_generate=True,
    generation_max_length=MAX_TARGET_LEN,
    ddp_find_unused_parameters=False,
    report_to="none"
)

"""# Trainer"""

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    tokenizer=tokenizer
)

"""# Avvia training"""

import torch
torch.cuda.empty_cache()
trainer.train()

"""# Salva modello e tokenizer"""

model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)
trainer.evaluate(test_tokenized, max_length=MAX_TARGET_LEN)

"""# Ricarica modello e tokenizer"""

from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# Ricarica il modello LoRA e tokenizer salvati
model = AutoModelForSeq2SeqLM.from_pretrained(output_dir)
tokenizer = AutoTokenizer.from_pretrained(output_dir)
model.to("cuda")

"""# Calcolo metriche (ROUGE, BLEU)"""

from evaluate import load
import numpy as np

# Carica metriche
rouge = load("rouge")
bleu = load("bleu")

# Funzione per generare e calcolare metriche
def evaluate_model(model, tokenizer, dataset, max_target_len=1024):
    predictions = []
    references = []

    for example in dataset:
        # Tokenizza input
        inputs = tokenizer(example["input"], return_tensors="pt", truncation=True, max_length=1024).to("cuda")

        # Genera output
        outputs = model.generate(**inputs, max_length=max_target_len)
        pred_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Aggiungi predizioni e reference
        predictions.append(pred_text)
        references.append(example["output"])

    # Calcolo ROUGE
    rouge_result = rouge.compute(predictions=predictions, references=references)

    # Calcolo BLEU (BLEU richiede lista di liste per references)
    bleu_result = bleu.compute(predictions=predictions, references=[[ref] for ref in references])

    return rouge_result, bleu_result, predictions, references

import torch
torch.cuda.empty_cache()
# Valutazione sul test set
rouge_result, bleu_result, preds, refs = evaluate_model(model, tokenizer, test_data)

print("=== ROUGE ===")
print(rouge_result)
print("\n=== BLEU ===")
print(bleu_result)

"""# Visualizza alcuni esempi"""

# Mostra alcuni esempi di output vs reference
for i in range(5):
    print(f"\n--- Esempio {i+1} ---")
    print(f"Input:\n{test_data[i]['input']}")
    print(f"\nOutput generato:\n{preds[i]}")
    print(f"\nOutput corretto:\n{refs[i]}")