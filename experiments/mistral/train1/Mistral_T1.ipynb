{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12670657,"sourceType":"datasetVersion","datasetId":8007103},{"sourceId":12697925,"sourceType":"datasetVersion","datasetId":8024799},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900,"modelId":1902}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"e6d7f104-03f7-4af7-abcd-48370f4d1a03","_cell_guid":"2be2a0a7-28b1-4b5f-b602-7539e657d086","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q \\\n \"transformers==4.41.2\" \\\n \"peft==0.10.0\" \\\n \"datasets>=2.19\" \\\n \"bitsandbytes>=0.43.0\" \\\n \"accelerate>=0.28.0\" \\\n evaluate rouge-score bert-score textstat\n","metadata":{"_uuid":"1ea4cee3-e8f4-4360-bd85-3fb0d041786d","_cell_guid":"2b8a5e77-ece2-4df4-8c30-c8f386d26582","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    DataCollatorForLanguageModeling,\n)\n\nfrom torch.optim import AdamW          # â† cambio import\nfrom datasets import Dataset\nfrom torch.utils.data import DataLoader\nimport torch\nimport json\n\nprint(f\"torchâ€‘cuda: {torch.version.cuda} | device count: {torch.cuda.device_count()}\")","metadata":{"_uuid":"dafe6e7e-0c35-44ba-abac-fd16aebfceab","_cell_guid":"147d3b80-c62a-429d-b6c3-847a22e89a0d","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-07T10:30:43.600657Z","iopub.execute_input":"2025-08-07T10:30:43.600999Z","iopub.status.idle":"2025-08-07T10:30:43.632446Z","shell.execute_reply.started":"2025-08-07T10:30:43.600972Z","shell.execute_reply":"2025-08-07T10:30:43.631882Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"torchâ€‘cuda: 12.4 | device count: 1\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"TRAIN_JSON = \"/kaggle/input/flan-t5/train_ready_trunc.json\"\nTEST_JSON  = \"/kaggle/input/flan-t5/test_ready_trunc.json\"\n\nwith open(TRAIN_JSON) as f: train_data = json.load(f)\nwith open(TEST_JSON)  as f: test_data  = json.load(f)\n\ntrain_ds = Dataset.from_list(train_data)\ntest_ds  = Dataset.from_list(test_data)\nprint(train_ds[0])","metadata":{"_uuid":"d129740b-362a-4f70-8db4-5f47be73ff49","_cell_guid":"466cadc3-1c09-4b7a-8e42-3e790a757988","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-08-07T10:30:45.535949Z","iopub.execute_input":"2025-08-07T10:30:45.536431Z","iopub.status.idle":"2025-08-07T10:30:45.800737Z","shell.execute_reply.started":"2025-08-07T10:30:45.536405Z","shell.execute_reply":"2025-08-07T10:30:45.799989Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stdout","text":"{'id_paper': '25', 'id_story': '25-01', 'id_chunk': '25-01_Abstract_chunk1', 'section': 'Abstract', 'input': 'Generate a story for these personas: Developer, Researcher, Policy Maker\\n\\nSection: Abstract\\nAbstract: Robust Speech Recognition via Large-Scale Weak Supervision Alec Radford * 1 Jong Wook Kim * 1 Tao Xu 1 Greg Brockman 1 Christine McLeavey 1 Ilya Sutskever 1 Abstract We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results but in a zeroshot transfer setting without the need for any finetuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing. arXiv:2212.04356v1 [eess.AS] 6 Dec 2022', 'output': 'Whisper is an automatic speech recognition (ASR) system trained on 680,000 hours of multilingual and multitask supervised data collected from the web. We show that the use of such a large and diverse dataset leads to improved robustness to accents, background noise and technical language. Moreover, it enables transcription in multiple languages, as well as translation from those languages into English. We are open-sourcing models and inference code to serve as a foundation for building useful applications and for further research on robust speech processing.\\nThe Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. Input audio is split into 30-second chunks, converted into a log-Mel spectrogram, and then passed into an encoder. A decoder is trained to predict the corresponding text caption, intermixed with special tokens that direct the single model to perform tasks such as language identification, phrase-level timestamps, multilingual speech transcription, and to-English speech translation.\\nOther existing approaches frequently use smaller, more closely paired audio-text training datasets,1 2, 3 or use broad but unsupervised audio pretraining.4, 5, 6 Because Whisper was trained on a large and diverse dataset and was not fine-tuned to any specific one, it does not beat models that specialize in LibriSpeech performance, a famously competitive benchmark in speech recognition. However, when we measure Whisperâ€™s zero-shot performance across many diverse datasets we find it is much more robust and makes 50% fewer errors than those models.\\n\\nAbout a third of Whisperâ€™s audio dataset is non-English, and it is alternately given the task of transcribing in the original language or translating to English. We find this approach is particularly effective at learning speech to text translation and outperforms the supervised SOTA on CoVoST2 to English translation zero-shot.\\nWe hope Whisperâ€™s high accuracy and ease of use will allow developers to add voice interfaces to a much wider set of applications. Check out the paper\\u2060(opens in a new window), model card\\u2060(opens in a new window), and code\\u2060(opens in a new window) to learn more details and to try out Whisper.\\n'}\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"\nfrom huggingface_hub import login\nfrom datasets import Dataset   # se non lâ€™hai giÃ  importato\nfrom transformers import AutoTokenizer\n\n# Inserisci il tuo token Hugging Face\nlogin(token=\"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\")\n\nMODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\nMAX_LEN = 1024          # lunghezza massima che vuoi realmente usare\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token        # OK\n\nPRE  = \"### Instruction:\\n\"\nMID  = \"\\n\\n### Response:\\n\"\nPOST = \"\"\n\ndef format_pair(example):\n    return {\"text\": f\"{PRE}{example['input']}{MID}{example['output']}{POST}\"}\n\ntrain_ds = train_ds.map(format_pair)\ntest_ds  = test_ds.map(format_pair)\n\ndef tok(batch):\n    return tokenizer(\n        batch[\"text\"],\n        padding=\"max_length\",\n        max_length=MAX_LEN,\n        truncation=True          # âœ”ï¸   (lascia True! evita overflow in VRAM)\n    )\n\ncols_to_drop = set(train_ds.column_names) - {\"text\"}\ntrain_tok = train_ds.map(tok, batched=True, remove_columns=list(cols_to_drop))\ntest_tok  = test_ds.map(tok,  batched=True, remove_columns=list(cols_to_drop))","metadata":{"_uuid":"f59db7ca-7f6f-4b78-b134-fb81752880f3","_cell_guid":"88cf4b65-ae12-4e69-b941-40c3f7c3094e","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-07T10:30:48.184150Z","iopub.execute_input":"2025-08-07T10:30:48.184791Z","iopub.status.idle":"2025-08-07T10:30:53.702263Z","shell.execute_reply.started":"2025-08-07T10:30:48.184766Z","shell.execute_reply":"2025-08-07T10:30:53.701508Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1866 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc6685f550f447793a6269a61f6ddb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/466 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c57face0e2ff4cab9a57b06fd2723447"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1866 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f3abc11c1f84334a66ce3d7a1fa8cdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/466 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4986e547542d40768e44b2b346386628"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# âš ï¸ esegui in una cella di notebook o nel terminale del runtime\n!pip install -U \"accelerate>=0.28.0\" \"bitsandbytes>=0.43.0\" --quiet\n# facoltativo ma consigliato:\n!pip install -U \"transformers>=4.41.0\" \"peft>=0.10.0\" --quiet\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, BitsAndBytesConfig\nfrom peft import LoraConfig, TaskType, get_peft_model\nfrom peft.utils import prepare_model_for_kbit_training   #  â† NUOVO\nimport torch\n\nMODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n# --- quantizzazione 4-bit -------------------------------------------------\nbnb_cfg = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.float16,\n)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=bnb_cfg,\n    trust_remote_code=True,\n    low_cpu_mem_usage=True,\n)\n\n# ğŸ”‘  PREPARA il modello per lâ€™add-on LoRA\nbase_model = prepare_model_for_kbit_training(\n    base_model,\n    use_gradient_checkpointing=True   # attiva anche il GC senza conflitti\n)\n\n# --- LoRA -----------------------------------------------------------------\nlora_cfg = LoraConfig(\n    task_type=TaskType.CAUSAL_LM,\n    r=4,\n    lora_alpha=16,\n    lora_dropout=0.05,\n    bias=\"none\",\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n)\n\nmodel = get_peft_model(base_model, lora_cfg)\nmodel.print_trainable_parameters()      # ora vedrai solo ~0,1 % â€œtrainableâ€\n","metadata":{"_uuid":"ba3958dc-d904-4a33-81c7-c0a9fc87f610","_cell_guid":"60a823e9-d3a6-49d6-9d23-bd0df4f00f0d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-08-07T10:31:24.887966Z","iopub.status.idle":"2025-08-07T10:31:24.888208Z","shell.execute_reply.started":"2025-08-07T10:31:24.888083Z","shell.execute_reply":"2025-08-07T10:31:24.888093Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nprint(next(model.parameters()).device)","metadata":{"_uuid":"8aee387b-9775-466a-b351-41f1098c8346","_cell_guid":"4b91d0c6-40eb-4e46-9d2c-d49f7e652db6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â€•â€•â€• dataset pronto per PyTorch â€•â€•â€•\ndef add_labels(batch):\n    # le label per un causal-LM sono identiche agli input_ids\n    batch[\"labels\"] = batch[\"input_ids\"]          # â† niente .clone() qui\n    return batch\n\ntrain_tok = train_tok.map(add_labels, batched=True)\n\ntrain_tok.set_format(\n    type=\"torch\",\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"]  # ora labels esiste\n)\n\n\n# â€•â€•â€• collator e dataloader â€•â€•â€•\nfrom transformers import DataCollatorForLanguageModeling\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer,   # â† minuscolo!\n    mlm=False              # causal-LM â†’ niente masked-LM\n)\n\ntrain_loader = DataLoader(\n    train_tok,\n    batch_size=1,\n    shuffle=True,\n    collate_fn=data_collator,\n)\n\n# â€•â€•â€• ottimizzatore â€•â€•â€•\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\n# â€•â€•â€• training loop minimale â€•â€•â€•\nmodel.train()\nepochs = 1\nfor epoch in range(epochs):\n    for step, batch in enumerate(train_loader):\n        # sposta batch su GPU\n        for k in batch:\n            if isinstance(batch[k], torch.Tensor):\n                batch[k] = batch[k].to(\"cuda\")\n\n        outputs = model(**batch)\n        loss = outputs.loss\n\n        loss.backward()\n        optimizer.step()\n        optimizer.zero_grad()\n\n        if step % 5 == 0:\n            print(f\"Epoch {epoch} | Step {step} | Loss {loss.item():.4f}\")\n","metadata":{"_uuid":"9ddd3af6-b86a-46b8-9cea-2fb2166ae155","_cell_guid":"a08528f0-a695-4fa7-bcd0-c0c3cbcedfd1","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.save_pretrained(\"/kaggle/working/mistral-lora-finetuned\")\ntokenizer.save_pretrained(\"/kaggle/working/mistral-lora-finetuned\")","metadata":{"_uuid":"b5a5349d-1f40-4581-924d-75c6fa80247b","_cell_guid":"5a054b77-68e1-4590-8720-774294c38646","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -U --quiet \\\n  \"transformers==4.41.2\" \\\n  \"peft==0.10.0\" \\\n  \"bitsandbytes==0.43.0\" \\\n  \"accelerate==0.28.0\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 0)  DISINSTALLA bitsandbytes (se câ€™Ã¨) E PATCHA PEFT IN MODO SICURO\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nimport subprocess, sys, importlib, os, json, shutil, types\n\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-yq\", \"bitsandbytes\"])\n\nimport peft\ndef _no_bnb() -> bool:          # ritorna sempre False\n    return False\n\n# helper che applica il patch solo se (modulo, attributo) esistono\ndef _safe_patch(module_path: str, attr_name: str):\n    try:\n        mod = importlib.import_module(module_path)\n        setattr(mod, attr_name, _no_bnb)\n    except (ModuleNotFoundError, AttributeError):\n        pass  # modulo o attributo non presenti in questa versione\n\n_safe_patch(\"peft.utils.other\",          \"is_bnb_available\")      # â‰¥ 0.7\n_safe_patch(\"peft.tuners.lora.model\",    \"is_bnb_available\")      # fallback\n_safe_patch(\"peft.import_utils\",         \"is_bnb_available\")      # alcune release\n_safe_patch(\"peft.import_utils\",         \"is_bnb_4bit_available\") # idem\n_safe_patch(\"peft.utils.import_utils\",   \"is_bnb_available\")      # versioni vecchie\n_safe_patch(\"peft.utils.import_utils\",   \"is_bnb_4bit_available\")\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 1)  CREA (una sola volta) Lâ€™ADAPTER â€œPATCHEDâ€ SENZA CHIAVI SPURIE\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nfrom peft import LoraConfig\nadapter_src = \"/kaggle/input/mistral-lora-finetuned\"   # path originale\nadapter_ok  = \"/kaggle/working/mistral-lora-patched\"   # destinazione ripulita\n\nif not os.path.exists(os.path.join(adapter_ok, \"adapter_config.json\")):\n    os.makedirs(adapter_ok, exist_ok=True)\n\n    with open(os.path.join(adapter_src, \"adapter_config.json\")) as f:\n        raw_cfg = json.load(f)\n\n    valid_keys = set(LoraConfig.__init__.__code__.co_varnames)\n    clean_cfg  = {k: v for k, v in raw_cfg.items() if k in valid_keys}\n\n    with open(os.path.join(adapter_ok, \"adapter_config.json\"), \"w\") as f:\n        json.dump(clean_cfg, f)\n\n    shutil.copy(os.path.join(adapter_src, \"adapter_model.safetensors\"), adapter_ok)\n\n# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n# 2)  CARICA MODELLO BASE + ADAPTER (solo PyTorch, niente bnb)\n# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndtype  = torch.float16 if torch.cuda.is_available() else torch.float32\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.2\",\n    device_map=\"auto\" if torch.cuda.is_available() else None,\n    torch_dtype=dtype,\n    trust_remote_code=True,\n)\n\nfrom peft import PeftModel\nmodel = PeftModel.from_pretrained(base_model, adapter_ok)  # carica LoRA\nmodel.eval()\n\ntokenizer = AutoTokenizer.from_pretrained(\n    \"mistralai/Mistral-7B-Instruct-v0.2\", trust_remote_code=True\n)\n\nprint(\"âœ…  Modello + LoRA caricati correttamente (senza bitsandbytes)\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import evaluate, pandas as pd, torch\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom bert_score import score as bert_score\nfrom textstat import flesch_kincaid_grade\nfrom tqdm import tqdm\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMAX_LEN    = 512        # token del prompt (troncatura)\nGEN_TOKENS = 256        # token generati\nassert isinstance(test_data, list) and all(\"input\" in x and \"output\" in x for x in test_data)\n\ninputs     = [ex[\"input\"]  for ex in test_data]\nreferences = [ex[\"output\"] for ex in test_data]\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  PREDIZIONI  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\npredictions = []\nfor prompt in tqdm(inputs, desc=\"Generating predictions\"):\n    text  = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n    enc   = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=MAX_LEN).to(model.device)\n    with torch.no_grad():\n        out_ids = model.generate(**enc, max_new_tokens=GEN_TOKENS, do_sample=False)\n    decoded = tokenizer.decode(out_ids[0], skip_special_tokens=True)\n    if \"### Response:\" in decoded:\n        decoded = decoded.split(\"### Response:\")[-1].strip()\n    predictions.append(decoded)\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  METRICHE  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nrouge = evaluate.load(\"rouge\")\nrouge_res = rouge.compute(predictions=predictions, references=references)\n\ntok_pred = [p.split() for p in predictions]\ntok_ref  = [[r.split()] for r in references]\nbleu     = corpus_bleu(tok_ref, tok_pred, smoothing_function=SmoothingFunction().method1)\n\n_, _, F1  = bert_score(predictions, references, lang=\"en\", rescale_with_baseline=True)\n\nfkgl = [flesch_kincaid_grade(p) for p in predictions]\navg_fkgl = sum(fkgl) / len(fkgl)\n\nprint(\"\\n===== Evaluation Results =====\")\nprint(f\"ROUGE-1 : {rouge_res['rouge1']:.4f}\")\nprint(f\"ROUGE-2 : {rouge_res['rouge2']:.4f}\")\nprint(f\"ROUGE-L : {rouge_res['rougeL']:.4f}\")\nprint(f\"BLEU    : {bleu:.4f}\")\nprint(f\"BERTScore F1 : {F1.mean().item():.4f}\")\nprint(f\"Average FKGL : {avg_fkgl:.2f}\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  CSV  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nout_df = pd.DataFrame({\n    \"input\":       inputs,\n    \"reference\":   references,\n    \"prediction\":  predictions,\n    \"fkgl\":        fkgl\n})\ncsv_path = \"/kaggle/working/mistral_lora_predictions.csv\"\nout_df.to_csv(csv_path, index=False)\nprint(f\"âœ…  Predizioni salvate in {csv_path}\")\n","metadata":{"_uuid":"8eec8c3f-ff26-407b-a03c-90e23e774070","_cell_guid":"cdf16ac0-ff9d-4d60-be78-7343ba4ff6f6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json, os\nsave_path = \"/kaggle/working/predictions.json\"\nwith open(save_path, \"w\") as f:\n    json.dump(predictions, f)\nprint(\"ğŸ”’  Predizioni salvate in\", save_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1) --- esegui SUBITO (prima di qualunque import di transformers)\n!pip uninstall -y -q transformers\n!pip install -q --no-cache-dir \"transformers==4.49.0\" \"bert_score==0.3.13\"\n\n# 2) (ri)avvia il kernel / la sessione notebook  â† passaggio importante\n#    In Kaggle: Menu > \"Runtime\" > \"Restart session\" (o lâ€™icona con le frecce).\n\n# 3) appena il kernel riparte, lancia SOLO la parte di metriche\nimport evaluate, pandas as pd, torch, json\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom bert_score import score as bert_score          # ora usa la 4.49\nfrom textstat import flesch_kincaid_grade\n\n# evita lâ€™avviso dei tokenizer (opzionale)\nimport os; os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nwith open(\"/kaggle/working/predictions.json\") as f:\n    predictions = json.load(f)\n\ninputs     = [ex[\"input\"]  for ex in test_data]\nreferences = [ex[\"output\"] for ex in test_data]\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 2)  CALCOLA LE METRICHE\nrouge = evaluate.load(\"rouge\")\nrouge_res = rouge.compute(predictions=predictions, references=references)\n\ntok_pred = [p.split() for p in predictions]\ntok_ref  = [[r.split()] for r in references]\nbleu     = corpus_bleu(tok_ref, tok_pred, smoothing_function=SmoothingFunction().method1)\n\n_, _, F1  = bert_score(predictions, references, lang=\"en\", rescale_with_baseline=True)\n\nfkgl = [flesch_kincaid_grade(p) for p in predictions]\navg_fkgl = sum(fkgl) / len(fkgl)\n\nprint(\"\\n===== Evaluation Results =====\")\nprint(f\"ROUGE-1 : {rouge_res['rouge1']:.4f}\")\nprint(f\"ROUGE-2 : {rouge_res['rouge2']:.4f}\")\nprint(f\"ROUGE-L : {rouge_res['rougeL']:.4f}\")\nprint(f\"BLEU    : {bleu:.4f}\")\nprint(f\"BERTScore F1 : {F1.mean().item():.4f}\")\nprint(f\"Average FKGL : {avg_fkgl:.2f}\")\n\n# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# 3)  SALVA IL CSV\nout_df = pd.DataFrame({\n    \"input\":       inputs,\n    \"reference\":   references,\n    \"prediction\":  predictions,\n    \"fkgl\":        fkgl\n})\ncsv_path = \"/kaggle/working/mistral_lora_predictions.csv\"\nout_df.to_csv(csv_path, index=False)\nprint(f\"âœ…  Predizioni salvate in {csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-07T10:31:31.728142Z","iopub.execute_input":"2025-08-07T10:31:31.728914Z","iopub.status.idle":"2025-08-07T10:32:38.938210Z","shell.execute_reply.started":"2025-08-07T10:31:31.728876Z","shell.execute_reply":"2025-08-07T10:32:38.937283Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m168.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n\u001b[?25h","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81751224d2f747d185d3b514f6839235"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nWarning: Empty candidate sentence detected; setting raw BERTscores to 0.\n","output_type":"stream"},{"name":"stdout","text":"\n===== Evaluation Results =====\nROUGE-1 : 0.2179\nROUGE-2 : 0.0385\nROUGE-L : 0.1191\nBLEU    : 0.0195\nBERTScore F1 : -0.1280\nAverage FKGL : 13.03\nâœ…  Predizioni salvate in /kaggle/working/mistral_lora_predictions.csv\n","output_type":"stream"}],"execution_count":6}]}