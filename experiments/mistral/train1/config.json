{
  "run": "mistral_training1",
  "base_model": "mistralai/Mistral-7B-Instruct-v0.2",
  "finetuning": "LoRA (PEFT)",
  "tokenizer": { "trust_remote_code": true },
  "dataset": {
    "format": "section-based (same as Flan experiments)",
    "train": "../../datasets/flan_story_v1/train_ready_trunc.json",
    "test":  "../../datasets/flan_story_v1/test_ready_trunc.json"
  },
  "generation_params_eval": { "max_new_tokens": 256, "do_sample": false },
  "notes": "Early Mistral run; outputs short; weak semantic alignment. No predictions saved for this run."
}
